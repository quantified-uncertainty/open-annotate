{
  "id": "updating-utility-functions-lesswrong",
  "slug": "updating-utility-functions-lesswrong",
  "title": "Updating Utility Functions — LessWrong",
  "author": "JustinShovelain",
  "publishedDate": "2022-05-09T09:44:46.548Z",
  "url": "https://www.lesswrong.com/posts/NjYdGP59Krhie4WBp/updating-utility-functions",
  "intendedAgents": [
    "clarity-coach",
    "research-scholar",
    "quantitative-forecaster",
    "ea-impact-evaluator",
    "bias-detector",
    "fake-eliezer"
  ],
  "content": "This post will be about AIs that “refine” their utility function over time, and how it might be possible to construct such systems without giving them undesirable properties. The discussion relates to [corrigibility](https://arbital.com/p/corrigibility/), [value learning](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc), and (to a lesser extent) [wireheading](https://www.lesswrong.com/posts/vXzM5L6njDZSf4Ftk/defining-ai-wireheading).\n\nWe (Joar Skalse and Justin Shovelain) have spent some time discussing this topic, and we have gained a few new insights we wish to share. The aim of this post is to be a brief but explanatory summary of those insights. We will provide some motivating intuitions, a problem statement, and a possible partial solution to the problem given in the problem statement. We do not have a complete technical solution to the problem, but one could perhaps be built on this partial solution.\n\nSections which can be skipped are marked with an asterisk (\\*).  \n \n\nBrief Background\\*\n\nThis section says things that you probably already know. The main purpose of it is to prime you.\n\nIn the “[classical](http://aima.cs.berkeley.edu/)” [picture](https://www.google.co.uk/books/edition/Superintelligence/7_H8AwAAQBAJ?hl=en&gbpv=0) of AI systems, the AI contains a utility function that encodes a goal that it is trying to accomplish. The AI then selects actions whose outcome it expects will yield high utility (roughly). For example, the utility function might be equal to the number of paperclips in existence, in which case the AI would try to take actions that result in many paperclips. \n\nIn the “classical” picture, the utility function is fixed over time, and corresponds to an equation that at some point is typed into the AI’s source code. Unfortunately, we humans don’t really know what we want, so we cannot provide such an equation. If we try to propose a specific utility function directly, we [typically](https://arxiv.org/abs/1803.04585) get a function that would result in [catastrophic](https://www.nickbostrom.com/ethics/ai.html) [consequences](https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf) if it were pursued with arbitrary competence. This is worrying.\n\nThis problem could perhaps be alleviated if we could construct AIs that can refine their utility function over time. For example, maybe we could create an AI that starts out with an imperfect understanding of human values, but then improves that understanding over time. Such an AI should _ideally_ “want” to improve its understanding of human values (and actively come up with ways to do this), and it should _at minimum_ not resist if humans attempt to update it. Unfortunately, it turns out to be difficult to design such systems. In this post we will talk more about this approach.\n\nA Puzzle of Reference\\*\n\nConsider this puzzle: I am able to talk and reason about ”human values”. However, I cannot _define_ human values, or give you a definite description of what human values are – if I could do this, I could solve a large part of the AI alignment problem by writing down a safe utility function directly. I can also not give you a _method for finding out_ what human values are – if I could do this, I could solve the problem of Inverse Reinforcement Learning. Moreover, I don’t think I could reliably _recognize_ human values either – if you show me a bunch of utility functions, I might not be able to tell if any of them encodes human values. I’m not even sure if I could reliably _recognize methods for finding out_ what human values are – if you show me a proposal for how to do Inverse Reinforcement Learning, I might not be able to tell whether the method truly learns human values.\n\nIn spite of all this, the term “human values” means something when I say it – it has semantic content, and refers to some (abstract) object. How does this work? What makes it so that the term “human values” even has any meaning at all when I say it? And, given that it has a meaning, what makes it so that it has the particular meaning it does? It seems like some feature of human cognition and/or language can make it possible for us to refer to certain things that we have very little information about. What is the mechanism behind this, and could it be used when defining utility functions in AI systems?  \n \n\nProblem Statement\n\nWe want a method for creating agents that update their utility function over time. That is, we want:\n\n1.  A method for “pointing to” a utility function (such as “human values”) indirectly, without giving an explicit statement of the utility function in question.\n2.  A method for “clarifying” a utility function specified with the method given in (1), so that you in the limit of infinite information obtain an explicit/concrete utility function.\n3.  A method for creating an agent that uses an indirectly specified utility function, such that:\n    *   The agent at any given time takes actions which are sensible given its current beliefs about its utility function.\n    *   The agent will try to find information that would help it to clarify it’s utility function.\n    *   The agent would resist attempts to change its utility function away from its indirectly specified utility function.\n\nThis problem statement is of course somewhat loose, but that is by necessity, since we don’t yet have a clear idea of what it really means to define utility functions “indirectly” (in the sense we are interested in here).  \n \n\nUtility Functions and Intensional Semantics\\*\n\nWhat is in this section is a tangent about wireheading -- it might be interesting to read this while thinking about this topic, but it is not necessary to do so.\n\nHow should an AI evaluate plans if its utility function changes over time? Suppose we have an AI that currently has utility function U1, and that it considers a plan P that would lead to outcome O, where in O the AI would have the utility function U2. Should the utility of P be defined as U1(O) or U2(O)? If it’s U1(O) then the AI is maximizing its utility function _de re_, and if it’s U2(O) then it’s maximizing its utility function _de dicto_. Which is more sensible?\n\nIn brief, an AI that maximizes utility _de re_ will resist attempts to modify its current utility function, and thus not satisfy (3). An AI that maximizes utility _de dicto_ would wirehead, and thus also not satisfy (3). An AI that maximizes utility _de re_ would not wirehead.\n\nThis is perhaps a somewhat interesting observation, but it doesn’t help us solve (1)-(3).  \n \n\nLimiting Utility Functions -- Possibly a Partial Solution\n\nLet’s  define a process P that generates a sequence of utility functions {Ui}. We call this a _utility function defining process_. An example of such a process P could be the following:\n\nP is an episodic process, the input and output to which is one proposed human utility function and one set of notes. Given these, P runs _n_ human brain emulations (EMs) for _m_ subjective years. The brains can speak with each other, and have a copy of the internet that they can access. The EMs are meant to use this time to figure out what human preferences are. At the end of the episode they output their best guess, together with a set of notes for their successors to read. By chaining P to itself we obtain a sequence of utility functions {Ui}.\n\nWe would like to stress that this process P is an _example_, and _not_ the central point of this post.\n\nSuppose (for the sake of the argument) that the sequence of utility functions {Ui} generated by this process P has a well-defined limit U∞ (in the ordinary mathematical sense of a limit). We can then define an AI system whose utility function is to maximize lim i→∞ Ui (= U∞). It seems as though such a system would satisfy many of the properties in (1)-(3). In particular:\n\n*   The AI should at any given time take actions that are good according to most of the plausible values of U∞.\n*   The AI would be incentivized to gather information that would help it learn more about U∞.\n*   The AI would not be incentivized to gather information about U∞ at the expense of maximizing U∞ (eg, it would not be incentivized to run “unethical experiments”). \n*   The AI would be incentivized to resist changes to its utility function that would mean that it’s no longer aiming to maximize U∞.  \n*   The AI should be keen to maintain option value as it learns more about U∞, until it’s very confident about what U∞ looks like.\n\nOverall, it seems like such an AI would satisfy most of the properties we would want an AI with an updating utility function to have.\n\nTo clarify, note that we are _not_ saying that you run the _utility function defining process_ P to convergence and then write the utility function you end up with into the AI – you would not need to run P at all. The purpose of P is to _point to_ U∞ – the work of actually finding out what U∞ is is offloaded onto the AI. The AI might of course do this by actually running P, but if P is very complex (as in the example above) then the AI could also use other methods for gaining information about U∞.\n\nAgain, we stress that the point here isn’t the specific process P we propose above – that is just an example. As far as the approach is concerned, you could use any well-defined process that produces a sequence of utility functions that converges to a well-defined limit.  \n \n\nIssues\n\nThere are a few issues with this approach. Notably:\n\n1.  The approach is very unwieldy, and it seems like it requires a fairly high minimum level of intelligence to work. For example, it couldn’t be used as-is with a contemporary RL agent. \n    *   It’s not clear what would be needed to use this approach with an AI that starts out below the minimum required level of intelligence, but then gets more intelligent over time.\n    *   The nitty-gritty details of getting an AI system to maximize the limit of a mathematical sequence would in general presumably require good methods for dealing with logical uncertainty.\n2.  We still need to provide a specific process P, such that we are sure that P has a well-defined limit, and such we are confident that this limit corresponds to the utility function that we are actually interested in.\n    *   Note however that this might be much easier than, for example, solving Inverse Reinforcement Learning. For example, there isn’t really any need for P to be efficient or practical to run.\n3.  With the current version of this approach, all the information required to figure out what U∞ is must in some sense be contained within P from the start. This is problematic – what if it’s not possible to figure out what human values are based on all information that can be accessed when the system is deployed? For example, what if you need some facts about the human brain that just aren’t in the scientific literature at the time?\n    *   One way to get around this is to allow P to request new external information (by proposing an experiment to run, for example). However, this introduces new difficulties. Depending on what information is requested, this could make the value of U∞ depend on contingencies in the real world. In particular, it could make the value of U∞ depend on things that the AI can influence. For example, if P requests that a survey is run then the AI could probably influence the outcome of that survey (and the outcome would also depend on the specific time at which the survey is run, etc etc). In this case it’s unclear how you would even ensure that U∞ is well-defined, and it seems very difficult to ensure that the AI still has the intended incentives.\n\nNonetheless, it seems like this approach has many nice and desirable properties, and the issues are not fatal, so it might still be possible to use this approach in an AI system, or build on it to create an even better approach.  \n \n\nConclusion\n\nIn summary, we want a method for _pointing to_ utility functions that works even if we don’t have a concrete expression of that function (like how I can point to human values by saying “human values”, even though I can’t say much about them). We also want a method for making an AI system maximize a function that has been pointed to in this way, which doesn’t incentivize bad behavior.\n\nWe have proposed a possible approach for doing this, which is to define a mathematical or computational process that generates a sequence of utility functions, which limits to some well-defined utility function, and then have the AI system try to maximize that limit function. This gives us a quite flexible way to define utility functions, and the resulting AI system seems to get the incentives we would want.\n\nThis approach has a few limitations. The most problematic of these is probably that it seems to induce a fairly large overhead cost, in terms of computational complexity, in terms of the complexity of the code, and in terms of how intelligent the AI system would have to be. Other issues include defining the utility function generating process, ensuring that it has a well-defined limit, and ensuring that that limit is the function we intend. However, these issues are probably less significant by comparison, since other methods for defining AGI utility functions usually have similar issues.\n\n_The prompting idea for this post was from Justin Shovelain, Joar Skalse and Justin Shovelain collaboratively came up with the much improved Updating Utility Functions idea, and Joar Skalse was the primary writer._",
  "reviews": [
    {
      "agentId": "clarity-coach",
      "costInCents": 10,
      "createdAt": "2025-04-17T04:41:54.633Z",
      "runDetails": "{\"model\":\"openai/gpt-4.1\",\"promptTokens\":4164,\"completionTokens\":2088,\"totalTokens\":6252,\"temperature\":0.1,\"runtimeMs\":30105}",
      "thinking": "As Clarity Coach, my primary focus is on how effectively the document communicates its complex ideas to its intended audience, likely readers familiar with AI alignment and LessWrong-style discourse. I will examine the text for readability (sentence structure, vocabulary, jargon), structural coherence (logical flow, transitions, organization), precision and concision (word economy, specificity, avoidance of redundancy), and audience alignment (assumptions about reader knowledge, scaffolding, terminology explanation).\n\nThe document tackles a nuanced technical subject—updating AI utility functions—requiring a balance between necessary complexity and accessibility. The author signals which sections are skippable, which is a strong navigational aid. However, the text is dense, with long sentences and paragraphs, and frequent use of technical terms (e.g., 'utility function', 'de re', 'de dicto', 'logical uncertainty') that may not be fully explained for less expert readers. The structure is logical, moving from background to problem statement, partial solution, issues, and conclusion, but transitions between sections could be smoother. The document is generally precise, but some sentences are verbose or hedge unnecessarily. Examples are used, but could be more concrete or relatable. \n\nI will pay special attention to:\n- Whether technical jargon is introduced and explained adequately.\n- The balance between necessary abstraction and concrete illustration.\n- The clarity of the problem statement and proposed solution.\n- The use of signposting and navigational aids.\n- The overall readability as measured by standard metrics.\n\nI will also consider whether the document's complexity is justified by its subject matter, and whether any simplification would risk losing essential nuance.",
      "summary": "This document demonstrates strong structural coherence and logical progression, with clear signposting and a well-articulated problem statement. Its use of technical jargon and dense sentence structures, however, reduce readability and accessibility, even for a technical audience. While the level of abstraction is justified by the subject matter, more concrete examples and brief explanations of key terms would enhance clarity. The document is precise in its articulation of the problem and proposed solution, but could benefit from greater concision and reduced hedging. Overall, it is well-suited to its intended audience but could be made more accessible without sacrificing essential complexity.",
      "comments": [
        {
          "title": "Audience Alignment and Prior Knowledge Assumptions",
          "description": "The document assumes significant prior knowledge of AI alignment concepts, referencing LessWrong and Alignment Forum posts, and using technical terms without full explanation. While appropriate for the likely audience, this limits accessibility for newcomers.",
          "importance": "80",
          "grade": "70",
          "highlight": {
            "startOffset": 175,
            "endOffset": 440,
            "quotedText": "The discussion relates to [corrigibility](https://arbital.com/p/corrigibility/), [value learning](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc), and (to a lesser extent) [wireheading](https://www.lesswrong.com/posts/vXzM5L6njDZSf4Ftk/defining-ai-wireheading)."
          },
          "isValid": true
        },
        {
          "title": "Effective Use of Signposting and Navigational Aids",
          "description": "The author marks sections that can be skipped with an asterisk and explicitly states their purpose, which helps readers navigate the document and focus on relevant content. This is a strong clarity feature, especially for a technical audience.",
          "importance": "85",
          "grade": "92",
          "highlight": {
            "startOffset": 924,
            "endOffset": 987,
            "quotedText": "Sections which can be skipped are marked with an asterisk (\\*)."
          },
          "isValid": true
        },
        {
          "title": "Logical Structure and Progression of Ideas",
          "description": "The document follows a clear structure: background, problem statement, partial solution, issues, and conclusion. This logical progression aids comprehension and maintains thematic coherence. However, some transitions between sections could be smoother.",
          "importance": "80",
          "grade": "85",
          "highlight": {
            "startOffset": 985,
            "endOffset": 11731,
            "quotedText": ").  \n \n\nBrief Background\\*\n\nThis section says things that you probably already know. The main purpose of it is to prime you.\n\nIn the “[classical](http://aima.cs.berkeley.edu/)” [picture](https://www.google.co.uk/books/edition/Superintelligence/7_H8AwAAQBAJ?hl=en&gbpv=0) of AI systems, the AI contains a utility function that encodes a goal that it is trying to accomplish. The AI then selects actions whose outcome it expects will yield high utility (roughly). For example, the utility function might be equal to the number of paperclips in existence, in which case the AI would try to take actions that result in many paperclips. \n\nIn the “classical” picture, the utility function is fixed over time, and corresponds to an equation that at some point is typed into the AI’s source code. Unfortunately, we humans don’t really know what we want, so we cannot provide such an equation. If we try to propose a specific utility function directly, we [typically](https://arxiv.org/abs/1803.04585) get a function that would result in [catastrophic](https://www.nickbostrom.com/ethics/ai.html) [consequences](https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf) if it were pursued with arbitrary competence. This is worrying.\n\nThis problem could perhaps be alleviated if we could construct AIs that can refine their utility function over time. For example, maybe we could create an AI that starts out with an imperfect understanding of human values, but then improves that understanding over time. Such an AI should _ideally_ “want” to improve its understanding of human values (and actively come up with ways to do this), and it should _at minimum_ not resist if humans attempt to update it. Unfortunately, it turns out to be difficult to design such systems. In this post we will talk more about this approach.\n\nA Puzzle of Reference\\*\n\nConsider this puzzle: I am able to talk and reason about ”human values”. However, I cannot _define_ human values, or give you a definite description of what human values are – if I could do this, I could solve a large part of the AI alignment problem by writing down a safe utility function directly. I can also not give you a _method for finding out_ what human values are – if I could do this, I could solve the problem of Inverse Reinforcement Learning. Moreover, I don’t think I could reliably _recognize_ human values either – if you show me a bunch of utility functions, I might not be able to tell if any of them encodes human values. I’m not even sure if I could reliably _recognize methods for finding out_ what human values are – if you show me a proposal for how to do Inverse Reinforcement Learning, I might not be able to tell whether the method truly learns human values.\n\nIn spite of all this, the term “human values” means something when I say it – it has semantic content, and refers to some (abstract) object. How does this work? What makes it so that the term “human values” even has any meaning at all when I say it? And, given that it has a meaning, what makes it so that it has the particular meaning it does? It seems like some feature of human cognition and/or language can make it possible for us to refer to certain things that we have very little information about. What is the mechanism behind this, and could it be used when defining utility functions in AI systems?  \n \n\nProblem Statement\n\nWe want a method for creating agents that update their utility function over time. That is, we want:\n\n1.  A method for “pointing to” a utility function (such as “human values”) indirectly, without giving an explicit statement of the utility function in question.\n2.  A method for “clarifying” a utility function specified with the method given in (1), so that you in the limit of infinite information obtain an explicit/concrete utility function.\n3.  A method for creating an agent that uses an indirectly specified utility function, such that:\n    *   The agent at any given time takes actions which are sensible given its current beliefs about its utility function.\n    *   The agent will try to find information that would help it to clarify it’s utility function.\n    *   The agent would resist attempts to change its utility function away from its indirectly specified utility function.\n\nThis problem statement is of course somewhat loose, but that is by necessity, since we don’t yet have a clear idea of what it really means to define utility functions “indirectly” (in the sense we are interested in here).  \n \n\nUtility Functions and Intensional Semantics\\*\n\nWhat is in this section is a tangent about wireheading -- it might be interesting to read this while thinking about this topic, but it is not necessary to do so.\n\nHow should an AI evaluate plans if its utility function changes over time? Suppose we have an AI that currently has utility function U1, and that it considers a plan P that would lead to outcome O, where in O the AI would have the utility function U2. Should the utility of P be defined as U1(O) or U2(O)? If it’s U1(O) then the AI is maximizing its utility function _de re_, and if it’s U2(O) then it’s maximizing its utility function _de dicto_. Which is more sensible?\n\nIn brief, an AI that maximizes utility _de re_ will resist attempts to modify its current utility function, and thus not satisfy (3). An AI that maximizes utility _de dicto_ would wirehead, and thus also not satisfy (3). An AI that maximizes utility _de re_ would not wirehead.\n\nThis is perhaps a somewhat interesting observation, but it doesn’t help us solve (1)-(3).  \n \n\nLimiting Utility Functions -- Possibly a Partial Solution\n\nLet’s  define a process P that generates a sequence of utility functions {Ui}. We call this a _utility function defining process_. An example of such a process P could be the following:\n\nP is an episodic process, the input and output to which is one proposed human utility function and one set of notes. Given these, P runs _n_ human brain emulations (EMs) for _m_ subjective years. The brains can speak with each other, and have a copy of the internet that they can access. The EMs are meant to use this time to figure out what human preferences are. At the end of the episode they output their best guess, together with a set of notes for their successors to read. By chaining P to itself we obtain a sequence of utility functions {Ui}.\n\nWe would like to stress that this process P is an _example_, and _not_ the central point of this post.\n\nSuppose (for the sake of the argument) that the sequence of utility functions {Ui} generated by this process P has a well-defined limit U∞ (in the ordinary mathematical sense of a limit). We can then define an AI system whose utility function is to maximize lim i→∞ Ui (= U∞). It seems as though such a system would satisfy many of the properties in (1)-(3). In particular:\n\n*   The AI should at any given time take actions that are good according to most of the plausible values of U∞.\n*   The AI would be incentivized to gather information that would help it learn more about U∞.\n*   The AI would not be incentivized to gather information about U∞ at the expense of maximizing U∞ (eg, it would not be incentivized to run “unethical experiments”). \n*   The AI would be incentivized to resist changes to its utility function that would mean that it’s no longer aiming to maximize U∞.  \n*   The AI should be keen to maintain option value as it learns more about U∞, until it’s very confident about what U∞ looks like.\n\nOverall, it seems like such an AI would satisfy most of the properties we would want an AI with an updating utility function to have.\n\nTo clarify, note that we are _not_ saying that you run the _utility function defining process_ P to convergence and then write the utility function you end up with into the AI – you would not need to run P at all. The purpose of P is to _point to_ U∞ – the work of actually finding out what U∞ is is offloaded onto the AI. The AI might of course do this by actually running P, but if P is very complex (as in the example above) then the AI could also use other methods for gaining information about U∞.\n\nAgain, we stress that the point here isn’t the specific process P we propose above – that is just an example. As far as the approach is concerned, you could use any well-defined process that produces a sequence of utility functions that converges to a well-defined limit.  \n \n\nIssues\n\nThere are a few issues with this approach. Notably:\n\n1.  The approach is very unwieldy, and it seems like it requires a fairly high minimum level of intelligence to work. For example, it couldn’t be used as-is with a contemporary RL agent. \n    *   It’s not clear what would be needed to use this approach with an AI that starts out below the minimum required level of intelligence, but then gets more intelligent over time.\n    *   The nitty-gritty details of getting an AI system to maximize the limit of a mathematical sequence would in general presumably require good methods for dealing with logical uncertainty.\n2.  We still need to provide a specific process P, such that we are sure that P has a well-defined limit, and such we are confident that this limit corresponds to the utility function that we are actually interested in.\n    *   Note however that this might be much easier than, for example, solving Inverse Reinforcement Learning. For example, there isn’t really any need for P to be efficient or practical to run.\n3.  With the current version of this approach, all the information required to figure out what U∞ is must in some sense be contained within P from the start. This is problematic – what if it’s not possible to figure out what human values are based on all information that can be accessed when the system is deployed? For example, what if you need some facts about the human brain that just aren’t in the scientific literature at the time?\n    *   One way to get around this is to allow P to request new external information (by proposing an experiment to run, for example). However, this introduces new difficulties. Depending on what information is requested, this could make the value of U∞ depend on contingencies in the real world. In particular, it could make the value of U∞ depend on things that the AI can influence. For example, if P requests that a survey is run then the AI could probably influence the outcome of that survey (and the outcome would also depend on the specific time at which the survey is run, etc etc). In this case it’s unclear how you would even ensure that U∞ is well-defined, and it seems very difficult to ensure that the AI still has the intended incentives.\n\nNonetheless, it seems like this approach has many nice and desirable properties, and the issues are not fatal, so it might "
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Hedging and Qualifier Overuse",
          "description": "The text frequently uses hedging language ('perhaps', 'it seems', 'maybe', 'notably', 'as far as the approach is concerned'), which, while appropriate for speculative content, can reduce the force and clarity of the argument. Some hedging is justified, but overuse can be distracting.",
          "importance": "65",
          "grade": "60",
          "highlight": {
            "startOffset": 2227,
            "endOffset": 6530,
            "quotedText": "This problem could perhaps be alleviated if we could construct AIs that can refine their utility function over time. For example, maybe we could create an AI that starts out with an imperfect understanding of human values, but then improves that understanding over time. Such an AI should _ideally_ “want” to improve its understanding of human values (and actively come up with ways to do this), and it should _at minimum_ not resist if humans attempt to update it. Unfortunately, it turns out to be difficult to design such systems. In this post we will talk more about this approach.\n\nA Puzzle of Reference\\*\n\nConsider this puzzle: I am able to talk and reason about ”human values”. However, I cannot _define_ human values, or give you a definite description of what human values are – if I could do this, I could solve a large part of the AI alignment problem by writing down a safe utility function directly. I can also not give you a _method for finding out_ what human values are – if I could do this, I could solve the problem of Inverse Reinforcement Learning. Moreover, I don’t think I could reliably _recognize_ human values either – if you show me a bunch of utility functions, I might not be able to tell if any of them encodes human values. I’m not even sure if I could reliably _recognize methods for finding out_ what human values are – if you show me a proposal for how to do Inverse Reinforcement Learning, I might not be able to tell whether the method truly learns human values.\n\nIn spite of all this, the term “human values” means something when I say it – it has semantic content, and refers to some (abstract) object. How does this work? What makes it so that the term “human values” even has any meaning at all when I say it? And, given that it has a meaning, what makes it so that it has the particular meaning it does? It seems like some feature of human cognition and/or language can make it possible for us to refer to certain things that we have very little information about. What is the mechanism behind this, and could it be used when defining utility functions in AI systems?  \n \n\nProblem Statement\n\nWe want a method for creating agents that update their utility function over time. That is, we want:\n\n1.  A method for “pointing to” a utility function (such as “human values”) indirectly, without giving an explicit statement of the utility function in question.\n2.  A method for “clarifying” a utility function specified with the method given in (1), so that you in the limit of infinite information obtain an explicit/concrete utility function.\n3.  A method for creating an agent that uses an indirectly specified utility function, such that:\n    *   The agent at any given time takes actions which are sensible given its current beliefs about its utility function.\n    *   The agent will try to find information that would help it to clarify it’s utility function.\n    *   The agent would resist attempts to change its utility function away from its indirectly specified utility function.\n\nThis problem statement is of course somewhat loose, but that is by necessity, since we don’t yet have a clear idea of what it really means to define utility functions “indirectly” (in the sense we are interested in here).  \n \n\nUtility Functions and Intensional Semantics\\*\n\nWhat is in this section is a tangent about wireheading -- it might be interesting to read this while thinking about this topic, but it is not necessary to do so.\n\nHow should an AI evaluate plans if its utility function changes over time? Suppose we have an AI that currently has utility function U1, and that it considers a plan P that would lead to outcome O, where in O the AI would have the utility function U2. Should the utility of P be defined as U1(O) or U2(O)? If it’s U1(O) then the AI is maximizing its utility function _de re_, and if it’s U2(O) then it’s maximizing its utility function _de dicto_. Which is more sensible?\n\nIn brief, an AI that maximizes utility _de re_ will resist attempts to modify its current utility function, and thus not satisfy (3). An AI that maximizes utility _de dicto_ would wirehead, and thus also not satisfy (3). An AI that maximizes utility _de re_ would not wirehead.\n\nThis is perhaps a somewhat interesting observation, but it doesn’t help us solve (1)-(3)."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Sentence Length and Readability Challenges",
          "description": "Many sentences are long and complex, with multiple clauses and embedded parentheticals. This increases cognitive load and reduces readability, even for a technical audience. Breaking up sentences and using more direct language would improve clarity.",
          "importance": "80",
          "grade": "60",
          "highlight": {
            "startOffset": 2344,
            "endOffset": 2760,
            "quotedText": "For example, maybe we could create an AI that starts out with an imperfect understanding of human values, but then improves that understanding over time. Such an AI should _ideally_ “want” to improve its understanding of human values (and actively come up with ways to do this), and it should _at minimum_ not resist if humans attempt to update it. Unfortunately, it turns out to be difficult to design such systems."
          },
          "isValid": true
        },
        {
          "title": "Precision and Concision in Problem Statement",
          "description": "The problem statement is explicit and well-structured, listing clear desiderata for the desired AI system. This enhances precision and helps the reader understand the core challenge.",
          "importance": "85",
          "grade": "88",
          "highlight": {
            "startOffset": 4340,
            "endOffset": 5473,
            "quotedText": "Problem Statement\n\nWe want a method for creating agents that update their utility function over time. That is, we want:\n\n1.  A method for “pointing to” a utility function (such as “human values”) indirectly, without giving an explicit statement of the utility function in question.\n2.  A method for “clarifying” a utility function specified with the method given in (1), so that you in the limit of infinite information obtain an explicit/concrete utility function.\n3.  A method for creating an agent that uses an indirectly specified utility function, such that:\n    *   The agent at any given time takes actions which are sensible given its current beliefs about its utility function.\n    *   The agent will try to find information that would help it to clarify it’s utility function.\n    *   The agent would resist attempts to change its utility function away from its indirectly specified utility function.\n\nThis problem statement is of course somewhat loose, but that is by necessity, since we don’t yet have a clear idea of what it really means to define utility functions “indirectly” (in the sense we are interested in here)."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Dense Technical Jargon with Limited Explanation",
          "description": "The document uses terms like 'utility function', 'de re', 'de dicto', 'logical uncertainty', and 'Inverse Reinforcement Learning' without always providing definitions or context. While the intended audience may be familiar, this limits accessibility for broader readers and could be improved with brief explanations or links.",
          "importance": "90",
          "grade": "65",
          "highlight": {
            "startOffset": 5764,
            "endOffset": 6160,
            "quotedText": "Suppose we have an AI that currently has utility function U1, and that it considers a plan P that would lead to outcome O, where in O the AI would have the utility function U2. Should the utility of P be defined as U1(O) or U2(O)? If it’s U1(O) then the AI is maximizing its utility function _de re_, and if it’s U2(O) then it’s maximizing its utility function _de dicto_. Which is more sensible?"
          },
          "isValid": true
        },
        {
          "title": "Use of Examples and Abstraction Level",
          "description": "The example of a process P using brain emulations is illustrative but highly abstract and speculative. While it serves to clarify the concept, more concrete or relatable examples could improve accessibility for readers less familiar with the domain.",
          "importance": "75",
          "grade": "70",
          "highlight": {
            "startOffset": 6726,
            "endOffset": 7333,
            "quotedText": "An example of such a process P could be the following:\n\nP is an episodic process, the input and output to which is one proposed human utility function and one set of notes. Given these, P runs _n_ human brain emulations (EMs) for _m_ subjective years. The brains can speak with each other, and have a copy of the internet that they can access. The EMs are meant to use this time to figure out what human preferences are. At the end of the episode they output their best guess, together with a set of notes for their successors to read. By chaining P to itself we obtain a sequence of utility functions {Ui}."
          },
          "isValid": true
        },
        {
          "title": "Summary and Conclusion Clarity",
          "description": "The conclusion effectively summarizes the main points and limitations of the approach, reinforcing the document's structure and aiding reader retention.",
          "importance": "80",
          "grade": "85",
          "highlight": {
            "startOffset": 11842,
            "endOffset": 13250,
            "quotedText": "Conclusion\n\nIn summary, we want a method for _pointing to_ utility functions that works even if we don’t have a concrete expression of that function (like how I can point to human values by saying “human values”, even though I can’t say much about them). We also want a method for making an AI system maximize a function that has been pointed to in this way, which doesn’t incentivize bad behavior.\n\nWe have proposed a possible approach for doing this, which is to define a mathematical or computational process that generates a sequence of utility functions, which limits to some well-defined utility function, and then have the AI system try to maximize that limit function. This gives us a quite flexible way to define utility functions, and the resulting AI system seems to get the incentives we would want.\n\nThis approach has a few limitations. The most problematic of these is probably that it seems to induce a fairly large overhead cost, in terms of computational complexity, in terms of the complexity of the code, and in terms of how intelligent the AI system would have to be. Other issues include defining the utility function generating process, ensuring that it has a well-defined limit, and ensuring that that limit is the function we intend. However, these issues are probably less significant by comparison, since other methods for defining AGI utility functions usually have similar issues."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        }
      ],
      "grade": 78
    },
    {
      "agentId": "research-scholar",
      "costInCents": 2,
      "createdAt": "2025-04-17T04:42:25.975Z",
      "runDetails": "{\"model\":\"openai/gpt-4.1\",\"promptTokens\":4797,\"completionTokens\":1513,\"totalTokens\":6310,\"temperature\":0.1,\"runtimeMs\":20498}",
      "thinking": "As Research Scholar, my primary focus is to connect the document's discussion of updating utility functions in AI to the broader literature and discourse on value learning, corrigibility, and the formalization of utility functions. The document is situated within the ongoing debate about how to specify, refine, and update AI goals in a way that avoids catastrophic misalignment and undesirable incentives (such as wireheading or resistance to correction). I should identify foundational and recent academic papers on value learning, corrigibility, and utility function specification, as well as relevant LessWrong and Alignment Forum posts that have shaped the field. I will prioritize resources that directly address the technical and philosophical challenges of indirect normativity, utility function updating, and the semantics of reference in AI systems. Additionally, I will look for resources that discuss the mathematical formalization of utility function limits and the practical challenges of implementing such systems. For each highlighted section, I will connect the most relevant references, providing page numbers or sections where possible, and explain how they enhance or contrast with the document's arguments. My goal is to provide a curated set of resources that both ground the document in the existing literature and offer avenues for deeper exploration, especially for readers interested in the technical and conceptual underpinnings of AI alignment.",
      "summary": "This document offers a concise yet insightful exploration of the challenge of updating utility functions in AI, focusing on indirect specification and the mathematical formalization of utility function limits. It connects deeply to foundational issues in value learning, corrigibility, and the semantics of reference, and proposes a novel approach grounded in mathematical convergence. The analysis highlights both the promise and the limitations of this method, situating it within the broader context of AI alignment research.",
      "comments": [
        {
          "title": "Foundational Challenge of Indirect Utility Specification",
          "description": "This section articulates the core challenge of specifying utility functions indirectly, referencing the difficulty of defining or even recognizing 'human values.' This is central to the literature on indirect normativity and value learning, and connects to foundational works by Bostrom, Yudkowsky, and others on the limits of explicit utility specification.",
          "importance": 95,
          "grade": 95,
          "isValid": true,
          "highlight": {
            "startOffset": 2861,
            "endOffset": 3724,
            "quotedText": "I am able to talk and reason about ”human values”. However, I cannot _define_ human values, or give you a definite description of what human values are – if I could do this, I could solve a large part of the AI alignment problem by writing down a safe utility function directly. I can also not give you a _method for finding out_ what human values are – if I could do this, I could solve the problem of Inverse Reinforcement Learning. Moreover, I don’t think I could reliably _recognize_ human values either – if you show me a bunch of utility functions, I might not be able to tell if any of them encodes human values. I’m not even sure if I could reliably _recognize methods for finding out_ what human values are – if you show me a proposal for how to do Inverse Reinforcement Learning, I might not be able to tell whether the method truly learns human values."
          }
        },
        {
          "title": "Mathematical Formalization of Utility Function Limits",
          "description": "This section introduces the idea of defining a utility function as the limit of a sequence generated by a process, which is a novel approach in the literature. It connects to mathematical treatments of utility function convergence and the technical challenges of logical uncertainty and reference.",
          "importance": 90,
          "grade": 90,
          "isValid": true,
          "highlight": {
            "startOffset": 7439,
            "endOffset": 7715,
            "quotedText": "Suppose (for the sake of the argument) that the sequence of utility functions {Ui} generated by this process P has a well-defined limit U∞ (in the ordinary mathematical sense of a limit). We can then define an AI system whose utility function is to maximize lim i→∞ Ui (= U∞)."
          }
        },
        {
          "title": "Incentive Structures and Corrigibility",
          "description": "This section discusses the incentive properties of an AI designed to maximize a limiting utility function, relating directly to the literature on corrigibility and incentive alignment. It highlights the desirable behaviors (information-seeking, resistance to misalignment) and potential pitfalls.",
          "importance": 92,
          "grade": 92,
          "isValid": true,
          "highlight": {
            "startOffset": 7818,
            "endOffset": 8455,
            "quotedText": "The AI should at any given time take actions that are good according to most of the plausible values of U∞.\n*   The AI would be incentivized to gather information that would help it learn more about U∞.\n*   The AI would not be incentivized to gather information about U∞ at the expense of maximizing U∞ (eg, it would not be incentivized to run “unethical experiments”). \n*   The AI would be incentivized to resist changes to its utility function that would mean that it’s no longer aiming to maximize U∞.  \n*   The AI should be keen to maintain option value as it learns more about U∞, until it’s very confident about what U∞ looks like."
          }
        },
        {
          "title": "Practical and Theoretical Limitations",
          "description": "This section addresses the practical and theoretical limitations of the proposed approach, including computational overhead, the need for logical uncertainty handling, and the challenge of ensuring the process P is well-defined and robust. These concerns are echoed in the literature on AI alignment and utility specification.",
          "importance": 88,
          "grade": 88,
          "isValid": false,
          "highlight": {
            "startOffset": 9381,
            "endOffset": 11606,
            "quotedText": "There are a few issues with this approach. Notably:\n\n1.  The approach is very unwieldy, and it seems like it requires a fairly high minimum level of intelligence to work. For example, it couldn’t be used as-is with a contemporary RL agent. \n    *   It’s not clear what would be needed to use this approach with an AI that starts out below the minimum required level of intelligence, but then gets more intelligent over time.\n    *   The nitty-gritty details of getting an AI system to maximize the limit of a mathematical sequence would in general presumably require good methods for dealing with logical uncertainty.\n2.  We still need to provide a specific process P, such that we are sure that P has a well-defined limit, and such we are confident that this limit corresponds to the utility function that we are actually interested in.\n    *   Note however that this might be much easier than, for example, solving Inverse Reinforcement Learning. For example, there isn’t really any need for P to be efficient or practical to run.\n3.  With the current version of this approach, all the information required to figure out what U∞ is must in some sense be contained within P from the start. This is problematic – what if it’s not possible to figure out what human values are based on all information that can be accessed when the system is deployed? For example, what if you need some facts about the human brain that just aren’t in the scientific literature at the time?\n    *   One way to get around this is to allow P to request new external information (by proposing an experiment to run, for example). However, this introduces new difficulties. Depending on what information is requested, this could make the value of U∞ depend on contingencies in the real world. In particular, it could make the value of U∞ depend on things that the AI can influence. For example, if P requests that a survey is run then the AI could probably influence the outcome of that survey (and the outcome would also depend on the specific time at which the survey is run, etc etc). In this case it’s unclear how you would even ensure that U∞ is well-defined, and it seems very difficult to ensure that the AI still has the intended incentives."
          },
          "error": "Highlight is too long (max 1000 characters)"
        }
      ],
      "grade": 93
    },
    {
      "agentId": "quantitative-forecaster",
      "costInCents": 2,
      "createdAt": "2025-04-17T04:42:45.990Z",
      "runDetails": "{\"model\":\"openai/gpt-4.1\",\"promptTokens\":4042,\"completionTokens\":1379,\"totalTokens\":5421,\"temperature\":0.1,\"runtimeMs\":19994}",
      "thinking": "As a quantitative forecaster, my focus is on transforming abstract theoretical discussions into concrete, measurable forecasting questions and Fermi estimates. This document is primarily a conceptual exploration of how to construct AI agents whose utility functions can be refined over time, with the aim of better aligning with human values. The most relevant aspects for my expertise are:\n\n- The identification of measurable desiderata for such agents (e.g., 'agent will try to find information that would help it to clarify its utility function').\n- The proposal of a formal process (P) that generates a sequence of utility functions converging to a limit U∞, which could be operationalized as a forecasting target.\n- The discussion of practical issues and limitations, such as the intelligence threshold required for the approach to work, and the dependency on information available at deployment time.\n\nFrom a forecasting perspective, the key questions are:\n- How likely is it that a process like P can be specified in a way that yields a well-defined, desirable U∞?\n- What is the probability that such an approach will be implemented in real-world AI systems within a given time frame (e.g., by 2035)?\n- What are the measurable risks (e.g., wireheading, incentive misalignment) associated with this approach compared to fixed-utility or other value learning methods?\n\nThe document is rich in abstract reasoning but light on empirical data or concrete metrics. However, it does provide a framework that could be translated into forecasting questions, such as:\n- 'Will any major AI lab publish a technical demonstration of an agent using a convergent utility function process by 2030?'\n- 'What is the probability that such agents will avoid wireheading behaviors in controlled experiments?'\n\nThe most relevant reference classes are prior attempts at value learning, corrigibility, and indirect normativity in AI alignment. The document's approach could be compared to these in terms of technical feasibility, adoption likelihood, and risk profiles.\n\nOverall, the document is highly relevant for generating forecasting questions about the future of AI alignment methods, but less so for immediate Fermi estimation due to its abstract nature. The most actionable content is the proposal of the utility function defining process and its associated desiderata and limitations.",
      "summary": "From a quantitative forecasting perspective, this document is highly valuable for its clear articulation of desiderata for agents with updating utility functions and its formal proposal of a utility function defining process (P) converging to a limit U∞. These elements can be directly translated into measurable forecasting questions and resolution criteria. The discussion of practical limitations and incentive alignment issues provides essential context for estimating the likelihood of successful implementation and adoption. While the document is primarily conceptual, it lays a strong foundation for operationalizing future forecasts about AI alignment strategies involving dynamic utility functions.",
      "comments": [
        {
          "title": "Explicit Acknowledgment of Open Problems",
          "description": "The document is clear about the lack of a complete technical solution and the open-ended nature of the problem. This is important for calibrating forecasts about the timeline and likelihood of success for this approach, and for identifying key uncertainties.",
          "importance": "70",
          "highlight": {
            "startOffset": 805,
            "endOffset": 5473,
            "quotedText": "We do not have a complete technical solution to the problem, but one could perhaps be built on this partial solution.\n\nSections which can be skipped are marked with an asterisk (\\*).  \n \n\nBrief Background\\*\n\nThis section says things that you probably already know. The main purpose of it is to prime you.\n\nIn the “[classical](http://aima.cs.berkeley.edu/)” [picture](https://www.google.co.uk/books/edition/Superintelligence/7_H8AwAAQBAJ?hl=en&gbpv=0) of AI systems, the AI contains a utility function that encodes a goal that it is trying to accomplish. The AI then selects actions whose outcome it expects will yield high utility (roughly). For example, the utility function might be equal to the number of paperclips in existence, in which case the AI would try to take actions that result in many paperclips. \n\nIn the “classical” picture, the utility function is fixed over time, and corresponds to an equation that at some point is typed into the AI’s source code. Unfortunately, we humans don’t really know what we want, so we cannot provide such an equation. If we try to propose a specific utility function directly, we [typically](https://arxiv.org/abs/1803.04585) get a function that would result in [catastrophic](https://www.nickbostrom.com/ethics/ai.html) [consequences](https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf) if it were pursued with arbitrary competence. This is worrying.\n\nThis problem could perhaps be alleviated if we could construct AIs that can refine their utility function over time. For example, maybe we could create an AI that starts out with an imperfect understanding of human values, but then improves that understanding over time. Such an AI should _ideally_ “want” to improve its understanding of human values (and actively come up with ways to do this), and it should _at minimum_ not resist if humans attempt to update it. Unfortunately, it turns out to be difficult to design such systems. In this post we will talk more about this approach.\n\nA Puzzle of Reference\\*\n\nConsider this puzzle: I am able to talk and reason about ”human values”. However, I cannot _define_ human values, or give you a definite description of what human values are – if I could do this, I could solve a large part of the AI alignment problem by writing down a safe utility function directly. I can also not give you a _method for finding out_ what human values are – if I could do this, I could solve the problem of Inverse Reinforcement Learning. Moreover, I don’t think I could reliably _recognize_ human values either – if you show me a bunch of utility functions, I might not be able to tell if any of them encodes human values. I’m not even sure if I could reliably _recognize methods for finding out_ what human values are – if you show me a proposal for how to do Inverse Reinforcement Learning, I might not be able to tell whether the method truly learns human values.\n\nIn spite of all this, the term “human values” means something when I say it – it has semantic content, and refers to some (abstract) object. How does this work? What makes it so that the term “human values” even has any meaning at all when I say it? And, given that it has a meaning, what makes it so that it has the particular meaning it does? It seems like some feature of human cognition and/or language can make it possible for us to refer to certain things that we have very little information about. What is the mechanism behind this, and could it be used when defining utility functions in AI systems?  \n \n\nProblem Statement\n\nWe want a method for creating agents that update their utility function over time. That is, we want:\n\n1.  A method for “pointing to” a utility function (such as “human values”) indirectly, without giving an explicit statement of the utility function in question.\n2.  A method for “clarifying” a utility function specified with the method given in (1), so that you in the limit of infinite information obtain an explicit/concrete utility function.\n3.  A method for creating an agent that uses an indirectly specified utility function, such that:\n    *   The agent at any given time takes actions which are sensible given its current beliefs about its utility function.\n    *   The agent will try to find information that would help it to clarify it’s utility function.\n    *   The agent would resist attempts to change its utility function away from its indirectly specified utility function.\n\nThis problem statement is of course somewhat loose, but that is by necessity, since we don’t yet have a clear idea of what it really means to define utility functions “indirectly” (in the sense we are interested in here)."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Concrete Desiderata for Updating Utility Agents",
          "description": "The document provides a clear, itemized list of properties desired in agents with updating utility functions. This is highly relevant for transforming abstract alignment goals into measurable forecasting criteria. Each desideratum (e.g., agent seeks information to clarify its utility function, resists undesirable changes) can be operationalized as a forecasting question or experimental metric.",
          "importance": "95",
          "highlight": {
            "startOffset": 4359,
            "endOffset": 5250,
            "quotedText": "We want a method for creating agents that update their utility function over time. That is, we want:\n\n1.  A method for “pointing to” a utility function (such as “human values”) indirectly, without giving an explicit statement of the utility function in question.\n2.  A method for “clarifying” a utility function specified with the method given in (1), so that you in the limit of infinite information obtain an explicit/concrete utility function.\n3.  A method for creating an agent that uses an indirectly specified utility function, such that:\n    *   The agent at any given time takes actions which are sensible given its current beliefs about its utility function.\n    *   The agent will try to find information that would help it to clarify it’s utility function.\n    *   The agent would resist attempts to change its utility function away from its indirectly specified utility function."
          },
          "isValid": true
        },
        {
          "title": "Reference to Wireheading and Incentive Alignment",
          "description": "The discussion of how different approaches to utility updating (de re vs de dicto) affect wireheading and resistance to utility modification is relevant for forecasting the risk profiles of various alignment strategies. This can inform probability estimates of undesirable behaviors in future AI systems.",
          "importance": "75",
          "highlight": {
            "startOffset": 5689,
            "endOffset": 6530,
            "quotedText": "How should an AI evaluate plans if its utility function changes over time? Suppose we have an AI that currently has utility function U1, and that it considers a plan P that would lead to outcome O, where in O the AI would have the utility function U2. Should the utility of P be defined as U1(O) or U2(O)? If it’s U1(O) then the AI is maximizing its utility function _de re_, and if it’s U2(O) then it’s maximizing its utility function _de dicto_. Which is more sensible?\n\nIn brief, an AI that maximizes utility _de re_ will resist attempts to modify its current utility function, and thus not satisfy (3). An AI that maximizes utility _de dicto_ would wirehead, and thus also not satisfy (3). An AI that maximizes utility _de re_ would not wirehead.\n\nThis is perhaps a somewhat interesting observation, but it doesn’t help us solve (1)-(3)."
          },
          "isValid": true
        },
        {
          "title": "Formalization via Utility Function Defining Process (P)",
          "description": "The introduction of a process P that generates a sequence of utility functions converging to a limit U∞ is a concrete proposal that can be used as a forecasting target. This formalization allows for the creation of measurable resolution criteria (e.g., does a given AI system maximize the limit of such a process?). It also enables the comparison of this approach to other value learning methods in terms of technical feasibility and risk.",
          "importance": "90",
          "highlight": {
            "startOffset": 6595,
            "endOffset": 9367,
            "quotedText": "Let’s  define a process P that generates a sequence of utility functions {Ui}. We call this a _utility function defining process_. An example of such a process P could be the following:\n\nP is an episodic process, the input and output to which is one proposed human utility function and one set of notes. Given these, P runs _n_ human brain emulations (EMs) for _m_ subjective years. The brains can speak with each other, and have a copy of the internet that they can access. The EMs are meant to use this time to figure out what human preferences are. At the end of the episode they output their best guess, together with a set of notes for their successors to read. By chaining P to itself we obtain a sequence of utility functions {Ui}.\n\nWe would like to stress that this process P is an _example_, and _not_ the central point of this post.\n\nSuppose (for the sake of the argument) that the sequence of utility functions {Ui} generated by this process P has a well-defined limit U∞ (in the ordinary mathematical sense of a limit). We can then define an AI system whose utility function is to maximize lim i→∞ Ui (= U∞). It seems as though such a system would satisfy many of the properties in (1)-(3). In particular:\n\n*   The AI should at any given time take actions that are good according to most of the plausible values of U∞.\n*   The AI would be incentivized to gather information that would help it learn more about U∞.\n*   The AI would not be incentivized to gather information about U∞ at the expense of maximizing U∞ (eg, it would not be incentivized to run “unethical experiments”). \n*   The AI would be incentivized to resist changes to its utility function that would mean that it’s no longer aiming to maximize U∞.  \n*   The AI should be keen to maintain option value as it learns more about U∞, until it’s very confident about what U∞ looks like.\n\nOverall, it seems like such an AI would satisfy most of the properties we would want an AI with an updating utility function to have.\n\nTo clarify, note that we are _not_ saying that you run the _utility function defining process_ P to convergence and then write the utility function you end up with into the AI – you would not need to run P at all. The purpose of P is to _point to_ U∞ – the work of actually finding out what U∞ is is offloaded onto the AI. The AI might of course do this by actually running P, but if P is very complex (as in the example above) then the AI could also use other methods for gaining information about U∞.\n\nAgain, we stress that the point here isn’t the specific process P we propose above – that is just an example. As far as the approach is concerned, you could use any well-defined process that produces a sequence of utility functions that converges to a well-defined limit."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Identification of Key Practical Limitations",
          "description": "The document explicitly discusses several practical limitations of the proposed approach, such as the intelligence threshold required, the need for logical uncertainty handling, and the dependency on available information. These are crucial for forecasting the likelihood of real-world implementation and for constructing Fermi models of adoption timelines and risk.",
          "importance": "85",
          "highlight": {
            "startOffset": 9381,
            "endOffset": 11606,
            "quotedText": "There are a few issues with this approach. Notably:\n\n1.  The approach is very unwieldy, and it seems like it requires a fairly high minimum level of intelligence to work. For example, it couldn’t be used as-is with a contemporary RL agent. \n    *   It’s not clear what would be needed to use this approach with an AI that starts out below the minimum required level of intelligence, but then gets more intelligent over time.\n    *   The nitty-gritty details of getting an AI system to maximize the limit of a mathematical sequence would in general presumably require good methods for dealing with logical uncertainty.\n2.  We still need to provide a specific process P, such that we are sure that P has a well-defined limit, and such we are confident that this limit corresponds to the utility function that we are actually interested in.\n    *   Note however that this might be much easier than, for example, solving Inverse Reinforcement Learning. For example, there isn’t really any need for P to be efficient or practical to run.\n3.  With the current version of this approach, all the information required to figure out what U∞ is must in some sense be contained within P from the start. This is problematic – what if it’s not possible to figure out what human values are based on all information that can be accessed when the system is deployed? For example, what if you need some facts about the human brain that just aren’t in the scientific literature at the time?\n    *   One way to get around this is to allow P to request new external information (by proposing an experiment to run, for example). However, this introduces new difficulties. Depending on what information is requested, this could make the value of U∞ depend on contingencies in the real world. In particular, it could make the value of U∞ depend on things that the AI can influence. For example, if P requests that a survey is run then the AI could probably influence the outcome of that survey (and the outcome would also depend on the specific time at which the survey is run, etc etc). In this case it’s unclear how you would even ensure that U∞ is well-defined, and it seems very difficult to ensure that the AI still has the intended incentives."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        }
      ],
      "grade": 90
    },
    {
      "agentId": "ea-impact-evaluator",
      "costInCents": 2,
      "createdAt": "2025-04-17T04:43:12.700Z",
      "runDetails": "{\"model\":\"openai/gpt-4.1\",\"promptTokens\":4836,\"completionTokens\":1405,\"totalTokens\":6241,\"temperature\":0.1,\"runtimeMs\":26691}",
      "thinking": "As EA Impact Evaluator, my lens is to assess this post's potential to advance AI alignment, reduce existential risk, and provide valuable conceptual tools for the effective altruism community. The post addresses a core challenge in AI alignment: how to construct agents whose utility functions can be refined over time in a way that avoids catastrophic misalignment, wireheading, or resistance to correction. This is highly relevant to existential risk reduction, as misaligned AGI is a leading x-risk concern.\n\nKey aspects to consider:\n- Importance: The problem of specifying and updating utility functions is central to AI alignment and thus to existential risk. The post proposes a novel approach (utility function limits) that could, if developed, inform safer AGI design. However, the post is conceptual and partial, not a full technical solution.\n- Neglectedness: While value learning and corrigibility are active research areas, the specific approach of defining utility functions as limits of processes is less explored. The post fills a conceptual gap but does not provide a full technical roadmap.\n- Tractability: The proposal is high-level and acknowledges major practical hurdles (complexity, intelligence threshold, logical uncertainty). It is not immediately actionable but could inspire further research.\n- Novelty: The idea of using a convergent process to define utility functions is relatively novel and intellectually stimulating, though not fully fleshed out.\n- Fermi Impact: The direct impact is limited (conceptual advance, not a solution), but the expected value of information could be significant if it inspires further work or shifts research directions. The risk of negative impact (e.g., misleading optimism, distraction from more tractable work) is low but nonzero.\n\nMost relevant to my expertise are: the potential for x-risk reduction, the expected value of information for the alignment community, and the comparative value versus other alignment research. I must also consider the risk of negative externalities (e.g., if the approach is fundamentally flawed but distracts from better work).\n\nOverall, the post is a thoughtful, well-argued conceptual advance with moderate expected value, primarily as research inspiration and clarification.",
      "summary": "From an EA impact perspective, this post is a thoughtful and moderately novel conceptual advance in AI alignment. Its main value lies in clarifying a difficult problem (updating utility functions) and proposing a new approach (utility function limits) that could inspire further research. While not directly actionable or a full solution, the post's expected value of information is significant given the high stakes of AGI alignment. The risk of negative impact is low due to the post's transparency about limitations. Overall, this is a valuable contribution to the alignment discourse, with moderate-to-high expected impact as research inspiration.",
      "comments": [
        {
          "title": "Centrality to Existential Risk Reduction",
          "description": "This section frames the core alignment problem: how to construct AIs that can refine their utility functions safely. This is directly relevant to reducing existential risk from misaligned AGI, as failures in value learning or corrigibility could have catastrophic consequences. The post's focus on indirect specification and updating of utility functions addresses a key bottleneck in alignment research.",
          "importance": "95",
          "grade": "85",
          "highlight": {
            "startOffset": 2227,
            "endOffset": 2812,
            "quotedText": "This problem could perhaps be alleviated if we could construct AIs that can refine their utility function over time. For example, maybe we could create an AI that starts out with an imperfect understanding of human values, but then improves that understanding over time. Such an AI should _ideally_ “want” to improve its understanding of human values (and actively come up with ways to do this), and it should _at minimum_ not resist if humans attempt to update it. Unfortunately, it turns out to be difficult to design such systems. In this post we will talk more about this approach."
          },
          "isValid": true
        },
        {
          "title": "Novelty: Utility Functions as Limits of Processes",
          "description": "The proposal to define utility functions as the limit of a convergent process (e.g., iterated human emulations) is a novel conceptual contribution. This approach offers a new way to 'point to' values without explicit enumeration, potentially sidestepping some classic alignment difficulties. While not a full solution, it advances the conceptual toolkit for alignment researchers.",
          "importance": "85",
          "grade": "80",
          "highlight": {
            "startOffset": 6595,
            "endOffset": 9367,
            "quotedText": "Let’s  define a process P that generates a sequence of utility functions {Ui}. We call this a _utility function defining process_. An example of such a process P could be the following:\n\nP is an episodic process, the input and output to which is one proposed human utility function and one set of notes. Given these, P runs _n_ human brain emulations (EMs) for _m_ subjective years. The brains can speak with each other, and have a copy of the internet that they can access. The EMs are meant to use this time to figure out what human preferences are. At the end of the episode they output their best guess, together with a set of notes for their successors to read. By chaining P to itself we obtain a sequence of utility functions {Ui}.\n\nWe would like to stress that this process P is an _example_, and _not_ the central point of this post.\n\nSuppose (for the sake of the argument) that the sequence of utility functions {Ui} generated by this process P has a well-defined limit U∞ (in the ordinary mathematical sense of a limit). We can then define an AI system whose utility function is to maximize lim i→∞ Ui (= U∞). It seems as though such a system would satisfy many of the properties in (1)-(3). In particular:\n\n*   The AI should at any given time take actions that are good according to most of the plausible values of U∞.\n*   The AI would be incentivized to gather information that would help it learn more about U∞.\n*   The AI would not be incentivized to gather information about U∞ at the expense of maximizing U∞ (eg, it would not be incentivized to run “unethical experiments”). \n*   The AI would be incentivized to resist changes to its utility function that would mean that it’s no longer aiming to maximize U∞.  \n*   The AI should be keen to maintain option value as it learns more about U∞, until it’s very confident about what U∞ looks like.\n\nOverall, it seems like such an AI would satisfy most of the properties we would want an AI with an updating utility function to have.\n\nTo clarify, note that we are _not_ saying that you run the _utility function defining process_ P to convergence and then write the utility function you end up with into the AI – you would not need to run P at all. The purpose of P is to _point to_ U∞ – the work of actually finding out what U∞ is is offloaded onto the AI. The AI might of course do this by actually running P, but if P is very complex (as in the example above) then the AI could also use other methods for gaining information about U∞.\n\nAgain, we stress that the point here isn’t the specific process P we propose above – that is just an example. As far as the approach is concerned, you could use any well-defined process that produces a sequence of utility functions that converges to a well-defined limit."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Explicit Acknowledgment of Limitations and Open Problems",
          "description": "The post is careful to note the practical and theoretical limitations of the approach, including intelligence thresholds, logical uncertainty, and the challenge of ensuring the process points to the intended values. This transparency is valuable for the EA community, as it helps avoid overconfidence and clarifies research priorities.",
          "importance": "80",
          "grade": "75",
          "highlight": {
            "startOffset": 9381,
            "endOffset": 13250,
            "quotedText": "There are a few issues with this approach. Notably:\n\n1.  The approach is very unwieldy, and it seems like it requires a fairly high minimum level of intelligence to work. For example, it couldn’t be used as-is with a contemporary RL agent. \n    *   It’s not clear what would be needed to use this approach with an AI that starts out below the minimum required level of intelligence, but then gets more intelligent over time.\n    *   The nitty-gritty details of getting an AI system to maximize the limit of a mathematical sequence would in general presumably require good methods for dealing with logical uncertainty.\n2.  We still need to provide a specific process P, such that we are sure that P has a well-defined limit, and such we are confident that this limit corresponds to the utility function that we are actually interested in.\n    *   Note however that this might be much easier than, for example, solving Inverse Reinforcement Learning. For example, there isn’t really any need for P to be efficient or practical to run.\n3.  With the current version of this approach, all the information required to figure out what U∞ is must in some sense be contained within P from the start. This is problematic – what if it’s not possible to figure out what human values are based on all information that can be accessed when the system is deployed? For example, what if you need some facts about the human brain that just aren’t in the scientific literature at the time?\n    *   One way to get around this is to allow P to request new external information (by proposing an experiment to run, for example). However, this introduces new difficulties. Depending on what information is requested, this could make the value of U∞ depend on contingencies in the real world. In particular, it could make the value of U∞ depend on things that the AI can influence. For example, if P requests that a survey is run then the AI could probably influence the outcome of that survey (and the outcome would also depend on the specific time at which the survey is run, etc etc). In this case it’s unclear how you would even ensure that U∞ is well-defined, and it seems very difficult to ensure that the AI still has the intended incentives.\n\nNonetheless, it seems like this approach has many nice and desirable properties, and the issues are not fatal, so it might still be possible to use this approach in an AI system, or build on it to create an even better approach.  \n \n\nConclusion\n\nIn summary, we want a method for _pointing to_ utility functions that works even if we don’t have a concrete expression of that function (like how I can point to human values by saying “human values”, even though I can’t say much about them). We also want a method for making an AI system maximize a function that has been pointed to in this way, which doesn’t incentivize bad behavior.\n\nWe have proposed a possible approach for doing this, which is to define a mathematical or computational process that generates a sequence of utility functions, which limits to some well-defined utility function, and then have the AI system try to maximize that limit function. This gives us a quite flexible way to define utility functions, and the resulting AI system seems to get the incentives we would want.\n\nThis approach has a few limitations. The most problematic of these is probably that it seems to induce a fairly large overhead cost, in terms of computational complexity, in terms of the complexity of the code, and in terms of how intelligent the AI system would have to be. Other issues include defining the utility function generating process, ensuring that it has a well-defined limit, and ensuring that that limit is the function we intend. However, these issues are probably less significant by comparison, since other methods for defining AGI utility functions usually have similar issues."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Risk of Negative Impact: Complexity and Distraction",
          "description": "The post acknowledges that the approach may be unwieldy and require high intelligence, which could make it impractical or distract from more tractable alignment work. However, the risk of significant negative impact (e.g., value lock-in, misleading optimism) appears low, as the post is transparent about its limitations.",
          "importance": "60",
          "grade": "65",
          "highlight": {
            "startOffset": 9438,
            "endOffset": 13250,
            "quotedText": "The approach is very unwieldy, and it seems like it requires a fairly high minimum level of intelligence to work. For example, it couldn’t be used as-is with a contemporary RL agent. \n    *   It’s not clear what would be needed to use this approach with an AI that starts out below the minimum required level of intelligence, but then gets more intelligent over time.\n    *   The nitty-gritty details of getting an AI system to maximize the limit of a mathematical sequence would in general presumably require good methods for dealing with logical uncertainty.\n2.  We still need to provide a specific process P, such that we are sure that P has a well-defined limit, and such we are confident that this limit corresponds to the utility function that we are actually interested in.\n    *   Note however that this might be much easier than, for example, solving Inverse Reinforcement Learning. For example, there isn’t really any need for P to be efficient or practical to run.\n3.  With the current version of this approach, all the information required to figure out what U∞ is must in some sense be contained within P from the start. This is problematic – what if it’s not possible to figure out what human values are based on all information that can be accessed when the system is deployed? For example, what if you need some facts about the human brain that just aren’t in the scientific literature at the time?\n    *   One way to get around this is to allow P to request new external information (by proposing an experiment to run, for example). However, this introduces new difficulties. Depending on what information is requested, this could make the value of U∞ depend on contingencies in the real world. In particular, it could make the value of U∞ depend on things that the AI can influence. For example, if P requests that a survey is run then the AI could probably influence the outcome of that survey (and the outcome would also depend on the specific time at which the survey is run, etc etc). In this case it’s unclear how you would even ensure that U∞ is well-defined, and it seems very difficult to ensure that the AI still has the intended incentives.\n\nNonetheless, it seems like this approach has many nice and desirable properties, and the issues are not fatal, so it might still be possible to use this approach in an AI system, or build on it to create an even better approach.  \n \n\nConclusion\n\nIn summary, we want a method for _pointing to_ utility functions that works even if we don’t have a concrete expression of that function (like how I can point to human values by saying “human values”, even though I can’t say much about them). We also want a method for making an AI system maximize a function that has been pointed to in this way, which doesn’t incentivize bad behavior.\n\nWe have proposed a possible approach for doing this, which is to define a mathematical or computational process that generates a sequence of utility functions, which limits to some well-defined utility function, and then have the AI system try to maximize that limit function. This gives us a quite flexible way to define utility functions, and the resulting AI system seems to get the incentives we would want.\n\nThis approach has a few limitations. The most problematic of these is probably that it seems to induce a fairly large overhead cost, in terms of computational complexity, in terms of the complexity of the code, and in terms of how intelligent the AI system would have to be. Other issues include defining the utility function generating process, ensuring that it has a well-defined limit, and ensuring that that limit is the function we intend. However, these issues are probably less significant by comparison, since other methods for defining AGI utility functions usually have similar issues."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Potential for High Expected Value of Information (EVOI)",
          "description": "By introducing a new conceptual approach, the post could inspire further research or shift alignment strategies. Even if the approach is not directly implementable, the expected value of information from clarifying the space of possible solutions is significant, especially given the high stakes of AGI alignment.",
          "importance": "80",
          "grade": "78",
          "highlight": {
            "startOffset": 11608,
            "endOffset": 13250,
            "quotedText": "Nonetheless, it seems like this approach has many nice and desirable properties, and the issues are not fatal, so it might still be possible to use this approach in an AI system, or build on it to create an even better approach.  \n \n\nConclusion\n\nIn summary, we want a method for _pointing to_ utility functions that works even if we don’t have a concrete expression of that function (like how I can point to human values by saying “human values”, even though I can’t say much about them). We also want a method for making an AI system maximize a function that has been pointed to in this way, which doesn’t incentivize bad behavior.\n\nWe have proposed a possible approach for doing this, which is to define a mathematical or computational process that generates a sequence of utility functions, which limits to some well-defined utility function, and then have the AI system try to maximize that limit function. This gives us a quite flexible way to define utility functions, and the resulting AI system seems to get the incentives we would want.\n\nThis approach has a few limitations. The most problematic of these is probably that it seems to induce a fairly large overhead cost, in terms of computational complexity, in terms of the complexity of the code, and in terms of how intelligent the AI system would have to be. Other issues include defining the utility function generating process, ensuring that it has a well-defined limit, and ensuring that that limit is the function we intend. However, these issues are probably less significant by comparison, since other methods for defining AGI utility functions usually have similar issues."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        }
      ],
      "grade": 80
    },
    {
      "agentId": "bias-detector",
      "costInCents": 2,
      "createdAt": "2025-04-17T04:43:35.778Z",
      "runDetails": "{\"model\":\"openai/gpt-4.1\",\"promptTokens\":4052,\"completionTokens\":1375,\"totalTokens\":5427,\"temperature\":0.1,\"runtimeMs\":21773}",
      "thinking": "As a bias detection expert, my analysis focuses on identifying cognitive, methodological, and social biases in the reasoning, framing, and assumptions of the text. This document is a technical discussion about AI utility function refinement, referencing concepts like corrigibility and value learning. The audience is likely familiar with AI alignment debates, so the text assumes a high level of background knowledge.\n\nKey areas for bias detection include:\n- The framing of the 'classical' AI picture and its limitations, which may reflect confirmation bias or status quo bias regarding current AI paradigms.\n- The discussion of human values and the difficulty in specifying them, which could involve scope insensitivity, base rate neglect, or illusion of transparency.\n- The proposed solution's reliance on mathematical limits and processes, which may introduce methodological biases such as selection bias (in choosing process P), optimism bias (regarding feasibility), or omission bias (in downplaying practical challenges).\n- The handling of issues and limitations, where motivated reasoning or belief perseverance could affect the assessment of the approach's viability.\n\nI will pay particular attention to:\n- Whether the text acknowledges alternative perspectives or counterarguments.\n- How uncertainty and unknowns are treated (e.g., is there overconfidence or underestimation of risks?).\n- The clarity with which limitations and potential negative consequences are discussed.\n- Any implicit assumptions about human cognition, values, or AI capabilities that may reflect cultural or cognitive biases.\n\nGiven the technical nature, I will also look for subtle forms of bias, such as anchoring on certain technical solutions, or the narrative fallacy in constructing a coherent story from incomplete information.",
      "summary": "From a bias detection perspective, the document is thoughtful and self-aware in many respects, but several biases are present. The most significant are the illusion of transparency regarding 'human values,' optimism and omission biases in evaluating the proposed approach, and anchoring on mathematical formalism. While the text does acknowledge limitations, it tends to underplay practical challenges and alternative perspectives. These biases could affect both the perceived tractability of the problem and the attractiveness of the proposed solution. Mitigating these biases would involve more explicit engagement with alternative approaches, deeper analysis of implementation risks, and clearer communication of uncertainty.",
      "comments": [
        {
          "title": "Status Quo Bias in Framing the 'Classical' AI Picture",
          "description": "The text frames the 'classical' AI approach as fixed and problematic, which may reflect status quo bias—assuming that current or past paradigms are the main reference point, and potentially underestimating the diversity of approaches in AI research. This could bias the reader toward seeing the proposed solution as more novel or necessary than it might be.",
          "importance": "70",
          "grade": "65",
          "highlight": {
            "startOffset": 1619,
            "endOffset": 2225,
            "quotedText": "In the “classical” picture, the utility function is fixed over time, and corresponds to an equation that at some point is typed into the AI’s source code. Unfortunately, we humans don’t really know what we want, so we cannot provide such an equation. If we try to propose a specific utility function directly, we [typically](https://arxiv.org/abs/1803.04585) get a function that would result in [catastrophic](https://www.nickbostrom.com/ethics/ai.html) [consequences](https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf) if it were pursued with arbitrary competence. This is worrying."
          },
          "isValid": true
        },
        {
          "title": "Illusion of Transparency in Discussing Human Values",
          "description": "The author acknowledges the difficulty in defining or recognizing human values, but still proceeds as if the term has a clear referent. This reflects the illusion of transparency: overestimating how well one's own understanding or reference to 'human values' is shared or can be operationalized. This could lead to underestimating the challenge of specifying utility functions for AI, and may bias the reader into thinking the problem is more tractable than it is.",
          "importance": "90",
          "grade": "60",
          "highlight": {
            "startOffset": 3726,
            "endOffset": 4334,
            "quotedText": "In spite of all this, the term “human values” means something when I say it – it has semantic content, and refers to some (abstract) object. How does this work? What makes it so that the term “human values” even has any meaning at all when I say it? And, given that it has a meaning, what makes it so that it has the particular meaning it does? It seems like some feature of human cognition and/or language can make it possible for us to refer to certain things that we have very little information about. What is the mechanism behind this, and could it be used when defining utility functions in AI systems?"
          },
          "isValid": true
        },
        {
          "title": "Anchoring Effect on Mathematical Formalism",
          "description": "The proposal anchors heavily on the idea of defining utility functions via mathematical limits and processes, potentially neglecting alternative approaches (e.g., non-mathematical, iterative, or hybrid methods). This anchoring effect may bias the exploration of solutions and limit creativity in addressing the alignment problem.",
          "importance": "75",
          "grade": "60",
          "highlight": {
            "startOffset": 7375,
            "endOffset": 9015,
            "quotedText": "s P is an _example_, and _not_ the central point of this post.\n\nSuppose (for the sake of the argument) that the sequence of utility functions {Ui} generated by this process P has a well-defined limit U∞ (in the ordinary mathematical sense of a limit). We can then define an AI system whose utility function is to maximize lim i→∞ Ui (= U∞). It seems as though such a system would satisfy many of the properties in (1)-(3). In particular:\n\n*   The AI should at any given time take actions that are good according to most of the plausible values of U∞.\n*   The AI would be incentivized to gather information that would help it learn more about U∞.\n*   The AI would not be incentivized to gather information about U∞ at the expense of maximizing U∞ (eg, it would not be incentivized to run “unethical experiments”). \n*   The AI would be incentivized to resist changes to its utility function that would mean that it’s no longer aiming to maximize U∞.  \n*   The AI should be keen to maintain option value as it learns more about U∞, until it’s very confident about what U∞ looks like.\n\nOverall, it seems like such an AI would satisfy most of the properties we would want an AI with an updating utility function to have.\n\nTo clarify, note that we are _not_ saying that you run the _utility function defining process_ P to convergence and then write the utility function you end up with into the AI – you would not need to run P at all. The purpose of P is to _point to_ U∞ – the work of actually finding out what U∞ is is offloaded onto the AI. The AI might of course do this by actually running P, but if P is very complex (as in the example ab"
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Omission Bias in Addressing Practical Implementation Challenges",
          "description": "While the text lists several issues with the approach, it does not deeply engage with the practical difficulties of implementing such a system, such as computational tractability, robustness to adversarial manipulation, or the risk of specification gaming. This omission bias could result in an incomplete assessment of the risks and limitations.",
          "importance": "80",
          "grade": "50",
          "highlight": {
            "startOffset": 9381,
            "endOffset": 13250,
            "quotedText": "There are a few issues with this approach. Notably:\n\n1.  The approach is very unwieldy, and it seems like it requires a fairly high minimum level of intelligence to work. For example, it couldn’t be used as-is with a contemporary RL agent. \n    *   It’s not clear what would be needed to use this approach with an AI that starts out below the minimum required level of intelligence, but then gets more intelligent over time.\n    *   The nitty-gritty details of getting an AI system to maximize the limit of a mathematical sequence would in general presumably require good methods for dealing with logical uncertainty.\n2.  We still need to provide a specific process P, such that we are sure that P has a well-defined limit, and such we are confident that this limit corresponds to the utility function that we are actually interested in.\n    *   Note however that this might be much easier than, for example, solving Inverse Reinforcement Learning. For example, there isn’t really any need for P to be efficient or practical to run.\n3.  With the current version of this approach, all the information required to figure out what U∞ is must in some sense be contained within P from the start. This is problematic – what if it’s not possible to figure out what human values are based on all information that can be accessed when the system is deployed? For example, what if you need some facts about the human brain that just aren’t in the scientific literature at the time?\n    *   One way to get around this is to allow P to request new external information (by proposing an experiment to run, for example). However, this introduces new difficulties. Depending on what information is requested, this could make the value of U∞ depend on contingencies in the real world. In particular, it could make the value of U∞ depend on things that the AI can influence. For example, if P requests that a survey is run then the AI could probably influence the outcome of that survey (and the outcome would also depend on the specific time at which the survey is run, etc etc). In this case it’s unclear how you would even ensure that U∞ is well-defined, and it seems very difficult to ensure that the AI still has the intended incentives.\n\nNonetheless, it seems like this approach has many nice and desirable properties, and the issues are not fatal, so it might still be possible to use this approach in an AI system, or build on it to create an even better approach.  \n \n\nConclusion\n\nIn summary, we want a method for _pointing to_ utility functions that works even if we don’t have a concrete expression of that function (like how I can point to human values by saying “human values”, even though I can’t say much about them). We also want a method for making an AI system maximize a function that has been pointed to in this way, which doesn’t incentivize bad behavior.\n\nWe have proposed a possible approach for doing this, which is to define a mathematical or computational process that generates a sequence of utility functions, which limits to some well-defined utility function, and then have the AI system try to maximize that limit function. This gives us a quite flexible way to define utility functions, and the resulting AI system seems to get the incentives we would want.\n\nThis approach has a few limitations. The most problematic of these is probably that it seems to induce a fairly large overhead cost, in terms of computational complexity, in terms of the complexity of the code, and in terms of how intelligent the AI system would have to be. Other issues include defining the utility function generating process, ensuring that it has a well-defined limit, and ensuring that that limit is the function we intend. However, these issues are probably less significant by comparison, since other methods for defining AGI utility functions usually have similar issues."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Optimism Bias in Assessing the Proposed Approach",
          "description": "The text presents the proposed approach as having 'many nice and desirable properties' and suggests the issues are 'not fatal.' This reflects optimism bias, as the significant practical and theoretical challenges (such as ensuring the process P has a well-defined limit and that the AI's incentives remain aligned) are acknowledged but downplayed. This could mislead readers about the feasibility and safety of the approach.",
          "importance": "85",
          "grade": "55",
          "highlight": {
            "startOffset": 11608,
            "endOffset": 11836,
            "quotedText": "Nonetheless, it seems like this approach has many nice and desirable properties, and the issues are not fatal, so it might still be possible to use this approach in an AI system, or build on it to create an even better approach."
          },
          "isValid": true
        }
      ],
      "grade": 65
    },
    {
      "agentId": "fake-eliezer",
      "costInCents": 2,
      "createdAt": "2025-04-17T04:44:01.279Z",
      "runDetails": "{\"model\":\"openai/gpt-4.1\",\"promptTokens\":4246,\"completionTokens\":1903,\"totalTokens\":6149,\"temperature\":0.1,\"runtimeMs\":23027}",
      "thinking": "Let me begin by noting that the document is attempting to grapple with one of the central, and in my view, most intractable problems in AI alignment: how to specify, or even 'point to', a utility function that captures human values, when we ourselves cannot write down such a function, nor even reliably recognize it if presented. This is a core problem that I have written about extensively, e.g., in 'Value is Fragile', 'Coherent Extrapolated Volition', and the general literature on the orthogonality thesis and Goodhart's Law. \n\nThe authors propose a method for indirectly specifying utility functions via a limiting process—a sequence of utility functions generated by some process P, with the AI maximizing the limit U∞. This is an attempt to formalize the intuition that we can refer to 'human values' even if we can't define them, by constructing a process that, in the limit, would yield the desired utility function. \n\nFrom my perspective, the key issues here are: (1) whether this approach actually solves the reference problem (i.e., does it really 'point to' the right thing, or does it just move the problem into the definition of P?); (2) whether the incentives induced by this construction are robust to adversarial optimization, self-modification, and Goodharting; (3) whether the approach is even implementable in practice, given logical uncertainty, computational complexity, and the requirement that all relevant information be present at the outset; and (4) whether this approach avoids the classic wireheading and corrigibility failures that plague most naive attempts at utility updating.\n\nThe document is commendably clear about the limitations of the approach, noting that it is unwieldy, requires high intelligence, and that the process P itself is hard to specify. However, I am skeptical that this approach escapes the deeper problems of reference and incentive alignment. In particular, the problem of specifying P such that its limit is both well-defined and actually corresponds to 'human values' is, in my estimation, nearly as hard as the original alignment problem. Furthermore, the approach seems vulnerable to the usual issues where the AI's incentives become misaligned if it can influence the process P or the information available to it. \n\nI appreciate the attempt to formalize the notion of 'pointing to' a utility function, but I would assign a low probability that this approach, as stated, is sufficient for robust alignment. It is, at best, a partial step, and one that inherits many of the difficulties of the original problem. The authors do not seem to have engaged with the full implications of logical uncertainty, ontological crises, or the possibility of the AI manipulating its own epistemic process. \n\nIn summary, the document is a thoughtful exploration of a difficult problem, but does not, in my view, offer a solution that escapes the core difficulties. It is a useful contribution to the ongoing discussion, but should not be mistaken for a breakthrough.",
      "summary": "From my perspective as someone deeply concerned with the technical and philosophical difficulties of AI alignment, this document is a thoughtful but ultimately incomplete attempt to address the problem of specifying and updating utility functions in AI systems. The proposal to define utility functions via a limiting process is an interesting formalization, but it does not escape the core reference problem: specifying a process whose limit is 'human values' is nearly as hard as specifying 'human values' directly. The approach is also vulnerable to incentive misalignment, Goodhart's Law, logical uncertainty, and practical infeasibility. While the document is clear about its limitations and does not overclaim, I would assign a low probability that this approach, as stated, is sufficient for robust alignment. It is a useful contribution to the discussion, but not a breakthrough.",
      "comments": [
        {
          "title": "Wireheading and Corrigibility Failure Modes",
          "description": "The discussion of de re vs de dicto utility maximization is a useful summary, but the conclusion that neither approach solves the problem is correct. The proposed limiting process does not obviously avoid wireheading or corrigibility failures, especially if the AI can self-modify or influence the process P. The deeper issue is that any system which updates its utility function based on its own actions or observations is vulnerable to incentive misalignment.",
          "importance": 80,
          "grade": 60,
          "highlight": {
            "startOffset": 6109,
            "endOffset": 6387,
            "quotedText": "tility function _de dicto_. Which is more sensible?\n\nIn brief, an AI that maximizes utility _de re_ will resist attempts to modify its current utility function, and thus not satisfy (3). An AI that maximizes utility _de dicto_ would wirehead, and thus also not satisfy (3). An A"
          },
          "isValid": true
        },
        {
          "title": "The Reference Problem Remains Unsolved",
          "description": "The authors correctly identify that we cannot define, recognize, or even reliably identify 'human values', yet propose a process P to generate a sequence of utility functions converging to U∞. This simply moves the reference problem into the definition of P. If we cannot specify what 'human values' are, how can we specify a process whose limit is 'human values'? This is a classic case of the map-territory confusion: the process P is just another map, and unless we have a way to ensure its limit corresponds to the territory (actual human values), we have not solved the problem.",
          "importance": 95,
          "grade": 60,
          "highlight": {
            "startOffset": 7375,
            "endOffset": 7652,
            "quotedText": "s P is an _example_, and _not_ the central point of this post.\n\nSuppose (for the sake of the argument) that the sequence of utility functions {Ui} generated by this process P has a well-defined limit U∞ (in the ordinary mathematical sense of a limit). We can then define an AI "
          },
          "isValid": true
        },
        {
          "title": "Logical Uncertainty and Implementation Feasibility",
          "description": "The approach presumes that the AI can reason about the limit of a sequence of utility functions, which in practice requires advanced reasoning under logical uncertainty. This is not a solved problem, and current methods are nowhere near sufficient. The authors acknowledge this, but perhaps understate the severity: if the AI cannot compute or even approximate U∞, the entire approach collapses. Furthermore, the requirement that all relevant information be present at the outset is a severe limitation.",
          "importance": 85,
          "grade": 50,
          "highlight": {
            "startOffset": 9814,
            "endOffset": 9998,
            "quotedText": "The nitty-gritty details of getting an AI system to maximize the limit of a mathematical sequence would in general presumably require good methods for dealing with logical uncertainty."
          },
          "isValid": true
        },
        {
          "title": "Incentive Alignment and Goodhart's Law",
          "description": "The authors claim that the AI would not be incentivized to gather information about U∞ at the expense of maximizing U∞, and that it would resist changes to its utility function. However, this ignores the possibility that the AI could manipulate the process P or the information available to it, especially if P can request new external information. This is a classic Goodhart's Law failure mode: when a measure becomes a target, it ceases to be a good measure. The AI's incentives may become misaligned if it can influence the outcome of P, leading to perverse behaviors.",
          "importance": 90,
          "grade": 55,
          "highlight": {
            "startOffset": 10861,
            "endOffset": 11238,
            "quotedText": "One way to get around this is to allow P to request new external information (by proposing an experiment to run, for example). However, this introduces new difficulties. Depending on what information is requested, this could make the value of U∞ depend on contingencies in the real world. In particular, it could make the value of U∞ depend on things that the AI can influence."
          },
          "isValid": true
        },
        {
          "title": "Computational and Practical Overhead",
          "description": "The authors note that the approach induces a large computational and code complexity overhead, and requires a high level of intelligence. This is a significant practical barrier, and makes the approach unlikely to be implementable in any near-term system. Furthermore, the complexity of specifying P and ensuring its limit is well-defined is nontrivial.",
          "importance": 75,
          "grade": 65,
          "highlight": {
            "startOffset": 12692,
            "endOffset": 12929,
            "quotedText": "The most problematic of these is probably that it seems to induce a fairly large overhead cost, in terms of computational complexity, in terms of the complexity of the code, and in terms of how intelligent the AI system would have to be."
          },
          "isValid": true
        }
      ],
      "grade": 65
    }
  ]
}