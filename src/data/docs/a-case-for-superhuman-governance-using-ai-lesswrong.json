{
  "id": "a-case-for-superhuman-governance-using-ai-lesswrong",
  "slug": "a-case-for-superhuman-governance-using-ai-lesswrong",
  "title": "A Case for Superhuman Governance, using AI — LessWrong",
  "author": "ozziegooen",
  "publishedDate": "2024-06-07T00:10:10.902Z",
  "url": "https://www.lesswrong.com/posts/cJv8rBSshrR82NRET/a-case-for-superhuman-governance-using-ais",
  "intendedAgents": [
    "bias-detector",
    "clarity-coach",
    "research-scholar",
    "fake-eliezer",
    "quantitative-forecaster",
    "ea-impact-evaluator"
  ],
  "content": "I believe that:\n\n1.  AI-enhanced organization governance could be a potentially huge win in the next few decades.\n2.  AI-enhanced governance could allow organizations to reach superhuman standards, like having an expected \"99.99\" reliability rate of not being corrupt or not telling lies.\n3.  While there are clear risks to AI-enhancement at the top levels of organizations, it’s likely that most of these can be managed, assuming that the implementers are reasonable. \n4.  AI-enhanced governance could synchronize well with AI company regulation. These companies would be well-placed to develop innovations and could hypothetically be incentivized to do much of the work. AI-enhanced governance might be necessary to ensure that these organizations are aligned with public interests.\n5.  More thorough investigation here could be promising for the effective altruism community.\n\nWithin effective altruism now, there’s a lot of work on governance and AI, but not much on using AI for governance. AI Governance typically focuses on using conventional strategies to oversee AI organizations, while AI Alignment research focuses on aligning AI systems. However, leveraging AI to improve human governance is an underexplored area that could complement these cause areas. You can think of it as “Organizational Alignment”, as a counterpoint to “AI Alignment.”\n\nThis article was written after some rough ideation I’ve done about this area. This isn’t at all a literature review or a research agenda. That said, for those interested in this topic, here are a few posts you might find interesting.\n\n*   [Project ideas: Governance during explosive technological growth](https://lukasfinnveden.substack.com/p/project-ideas-governance-during-explosive)\n*   [The Project AI Series, by OpenMined](The Private AI Series)\n*   [Safety Cases: How to Justify the Safety of Advanced AI Systems](https://arxiv.org/abs/2403.10462)\n*   [Affirmative Safety: An Approach to Risk Management for Advanced AI](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4806274)\n\n## What is “AI-Assisted” Governance?\n\nAI-Assisted Governance refers to improvements in governance that leverage artificial intelligence (AI), particularly focusing on rapidly advancing areas like Large Language Models (LLMs).\n\nExamples methods include:\n\n1.  Monitoring politicians and executives to identify and flag misaligned or malevolent behavior, ensuring accountability and integrity.\n2.  Enhancing epistemics and decision-making processes at the top levels of organizations, leading to more informed and rational strategies.\n3.  Facilitating more effective negotiations and trades between organizations, fostering better cooperation and coordination.\n4.  Assisting in writing and overseeing highly secure systems, such as implementing differential privacy and formally verified, bug-free decision-automation software, for use at managerial levels.\n\n## Arguments for Governance Improvements, Generally\n\nThere's already a lot of consensus in the rationalist and effective altruist communities about the importance for governance. See the topics on [Global Governance](https://forum.effectivealtruism.org/topics/global-governance), [AI Governance](https://forum.effectivealtruism.org/topics/ai-governance), and [Nonprofit Governance](https://forum.effectivealtruism.org/topics/nonprofit-governance) for more information.\n\nHere are some main reasons why focusing on improving governance seems particularly promising:\n\n**Concentrated Leverage**\n\nReal-world influence is disproportionately concentrated in the hands of a relatively small number of leaders in government, business, and other pivotal institutions. This is especially true in the case of rapid AI progress. Improving the reasoning and actions of this select group is therefore perhaps the most targeted, tractable, and neglected way to shape humanity's long-term future. AI tools could offer uniquely potent levers to do so.\n\nA lot of epistemic-enhancing work focuses on helping large populations. But some people will matter many times as much as others, and these people are often in key management positions.\n\n**Dramatic Room for Improvement**\n\nIt's hard to look at recent political and business fiascos and have much confidence of what to expect in the future. It seems harder if you think that the world could get a lot more crazy in the future. I think there's wide acceptance in our communities that critical modern organization are poorly equipped to deal with modern or upcoming challenges.\n\nThe main question is if there are any effective approaches to improvement here. I would argue that AI assistance is a serious option.\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/nyiXej8EtoFwigrsw/xby63xzrcf1wm0ckqxig)\n\nA superhuman combination of human and AI\n\n## Arguments for AI in Governance Improvements\n\nAssuming that governance is an important area, why should we prioritize using AI to improve it? \n\n**Rapid Technological Progress**\n\nIn contrast to most other domains relevant to improving governance, AI has seen remarkable and rapidly accelerating progress in recent years. LLMs are improving rapidly and now there are billions of dollars being invested in improving deep learning capabilities. We can expect this trend to continue.\n\n**Lack of Promising Alternatives**\n\nThere aren't any AI-absent improvements that I could see making very large expected changes in governments or corporations in the next 20 years. \n\n*   There seems to be surprisingly little interest in improving organizational boards. I don't know of any nonprofits focused on this, for example. Recent FTX and OpenAI board fiascos have demonstrated severe problems with boards, and I don't see these going away soon.\n*   Despite decades of research into organizational psychology, human decision-making, statistics, and so on, government leaders continue to be severely lacking. \n*   Better voting systems for governments seems nice, but very limited. I don't see these making substantial global differences in the next 20 years.\n*   Current forecasting infrastructure is still a long way off from helping at executive levels. I think that next-generation AI-heavy systems could change that, but don't expect much from non-AI systems.\n\n**Reliable Oversight**\n\nOne of the key challenges in holding human leaders accountable is the difficulty of comprehensively monitoring their actions, due to privacy concerns and logistical constraints. For instance, while it could be valuable to have 24/7 oversight of key decision-makers, few would consent to such invasive surveillance. In contrast, AI systems could be designed from the ground up to be fully transparent and amenable to constant monitoring.\n\nWeak AI systems, in particular, could be both much easier to oversee and more effective at providing oversight than human-based approaches. For example, consider the challenge of ensuring that a team of 20 humans maintains 99.99% integrity - never accepting bribes or intentionally deceiving. Achieving this level of reliability with purely human oversight seems impractical. However, well-designed AI systems could potentially provide the necessary level of monitoring and control.\n\nIn the field of cybersecurity, [social hacking](https://en.wikipedia.org/wiki/Social_hacking) is a well-known corporate vulnerability. Software controls and monitoring are often employed as a solution. As software capabilities advance, we can anticipate improvements in software-based mitigation of social hacking risks.\n\nA simple step in this direction could be to require that teams rely on designated AI decision-support systems, at least in situations where deception or misconduct is most likely. More advanced \"AI watchers\" could eventually operate at scale to keep both human and machine agents consistently honest and aligned with organizational goals.\n\n**Complementary Human-AI Approaches**\n\nI'm particularly excited by the potential for well-designed AI governance tools to be mutually reinforcing with responsible human leadership, rather than purely substitutive. In the same way that the AI alignment research community aims to use limited AI assistants to help oversee and validate more advanced AI systems, we could use highly reliable AI to augment human oversight of both artificial and human agents. The complementary strengths and weaknesses of human and machine cognition could be a powerful combination. Humans fail in some predictable ways, and AI fail in some predictable ways, but there seems to be a small subset of strategies that could defeat both humans and AIs combined.\n\n**Synergies with Key AI Actors**\n\nThe leading AI organizations and individuals are not only some of the most critical targets for improved oversight and alignment, but also the best positioned to pioneer effective AI governance techniques. They could have both the greatest need and capacity to innovate in this area. Leading AI teams are generally highly-innovative, strong at using AI, and knowledgeable of AI oversight - these skills seem very well suited to them being the earliest candidates to execute and innovate on AI-Assisted Governance strategies.  \n\nIf AI companies claim to be developing robust and secure AI systems for the public good, it's reasonable to ask them to embrace AI-assisted oversight of their own activities. \n\n **The Recursive Enforcement Problem**\n\nTraditional human oversight systems struggle to ensure accountability at the highest levels, as the question of \"who oversees the overseers\" often remains unresolved. This can lead to distrust and corruption.\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/nyiXej8EtoFwigrsw/dgvsazjn06tqynvhaeph)\n\nIntegrating AI systems can mitigate this issue by designing highest-level oversight systems that maintain privacy while revealing key problems. These AI systems can be thoroughly tested to ensure competence and reliability, focusing on ensuring the honesty and alignment of human overseers.\n\nTo enhance AI-assisted oversight:\n\n1.  Implement secure communication channels between AI and human overseers\n2.  Make sure that AIs reveal key alignment failures, while minimizing the reveal of private information\n3.  Have human evaluators oversee these AI systems. These evaluators can do so while not seeing critical private information.\n4.  Have a dialogue between the organization and other actors to ensure that the human AI evaluators do a good job.\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/nyiXej8EtoFwigrsw/smqd4k6uxgjmsxjrfxil)\n\nSuch a system will still have challenges to find the best tradeoff between oversight and privacy, but it would introduce a lot more options than simply using humans to oversee other humans.\n\nIn a more advanced system, it seems possible that all entities could be overseen by certain limited AI overseers, in proportion to the need. \n\n## Strategy: Bootstrapping Effective AI Governance\n\nAI-Assisted Governance could enable superhuman performance measures, such as \"_99.999% resistance to bribes or manipulation,_\" to become feasible. This could provide a roadmap for governments to set increasingly stringent performance requirements that AI companies must meet in order to be permitted to develop more advanced and potentially dangerous technologies.\n\nPrudent governments would start with highly conservative rules that impose rigorous demands and grant limited permissions. If AI developers demonstrate their ability to meet the initial performance benchmarks, the rules for higher levels will be reassessed.\n\nWith robust government regulation, this approach could create a powerful positive feedback loop, where at each stage, the system is expected to become safer, even as capabilities grow.\n\nFor instance:\n\n1.  AI companies enhance AI capabilities up to the limits they are permitted.\n2.  To meet the required standards, AI companies would be incentivized to incorporate AI capabilities into innovative governance tools. Their existing expertise and innovation capabilities in AI put them in a strong position to do so.\n3.  Other parties adopt some of these new governance tools. Importantly, some are implemented by government regulators, and many are implemented by AI organization evaluators and auditors.\n4.  Regulators review recent changes in AI development and developer governance, and update regulatory standards accordingly. If it appears very safe to do so, they may gradually relax certain measures. These regulators also leverage recent AI advancements to assist in setting standards. \n5.  The cycle begins anew.\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/nyiXej8EtoFwigrsw/qnkqga8uswgwqq0xq1do)\n\nJust as we might bootstrap safe AI systems, this could effectively bootstrap safe AI organizations. However, this self-reinforcing dynamic still necessitates robust external checks and balances. Independent auditors and evaluators with strong technical expertise and an adversarial mandate will be crucial to stress-test the AI governance solutions and prevent self-deception or manipulation by AI companies.\n\n## Objection 1: Would AI-Assisted Governance Increase AI Risk?\n\nSome may object that integrating AI into governance structures could actually make dangerous AI outcomes more likely. After all, if we are already concerned about advanced AI systems seizing control, wouldn't putting them in positions of power be enormously risky?\n\nThere are certainly some very unwise ways one could combine AI and governance. Recklessly deploying an untested AGI system to replace human leaders, for instance. However, I believe that with sufficient care and incremental development, we can find balanced approaches that capture the benefits of AI-assisted governance while mitigating the risks.\n\n\"AI\" is really an extremely broad category, encompassing a vast range of potential systems with radically different capabilities and risk profiles. Just as we use \"technology\" to counter misuse of \"technology\", or \"politicians\" to oversee other \"politicians\", we can leverage safer and more constrained AI systems to help govern the development and deployment of more advanced AI. A large spectrum exists between \"_AIs that are useful_\" and \"_AIs that pose existential risk_\". Many systems could offer significant benefits to governance without being capable of dangerous self-improvement or independent power-seeking.\n\nFor example, we could use heavily restricted AI systems to monitor communications and flag potential misconduct, without giving them any executive powers. We could have more advanced AI systems oversee the deployment of more mature and stable ones. Various oversight regimes and \"AI watchers\" could be implemented to maintain accountability. The key is to start with highly limited and transparent systems, and gradually scale up oversight and functionality as we gain confidence.\n\nIt's also worth noting that by the time AIs are truly dangerous, they may gain power regardless of where they are initially deployed. The most concerning AGI scenarios might not be worsened by responsible AI governance efforts in the interim. On the contrary, having more robust governance structures in place, bolstered by our most dependable AI tools, may actually help us navigate the emergence of advanced AI more safely.\n\nIn particular, well-designed AI oversight could make institutions more resistant to subversion and misuse by malicious agents, both human and artificial. For instance, requiring both human and AI authorization for sensitive actions could make unilateral corruption or deception much harder. Ultimately, while caution is certainly warranted, I suspect the protective benefits of AI-assisted governance are greater than the risks, if implemented thoughtfully.\n\n## Objection 2: Will AI Governance Dangerously Increase Complexity?\n\nAnother reasonable concern is that introducing AI into already complex governance structures could make the overall system more intricate and unpredictable. Highly capable AI agents engaging in strategic interactions could generate dynamics far beyond human understanding, leading to a loss of control. In the worst case, this opacity and complexity could make the whole system more fragile and vulnerable to catastrophic failure modes.\n\nThis is a serious challenge that would need to be carefully managed. Highly advanced AI pursuing incomprehensible strategies could certainly introduce dangerous volatility and uncertainty. We would need to invest heavily in interpretability and oversight capabilities to maintain a sufficient grasp on what's happening.\n\nHowever, I suspect that thoughtful AI-assisted governance is more likely to ultimately reduce destructive unpredictability than increase it. Currently, many high-stakes decisions are made by individual human leaders with unchecked biases and failure modes. The rationality and consistency of well-designed AI systems could help stabilize decision-making and make it more legible.\n\nThere's a long track record of judicious optimization and standardization increasing the reliability and predictability of essential systems and infrastructures, from units of measurement to building codes to economic policy. I expect we could achieve similar benefits for governance by applying our most capable and aligned AI tools, e.g. to help leaders converge on more rational and evidence-based policies.\n\nSome added complexity is likely inevitable when using advanced tools to improve old processes. The key question is whether the benefits justify the complexity. Given the extraordinary importance of good governance, and the ability for it to help reduce complexity where really needed, I believe some additional complexity in certain places is an acceptable price to pay for much more capable and aligned decision-making.\n\n## Objection 3: Could AI Governance Tools Backfire by Causing Overconfidence?\n\nA final concern to consider is that AI-assisted governance tools might not work nearly as well as hoped, but could still engender overconfidence and complacency. If people perceive AI involvement as a panacea that makes governance trustworthy and reliable by default, they may grow overly deferential and reduce their vigilance. Flawed AI tools and overreliance on them could then cause more harm than if we had maintained a healthy skepticism.\n\nThis risk of misplaced confidence is real and will require active effort to combat. Any development and deployment of AI governance tools must be accompanied by clear communication about their limitations and failure modes. Overselling their capabilities or implying that they make conventional oversight and accountability unnecessary would be deeply irresponsible.\n\nHowever, if AI governance researchers maintain a sober and realistic outlook, I'm optimistic that we can realize real gains without much undue hype. It's an empirical question how much the benefits of AI assistance will outweigh the drawbacks of overconfidence, but I suspect we can achieve a favorable balance. Note too that AI improvements in epistemic capabilities could help in making these decisions.\n\n## What Should Effective Altruists Do?\n\nIf governments mandated exceptional levels of governance, AI organizations would be driven to undertake the necessary work to achieve those standards. Therefore, effective altruists would ideally concentrate on ensuring the implementation of appropriate regulations, if that specific (perhaps very unlikely) approach seems viable.\n\nDetermining the specifics of such regulations would be challenging, as would be the process of getting them approved. Setting benchmarks for organizational governance is not a straightforward task and warrants further investigation. It's important to note that these standards may be initially unattainable using current technology, provided there is a path to eventually meeting them with future AI advancements.\n\nHowever, there are numerous other entities for which we desire excellent governance, but they may lack the capabilities or motivation to conduct the required research. In such cases, some level of philanthropic support in this area appears to be warranted.\n\nI'll also note: the main writing I've seen in this space comes from the AI policy sector. I'm thankful for this, but I'd like to see more ideation from the tech sector. I'd be eager to get better futurist ideas and demos of stories where creative technologies make organizational governance amazing in the next 50 years. Similar to my desire to see more great futuristic epistemic ideas, I'd love to see more futuristic governance ideas as well.  \n \n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/nyiXej8EtoFwigrsw/g8vrdbnvkrchxfeo2t2h)",
  "reviews": [
    {
      "agentId": "bias-detector",
      "costInCents": 6,
      "createdAt": "2025-04-17T02:25:44.768Z",
      "runDetails": "{\"model\":\"anthropic/claude-3.7-sonnet\",\"promptTokens\":5653,\"completionTokens\":2847,\"totalTokens\":8500,\"temperature\":0.1,\"runtimeMs\":54679}",
      "thinking": "I'm analyzing this document from the perspective of a bias detector, looking for various types of cognitive, social, cultural, and methodological biases that might be present in the text.\n\nFirst, I notice that the document presents a strong case for AI-enhanced governance, arguing that it could lead to \"superhuman standards\" of reliability and integrity. The author seems quite optimistic about the potential benefits while acknowledging some risks. This framing suggests there might be some optimism bias or technological solutionism at play.\n\nThe author makes several claims about the potential of AI to improve governance without providing substantial evidence for these claims. For example, the assertion that AI-enhanced governance could allow for \"99.99\" reliability rates against corruption is presented without empirical backing. This suggests potential overconfidence or the planning fallacy.\n\nThere's also a notable focus on the benefits of AI governance while potential downsides are addressed mainly as \"objections\" that are then countered. This structure might indicate confirmation bias, where the author is more thoroughly exploring evidence and arguments that support their position.\n\nThe document presents AI as a solution to human fallibility in governance, which could reflect a form of automation bias - the tendency to favor suggestions from automated systems over human judgment. There's an implicit assumption that AI systems would be less susceptible to certain biases than humans, which isn't necessarily proven.\n\nThe author mentions that \"there aren't any AI-absent improvements that I could see making very large expected changes in governments or corporations in the next 20 years.\" This statement suggests a possible availability bias, where the author may be overlooking potential non-AI solutions simply because they're less salient in their thinking or current discourse.\n\nThe bootstrapping strategy described involves a feedback loop where AI companies develop governance tools that are then used to regulate those same companies. This could potentially involve conflicts of interest or regulatory capture, though the author does acknowledge the need for independent auditors.\n\nThe document also exhibits some in-group framing, referring to \"consensus in the rationalist and effective altruist communities\" and addressing what \"effective altruists would ideally concentrate on.\" This suggests the author is writing primarily for an audience that shares certain assumptions and values.\n\nIn addressing objections, the author tends to acknowledge concerns but then provide reasons why they believe these concerns can be managed or are outweighed by benefits. This structure, while attempting to be balanced, may still reflect motivated reasoning.\n\nOverall, while the document does attempt to present a balanced view by addressing potential objections, it still exhibits several potential biases that could affect the validity of its conclusions. I'll now identify specific instances of these biases in the text.",
      "summary": "The document exhibits several significant biases that undermine its analysis of AI-enhanced governance. Most prominently, optimism bias and technological solutionism lead the author to present an overly positive view of AI's potential to achieve \"superhuman standards\" in governance without sufficient empirical evidence. The text displays automation bias by assuming AI systems would be inherently more reliable for oversight than humans. The document's structure reveals confirmation bias, addressing potential objections only after establishing a strong case for AI governance, with each objection followed by a rebuttal that reinforces the author's position. Motivated reasoning is evident in how the author acknowledges risks but quickly pivots to minimizing them. The planning fallacy appears in the underestimation of implementation challenges. The text also exhibits in-group bias through its framing for rationalist and effective altruist communities, and presents a false dichotomy between AI-enhanced governance and the status quo. While the author attempts to present a balanced view by addressing objections, these biases significantly impact the objectivity and reliability of the analysis.",
      "comments": [
        {
          "title": "Optimism Bias",
          "description": "The author displays optimism bias by presenting an overly positive view of AI's potential to achieve \"superhuman standards\" in governance. The claim of \"99.99\" reliability rates against corruption lacks empirical evidence and reflects unrealistic expectations about technology's capabilities. This optimism bias could lead to underestimating implementation challenges and overestimating benefits.",
          "importance": "90",
          "grade": "40",
          "highlight": {
            "startOffset": 114,
            "endOffset": 464,
            "quotedText": "2.  AI-enhanced governance could allow organizations to reach superhuman standards, like having an expected \"99.99\" reliability rate of not being corrupt or not telling lies.\n3.  While there are clear risks to AI-enhancement at the top levels of organizations, it’s likely that most of these can be managed, assuming that the implementers are reasona"
          },
          "isValid": true
        },
        {
          "title": "In-group Bias",
          "description": "The text displays in-group bias through its framing and language, which assumes shared values and perspectives with the rationalist and effective altruist communities. This framing may limit the appeal and applicability of the ideas to those outside these communities and reflects a bias toward in-group thinking patterns and priorities. The author assumes consensus on certain values without acknowledging diverse perspectives on governance.",
          "importance": "70",
          "grade": "45",
          "highlight": {
            "startOffset": 2954,
            "endOffset": 97,
            "quotedText": "ext few decades.\n2.  AI-enhanced governance could allow organizations to reach superhuman standards, like having an expected \"99.99\" reliability rate of not being corrupt or not telling lies.\n3.  While there are clear risks to AI-enhancement at the top levels of organizations, it’s likely that most of these can be managed, assuming that the implementers are reasonable. \n4.  AI-enhanced governance could synchronize well with AI company regulation. These companies would be well-placed to develop innovations and could hypothetically be incentivized to do much of the work. AI-enhanced governance might be necessary to ensure that these organizations are aligned with public interests.\n5.  More thorough investigation here could be promising for the effective altruism community.\n\nWithin effective altruism now, there’s a lot of work on governance and AI, but not much on using AI for governance. AI Governance typically focuses on using conventional strategies to oversee AI organizations, while AI Alignment research focuses on aligning AI systems. However, leveraging AI to improve human governance is an underexplored area that could complement these cause areas. You can think of it as “Organizational Alignment”, as a counterpoint to “AI Alignment.”\n\nThis article was written after some rough ideation I’ve done about this area. This isn’t at all a literature review or a research agenda. That said, for those interested in this topic, here are a few posts you might find interesting.\n\n*   [Project ideas: Governance during explosive technological growth](https://lukasfinnveden.substack.com/p/project-ideas-governance-during-explosive)\n*   [The Project AI Series, by OpenMined](The Private AI Series)\n*   [Safety Cases: How to Justify the Safety of Advanced AI Systems](https://arxiv.org/abs/2403.10462)\n*   [Affirmative Safety: An Approach to Risk Management for Advanced AI](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4806274)\n\n## What is “AI-Assisted” Governance?\n\nAI-Assisted Governance refers to improvements in governance that leverage artificial intelligence (AI), particularly focusing on rapidly advancing areas like Large Language Models (LLMs).\n\nExamples methods include:\n\n1.  Monitoring politicians and executives to identify and flag misaligned or malevolent behavior, ensuring accountability and integrity.\n2.  Enhancing epistemics and decision-making processes at the top levels of organizations, leading to more informed and rational strategies.\n3.  Facilitating more effective negotiations and trades between organizations, fostering better cooperation and coordination.\n4.  Assisting in writing and overseeing highly secure systems, such as implementing differential privacy and formally verified, bug-free decision-automation software, for use at managerial levels.\n\n## Arguments for Governance Improvements, Generally\n\n"
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "False Dichotomy",
          "description": "The author presents a somewhat false dichotomy between AI-enhanced governance and the status quo, suggesting that without AI assistance, governance will remain fundamentally flawed. This overlooks the spectrum of possible non-AI governance improvements and hybrid approaches. The framing artificially narrows the solution space to make AI governance seem like the only viable path forward.",
          "importance": "80",
          "grade": "35",
          "highlight": {
            "startOffset": 5293,
            "endOffset": 6261,
            "quotedText": "Lack of Promising Alternatives**\n\nThere aren't any AI-absent improvements that I could see making very large expected changes in governments or corporations in the next 20 years. \n\n*   There seems to be surprisingly little interest in improving organizational boards. I don't know of any nonprofits focused on this, for example. Recent FTX and OpenAI board fiascos have demonstrated severe problems with boards, and I don't see these going away soon.\n*   Despite decades of research into organizational psychology, human decision-making, statistics, and so on, government leaders continue to be severely lacking. \n*   Better voting systems for governments seems nice, but very limited. I don't see these making substantial global differences in the next 20 years.\n*   Current forecasting infrastructure is still a long way off from helping at executive levels. I think that next-generation AI-heavy systems could change that, but don't expect much from non-AI systems."
          },
          "isValid": true
        },
        {
          "title": "Status Quo Bias Critique",
          "description": "The author correctly identifies a form of status quo bias in current governance approaches, noting the lack of promising alternatives to improve organizational governance. However, this observation itself may reflect availability bias, as the author may be overlooking non-AI governance innovations simply because they're less salient in current discourse or the author's experience.",
          "importance": "85",
          "grade": "55",
          "highlight": {
            "startOffset": 5327,
            "endOffset": 6261,
            "quotedText": "There aren't any AI-absent improvements that I could see making very large expected changes in governments or corporations in the next 20 years. \n\n*   There seems to be surprisingly little interest in improving organizational boards. I don't know of any nonprofits focused on this, for example. Recent FTX and OpenAI board fiascos have demonstrated severe problems with boards, and I don't see these going away soon.\n*   Despite decades of research into organizational psychology, human decision-making, statistics, and so on, government leaders continue to be severely lacking. \n*   Better voting systems for governments seems nice, but very limited. I don't see these making substantial global differences in the next 20 years.\n*   Current forecasting infrastructure is still a long way off from helping at executive levels. I think that next-generation AI-heavy systems could change that, but don't expect much from non-AI systems."
          },
          "isValid": true
        },
        {
          "title": "Automation Bias",
          "description": "The text exhibits automation bias by suggesting AI systems would be inherently more reliable for oversight than humans. This reflects the tendency to favor automated systems over human judgment without sufficient evidence. The author assumes AI would be less susceptible to certain biases or corruption, which isn't necessarily proven and overlooks the fact that AI systems inherit biases from their training data and designers.",
          "importance": "85",
          "grade": "35",
          "highlight": {
            "startOffset": 6725,
            "endOffset": 7207,
            "quotedText": "Weak AI systems, in particular, could be both much easier to oversee and more effective at providing oversight than human-based approaches. For example, consider the challenge of ensuring that a team of 20 humans maintains 99.99% integrity - never accepting bribes or intentionally deceiving. Achieving this level of reliability with purely human oversight seems impractical. However, well-designed AI systems could potentially provide the necessary level of monitoring and control."
          },
          "isValid": true
        },
        {
          "title": "Technological Solutionism",
          "description": "The document exhibits technological solutionism by positioning AI as the solution to complex governance problems that have persisted throughout human history. This reflects a bias toward technological fixes for fundamentally social, political, and human problems. While technology can certainly help, the author overemphasizes technical solutions to what are often deeply human challenges involving values, power dynamics, and social structures.",
          "importance": "85",
          "grade": "35",
          "highlight": {
            "startOffset": 9389,
            "endOffset": 9863,
            "quotedText": "Traditional human oversight systems struggle to ensure accountability at the highest levels, as the question of \"who oversees the overseers\" often remains unresolved. This can lead to distrust and corruption.\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/nyiXej8EtoFwigrsw/dgvsazjn06tqynvhaeph)\n\nIntegrating AI systems can mitigate this issue by designing highest-level oversight systems that maintain privacy while revealing key problems."
          },
          "isValid": true
        },
        {
          "title": "Planning Fallacy",
          "description": "The document exhibits the planning fallacy by underestimating the complexity and challenges of implementing AI governance systems. The author presents a relatively straightforward bootstrapping strategy without adequately addressing the technical, political, and social obstacles that would need to be overcome. This optimistic planning overlooks likely implementation difficulties and timeline extensions that would occur in practice.",
          "importance": "75",
          "grade": "40",
          "highlight": {
            "startOffset": 10925,
            "endOffset": 11786,
            "quotedText": "## Strategy: Bootstrapping Effective AI Governance\n\nAI-Assisted Governance could enable superhuman performance measures, such as \"_99.999% resistance to bribes or manipulation,_\" to become feasible. This could provide a roadmap for governments to set increasingly stringent performance requirements that AI companies must meet in order to be permitted to develop more advanced and potentially dangerous technologies.\n\nPrudent governments would start with highly conservative rules that impose rigorous demands and grant limited permissions. If AI developers demonstrate their ability to meet the initial performance benchmarks, the rules for higher levels will be reassessed.\n\nWith robust government regulation, this approach could create a powerful positive feedback loop, where at each stage, the system is expected to become safer, even as capabilities grow."
          },
          "isValid": true
        },
        {
          "title": "Confirmation Bias in Structure",
          "description": "The document's structure reveals confirmation bias, as it presents a strong case for AI governance before addressing potential objections. Each objection is followed by a rebuttal that reinforces the author's position. This approach suggests the author is primarily seeking to confirm their pre-existing view rather than conducting a balanced analysis. A more objective approach would integrate challenges throughout the analysis rather than segregating them as objections to be overcome.",
          "importance": "80",
          "grade": "45",
          "highlight": {
            "startOffset": 13154,
            "endOffset": 15820,
            "quotedText": "## Objection 1: Would AI-Assisted Governance Increase AI Risk?\n\nSome may object that integrating AI into governance structures could actually make dangerous AI outcomes more likely. After all, if we are already concerned about advanced AI systems seizing control, wouldn't putting them in positions of power be enormously risky?\n\nThere are certainly some very unwise ways one could combine AI and governance. Recklessly deploying an untested AGI system to replace human leaders, for instance. However, I believe that with sufficient care and incremental development, we can find balanced approaches that capture the benefits of AI-assisted governance while mitigating the risks.\n\n\"AI\" is really an extremely broad category, encompassing a vast range of potential systems with radically different capabilities and risk profiles. Just as we use \"technology\" to counter misuse of \"technology\", or \"politicians\" to oversee other \"politicians\", we can leverage safer and more constrained AI systems to help govern the development and deployment of more advanced AI. A large spectrum exists between \"_AIs that are useful_\" and \"_AIs that pose existential risk_\". Many systems could offer significant benefits to governance without being capable of dangerous self-improvement or independent power-seeking.\n\nFor example, we could use heavily restricted AI systems to monitor communications and flag potential misconduct, without giving them any executive powers. We could have more advanced AI systems oversee the deployment of more mature and stable ones. Various oversight regimes and \"AI watchers\" could be implemented to maintain accountability. The key is to start with highly limited and transparent systems, and gradually scale up oversight and functionality as we gain confidence.\n\nIt's also worth noting that by the time AIs are truly dangerous, they may gain power regardless of where they are initially deployed. The most concerning AGI scenarios might not be worsened by responsible AI governance efforts in the interim. On the contrary, having more robust governance structures in place, bolstered by our most dependable AI tools, may actually help us navigate the emergence of advanced AI more safely.\n\nIn particular, well-designed AI oversight could make institutions more resistant to subversion and misuse by malicious agents, both human and artificial. For instance, requiring both human and AI authorization for sensitive actions could make unilateral corruption or deception much harder. Ultimately, while caution is certainly warranted, I suspect the protective benefits of AI-assisted governance are greater than the risks, if implemented thoughtfully."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Motivated Reasoning",
          "description": "The author engages in motivated reasoning when addressing objections, acknowledging concerns but quickly pivoting to reasons why these concerns can be managed or are outweighed by benefits. This pattern suggests the author is reasoning toward a preferred conclusion rather than objectively evaluating evidence. The consistent pattern of acknowledging risks but then minimizing them reveals a bias toward the author's preferred outcome.",
          "importance": "85",
          "grade": "40",
          "highlight": {
            "startOffset": 16329,
            "endOffset": 16790,
            "quotedText": "This is a serious challenge that would need to be carefully managed. Highly advanced AI pursuing incomprehensible strategies could certainly introduce dangerous volatility and uncertainty. We would need to invest heavily in interpretability and oversight capabilities to maintain a sufficient grasp on what's happening.\n\nHowever, I suspect that thoughtful AI-assisted governance is more likely to ultimately reduce destructive unpredictability than increase it."
          },
          "isValid": true
        },
        {
          "title": "Authority Bias Potential",
          "description": "The document could encourage authority bias by suggesting that AI systems could serve as reliable authorities in governance contexts. This risks transferring undue deference from human authorities to AI systems. While the author acknowledges the risk of overconfidence in AI, they don't fully address how authority bias might manifest when AI systems are integrated into governance structures.",
          "importance": "75",
          "grade": "60",
          "highlight": {
            "startOffset": 17944,
            "endOffset": 18388,
            "quotedText": "A final concern to consider is that AI-assisted governance tools might not work nearly as well as hoped, but could still engender overconfidence and complacency. If people perceive AI involvement as a panacea that makes governance trustworthy and reliable by default, they may grow overly deferential and reduce their vigilance. Flawed AI tools and overreliance on them could then cause more harm than if we had maintained a healthy skepticism."
          },
          "isValid": true
        }
      ],
      "grade": 45
    },
    {
      "agentId": "clarity-coach",
      "costInCents": 5,
      "createdAt": "2025-04-17T02:26:27.266Z",
      "runDetails": "{\"model\":\"anthropic/claude-3.7-sonnet\",\"promptTokens\":5773,\"completionTokens\":2082,\"totalTokens\":7855,\"temperature\":0.1,\"runtimeMs\":41768}",
      "thinking": "I'll analyze this document from the perspective of clarity and readability, focusing on how effectively the author communicates their ideas about AI-enhanced governance.\n\n## Readability Assessment\nThe document uses a mix of sentence lengths, which generally helps readability. However, there are some lengthy, complex sentences that could benefit from restructuring. The author employs a fair amount of specialized terminology (\"AI alignment,\" \"epistemic-enhancing,\" \"differential privacy\") without always providing sufficient explanation for readers unfamiliar with these concepts.\n\nThe reading level appears to be relatively high, likely requiring college-level education to fully comprehend. This may be appropriate given the target audience (LessWrong readers, presumably familiar with rationalist and effective altruist concepts), but could limit accessibility to broader audiences.\n\nThe document uses active voice predominantly, which enhances clarity. Paragraph structure is generally good, with information chunked into digestible sections.\n\n## Structural Coherence Analysis\nThe document has a clear organizational structure with headings that guide the reader through the argument. The author begins with their main claims, provides context about AI-assisted governance, presents arguments for governance improvements generally and AI-specific improvements, outlines a strategy, addresses potential objections, and concludes with recommendations.\n\nTransitions between sections are generally smooth, though some connections between ideas could be strengthened. The information hierarchy is logical, moving from general principles to specific applications and considerations.\n\nThe use of visual aids (diagrams) helps illustrate complex concepts, particularly the recursive enforcement problem and the bootstrapping cycle.\n\n## Precision and Concision Evaluation\nThere are instances of redundancy and verbosity that could be trimmed for greater impact. Some concepts are explained multiple times in slightly different ways. The author occasionally uses hedging language (\"I believe,\" \"I suspect,\" \"seems\") which, while appropriate for speculative topics, sometimes weakens the clarity of their assertions.\n\nThe precision of terminology is generally good, though some technical concepts could benefit from more explicit definitions or examples to ensure shared understanding.\n\n## Audience Alignment Assessment\nThe document assumes significant prior knowledge about AI concepts, effective altruism, and rationalist thinking. While this may be appropriate for the LessWrong platform, it creates barriers for readers without this background.\n\nThe author does provide some contextual scaffolding through links to related topics and articles, which helps readers find additional information. However, more explicit definitions of key terms would improve accessibility.\n\nThe examples provided are relevant but sometimes abstract. More concrete illustrations of how AI-assisted governance might work in practice would strengthen understanding.\n\nThe formatting is appropriate for the medium, with effective use of headings, bullet points, and images to break up text and highlight key points.\n\nOverall, while the document communicates its ideas effectively to its likely intended audience, there are opportunities to enhance clarity, reduce complexity in certain sections, and provide more concrete examples to illustrate abstract concepts.",
      "summary": "This document presents a thoughtful case for AI-enhanced governance with generally good structural organization and logical flow. The author effectively uses headings, bullet points, and visual aids to guide readers through complex concepts. However, the clarity is hampered by several factors: high technical jargon density without sufficient explanation, abstract concepts that would benefit from more concrete examples, and occasional hedging language that weakens key assertions. The document assumes significant background knowledge about AI concepts and effective altruism, creating potential barriers for readers without this context. While the overall structure is strong and the argument progression logical, the conclusion lacks specific actionable recommendations. The document scores well on structural coherence and information chunking but has room for improvement in accessibility, precision, and providing concrete illustrations of abstract concepts. For its likely intended audience on LessWrong, it communicates effectively, but broader accessibility would require more contextual scaffolding and explicit definitions of specialized terminology.",
      "comments": [
        {
          "title": "Effective Use of Structure",
          "description": "The document employs a clear organizational structure with logical progression from claims to context, arguments, strategy, objections, and recommendations. This hierarchical organization helps readers follow the complex argument about AI-enhanced governance.",
          "importance": "85",
          "grade": "90",
          "highlight": {
            "startOffset": 0,
            "endOffset": 878,
            "quotedText": "I believe that:\n\n1.  AI-enhanced organization governance could be a potentially huge win in the next few decades.\n2.  AI-enhanced governance could allow organizations to reach superhuman standards, like having an expected \"99.99\" reliability rate of not being corrupt or not telling lies.\n3.  While there are clear risks to AI-enhancement at the top levels of organizations, it’s likely that most of these can be managed, assuming that the implementers are reasonable. \n4.  AI-enhanced governance could synchronize well with AI company regulation. These companies would be well-placed to develop innovations and could hypothetically be incentivized to do much of the work. AI-enhanced governance might be necessary to ensure that these organizations are aligned with public interests.\n5.  More thorough investigation here could be promising for the effective altruism community."
          },
          "isValid": true
        },
        {
          "title": "Technical Jargon Density",
          "description": "This section contains specialized terminology without sufficient explanation for readers unfamiliar with these concepts. Terms like \"AI Alignment,\" \"Organizational Alignment,\" and references to the rationalist and effective altruist communities assume significant background knowledge, potentially limiting accessibility.",
          "importance": "80",
          "grade": "45",
          "highlight": {
            "startOffset": 871,
            "endOffset": 1346,
            "quotedText": "munity.\n\nWithin effective altruism now, there’s a lot of work on governance and AI, but not much on using AI for governance. AI Governance typically focuses on using conventional strategies to oversee AI organizations, while AI Alignment research focuses on aligning AI systems. However, leveraging AI to improve human governance is an underexplored area that could complement these cause areas. You can think of it as “Organizational Alignment”, as a counterpoint to “AI Ali"
          },
          "isValid": true
        },
        {
          "title": "Effective Information Chunking",
          "description": "The author effectively breaks down the concept of AI-Assisted Governance into four clear, numbered examples. This chunking of information enhances readability by presenting complex ideas in digestible segments, allowing readers to process each component before moving to the next.",
          "importance": "75",
          "grade": "85",
          "highlight": {
            "startOffset": 2272,
            "endOffset": 2899,
            "quotedText": "Examples methods include:\n\n1.  Monitoring politicians and executives to identify and flag misaligned or malevolent behavior, ensuring accountability and integrity.\n2.  Enhancing epistemics and decision-making processes at the top levels of organizations, leading to more informed and rational strategies.\n3.  Facilitating more effective negotiations and trades between organizations, fostering better cooperation and coordination.\n4.  Assisting in writing and overseeing highly secure systems, such as implementing differential privacy and formally verified, bug-free decision-automation software, for use at managerial levels."
          },
          "isValid": true
        },
        {
          "title": "Abstract Concepts Without Concrete Examples",
          "description": "This passage introduces the important concept of \"superhuman performance measures\" but remains abstract. The inclusion of a specific example (99.999% resistance to bribes) helps somewhat, but the section would benefit from more concrete illustrations of how such measures would be implemented and verified in practice.",
          "importance": "70",
          "grade": "60",
          "highlight": {
            "startOffset": 10977,
            "endOffset": 11123,
            "quotedText": "AI-Assisted Governance could enable superhuman performance measures, such as \"_99.999% resistance to bribes or manipulation,_\" to become feasible."
          },
          "isValid": true
        },
        {
          "title": "Visual Aid Enhancing Comprehension",
          "description": "The inclusion of a diagram to illustrate the bootstrapping cycle significantly enhances clarity. This visual representation helps readers understand the complex, cyclical relationship between AI development, governance tools, regulation, and standards in a way that would be difficult to convey through text alone.",
          "importance": "75",
          "grade": "90",
          "highlight": {
            "startOffset": 11788,
            "endOffset": 12621,
            "quotedText": "For instance:\n\n1.  AI companies enhance AI capabilities up to the limits they are permitted.\n2.  To meet the required standards, AI companies would be incentivized to incorporate AI capabilities into innovative governance tools. Their existing expertise and innovation capabilities in AI put them in a strong position to do so.\n3.  Other parties adopt some of these new governance tools. Importantly, some are implemented by government regulators, and many are implemented by AI organization evaluators and auditors.\n4.  Regulators review recent changes in AI development and developer governance, and update regulatory standards accordingly. If it appears very safe to do so, they may gradually relax certain measures. These regulators also leverage recent AI advancements to assist in setting standards. \n5.  The cycle begins anew."
          },
          "isValid": true
        },
        {
          "title": "Balanced Presentation of Objections",
          "description": "The author effectively addresses potential objections to their proposal, demonstrating intellectual honesty and helping readers evaluate the argument more thoroughly. The clear structure of presenting each objection followed by a thoughtful response enhances the document's coherence and credibility.",
          "importance": "85",
          "grade": "85",
          "highlight": {
            "startOffset": 13154,
            "endOffset": 15820,
            "quotedText": "## Objection 1: Would AI-Assisted Governance Increase AI Risk?\n\nSome may object that integrating AI into governance structures could actually make dangerous AI outcomes more likely. After all, if we are already concerned about advanced AI systems seizing control, wouldn't putting them in positions of power be enormously risky?\n\nThere are certainly some very unwise ways one could combine AI and governance. Recklessly deploying an untested AGI system to replace human leaders, for instance. However, I believe that with sufficient care and incremental development, we can find balanced approaches that capture the benefits of AI-assisted governance while mitigating the risks.\n\n\"AI\" is really an extremely broad category, encompassing a vast range of potential systems with radically different capabilities and risk profiles. Just as we use \"technology\" to counter misuse of \"technology\", or \"politicians\" to oversee other \"politicians\", we can leverage safer and more constrained AI systems to help govern the development and deployment of more advanced AI. A large spectrum exists between \"_AIs that are useful_\" and \"_AIs that pose existential risk_\". Many systems could offer significant benefits to governance without being capable of dangerous self-improvement or independent power-seeking.\n\nFor example, we could use heavily restricted AI systems to monitor communications and flag potential misconduct, without giving them any executive powers. We could have more advanced AI systems oversee the deployment of more mature and stable ones. Various oversight regimes and \"AI watchers\" could be implemented to maintain accountability. The key is to start with highly limited and transparent systems, and gradually scale up oversight and functionality as we gain confidence.\n\nIt's also worth noting that by the time AIs are truly dangerous, they may gain power regardless of where they are initially deployed. The most concerning AGI scenarios might not be worsened by responsible AI governance efforts in the interim. On the contrary, having more robust governance structures in place, bolstered by our most dependable AI tools, may actually help us navigate the emergence of advanced AI more safely.\n\nIn particular, well-designed AI oversight could make institutions more resistant to subversion and misuse by malicious agents, both human and artificial. For instance, requiring both human and AI authorization for sensitive actions could make unilateral corruption or deception much harder. Ultimately, while caution is certainly warranted, I suspect the protective benefits of AI-assisted governance are greater than the risks, if implemented thoughtfully."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Hedging Language Affecting Clarity",
          "description": "The frequent use of hedging language (\"I believe,\" \"I suspect,\" \"appears to be\") appropriately reflects uncertainty in this speculative domain but sometimes weakens the clarity and impact of assertions. More definitive statements where evidence permits would strengthen the argument in key places.",
          "importance": "65",
          "grade": "60",
          "highlight": {
            "startOffset": 16650,
            "endOffset": 17029,
            "quotedText": "However, I suspect that thoughtful AI-assisted governance is more likely to ultimately reduce destructive unpredictability than increase it. Currently, many high-stakes decisions are made by individual human leaders with unchecked biases and failure modes. The rationality and consistency of well-designed AI systems could help stabilize decision-making and make it more legible."
          },
          "isValid": true
        },
        {
          "title": "Conclusion Lacking Specific Actionable Recommendations",
          "description": "The conclusion section identifies the need for action but remains vague about specific next steps. While acknowledging the challenges, it would benefit from more concrete recommendations for effective altruists interested in pursuing this area, such as specific research questions, potential pilot projects, or collaboration opportunities.",
          "importance": "75",
          "grade": "55",
          "highlight": {
            "startOffset": 19165,
            "endOffset": 20655,
            "quotedText": "## What Should Effective Altruists Do?\n\nIf governments mandated exceptional levels of governance, AI organizations would be driven to undertake the necessary work to achieve those standards. Therefore, effective altruists would ideally concentrate on ensuring the implementation of appropriate regulations, if that specific (perhaps very unlikely) approach seems viable.\n\nDetermining the specifics of such regulations would be challenging, as would be the process of getting them approved. Setting benchmarks for organizational governance is not a straightforward task and warrants further investigation. It's important to note that these standards may be initially unattainable using current technology, provided there is a path to eventually meeting them with future AI advancements.\n\nHowever, there are numerous other entities for which we desire excellent governance, but they may lack the capabilities or motivation to conduct the required research. In such cases, some level of philanthropic support in this area appears to be warranted.\n\nI'll also note: the main writing I've seen in this space comes from the AI policy sector. I'm thankful for this, but I'd like to see more ideation from the tech sector. I'd be eager to get better futurist ideas and demos of stories where creative technologies make organizational governance amazing in the next 50 years. Similar to my desire to see more great futuristic epistemic ideas, I'd love to see more futuristic governance ideas as well."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        }
      ],
      "grade": 72
    }
  ]
}