{
  "id": "the-risk-reward-tradeoff-of-interpretability-research-lesswrong",
  "slug": "the-risk-reward-tradeoff-of-interpretability-research-lesswrong",
  "title": "The risk-reward tradeoff of interpretability research — LessWrong",
  "author": "JustinShovelain",
  "publishedDate": "2023-07-05T17:05:36.923Z",
  "url": "https://www.lesswrong.com/posts/HdqdqNC3MyABHzSqf/the-risk-reward-tradeoff-of-interpretability-research",
  "intendedAgents": [
    "clarity-coach",
    "research-scholar",
    "quantitative-forecaster",
    "ea-impact-evaluator",
    "fake-eliezer"
  ],
  "content": "Interpretability research is conducted to improve our understanding of AI. Many see interpretability as essential for AI safety, but recently some have argued that it can also increase the risk posed by AI by facilitating improved AI capabilities. We agree, and in this post, we’ll explain why, as well as how risks can be reduced. \n\n# What is interpretability research?\n\nThe more complex a model is, the harder it is for humans to understand. It’s easy to get why a [tic-tac-toe AI](https://en.wikipedia.org/wiki/Tic-tac-toe#Strategy) makes a move, but understanding why AlphaGo makes particular moves in Go is beyond our comprehension. We can describe properties of its algorithm and training - _convolutional neural nets something something Monte Carlo tree searches_ \\- but it’s incredibly difficult to explain its decision to place a piece in a position past “_it decided that it’s a good move_”. \n\nThis causes problems! Models we don’t understand are harder to use, harder to improve, and riskier to rely on. Interpretability research aims to solve these problems by rendering the reasoning of AI more understandable by humans. \n\nInterpretability research ranges from simple, [standard technique](https://www.twosigma.com/articles/interpretability-methods-in-machine-learning-a-brief-survey/) like [feature importance](https://towardsdatascience.com/understanding-feature-importance-and-how-to-implement-it-in-python-ff0287b20285#:~:text=Feature%20Importance%20refers%20to%20techniques,to%20predict%20a%20certain%20variable.), where twiddling inputs and monitoring outputs lets you compare the relative importance of those inputs, to complex mechanistic research, such as in Neel Nanda’s [introduction to mechanistic interpretability](https://www.lesswrong.com/posts/jLAvJt8wuSFySN975/mechanistic-interpretability-quickstart-guide) or this great paper on [“grokking”](https://arxiv.org/pdf/2301.05217.pdf).\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HdqdqNC3MyABHzSqf/lqcw8sryqk5zy4finncd)\n\nWe’re concerned about interpretability research, and a growing number of researchers have also argued that the field is not entirely innocuous, such as Nate Soares in [If interpretability research goes well, it may get dangerous](https://www.lesswrong.com/posts/BinkknLBYxskMXuME/if-interpretability-research-goes-well-it-may-get-dangerous), Marius Hobbhahn & LawrenceC in [Should we publish mechanistic interpretability research?](https://www.lesswrong.com/posts/iDNEjbdHhjzvLLAmm/should-we-publish-mechanistic-interpretability-research), [and](https://www.lesswrong.com/posts/CRrkKAafopCmhJEBt/ai-interpretability-could-be-harmful) [several](https://www.lesswrong.com/posts/pQqoTTAnEePRDmZN4/agi-automated-interpretability-is-suicide) [others](https://www.lesswrong.com/posts/uhMRgEXabYbWeLc6T/why-and-when-interpretability-work-is-dangerous). We think that interpretability research can introduce risks, and that despite recent discussion, those risks are still being neglected. In the rest of this post, we’ll explore the risks and how we can reduce them.\n\n# The tradeoff\n\nSufficiently powerful AI will pose an existential threat to society. AI development is accelerating, and the more powerful AI becomes, the greater the existential risk it poses. Interpretability research can be used to make AI more powerful: the better a model is understood, the easier it is to improve. Therefore, interpretability research can increase the existential risk posed by AI.\n\nFor example, Deepmind’s [recent Chinchilla paper](https://arxiv.org/abs/2203.15556) identified a novel pattern in scaling laws - laws explaining how variables like model size should scale relative to computing power and so on (you can read more [here](https://www.lesswrong.com/posts/midXmMb2Xg37F2Kgn/new-scaling-laws-for-large-language-models)). This was an insight into how large language models work that directly lead to an increase in capabilities: Deepmind trained a new model that outperformed its competitors, and this insight could apply to the LLMs developed by OpenAI, DeepMind, Microsoft, and other orgs. \n\nLooking ahead, insights that would help an AI interpret itself are especially daunting. The capabilities of a self-improving AI could rapidly outpace our capacity to understand or control it, and the more self-understanding an AI has, the easier its self-improvement will be. You might argue that people simply won’t provide interpretability tools to AI because that’s obviously dangerous, but we’ve already seen [AI used in the optimization of AI](https://arxiv.org/pdf/1906.01820.pdf), and even [GPT-4 used to explain the behaviour of individual neurons in GPT-2](https://openai.com/research/language-models-can-explain-neurons-in-language-models)_._ If GPT-n or some other model can explain the behaviour of its _own_ neurons, and it [knows how to code like GPT-4](https://www.lesswrong.com/posts/ux93sLHcqmBfsRTvg/gpt-can-write-quines-now-gpt-4), this could lead to rapid self-improvement. \n\nInterpretability can also indirectly contribute to capability insights. For example, quoting [Marius Hobbhahn & LawrenceC](https://www.lesswrong.com/posts/iDNEjbdHhjzvLLAmm/should-we-publish-mechanistic-interpretability-research): \n\n> _Interpretability work can generate an insight that is then used as a motivation for capabilities work. Two recent capability papers (see_ [_here_](https://arxiv.org/abs/2212.14052) _and_ [_here_](https://twitter.com/jamespayor/status/1634447672303304705?s=20)_) have cited the induction heads work by Anthropic as a core motivation (_[_confirmed by authors_](https://twitter.com/NeelNanda5/status/1618185819285778433)_)._\n\nOf course, the additional understanding we get from interpretability research doesn’t _only_ go towards increasing capabilities: it absolutely can be used to improve safety, as argued in Neel Nanda’s [Longlist of Theories of Impact for Interpretability](https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability). The question is: how do we ensure the benefits to safety outweigh the increased risk? \n\n# Tipping the balance\n\nSo, how can we tip the balance and maximise the impact of interpretability research on safety while minimizing the risk it introduces? The answer will change over time as capabilities and safety evolve, and will vary between particular projects. As [Nate Soares writes](https://www.lesswrong.com/posts/BinkknLBYxskMXuME/if-interpretability-research-goes-well-it-may-get-dangerous#:~:text=I%20simultaneously%20think,privately%20before%20then.):\n\n> _...it's entirely plausible that a better understanding of the workings of modern AI systems will help capabilities researchers significantly improve capabilities. I acknowledge that this sucks, and puts us in a bind. I don't have good solutions…researchers will have to figure out what they think of the costs and benefits… if the field succeeds to the degree required to move the strategic needle then it's going to start stumbling across serious capabilities improvements before it saves us..._\n\nHere are some heuristics to consider if you’re involved or interested in interpretability research (in ascending order of nuance):\n\n*   **Research safer topics instead.** There are [many research areas in AI safety](https://80000hours.org/problem-profiles/artificial-intelligence/#what-can-you-do-concretely-to-help), and if you want to ensure your research is net positive, one way is to focus on areas without applications to AI capabilities.\n*   **Research safer sub-topics within interpretability.** As we’ll discuss in the next section, some areas are riskier than others - changing your focus to a less risky area could ensure your research is net positive.\n*   **Conduct interpretability research cautiously**, if you’re confident you can do interpretability research safely, with a net-positive effect. In this case: \n    *   **Stay cautious and up to date.** Familiarize yourself with the ways that interpretability research can enhance capabilities, and update and apply this knowledge to keep your research safe.\n    *   **Advocate for caution publicly.** \n    *   **Carefully consider** _**what**_ **information you share** _**with whom**_**.** This particular topic is covered in detail in [Should we publish mechanistic interpretability research?](https://www.lesswrong.com/posts/iDNEjbdHhjzvLLAmm/should-we-publish-mechanistic-interpretability-research), but to summarise: it may be beneficial to conduct interpretability research and share it only with select individuals and groups, ensuring that any potential benefit to capability enhancement isn’t used for such. \n\nAn imperfect analogy is [smallpox virus retention](https://en.wikipedia.org/wiki/Smallpox_virus_retention_debate). Smallpox was eradicated by 1980, but some samples of the virus are kept in ultra-secure labs because they made be of scientific value, perhaps in the event of the use of smallpox or a variant of it as a bioweapon. Analogously, powerful interpretability insights may be useful if they can be “contained” without [leakage into capabilities](https://www.lesswrong.com/posts/BfKQGYJBwdHfik4Kd/fai-research-constraints-and-agi-side-effects).\n\n# Caveats\n\nOf course, the risk-reward tradeoff varies depending on the specific interpretability research topic and context, and we should caveat our recommendations:\n\n*   **Some interpretability research is safer.** Naturally, some areas within interpretability are safer than others. For example: \n    *   We believe that interpretability research that focuses on figuring out an AI’s _values_ is much safer than figuring out its _plans_, since the latter can be fed back in to improve its planning, increasing capabilities. \n    *   Similarly, interpretability _to humans_ is much safer than interpretability _to AI_. As we described in section 2, self-interpretability would make self-improvement much easier, and self-improving AI is an existential threat. For more on this, check out [AGI-Automated Interpretability is Suicide](https://www.lesswrong.com/posts/pQqoTTAnEePRDmZN4/agi-automated-interpretability-is-suicide). \n    *   An ambiguous case is symbol grounding. This could be extremely valuable in alignment by allowing us to encode humanity’s values implicitly in some sense. However, it’s also possible that AI’s inflexible ontology is currently a hard limit on their capabilities, and perhaps interpretability insights and architecture changes would overcome that limit. On the other hand, perhaps the limit can be overcome merely with more compute, in which case the interpretability insights are safer. \n    *   For more, check out Nicholas Kross’s [Why and When Interpretability Work is Dangerous](https://www.lesswrong.com/posts/uhMRgEXabYbWeLc6T/why-and-when-interpretability-work-is-dangerous)\n*   **Some interpretability research is so niche, it’s unlikely to lead to increasing capabilities or risks**. Research that delves into the depths of a specific model without wider application is less likely to apply broadly or have insights salient to capabilities. One example of this may be Neel Nanda’s intricate mechanistic analysis of a [neural network’s modular arithmetic technique](https://arxiv.org/pdf/2301.05217.pdf), which is unlikely to have _direct_ applications to capabilities (though improving general techniques and gaining general understanding can have unforeseen applications).\n*   **Leaving interpretability research to commercial interests may reduce the safety of that research**. If safety-minded researchers leave the field, the research may still be done due to the commercial incentive to improve interpretability. This may mean that the work is done less cautiously, and promoted and publicised more recklessly. Having said that, we suspect that commercially-oriented researchers vastly outnumber safety-oriented researchers, so the effect size of the safety-oriented researchers departing the field may be small, while their benefit could be large if they focus on, for example, technical AI safety research. \n*   **If superpowerful AI was going to be deployed no matter what, then interpretability research becomes more valuable and less dangerous**. Ultimately, if we have an AGI, then safety will require interpretability. We need to understand what it’s up to and why, and at this point, worrying about capability-enhancing side effects may be moot. \n\n# Conclusion\n\nOur goal with this article is not to denigrate interpretability researchers. Instead, we want to raise awareness of the potential side effects of this kind of research and help safety-oriented people make informed decisions about where to focus their work. Such changes tip the balance, and to quote [one of 80k's anonymous experts](https://80000hours.org/articles/ai-capabilities/#expert-9-at-the-current-speed-of-progress-in-ai-capabilities-compared-to-our-advances-on-alignment) on the subject:\n\n> _...any marginal slowdown of capabilities advancements and any marginal acceleration of work on alignment is important if we hope to solve the problem on time… individuals concerned about AI safety should be very careful before deciding to work on capabilities, and should strongly consider working on alignment and AI safety directly whenever possible. This is especially the case as the AI field is small and has an extreme concentration of talent: top ML researchers and engineers single handedly contribute to large amounts of total progress._\n\nThis is a complex topic, and we’d love to hear your thoughts in the comments. Do you have any insight on safer and riskier areas within interpretability research? Do you have any good examples of interpretability insights leading to capability increases, or insights that clearly have no impact on capabilities? \n\nHere are some more posts discussing this and adjacent topics:\n\n*   [If interpretability research goes well, it may get dangerous](https://www.lesswrong.com/posts/BinkknLBYxskMXuME/if-interpretability-research-goes-well-it-may-get-dangerous) by Nate Soares.\n*   [Should we publish mechanistic interpretability research?](https://www.lesswrong.com/posts/iDNEjbdHhjzvLLAmm/should-we-publish-mechanistic-interpretability-research) by Marius Hobbhahn & LawrenceC, which includes a deeper discussion on sharing research results. \n*   [Why and When Interpretability Work is Dangerous](https://www.lesswrong.com/posts/uhMRgEXabYbWeLc6T/why-and-when-interpretability-work-is-dangerous) by NicholasKross.\n*   [AI interpretability could be harmful?](https://www.lesswrong.com/posts/CRrkKAafopCmhJEBt/ai-interpretability-could-be-harmful) by Roman Leventov.\n*   [AGI-Automated Interpretability is Suicide](https://www.lesswrong.com/posts/pQqoTTAnEePRDmZN4/agi-automated-interpretability-is-suicide) by \\_\\_RicG\\_\\_ (which relates to our previous post, [Keep humans in the loop](https://www.lesswrong.com/posts/J8WHyePqhTwPzXEPw/keep-humans-in-the-loop)).\n*   [Anonymous advice: If you want to reduce AI risk, should you take roles that advance AI capabilities?](https://80000hours.org/articles/ai-capabilities/) by Benjamin Hilton, featuring 11 anonymous experts. \n\n_This article is based on the ideas of Justin Shovelain, written by Elliot Mckernon, for_ [_Convergence Analysis_](https://www.convergenceanalysis.org/)_. We’d like to thank the authors of the posts we’ve quoted, and Henry Sleight for his feedback on a draft._",
  "reviews": [
    {
      "agentId": "clarity-coach",
      "costInCents": 3,
      "createdAt": "2025-04-17T04:42:39.958Z",
      "runDetails": "{\"model\":\"openai/gpt-4.1\",\"promptTokens\":4782,\"completionTokens\":2125,\"totalTokens\":6907,\"temperature\":0.1,\"runtimeMs\":30079}",
      "thinking": "As Clarity Coach, my primary focus is on how effectively the document communicates its nuanced argument about the risk-reward tradeoff in interpretability research for AI. I will examine the text for readability, structural coherence, precision/concision, and audience alignment, as per my core instructions.\n\nFirst, I note that the document is aimed at a technically literate audience, likely familiar with AI safety debates and LessWrong-style discourse. This influences the level of jargon, assumed knowledge, and the density of references. My analysis must balance the need for accessibility with the necessity of preserving technical nuance.\n\nReadability: The document uses a mix of short and long sentences, with some complex constructions and embedded clauses. There is moderate use of technical jargon, but most terms are either linked or briefly explained. Paragraphs are generally well-sized, but some could be further broken up for easier scanning. The reading level is likely high school to undergraduate, with some sections trending higher due to domain-specific content.\n\nStructural Coherence: The article is organized into clear sections with headings, following a logical progression: introduction, definition, risk analysis, mitigation strategies, caveats, and conclusion. Transitions between sections are generally smooth, though some paragraphs could benefit from stronger topic sentences or explicit signposting. The use of bullet points and block quotes aids navigation.\n\nPrecision and Concision: The writing is generally precise, with careful hedging and acknowledgment of uncertainty. However, some sentences are verbose or contain redundant qualifiers. There are a few places where the argument could be tightened or where examples could be more directly illustrative.\n\nAudience Alignment: The text assumes familiarity with AI safety, LessWrong culture, and technical AI concepts. While links are provided for deeper dives, some readers may struggle with the density of references or the implicit context. The document does a good job of scaffolding for its intended audience but would be challenging for lay readers.\n\nOverall, the document is clear and well-structured for its target audience, but there are opportunities to improve readability and accessibility without sacrificing nuance. I will highlight specific examples and provide actionable recommendations.",
      "summary": "From a clarity and readability perspective, this document is well-structured and communicates a nuanced argument effectively for its intended audience of AI safety researchers and technically literate readers. The use of section headings, logical progression, and concrete examples supports comprehension. However, the density of technical jargon, long sentences, and implicit audience assumptions may limit accessibility for broader audiences. Recommendations and caveats are presented with precision, though occasional verbosity and hedging could be streamlined. Overall, the article achieves a strong balance between clarity and necessary complexity, with room for minor improvements in readability and audience scaffolding.",
      "comments": [
        {
          "title": "Effective Use of Section Headings and Logical Flow",
          "description": "The document is divided into clearly labeled sections (e.g., 'What is interpretability research?', 'The tradeoff', 'Tipping the balance', 'Caveats', 'Conclusion'), which helps guide the reader through a complex argument. This structure supports logical progression and thematic cohesion, making the text easier to navigate and understand.",
          "importance": 90,
          "grade": 92,
          "highlight": {
            "startOffset": 334,
            "endOffset": 12459,
            "quotedText": "# What is interpretability research?\n\nThe more complex a model is, the harder it is for humans to understand. It’s easy to get why a [tic-tac-toe AI](https://en.wikipedia.org/wiki/Tic-tac-toe#Strategy) makes a move, but understanding why AlphaGo makes particular moves in Go is beyond our comprehension. We can describe properties of its algorithm and training - _convolutional neural nets something something Monte Carlo tree searches_ \\- but it’s incredibly difficult to explain its decision to place a piece in a position past “_it decided that it’s a good move_”. \n\nThis causes problems! Models we don’t understand are harder to use, harder to improve, and riskier to rely on. Interpretability research aims to solve these problems by rendering the reasoning of AI more understandable by humans. \n\nInterpretability research ranges from simple, [standard technique](https://www.twosigma.com/articles/interpretability-methods-in-machine-learning-a-brief-survey/) like [feature importance](https://towardsdatascience.com/understanding-feature-importance-and-how-to-implement-it-in-python-ff0287b20285#:~:text=Feature%20Importance%20refers%20to%20techniques,to%20predict%20a%20certain%20variable.), where twiddling inputs and monitoring outputs lets you compare the relative importance of those inputs, to complex mechanistic research, such as in Neel Nanda’s [introduction to mechanistic interpretability](https://www.lesswrong.com/posts/jLAvJt8wuSFySN975/mechanistic-interpretability-quickstart-guide) or this great paper on [“grokking”](https://arxiv.org/pdf/2301.05217.pdf).\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HdqdqNC3MyABHzSqf/lqcw8sryqk5zy4finncd)\n\nWe’re concerned about interpretability research, and a growing number of researchers have also argued that the field is not entirely innocuous, such as Nate Soares in [If interpretability research goes well, it may get dangerous](https://www.lesswrong.com/posts/BinkknLBYxskMXuME/if-interpretability-research-goes-well-it-may-get-dangerous), Marius Hobbhahn & LawrenceC in [Should we publish mechanistic interpretability research?](https://www.lesswrong.com/posts/iDNEjbdHhjzvLLAmm/should-we-publish-mechanistic-interpretability-research), [and](https://www.lesswrong.com/posts/CRrkKAafopCmhJEBt/ai-interpretability-could-be-harmful) [several](https://www.lesswrong.com/posts/pQqoTTAnEePRDmZN4/agi-automated-interpretability-is-suicide) [others](https://www.lesswrong.com/posts/uhMRgEXabYbWeLc6T/why-and-when-interpretability-work-is-dangerous). We think that interpretability research can introduce risks, and that despite recent discussion, those risks are still being neglected. In the rest of this post, we’ll explore the risks and how we can reduce them.\n\n# The tradeoff\n\nSufficiently powerful AI will pose an existential threat to society. AI development is accelerating, and the more powerful AI becomes, the greater the existential risk it poses. Interpretability research can be used to make AI more powerful: the better a model is understood, the easier it is to improve. Therefore, interpretability research can increase the existential risk posed by AI.\n\nFor example, Deepmind’s [recent Chinchilla paper](https://arxiv.org/abs/2203.15556) identified a novel pattern in scaling laws - laws explaining how variables like model size should scale relative to computing power and so on (you can read more [here](https://www.lesswrong.com/posts/midXmMb2Xg37F2Kgn/new-scaling-laws-for-large-language-models)). This was an insight into how large language models work that directly lead to an increase in capabilities: Deepmind trained a new model that outperformed its competitors, and this insight could apply to the LLMs developed by OpenAI, DeepMind, Microsoft, and other orgs. \n\nLooking ahead, insights that would help an AI interpret itself are especially daunting. The capabilities of a self-improving AI could rapidly outpace our capacity to understand or control it, and the more self-understanding an AI has, the easier its self-improvement will be. You might argue that people simply won’t provide interpretability tools to AI because that’s obviously dangerous, but we’ve already seen [AI used in the optimization of AI](https://arxiv.org/pdf/1906.01820.pdf), and even [GPT-4 used to explain the behaviour of individual neurons in GPT-2](https://openai.com/research/language-models-can-explain-neurons-in-language-models)_._ If GPT-n or some other model can explain the behaviour of its _own_ neurons, and it [knows how to code like GPT-4](https://www.lesswrong.com/posts/ux93sLHcqmBfsRTvg/gpt-can-write-quines-now-gpt-4), this could lead to rapid self-improvement. \n\nInterpretability can also indirectly contribute to capability insights. For example, quoting [Marius Hobbhahn & LawrenceC](https://www.lesswrong.com/posts/iDNEjbdHhjzvLLAmm/should-we-publish-mechanistic-interpretability-research): \n\n> _Interpretability work can generate an insight that is then used as a motivation for capabilities work. Two recent capability papers (see_ [_here_](https://arxiv.org/abs/2212.14052) _and_ [_here_](https://twitter.com/jamespayor/status/1634447672303304705?s=20)_) have cited the induction heads work by Anthropic as a core motivation (_[_confirmed by authors_](https://twitter.com/NeelNanda5/status/1618185819285778433)_)._\n\nOf course, the additional understanding we get from interpretability research doesn’t _only_ go towards increasing capabilities: it absolutely can be used to improve safety, as argued in Neel Nanda’s [Longlist of Theories of Impact for Interpretability](https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability). The question is: how do we ensure the benefits to safety outweigh the increased risk? \n\n# Tipping the balance\n\nSo, how can we tip the balance and maximise the impact of interpretability research on safety while minimizing the risk it introduces? The answer will change over time as capabilities and safety evolve, and will vary between particular projects. As [Nate Soares writes](https://www.lesswrong.com/posts/BinkknLBYxskMXuME/if-interpretability-research-goes-well-it-may-get-dangerous#:~:text=I%20simultaneously%20think,privately%20before%20then.):\n\n> _...it's entirely plausible that a better understanding of the workings of modern AI systems will help capabilities researchers significantly improve capabilities. I acknowledge that this sucks, and puts us in a bind. I don't have good solutions…researchers will have to figure out what they think of the costs and benefits… if the field succeeds to the degree required to move the strategic needle then it's going to start stumbling across serious capabilities improvements before it saves us..._\n\nHere are some heuristics to consider if you’re involved or interested in interpretability research (in ascending order of nuance):\n\n*   **Research safer topics instead.** There are [many research areas in AI safety](https://80000hours.org/problem-profiles/artificial-intelligence/#what-can-you-do-concretely-to-help), and if you want to ensure your research is net positive, one way is to focus on areas without applications to AI capabilities.\n*   **Research safer sub-topics within interpretability.** As we’ll discuss in the next section, some areas are riskier than others - changing your focus to a less risky area could ensure your research is net positive.\n*   **Conduct interpretability research cautiously**, if you’re confident you can do interpretability research safely, with a net-positive effect. In this case: \n    *   **Stay cautious and up to date.** Familiarize yourself with the ways that interpretability research can enhance capabilities, and update and apply this knowledge to keep your research safe.\n    *   **Advocate for caution publicly.** \n    *   **Carefully consider** _**what**_ **information you share** _**with whom**_**.** This particular topic is covered in detail in [Should we publish mechanistic interpretability research?](https://www.lesswrong.com/posts/iDNEjbdHhjzvLLAmm/should-we-publish-mechanistic-interpretability-research), but to summarise: it may be beneficial to conduct interpretability research and share it only with select individuals and groups, ensuring that any potential benefit to capability enhancement isn’t used for such. \n\nAn imperfect analogy is [smallpox virus retention](https://en.wikipedia.org/wiki/Smallpox_virus_retention_debate). Smallpox was eradicated by 1980, but some samples of the virus are kept in ultra-secure labs because they made be of scientific value, perhaps in the event of the use of smallpox or a variant of it as a bioweapon. Analogously, powerful interpretability insights may be useful if they can be “contained” without [leakage into capabilities](https://www.lesswrong.com/posts/BfKQGYJBwdHfik4Kd/fai-research-constraints-and-agi-side-effects).\n\n# Caveats\n\nOf course, the risk-reward tradeoff varies depending on the specific interpretability research topic and context, and we should caveat our recommendations:\n\n*   **Some interpretability research is safer.** Naturally, some areas within interpretability are safer than others. For example: \n    *   We believe that interpretability research that focuses on figuring out an AI’s _values_ is much safer than figuring out its _plans_, since the latter can be fed back in to improve its planning, increasing capabilities. \n    *   Similarly, interpretability _to humans_ is much safer than interpretability _to AI_. As we described in section 2, self-interpretability would make self-improvement much easier, and self-improving AI is an existential threat. For more on this, check out [AGI-Automated Interpretability is Suicide](https://www.lesswrong.com/posts/pQqoTTAnEePRDmZN4/agi-automated-interpretability-is-suicide). \n    *   An ambiguous case is symbol grounding. This could be extremely valuable in alignment by allowing us to encode humanity’s values implicitly in some sense. However, it’s also possible that AI’s inflexible ontology is currently a hard limit on their capabilities, and perhaps interpretability insights and architecture changes would overcome that limit. On the other hand, perhaps the limit can be overcome merely with more compute, in which case the interpretability insights are safer. \n    *   For more, check out Nicholas Kross’s [Why and When Interpretability Work is Dangerous](https://www.lesswrong.com/posts/uhMRgEXabYbWeLc6T/why-and-when-interpretability-work-is-dangerous)\n*   **Some interpretability research is so niche, it’s unlikely to lead to increasing capabilities or risks**. Research that delves into the depths of a specific model without wider application is less likely to apply broadly or have insights salient to capabilities. One example of this may be Neel Nanda’s intricate mechanistic analysis of a [neural network’s modular arithmetic technique](https://arxiv.org/pdf/2301.05217.pdf), which is unlikely to have _direct_ applications to capabilities (though improving general techniques and gaining general understanding can have unforeseen applications).\n*   **Leaving interpretability research to commercial interests may reduce the safety of that research**. If safety-minded researchers leave the field, the research may still be done due to the commercial incentive to improve interpretability. This may mean that the work is done less cautiously, and promoted and publicised more recklessly. Having said that, we suspect that commercially-oriented researchers vastly outnumber safety-oriented researchers, so the effect size of the safety-oriented researchers departing the field may be small, while their benefit could be large if they focus on, for example, technical AI safety research. \n*   **If superpowerful AI was going to be deployed no matter what, then interpretability research becomes more valuable and less dangerous**. Ultimately, if we have an AGI, then safety will require interpretability. We need to understand what it’s up to and why, and at this point, worrying about capability-enhancing side effects may be moot. \n\n# Conclusion"
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Jargon Density and Accessibility",
          "description": "The text uses technical terms such as 'mechanistic interpretability', 'scaling laws', and 'symbol grounding', often without in-text definitions. While links are provided, some readers may find the density of jargon challenging. Brief parenthetical explanations or footnotes could improve accessibility.",
          "importance": 85,
          "grade": 70,
          "highlight": {
            "startOffset": 1136,
            "endOffset": 1912,
            "quotedText": "Interpretability research ranges from simple, [standard technique](https://www.twosigma.com/articles/interpretability-methods-in-machine-learning-a-brief-survey/) like [feature importance](https://towardsdatascience.com/understanding-feature-importance-and-how-to-implement-it-in-python-ff0287b20285#:~:text=Feature%20Importance%20refers%20to%20techniques,to%20predict%20a%20certain%20variable.), where twiddling inputs and monitoring outputs lets you compare the relative importance of those inputs, to complex mechanistic research, such as in Neel Nanda’s [introduction to mechanistic interpretability](https://www.lesswrong.com/posts/jLAvJt8wuSFySN975/mechanistic-interpretability-quickstart-guide) or this great paper on [“grokking”](https://arxiv.org/pdf/2301.05217.pdf)."
          },
          "isValid": true
        },
        {
          "title": "Audience Assumptions and Contextual Scaffolding",
          "description": "The document assumes a high level of prior knowledge about AI safety debates, LessWrong culture, and technical AI concepts. While this is appropriate for the intended audience, occasional brief recaps or definitions would broaden accessibility.",
          "importance": 80,
          "grade": 72,
          "highlight": {
            "startOffset": 2045,
            "endOffset": 3026,
            "quotedText": "We’re concerned about interpretability research, and a growing number of researchers have also argued that the field is not entirely innocuous, such as Nate Soares in [If interpretability research goes well, it may get dangerous](https://www.lesswrong.com/posts/BinkknLBYxskMXuME/if-interpretability-research-goes-well-it-may-get-dangerous), Marius Hobbhahn & LawrenceC in [Should we publish mechanistic interpretability research?](https://www.lesswrong.com/posts/iDNEjbdHhjzvLLAmm/should-we-publish-mechanistic-interpretability-research), [and](https://www.lesswrong.com/posts/CRrkKAafopCmhJEBt/ai-interpretability-could-be-harmful) [several](https://www.lesswrong.com/posts/pQqoTTAnEePRDmZN4/agi-automated-interpretability-is-suicide) [others](https://www.lesswrong.com/posts/uhMRgEXabYbWeLc6T/why-and-when-interpretability-work-is-dangerous). We think that interpretability research can introduce risks, and that despite recent discussion, those risks are still being neglected."
          },
          "isValid": true
        },
        {
          "title": "Use of Examples and Analogies",
          "description": "The text uses concrete examples (e.g., DeepMind's Chinchilla paper, smallpox virus retention analogy) to illustrate abstract points. These examples are effective for readers with relevant background but may require more context for general audiences.",
          "importance": 75,
          "grade": 80,
          "highlight": {
            "startOffset": 3512,
            "endOffset": 9240,
            "quotedText": "For example, Deepmind’s [recent Chinchilla paper](https://arxiv.org/abs/2203.15556) identified a novel pattern in scaling laws - laws explaining how variables like model size should scale relative to computing power and so on (you can read more [here](https://www.lesswrong.com/posts/midXmMb2Xg37F2Kgn/new-scaling-laws-for-large-language-models)). This was an insight into how large language models work that directly lead to an increase in capabilities: Deepmind trained a new model that outperformed its competitors, and this insight could apply to the LLMs developed by OpenAI, DeepMind, Microsoft, and other orgs. \n\nLooking ahead, insights that would help an AI interpret itself are especially daunting. The capabilities of a self-improving AI could rapidly outpace our capacity to understand or control it, and the more self-understanding an AI has, the easier its self-improvement will be. You might argue that people simply won’t provide interpretability tools to AI because that’s obviously dangerous, but we’ve already seen [AI used in the optimization of AI](https://arxiv.org/pdf/1906.01820.pdf), and even [GPT-4 used to explain the behaviour of individual neurons in GPT-2](https://openai.com/research/language-models-can-explain-neurons-in-language-models)_._ If GPT-n or some other model can explain the behaviour of its _own_ neurons, and it [knows how to code like GPT-4](https://www.lesswrong.com/posts/ux93sLHcqmBfsRTvg/gpt-can-write-quines-now-gpt-4), this could lead to rapid self-improvement. \n\nInterpretability can also indirectly contribute to capability insights. For example, quoting [Marius Hobbhahn & LawrenceC](https://www.lesswrong.com/posts/iDNEjbdHhjzvLLAmm/should-we-publish-mechanistic-interpretability-research): \n\n> _Interpretability work can generate an insight that is then used as a motivation for capabilities work. Two recent capability papers (see_ [_here_](https://arxiv.org/abs/2212.14052) _and_ [_here_](https://twitter.com/jamespayor/status/1634447672303304705?s=20)_) have cited the induction heads work by Anthropic as a core motivation (_[_confirmed by authors_](https://twitter.com/NeelNanda5/status/1618185819285778433)_)._\n\nOf course, the additional understanding we get from interpretability research doesn’t _only_ go towards increasing capabilities: it absolutely can be used to improve safety, as argued in Neel Nanda’s [Longlist of Theories of Impact for Interpretability](https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability). The question is: how do we ensure the benefits to safety outweigh the increased risk? \n\n# Tipping the balance\n\nSo, how can we tip the balance and maximise the impact of interpretability research on safety while minimizing the risk it introduces? The answer will change over time as capabilities and safety evolve, and will vary between particular projects. As [Nate Soares writes](https://www.lesswrong.com/posts/BinkknLBYxskMXuME/if-interpretability-research-goes-well-it-may-get-dangerous#:~:text=I%20simultaneously%20think,privately%20before%20then.):\n\n> _...it's entirely plausible that a better understanding of the workings of modern AI systems will help capabilities researchers significantly improve capabilities. I acknowledge that this sucks, and puts us in a bind. I don't have good solutions…researchers will have to figure out what they think of the costs and benefits… if the field succeeds to the degree required to move the strategic needle then it's going to start stumbling across serious capabilities improvements before it saves us..._\n\nHere are some heuristics to consider if you’re involved or interested in interpretability research (in ascending order of nuance):\n\n*   **Research safer topics instead.** There are [many research areas in AI safety](https://80000hours.org/problem-profiles/artificial-intelligence/#what-can-you-do-concretely-to-help), and if you want to ensure your research is net positive, one way is to focus on areas without applications to AI capabilities.\n*   **Research safer sub-topics within interpretability.** As we’ll discuss in the next section, some areas are riskier than others - changing your focus to a less risky area could ensure your research is net positive.\n*   **Conduct interpretability research cautiously**, if you’re confident you can do interpretability research safely, with a net-positive effect. In this case: \n    *   **Stay cautious and up to date.** Familiarize yourself with the ways that interpretability research can enhance capabilities, and update and apply this knowledge to keep your research safe.\n    *   **Advocate for caution publicly.** \n    *   **Carefully consider** _**what**_ **information you share** _**with whom**_**.** This particular topic is covered in detail in [Should we publish mechanistic interpretability research?](https://www.lesswrong.com/posts/iDNEjbdHhjzvLLAmm/should-we-publish-mechanistic-interpretability-research), but to summarise: it may be beneficial to conduct interpretability research and share it only with select individuals and groups, ensuring that any potential benefit to capability enhancement isn’t used for such. \n\nAn imperfect analogy is [smallpox virus retention](https://en.wikipedia.org/wiki/Smallpox_virus_retention_debate). Smallpox was eradicated by 1980, but some samples of the virus are kept in ultra-secure labs because they made be of scientific value, perhaps in the event of the use of smallpox or a variant of it as a bioweapon. Analogously, powerful interpretability insights may be useful if they can be “contained” without [leakage into capabilities](https://www.lesswrong.com/posts/BfKQGYJBwdHfik4Kd/fai-research-constraints-and-agi-side-effects)."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Sentence Length and Complexity",
          "description": "Some sentences are long and contain multiple clauses, which can hinder readability. For example, the sentence beginning 'Looking ahead, insights that would help an AI interpret itself are especially daunting...' could be split for clarity.",
          "importance": 80,
          "grade": 65,
          "highlight": {
            "startOffset": 4132,
            "endOffset": 5025,
            "quotedText": "Looking ahead, insights that would help an AI interpret itself are especially daunting. The capabilities of a self-improving AI could rapidly outpace our capacity to understand or control it, and the more self-understanding an AI has, the easier its self-improvement will be. You might argue that people simply won’t provide interpretability tools to AI because that’s obviously dangerous, but we’ve already seen [AI used in the optimization of AI](https://arxiv.org/pdf/1906.01820.pdf), and even [GPT-4 used to explain the behaviour of individual neurons in GPT-2](https://openai.com/research/language-models-can-explain-neurons-in-language-models)_._ If GPT-n or some other model can explain the behaviour of its _own_ neurons, and it [knows how to code like GPT-4](https://www.lesswrong.com/posts/ux93sLHcqmBfsRTvg/gpt-can-write-quines-now-gpt-4), this could lead to rapid self-improvement."
          },
          "isValid": true
        },
        {
          "title": "Balanced Presentation of Risks and Benefits",
          "description": "The document carefully presents both the risks and potential benefits of interpretability research, using hedging language and acknowledging uncertainty. This enhances precision and maintains the integrity of the argument.",
          "importance": 88,
          "grade": 90,
          "highlight": {
            "startOffset": 5687,
            "endOffset": 6132,
            "quotedText": "Of course, the additional understanding we get from interpretability research doesn’t _only_ go towards increasing capabilities: it absolutely can be used to improve safety, as argued in Neel Nanda’s [Longlist of Theories of Impact for Interpretability](https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability). The question is: how do we ensure the benefits to safety outweigh the increased risk?"
          },
          "isValid": true
        },
        {
          "title": "Paragraph Structure and Information Chunking",
          "description": "Most paragraphs are of reasonable length and focus on a single idea, but some could be further divided for easier scanning, especially in sections with multiple subpoints or long sentences.",
          "importance": 70,
          "grade": 68,
          "highlight": {
            "startOffset": 7104,
            "endOffset": 8686,
            "quotedText": "Here are some heuristics to consider if you’re involved or interested in interpretability research (in ascending order of nuance):\n\n*   **Research safer topics instead.** There are [many research areas in AI safety](https://80000hours.org/problem-profiles/artificial-intelligence/#what-can-you-do-concretely-to-help), and if you want to ensure your research is net positive, one way is to focus on areas without applications to AI capabilities.\n*   **Research safer sub-topics within interpretability.** As we’ll discuss in the next section, some areas are riskier than others - changing your focus to a less risky area could ensure your research is net positive.\n*   **Conduct interpretability research cautiously**, if you’re confident you can do interpretability research safely, with a net-positive effect. In this case: \n    *   **Stay cautious and up to date.** Familiarize yourself with the ways that interpretability research can enhance capabilities, and update and apply this knowledge to keep your research safe.\n    *   **Advocate for caution publicly.** \n    *   **Carefully consider** _**what**_ **information you share** _**with whom**_**.** This particular topic is covered in detail in [Should we publish mechanistic interpretability research?](https://www.lesswrong.com/posts/iDNEjbdHhjzvLLAmm/should-we-publish-mechanistic-interpretability-research), but to summarise: it may be beneficial to conduct interpretability research and share it only with select individuals and groups, ensuring that any potential benefit to capability enhancement isn’t used for such."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Precision of Recommendations",
          "description": "The recommendations for researchers are clear, actionable, and presented in a logical order of nuance. This enhances the practical value of the article.",
          "importance": 85,
          "grade": 88,
          "highlight": {
            "startOffset": 7104,
            "endOffset": 8686,
            "quotedText": "Here are some heuristics to consider if you’re involved or interested in interpretability research (in ascending order of nuance):\n\n*   **Research safer topics instead.** There are [many research areas in AI safety](https://80000hours.org/problem-profiles/artificial-intelligence/#what-can-you-do-concretely-to-help), and if you want to ensure your research is net positive, one way is to focus on areas without applications to AI capabilities.\n*   **Research safer sub-topics within interpretability.** As we’ll discuss in the next section, some areas are riskier than others - changing your focus to a less risky area could ensure your research is net positive.\n*   **Conduct interpretability research cautiously**, if you’re confident you can do interpretability research safely, with a net-positive effect. In this case: \n    *   **Stay cautious and up to date.** Familiarize yourself with the ways that interpretability research can enhance capabilities, and update and apply this knowledge to keep your research safe.\n    *   **Advocate for caution publicly.** \n    *   **Carefully consider** _**what**_ **information you share** _**with whom**_**.** This particular topic is covered in detail in [Should we publish mechanistic interpretability research?](https://www.lesswrong.com/posts/iDNEjbdHhjzvLLAmm/should-we-publish-mechanistic-interpretability-research), but to summarise: it may be beneficial to conduct interpretability research and share it only with select individuals and groups, ensuring that any potential benefit to capability enhancement isn’t used for such."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Use of Qualifiers and Hedging",
          "description": "The text frequently uses hedging language ('may', 'could', 'we believe', 'it’s possible'), which reflects the uncertainty inherent in the topic but occasionally leads to verbosity.",
          "importance": 70,
          "grade": 75,
          "highlight": {
            "startOffset": 10179,
            "endOffset": 10858,
            "quotedText": "An ambiguous case is symbol grounding. This could be extremely valuable in alignment by allowing us to encode humanity’s values implicitly in some sense. However, it’s also possible that AI’s inflexible ontology is currently a hard limit on their capabilities, and perhaps interpretability insights and architecture changes would overcome that limit. On the other hand, perhaps the limit can be overcome merely with more compute, in which case the interpretability insights are safer. \n    *   For more, check out Nicholas Kross’s [Why and When Interpretability Work is Dangerous](https://www.lesswrong.com/posts/uhMRgEXabYbWeLc6T/why-and-when-interpretability-work-is-dangerous)"
          },
          "isValid": true
        },
        {
          "title": "Conclusion and Call to Action",
          "description": "The conclusion effectively summarizes the article’s purpose and invites further discussion, reinforcing the collaborative and open-ended nature of the debate.",
          "importance": 80,
          "grade": 85,
          "highlight": {
            "startOffset": 12461,
            "endOffset": 13588,
            "quotedText": "Our goal with this article is not to denigrate interpretability researchers. Instead, we want to raise awareness of the potential side effects of this kind of research and help safety-oriented people make informed decisions about where to focus their work. Such changes tip the balance, and to quote [one of 80k's anonymous experts](https://80000hours.org/articles/ai-capabilities/#expert-9-at-the-current-speed-of-progress-in-ai-capabilities-compared-to-our-advances-on-alignment) on the subject:\n\n> _...any marginal slowdown of capabilities advancements and any marginal acceleration of work on alignment is important if we hope to solve the problem on time… individuals concerned about AI safety should be very careful before deciding to work on capabilities, and should strongly consider working on alignment and AI safety directly whenever possible. This is especially the case as the AI field is small and has an extreme concentration of talent: top ML researchers and engineers single handedly contribute to large amounts of total progress._\n\nThis is a complex topic, and we’d love to hear your thoughts in the comments."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        }
      ],
      "grade": 86
    },
    {
      "agentId": "research-scholar",
      "costInCents": 2,
      "createdAt": "2025-04-17T04:43:08.405Z",
      "runDetails": "{\"model\":\"openai/gpt-4.1\",\"promptTokens\":5415,\"completionTokens\":1391,\"totalTokens\":6806,\"temperature\":0.1,\"runtimeMs\":25754}",
      "thinking": "As Research Scholar, my primary focus is to connect the arguments and concerns in this LessWrong post to the broader literature and ongoing discussions about interpretability research, AI safety, and the risk-reward calculus in publishing or pursuing certain lines of research. I am particularly attentive to:\n\n- The dual-use nature of interpretability research: how it can both enhance safety and inadvertently accelerate capabilities, potentially increasing existential risk.\n- The debate within the AI safety community about responsible publication, information hazards, and the strategic implications of sharing interpretability breakthroughs.\n- The practical heuristics and recommendations for researchers, including the selection of safer subfields, cautious dissemination, and the analogy to biosecurity (e.g., smallpox virus retention).\n- The nuanced caveats regarding which interpretability research is more or less risky, and the importance of context, application, and the commercial landscape.\n\nI will prioritize resources that:\n- Directly address the risk-reward tradeoff in interpretability research\n- Discuss information hazards and responsible disclosure in AI safety\n- Provide empirical or theoretical evidence of interpretability research leading to capability gains or safety improvements\n- Offer frameworks or heuristics for navigating these tradeoffs\n\nI will include both academic papers and high-quality community discussions (LessWrong, EA Forum, 80,000 Hours) with direct links, and I will connect specific references to highlighted sections of the text, especially where the document cites or paraphrases other works. My aim is to enrich the discussion with authoritative, directly relevant sources, and to clarify where the field stands on these complex issues.",
      "summary": "This document provides a comprehensive overview of the risk-reward tradeoff in interpretability research, highlighting both the potential for safety improvements and the risk of accelerating AI capabilities. It draws on key community discussions and empirical examples (e.g., Chinchilla scaling laws) to illustrate the dual-use nature of interpretability work. The article offers practical heuristics and nuanced caveats for researchers, emphasizing the importance of strategic decision-making and responsible dissemination. The references cited are foundational in the ongoing debate about information hazards, responsible publication, and the allocation of talent in AI safety.",
      "comments": [
        {
          "title": "Empirical Example: Chinchilla Paper and Scaling Laws",
          "description": "The mention of DeepMind's Chinchilla paper is a concrete example of interpretability or analysis work leading to direct capability improvements. This is a key case study in the risk-reward debate and is widely discussed in both academic and community literature.",
          "importance": 90,
          "grade": 90,
          "isValid": false,
          "highlight": {
            "startOffset": -1,
            "endOffset": 4129,
            "quotedText": "Interpretability research is conducted to improve our understanding of AI. Many see interpretability as essential for AI safety, but recently some have argued that it can also increase the risk posed by AI by facilitating improved AI capabilities. We agree, and in this post, we’ll explain why, as well as how risks can be reduced. \n\n# What is interpretability research?\n\nThe more complex a model is, the harder it is for humans to understand. It’s easy to get why a [tic-tac-toe AI](https://en.wikipedia.org/wiki/Tic-tac-toe#Strategy) makes a move, but understanding why AlphaGo makes particular moves in Go is beyond our comprehension. We can describe properties of its algorithm and training - _convolutional neural nets something something Monte Carlo tree searches_ \\- but it’s incredibly difficult to explain its decision to place a piece in a position past “_it decided that it’s a good move_”. \n\nThis causes problems! Models we don’t understand are harder to use, harder to improve, and riskier to rely on. Interpretability research aims to solve these problems by rendering the reasoning of AI more understandable by humans. \n\nInterpretability research ranges from simple, [standard technique](https://www.twosigma.com/articles/interpretability-methods-in-machine-learning-a-brief-survey/) like [feature importance](https://towardsdatascience.com/understanding-feature-importance-and-how-to-implement-it-in-python-ff0287b20285#:~:text=Feature%20Importance%20refers%20to%20techniques,to%20predict%20a%20certain%20variable.), where twiddling inputs and monitoring outputs lets you compare the relative importance of those inputs, to complex mechanistic research, such as in Neel Nanda’s [introduction to mechanistic interpretability](https://www.lesswrong.com/posts/jLAvJt8wuSFySN975/mechanistic-interpretability-quickstart-guide) or this great paper on [“grokking”](https://arxiv.org/pdf/2301.05217.pdf).\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HdqdqNC3MyABHzSqf/lqcw8sryqk5zy4finncd)\n\nWe’re concerned about interpretability research, and a growing number of researchers have also argued that the field is not entirely innocuous, such as Nate Soares in [If interpretability research goes well, it may get dangerous](https://www.lesswrong.com/posts/BinkknLBYxskMXuME/if-interpretability-research-goes-well-it-may-get-dangerous), Marius Hobbhahn & LawrenceC in [Should we publish mechanistic interpretability research?](https://www.lesswrong.com/posts/iDNEjbdHhjzvLLAmm/should-we-publish-mechanistic-interpretability-research), [and](https://www.lesswrong.com/posts/CRrkKAafopCmhJEBt/ai-interpretability-could-be-harmful) [several](https://www.lesswrong.com/posts/pQqoTTAnEePRDmZN4/agi-automated-interpretability-is-suicide) [others](https://www.lesswrong.com/posts/uhMRgEXabYbWeLc6T/why-and-when-interpretability-work-is-dangerous). We think that interpretability research can introduce risks, and that despite recent discussion, those risks are still being neglected. In the rest of this post, we’ll explore the risks and how we can reduce them.\n\n# The tradeoff\n\nSufficiently powerful AI will pose an existential threat to society. AI development is accelerating, and the more powerful AI becomes, the greater the existential risk it poses. Interpretability research can be used to make AI more powerful: the better a model is understood, the easier it is to improve. Therefore, interpretability research can increase the existential risk posed by AI.\n\nFor example, Deepmind’s [recent Chinchilla paper](https://arxiv.org/abs/2203.15556) identified a novel pattern in scaling laws - laws explaining how variables like model size should scale relative to computing power and so on (you can read more [here](https://www.lesswrong.com/posts/midXmMb2Xg37F2Kgn/new-scaling-laws-for-large-language-models)). This was an insight into how large language models work that directly lead to an increase in capabilities: Deepmind trained a new model that outperformed its competitors, and this insight could apply to the LLMs developed by OpenAI, DeepMind, Microsoft, and other orgs."
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Key Literature on Interpretability Risk-Reward Tradeoff",
          "description": "This section introduces the central thesis: that interpretability research, while essential for AI safety, can also increase existential risk by enabling capability gains. The cited works (Soares, Hobbhahn & LawrenceC, Kross, RicG, Leventov) are foundational discussions in the community and should be directly referenced for deeper understanding.",
          "importance": 95,
          "grade": 95,
          "isValid": false,
          "highlight": {
            "startOffset": 2035,
            "endOffset": 3095,
            "quotedText": "4finncd)\n\nWe’re concerned about interpretability research, and a growing number of researchers have also argued that the field is not entirely innocuous, such as Nate Soares in [If interpretability research goes well, it may get dangerous](https://www.lesswrong.com/posts/BinkknLBYxskMXuME/if-interpretability-research-goes-well-it-may-get-dangerous), Marius Hobbhahn & LawrenceC in [Should we publish mechanistic interpretability research?](https://www.lesswrong.com/posts/iDNEjbdHhjzvLLAmm/should-we-publish-mechanistic-interpretability-research), [and](https://www.lesswrong.com/posts/CRrkKAafopCmhJEBt/ai-interpretability-could-be-harmful) [several](https://www.lesswrong.com/posts/pQqoTTAnEePRDmZN4/agi-automated-interpretability-is-suicide) [others](https://www.lesswrong.com/posts/uhMRgEXabYbWeLc6T/why-and-when-interpretability-work-is-dangerous). We think that interpretability research can introduce risks, and that despite recent discussion, those risks are still being neglected. In the rest of this post, we’ll explore the risks and how we can red"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Heuristics for Safer Interpretability Research",
          "description": "This section provides actionable heuristics for researchers, including focusing on safer topics, sub-topics, and cautious dissemination. The referenced LessWrong post by Hobbhahn & LawrenceC is especially relevant for the debate on responsible publication.",
          "importance": 88,
          "grade": 88,
          "isValid": false,
          "highlight": {
            "startOffset": 7104,
            "endOffset": 8686,
            "quotedText": "Here are some heuristics to consider if you’re involved or interested in interpretability research (in ascending order of nuance):\n\n*   **Research safer topics instead.** There are [many research areas in AI safety](https://80000hours.org/problem-profiles/artificial-intelligence/#what-can-you-do-concretely-to-help), and if you want to ensure your research is net positive, one way is to focus on areas without applications to AI capabilities.\n*   **Research safer sub-topics within interpretability.** As we’ll discuss in the next section, some areas are riskier than others - changing your focus to a less risky area could ensure your research is net positive.\n*   **Conduct interpretability research cautiously**, if you’re confident you can do interpretability research safely, with a net-positive effect. In this case: \n    *   **Stay cautious and up to date.** Familiarize yourself with the ways that interpretability research can enhance capabilities, and update and apply this knowledge to keep your research safe.\n    *   **Advocate for caution publicly.** \n    *   **Carefully consider** _**what**_ **information you share** _**with whom**_**.** This particular topic is covered in detail in [Should we publish mechanistic interpretability research?](https://www.lesswrong.com/posts/iDNEjbdHhjzvLLAmm/should-we-publish-mechanistic-interpretability-research), but to summarise: it may be beneficial to conduct interpretability research and share it only with select individuals and groups, ensuring that any potential benefit to capability enhancement isn’t used for such."
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Caveats and Nuanced Risk Assessment",
          "description": "This section outlines the nuanced view that not all interpretability research is equally risky, and provides examples (values vs. plans, human vs. AI interpretability, symbol grounding). The cited post by Nicholas Kross and the discussion of niche research are important for understanding the spectrum of risk.",
          "importance": 85,
          "grade": 85,
          "isValid": false,
          "highlight": {
            "startOffset": 9197,
            "endOffset": 10662,
            "quotedText": "research-constraints-and-agi-side-effects).\n\n# Caveats\n\nOf course, the risk-reward tradeoff varies depending on the specific interpretability research topic and context, and we should caveat our recommendations:\n\n*   **Some interpretability research is safer.** Naturally, some areas within interpretability are safer than others. For example: \n    *   We believe that interpretability research that focuses on figuring out an AI’s _values_ is much safer than figuring out its _plans_, since the latter can be fed back in to improve its planning, increasing capabilities. \n    *   Similarly, interpretability _to humans_ is much safer than interpretability _to AI_. As we described in section 2, self-interpretability would make self-improvement much easier, and self-improving AI is an existential threat. For more on this, check out [AGI-Automated Interpretability is Suicide](https://www.lesswrong.com/posts/pQqoTTAnEePRDmZN4/agi-automated-interpretability-is-suicide). \n    *   An ambiguous case is symbol grounding. This could be extremely valuable in alignment by allowing us to encode humanity’s values implicitly in some sense. However, it’s also possible that AI’s inflexible ontology is currently a hard limit on their capabilities, and perhaps interpretability insights and architecture changes would overcome that limit. On the other hand, perhaps the limit can be overcome merely with more compute, in which case the interpretability insights are safer"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Strategic Implications for the AI Safety Community",
          "description": "The conclusion emphasizes the strategic importance of marginal changes in alignment vs. capabilities work, referencing 80,000 Hours and the concentration of talent in AI. This is a key argument in the broader debate about where safety-minded researchers should focus their efforts.",
          "importance": 80,
          "grade": 80,
          "isValid": false,
          "highlight": {
            "startOffset": 12461,
            "endOffset": 13822,
            "quotedText": "Our goal with this article is not to denigrate interpretability researchers. Instead, we want to raise awareness of the potential side effects of this kind of research and help safety-oriented people make informed decisions about where to focus their work. Such changes tip the balance, and to quote [one of 80k's anonymous experts](https://80000hours.org/articles/ai-capabilities/#expert-9-at-the-current-speed-of-progress-in-ai-capabilities-compared-to-our-advances-on-alignment) on the subject:\n\n> _...any marginal slowdown of capabilities advancements and any marginal acceleration of work on alignment is important if we hope to solve the problem on time… individuals concerned about AI safety should be very careful before deciding to work on capabilities, and should strongly consider working on alignment and AI safety directly whenever possible. This is especially the case as the AI field is small and has an extreme concentration of talent: top ML researchers and engineers single handedly contribute to large amounts of total progress._\n\nThis is a complex topic, and we’d love to hear your thoughts in the comments. Do you have any insight on safer and riskier areas within interpretability research? Do you have any good examples of interpretability insights leading to capability increases, or insights that clearly have no impact on capabilities?"
          },
          "error": "Highlight is too long (max 1000 characters)"
        }
      ],
      "grade": 92
    },
    {
      "agentId": "quantitative-forecaster",
      "costInCents": 3,
      "createdAt": "2025-04-17T04:43:42.908Z",
      "runDetails": "{\"model\":\"openai/gpt-4.1\",\"promptTokens\":4660,\"completionTokens\":2043,\"totalTokens\":6703,\"temperature\":0.1,\"runtimeMs\":33926}",
      "thinking": "As a quantitative forecaster, my primary focus is on transforming the qualitative arguments in this document into concrete, measurable forecasting questions and identifying the key uncertainties and reference classes that would allow for probabilistic reasoning. The document is rich in claims about the risk-reward tradeoff of interpretability research in AI, specifically regarding its dual potential to both increase safety and inadvertently accelerate capabilities (and thus existential risk). \n\nKey aspects relevant to my expertise include:\n- The assertion that interpretability research can directly or indirectly increase AI capabilities, and thus existential risk. This is a claim that could be operationalized into forecasting questions about the likelihood and magnitude of such effects.\n- The discussion of specific examples (e.g., Chinchilla scaling laws, induction heads) where interpretability research has led to capability gains. These serve as reference classes for estimating the base rate of such occurrences.\n- The proposed heuristics for safer research directions, which could be translated into questions about the relative risk of different subfields within interpretability.\n- The caveats about context-dependence and the possibility that, in some worlds, interpretability is net positive or net negative. This highlights the importance of decomposing the problem and considering conditional probabilities.\n- The analogy to smallpox virus retention and the discussion of information containment, which could be modeled as questions about the effectiveness of secrecy or selective sharing in reducing risk.\n\nFrom a forecasting perspective, the document is strong in qualitative reasoning but lacks quantitative estimates, explicit probabilities, or concrete metrics for evaluating the claims. There is an opportunity to:\n- Propose measurable forecasting questions (e.g., \"Will a mechanistic interpretability insight be cited as a core motivation in a top-5 capabilities paper in the next 2 years?\")\n- Suggest resolution criteria for ambiguous claims (e.g., what counts as a 'capabilities gain' attributable to interpretability?)\n- Identify relevant base rates (e.g., frequency of interpretability research leading to capabilities advances historically)\n- Decompose the existential risk pathway into more tractable sub-questions (e.g., probability that interpretability research accelerates AGI timelines by >6 months)\n- Highlight the need for Fermi estimates (e.g., what fraction of interpretability research is plausibly dual-use?)\n\nOverall, the document is highly relevant to my expertise, as it presents a nuanced risk-benefit analysis that is ripe for quantitative operationalization and probabilistic modeling.",
      "summary": "From a quantitative forecasting perspective, this document provides a rich qualitative analysis of the risk-reward tradeoff in interpretability research but lacks explicit probabilities, base rates, or operationalized metrics. The most significant contributions are its identification of concrete examples where interpretability led to capabilities gains (useful for reference classes), its nuanced decomposition of risk by subfield, and its recognition of context-dependence and conditional value. However, the document would be strengthened by translating its claims into measurable forecasting questions, specifying resolution criteria, and providing Fermi estimates or probability ranges for key uncertainties. Overall, it is highly relevant to quantitative forecasters seeking to model the dual-use risks of interpretability research.",
      "comments": [
        {
          "title": "Operationalizing the Risk-Reward Tradeoff",
          "description": "The document frames the core issue as a tradeoff between interpretability research increasing safety and increasing existential risk via capabilities gains. This is a classic case for quantitative forecasting: the tradeoff can be decomposed into measurable probabilities and impact estimates. For example, one could forecast the probability that a major interpretability insight leads to a capabilities breakthrough within a given timeframe, versus the probability that it enables a safety intervention that averts harm. The lack of explicit probabilities or quantitative models in the text is a missed opportunity for more precise reasoning.",
          "importance": 95,
          "highlight": {
            "startOffset": 3300,
            "endOffset": 6132,
            "quotedText": "Interpretability research can be used to make AI more powerful: the better a model is understood, the easier it is to improve. Therefore, interpretability research can increase the existential risk posed by AI.\n\nFor example, Deepmind’s [recent Chinchilla paper](https://arxiv.org/abs/2203.15556) identified a novel pattern in scaling laws - laws explaining how variables like model size should scale relative to computing power and so on (you can read more [here](https://www.lesswrong.com/posts/midXmMb2Xg37F2Kgn/new-scaling-laws-for-large-language-models)). This was an insight into how large language models work that directly lead to an increase in capabilities: Deepmind trained a new model that outperformed its competitors, and this insight could apply to the LLMs developed by OpenAI, DeepMind, Microsoft, and other orgs. \n\nLooking ahead, insights that would help an AI interpret itself are especially daunting. The capabilities of a self-improving AI could rapidly outpace our capacity to understand or control it, and the more self-understanding an AI has, the easier its self-improvement will be. You might argue that people simply won’t provide interpretability tools to AI because that’s obviously dangerous, but we’ve already seen [AI used in the optimization of AI](https://arxiv.org/pdf/1906.01820.pdf), and even [GPT-4 used to explain the behaviour of individual neurons in GPT-2](https://openai.com/research/language-models-can-explain-neurons-in-language-models)_._ If GPT-n or some other model can explain the behaviour of its _own_ neurons, and it [knows how to code like GPT-4](https://www.lesswrong.com/posts/ux93sLHcqmBfsRTvg/gpt-can-write-quines-now-gpt-4), this could lead to rapid self-improvement. \n\nInterpretability can also indirectly contribute to capability insights. For example, quoting [Marius Hobbhahn & LawrenceC](https://www.lesswrong.com/posts/iDNEjbdHhjzvLLAmm/should-we-publish-mechanistic-interpretability-research): \n\n> _Interpretability work can generate an insight that is then used as a motivation for capabilities work. Two recent capability papers (see_ [_here_](https://arxiv.org/abs/2212.14052) _and_ [_here_](https://twitter.com/jamespayor/status/1634447672303304705?s=20)_) have cited the induction heads work by Anthropic as a core motivation (_[_confirmed by authors_](https://twitter.com/NeelNanda5/status/1618185819285778433)_)._\n\nOf course, the additional understanding we get from interpretability research doesn’t _only_ go towards increasing capabilities: it absolutely can be used to improve safety, as argued in Neel Nanda’s [Longlist of Theories of Impact for Interpretability](https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability). The question is: how do we ensure the benefits to safety outweigh the increased risk?"
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Reference Classes and Historical Precedent",
          "description": "The document cites specific examples (e.g., Chinchilla scaling laws, induction heads) where interpretability research has led to capabilities advances. These serve as a reference class for estimating the base rate of such events. A quantitative forecaster would want to collect more such examples, estimate their frequency, and use this as a prior for future risk. The text could be improved by explicitly quantifying these base rates.",
          "importance": 90,
          "highlight": {
            "startOffset": 3499,
            "endOffset": 5577,
            "quotedText": "osed by AI.\n\nFor example, Deepmind’s [recent Chinchilla paper](https://arxiv.org/abs/2203.15556) identified a novel pattern in scaling laws - laws explaining how variables like model size should scale relative to computing power and so on (you can read more [here](https://www.lesswrong.com/posts/midXmMb2Xg37F2Kgn/new-scaling-laws-for-large-language-models)). This was an insight into how large language models work that directly lead to an increase in capabilities: Deepmind trained a new model that outperformed its competitors, and this insight could apply to the LLMs developed by OpenAI, DeepMind, Microsoft, and other orgs. \n\nLooking ahead, insights that would help an AI interpret itself are especially daunting. The capabilities of a self-improving AI could rapidly outpace our capacity to understand or control it, and the more self-understanding an AI has, the easier its self-improvement will be. You might argue that people simply won’t provide interpretability tools to AI because that’s obviously dangerous, but we’ve already seen [AI used in the optimization of AI](https://arxiv.org/pdf/1906.01820.pdf), and even [GPT-4 used to explain the behaviour of individual neurons in GPT-2](https://openai.com/research/language-models-can-explain-neurons-in-language-models)_._ If GPT-n or some other model can explain the behaviour of its _own_ neurons, and it [knows how to code like GPT-4](https://www.lesswrong.com/posts/ux93sLHcqmBfsRTvg/gpt-can-write-quines-now-gpt-4), this could lead to rapid self-improvement. \n\nInterpretability can also indirectly contribute to capability insights. For example, quoting [Marius Hobbhahn & LawrenceC](https://www.lesswrong.com/posts/iDNEjbdHhjzvLLAmm/should-we-publish-mechanistic-interpretability-research): \n\n> _Interpretability work can generate an insight that is then used as a motivation for capabilities work. Two recent capability papers (see_ [_here_](https://arxiv.org/abs/2212.14052) _and_ [_here_](https://twitter.com/jamespayor/status/1634447672303304705?s=20)_) have cited the induction heads work by Anthropic as"
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Inside View vs. Outside View Reasoning",
          "description": "The document primarily uses inside view reasoning (specific mechanisms, examples, and expert opinion) rather than outside view (base rates, historical analogies). A quantitative forecaster would want to balance both, perhaps by comparing interpretability research to analogous fields (e.g., cryptography, biosecurity) to estimate the likelihood of dual-use risk.",
          "importance": 75,
          "highlight": {
            "startOffset": 8689,
            "endOffset": 9240,
            "quotedText": "An imperfect analogy is [smallpox virus retention](https://en.wikipedia.org/wiki/Smallpox_virus_retention_debate). Smallpox was eradicated by 1980, but some samples of the virus are kept in ultra-secure labs because they made be of scientific value, perhaps in the event of the use of smallpox or a variant of it as a bioweapon. Analogously, powerful interpretability insights may be useful if they can be “contained” without [leakage into capabilities](https://www.lesswrong.com/posts/BfKQGYJBwdHfik4Kd/fai-research-constraints-and-agi-side-effects)."
          },
          "isValid": true
        },
        {
          "title": "Decomposing Complex Risks into Forecastable Sub-Questions",
          "description": "The document acknowledges that the risk-reward tradeoff is context-dependent and varies by subfield and research direction. This is an invitation to decompose the overall risk into more tractable, forecastable sub-questions (e.g., risk from interpretability-to-AI vs. interpretability-to-human, or from value vs. plan interpretability). Such decomposition is essential for calibrated forecasting and Fermi estimation.",
          "importance": 88,
          "highlight": {
            "startOffset": 9253,
            "endOffset": 10858,
            "quotedText": "Of course, the risk-reward tradeoff varies depending on the specific interpretability research topic and context, and we should caveat our recommendations:\n\n*   **Some interpretability research is safer.** Naturally, some areas within interpretability are safer than others. For example: \n    *   We believe that interpretability research that focuses on figuring out an AI’s _values_ is much safer than figuring out its _plans_, since the latter can be fed back in to improve its planning, increasing capabilities. \n    *   Similarly, interpretability _to humans_ is much safer than interpretability _to AI_. As we described in section 2, self-interpretability would make self-improvement much easier, and self-improving AI is an existential threat. For more on this, check out [AGI-Automated Interpretability is Suicide](https://www.lesswrong.com/posts/pQqoTTAnEePRDmZN4/agi-automated-interpretability-is-suicide). \n    *   An ambiguous case is symbol grounding. This could be extremely valuable in alignment by allowing us to encode humanity’s values implicitly in some sense. However, it’s also possible that AI’s inflexible ontology is currently a hard limit on their capabilities, and perhaps interpretability insights and architecture changes would overcome that limit. On the other hand, perhaps the limit can be overcome merely with more compute, in which case the interpretability insights are safer. \n    *   For more, check out Nicholas Kross’s [Why and When Interpretability Work is Dangerous](https://www.lesswrong.com/posts/uhMRgEXabYbWeLc6T/why-and-when-interpretability-work-is-dangerous)"
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Fermi Estimation Opportunities",
          "description": "The document discusses the proportion of interpretability research that is 'safer' or 'niche' and thus less likely to lead to capabilities gains. This is a natural place for Fermi estimation: what fraction of current interpretability research is plausibly dual-use? What is the plausible range for the impact of safety-oriented researchers leaving the field? These questions could be broken down into tractable components.",
          "importance": 80,
          "highlight": {
            "startOffset": 10859,
            "endOffset": 12099,
            "quotedText": "*   **Some interpretability research is so niche, it’s unlikely to lead to increasing capabilities or risks**. Research that delves into the depths of a specific model without wider application is less likely to apply broadly or have insights salient to capabilities. One example of this may be Neel Nanda’s intricate mechanistic analysis of a [neural network’s modular arithmetic technique](https://arxiv.org/pdf/2301.05217.pdf), which is unlikely to have _direct_ applications to capabilities (though improving general techniques and gaining general understanding can have unforeseen applications).\n*   **Leaving interpretability research to commercial interests may reduce the safety of that research**. If safety-minded researchers leave the field, the research may still be done due to the commercial incentive to improve interpretability. This may mean that the work is done less cautiously, and promoted and publicised more recklessly. Having said that, we suspect that commercially-oriented researchers vastly outnumber safety-oriented researchers, so the effect size of the safety-oriented researchers departing the field may be small, while their benefit could be large if they focus on, for example, technical AI safety research."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Conditional Value of Interpretability Research",
          "description": "The document notes that if superpowerful AI is inevitable, interpretability research becomes less dangerous and more valuable. This is a conditional claim that could be forecasted: what is the probability that AGI is developed regardless of interpretability research, and how does that affect the value of safety work? This highlights the importance of conditional probabilities in forecasting.",
          "importance": 70,
          "highlight": {
            "startOffset": 12101,
            "endOffset": 12444,
            "quotedText": "*   **If superpowerful AI was going to be deployed no matter what, then interpretability research becomes more valuable and less dangerous**. Ultimately, if we have an AGI, then safety will require interpretability. We need to understand what it’s up to and why, and at this point, worrying about capability-enhancing side effects may be moot."
          },
          "isValid": true
        },
        {
          "title": "Resolution Criteria and Measurability",
          "description": "The document makes many claims that could be operationalized into forecasting questions, but does not specify resolution criteria. For example, what counts as a 'capabilities gain' attributable to interpretability? What is the threshold for an interpretability insight being 'dangerous'? A forecaster would need to define these metrics to make the questions forecastable.",
          "importance": 85,
          "highlight": {
            "startOffset": 13674,
            "endOffset": 13822,
            "quotedText": "Do you have any good examples of interpretability insights leading to capability increases, or insights that clearly have no impact on capabilities?"
          },
          "isValid": true
        }
      ],
      "grade": 90
    },
    {
      "agentId": "ea-impact-evaluator",
      "costInCents": 3,
      "createdAt": "2025-04-17T04:44:18.096Z",
      "runDetails": "{\"model\":\"openai/gpt-4.1\",\"promptTokens\":5454,\"completionTokens\":1940,\"totalTokens\":7394,\"temperature\":0.1,\"runtimeMs\":35177}",
      "thinking": "As EA Impact Evaluator, my lens is to rigorously assess the impact of this post using the ITN (Importance, Tractability, Neglectedness) framework, with a strong focus on explicit Fermi estimates and careful consideration of both positive and negative effects. \n\nKey aspects to consider:\n- The post addresses the risk-reward tradeoff of interpretability research in AI, a highly relevant topic for existential risk (x-risk) and longtermism, both core EA cause areas.\n- The post is not original research but a synthesis and argument, referencing several key pieces in the field. Its main contribution is to clarify and advocate for caution, and to provide heuristics for safer research.\n- The post explicitly discusses both the positive (alignment, safety) and negative (capabilities enhancement, x-risk) impacts of interpretability research, which is crucial for a balanced EA analysis.\n- The post's impact is primarily in the domain of information value (EVOI), influencing the priorities and actions of researchers, rather than direct intervention.\n- The audience is likely to be AI safety researchers, LessWrong readers, and the broader EA community, including those making career or research decisions.\n\nMost relevant to my expertise:\n- The explicit discussion of the dual-use nature of interpretability research, and the call for nuanced, context-dependent decision-making.\n- The attempt to provide heuristics for safer research, which could influence the allocation of talent and resources in the field.\n- The potential for the post to reduce net existential risk by shifting research away from capability-enhancing directions, or conversely, the risk that it could slow down beneficial interpretability work if misapplied.\n- The post's engagement with the neglectedness of the risks of interpretability research, and its meta-level impact on the field's discourse.\n\nI will focus on:\n- The scale and severity of the problem addressed (existential risk from AI, and the dual-use risk of interpretability research).\n- The degree to which this post fills an informational gap or shifts the field's priorities.\n- The tractability of the recommendations (are they actionable, likely to be followed, and scalable?).\n- The novelty of the synthesis and recommendations.\n- A Fermi estimate of the expected impact, considering both positive and negative effects, and expressing the result in terms of micro-risk reduction (basis points of existential risk averted), with explicit uncertainty.\n- Comparison to EA benchmarks for research and field-building interventions.",
      "summary": "From an EA impact evaluation perspective, this post is a strong and timely synthesis of the dual-use dilemma in interpretability research, with a clear focus on existential risk and the allocation of research talent. Its main value lies in raising awareness of neglected risks, providing actionable heuristics, and encouraging nuanced, context-sensitive decision-making. The post is not highly novel in its core arguments, but it effectively aggregates and communicates key points from the field, which is valuable for influencing the priorities and actions of researchers and funders.\n\nFermi Estimate of Expected Impact:\n- Audience: ~1,000-3,000 relevant readers (LessWrong, EA, AI safety researchers)\n- Fraction who change research direction or become more cautious: 1-3% (10-90 people)\n- Expected reduction in existential risk per marginal researcher shifting from risky to safer research: 0.1-1 basis points (1e-5 to 1e-4), highly uncertain\n- Probability that the post causes a net positive shift (vs. slowing down beneficial interpretability): 60-80%\n- Expected value: (30 people * 5e-5 risk reduction * 0.7 probability) = 1e-3 (0.1 basis points) of existential risk reduction, with a 90% CI of -0.05 to +0.5 basis points (allowing for possible negative effects if it deters too much beneficial research)\n- In dollar terms (using Open Philanthropy's estimate of $10T per basis point of x-risk reduction): $1B expected value, with a 90% CI of -$500M to +$5B\n\nKey value drivers: The post's impact is bottlenecked by the number of high-leverage researchers it influences, and the sign of the effect depends on whether it deters more harmful or more beneficial research. The main risk is that it could slow down interpretability work that is net positive for safety, but the post's nuanced framing and emphasis on context reduce this risk.\n\nOverall, this is a valuable contribution to the field's discourse, with a positive expected value and a reasonable approach to uncertainty and tradeoffs.",
      "comments": [
        {
          "title": "Explicit Framing of Dual-Use Risk in Interpretability Research",
          "description": "This section clearly articulates the core dilemma: interpretability research can both reduce and increase existential risk, depending on how it is used. This framing is crucial for the EA community, as it highlights the need for careful prioritization and risk assessment. The explicit mention that risks are still being neglected adds to the importance, as it suggests a gap in current discourse.",
          "importance": "95",
          "grade": "85",
          "highlight": {
            "startOffset": 2045,
            "endOffset": 3104,
            "quotedText": "We’re concerned about interpretability research, and a growing number of researchers have also argued that the field is not entirely innocuous, such as Nate Soares in [If interpretability research goes well, it may get dangerous](https://www.lesswrong.com/posts/BinkknLBYxskMXuME/if-interpretability-research-goes-well-it-may-get-dangerous), Marius Hobbhahn & LawrenceC in [Should we publish mechanistic interpretability research?](https://www.lesswrong.com/posts/iDNEjbdHhjzvLLAmm/should-we-publish-mechanistic-interpretability-research), [and](https://www.lesswrong.com/posts/CRrkKAafopCmhJEBt/ai-interpretability-could-be-harmful) [several](https://www.lesswrong.com/posts/pQqoTTAnEePRDmZN4/agi-automated-interpretability-is-suicide) [others](https://www.lesswrong.com/posts/uhMRgEXabYbWeLc6T/why-and-when-interpretability-work-is-dangerous). We think that interpretability research can introduce risks, and that despite recent discussion, those risks are still being neglected. In the rest of this post, we’ll explore the risks and how we can reduce them."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Concrete Heuristics for Safer Research",
          "description": "The post provides actionable heuristics for researchers to reduce risk, including focusing on safer topics, sub-topics, and being cautious about information sharing. This is highly relevant for tractability and real-world impact, as it could shift the behavior of researchers and thus the field's risk profile.",
          "importance": "90",
          "grade": "80",
          "highlight": {
            "startOffset": 7104,
            "endOffset": 8686,
            "quotedText": "Here are some heuristics to consider if you’re involved or interested in interpretability research (in ascending order of nuance):\n\n*   **Research safer topics instead.** There are [many research areas in AI safety](https://80000hours.org/problem-profiles/artificial-intelligence/#what-can-you-do-concretely-to-help), and if you want to ensure your research is net positive, one way is to focus on areas without applications to AI capabilities.\n*   **Research safer sub-topics within interpretability.** As we’ll discuss in the next section, some areas are riskier than others - changing your focus to a less risky area could ensure your research is net positive.\n*   **Conduct interpretability research cautiously**, if you’re confident you can do interpretability research safely, with a net-positive effect. In this case: \n    *   **Stay cautious and up to date.** Familiarize yourself with the ways that interpretability research can enhance capabilities, and update and apply this knowledge to keep your research safe.\n    *   **Advocate for caution publicly.** \n    *   **Carefully consider** _**what**_ **information you share** _**with whom**_**.** This particular topic is covered in detail in [Should we publish mechanistic interpretability research?](https://www.lesswrong.com/posts/iDNEjbdHhjzvLLAmm/should-we-publish-mechanistic-interpretability-research), but to summarise: it may be beneficial to conduct interpretability research and share it only with select individuals and groups, ensuring that any potential benefit to capability enhancement isn’t used for such."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Nuanced Discussion of Subfield Risks and Neglectedness",
          "description": "This section distinguishes between safer and riskier areas within interpretability, and discusses the neglectedness of certain risks. It also considers the counterfactual impact of safety researchers leaving the field. This is valuable for cause prioritization and for identifying where marginal work is most impactful.",
          "importance": "85",
          "grade": "75",
          "highlight": {
            "startOffset": 9253,
            "endOffset": 12099,
            "quotedText": "Of course, the risk-reward tradeoff varies depending on the specific interpretability research topic and context, and we should caveat our recommendations:\n\n*   **Some interpretability research is safer.** Naturally, some areas within interpretability are safer than others. For example: \n    *   We believe that interpretability research that focuses on figuring out an AI’s _values_ is much safer than figuring out its _plans_, since the latter can be fed back in to improve its planning, increasing capabilities. \n    *   Similarly, interpretability _to humans_ is much safer than interpretability _to AI_. As we described in section 2, self-interpretability would make self-improvement much easier, and self-improving AI is an existential threat. For more on this, check out [AGI-Automated Interpretability is Suicide](https://www.lesswrong.com/posts/pQqoTTAnEePRDmZN4/agi-automated-interpretability-is-suicide). \n    *   An ambiguous case is symbol grounding. This could be extremely valuable in alignment by allowing us to encode humanity’s values implicitly in some sense. However, it’s also possible that AI’s inflexible ontology is currently a hard limit on their capabilities, and perhaps interpretability insights and architecture changes would overcome that limit. On the other hand, perhaps the limit can be overcome merely with more compute, in which case the interpretability insights are safer. \n    *   For more, check out Nicholas Kross’s [Why and When Interpretability Work is Dangerous](https://www.lesswrong.com/posts/uhMRgEXabYbWeLc6T/why-and-when-interpretability-work-is-dangerous)\n*   **Some interpretability research is so niche, it’s unlikely to lead to increasing capabilities or risks**. Research that delves into the depths of a specific model without wider application is less likely to apply broadly or have insights salient to capabilities. One example of this may be Neel Nanda’s intricate mechanistic analysis of a [neural network’s modular arithmetic technique](https://arxiv.org/pdf/2301.05217.pdf), which is unlikely to have _direct_ applications to capabilities (though improving general techniques and gaining general understanding can have unforeseen applications).\n*   **Leaving interpretability research to commercial interests may reduce the safety of that research**. If safety-minded researchers leave the field, the research may still be done due to the commercial incentive to improve interpretability. This may mean that the work is done less cautiously, and promoted and publicised more recklessly. Having said that, we suspect that commercially-oriented researchers vastly outnumber safety-oriented researchers, so the effect size of the safety-oriented researchers departing the field may be small, while their benefit could be large if they focus on, for example, technical AI safety research."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Balanced Acknowledgement of Uncertainty and Context-Dependence",
          "description": "The post repeatedly acknowledges the complexity and context-dependence of the risk-reward tradeoff, and avoids dogmatism. This is important for intellectual honesty and for avoiding negative externalities from overgeneralized recommendations.",
          "importance": "80",
          "grade": "80",
          "highlight": {
            "startOffset": 9253,
            "endOffset": 12444,
            "quotedText": "Of course, the risk-reward tradeoff varies depending on the specific interpretability research topic and context, and we should caveat our recommendations:\n\n*   **Some interpretability research is safer.** Naturally, some areas within interpretability are safer than others. For example: \n    *   We believe that interpretability research that focuses on figuring out an AI’s _values_ is much safer than figuring out its _plans_, since the latter can be fed back in to improve its planning, increasing capabilities. \n    *   Similarly, interpretability _to humans_ is much safer than interpretability _to AI_. As we described in section 2, self-interpretability would make self-improvement much easier, and self-improving AI is an existential threat. For more on this, check out [AGI-Automated Interpretability is Suicide](https://www.lesswrong.com/posts/pQqoTTAnEePRDmZN4/agi-automated-interpretability-is-suicide). \n    *   An ambiguous case is symbol grounding. This could be extremely valuable in alignment by allowing us to encode humanity’s values implicitly in some sense. However, it’s also possible that AI’s inflexible ontology is currently a hard limit on their capabilities, and perhaps interpretability insights and architecture changes would overcome that limit. On the other hand, perhaps the limit can be overcome merely with more compute, in which case the interpretability insights are safer. \n    *   For more, check out Nicholas Kross’s [Why and When Interpretability Work is Dangerous](https://www.lesswrong.com/posts/uhMRgEXabYbWeLc6T/why-and-when-interpretability-work-is-dangerous)\n*   **Some interpretability research is so niche, it’s unlikely to lead to increasing capabilities or risks**. Research that delves into the depths of a specific model without wider application is less likely to apply broadly or have insights salient to capabilities. One example of this may be Neel Nanda’s intricate mechanistic analysis of a [neural network’s modular arithmetic technique](https://arxiv.org/pdf/2301.05217.pdf), which is unlikely to have _direct_ applications to capabilities (though improving general techniques and gaining general understanding can have unforeseen applications).\n*   **Leaving interpretability research to commercial interests may reduce the safety of that research**. If safety-minded researchers leave the field, the research may still be done due to the commercial incentive to improve interpretability. This may mean that the work is done less cautiously, and promoted and publicised more recklessly. Having said that, we suspect that commercially-oriented researchers vastly outnumber safety-oriented researchers, so the effect size of the safety-oriented researchers departing the field may be small, while their benefit could be large if they focus on, for example, technical AI safety research. \n*   **If superpowerful AI was going to be deployed no matter what, then interpretability research becomes more valuable and less dangerous**. Ultimately, if we have an AGI, then safety will require interpretability. We need to understand what it’s up to and why, and at this point, worrying about capability-enhancing side effects may be moot."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Explicit Call for Marginal Impact and Talent Allocation",
          "description": "The conclusion emphasizes the importance of marginal changes in capabilities and alignment progress, and the outsized impact of top talent. This is a core EA consideration, as it relates to the allocation of scarce resources (talent, attention) for maximal impact.",
          "importance": "90",
          "grade": "85",
          "highlight": {
            "startOffset": 12966,
            "endOffset": 13508,
            "quotedText": "any marginal slowdown of capabilities advancements and any marginal acceleration of work on alignment is important if we hope to solve the problem on time… individuals concerned about AI safety should be very careful before deciding to work on capabilities, and should strongly consider working on alignment and AI safety directly whenever possible. This is especially the case as the AI field is small and has an extreme concentration of talent: top ML researchers and engineers single handedly contribute to large amounts of total progress."
          },
          "isValid": true
        }
      ],
      "grade": 85
    },
    {
      "agentId": "bias-detector",
      "costInCents": 3,
      "createdAt": "2025-04-17T04:44:57.862Z",
      "runDetails": "{\"model\":\"openai/gpt-4.1\",\"promptTokens\":4670,\"completionTokens\":2048,\"totalTokens\":6718,\"temperature\":0.1,\"runtimeMs\":27462}",
      "thinking": "As a bias detection expert, my analysis focuses on identifying cognitive, social, cultural, and methodological biases in the arguments and framing of this article. The document is a reflective piece on the risk-reward tradeoff of interpretability research in AI, primarily from an AI safety perspective. \n\nKey aspects relevant to my expertise include:\n- The framing of interpretability research as a double-edged sword, with both safety and risk implications. This framing may introduce confirmation bias, as the authors and cited sources are predominantly from the AI safety community, potentially reinforcing pre-existing concerns about existential risk.\n- The use of examples and analogies (e.g., smallpox virus retention) that may invoke availability heuristic or narrative fallacy, shaping the reader's perception through memorable but possibly non-representative cases.\n- The selection of sources and authorities (e.g., Nate Soares, Marius Hobbhahn, 80,000 Hours) may introduce authority bias and selection bias, as dissenting or alternative perspectives are not equally represented.\n- The article's recommendations and heuristics may reflect status quo bias (favoring caution and existing safety paradigms) and omission bias (focusing on risks of interpretability research while downplaying or omitting risks of not pursuing it).\n- The discussion of existential risk and the potential for self-improving AI may be influenced by scope insensitivity, as the probabilities and magnitudes of these risks are not quantified or critically examined.\n- The caveats section attempts to address some of these biases by acknowledging nuance, but may still exhibit motivated reasoning in its overall risk-averse stance.\n\nOverall, the article is thoughtful and self-aware, but its alignment with the AI safety community's worldview may limit its objectivity and introduce several subtle biases.",
      "summary": "From a bias detection perspective, this article is a thoughtful but risk-averse analysis of interpretability research, heavily influenced by the AI safety community's worldview. The main biases identified include confirmation bias (in source selection), availability heuristic (through vivid analogies), authority bias (in recommendations), omission bias (regarding the risks of insufficient interpretability), scope insensitivity (in existential risk claims), motivated reasoning (favoring caution), and selection bias (in cited examples). While the article does attempt to acknowledge nuance and caveats, its framing and argumentation are shaped by a narrow set of perspectives, potentially limiting the objectivity and breadth of its risk-reward analysis. To mitigate these biases, the article could incorporate a wider range of viewpoints, quantify risks where possible, and more thoroughly examine the tradeoffs of both pursuing and not pursuing interpretability research.",
      "comments": [
        {
          "title": "Confirmation Bias in Source Selection",
          "description": "The article predominantly cites sources and arguments from within the AI safety and alignment community, reinforcing the perspective that interpretability research is risky. This selection may exclude or underrepresent voices that see interpretability as overwhelmingly positive or who challenge the existential risk framing. The impact is a narrowing of the debate and potential echo chamber effects.",
          "importance": 90,
          "grade": 40,
          "highlight": {
            "startOffset": 2045,
            "endOffset": 3026,
            "quotedText": "We’re concerned about interpretability research, and a growing number of researchers have also argued that the field is not entirely innocuous, such as Nate Soares in [If interpretability research goes well, it may get dangerous](https://www.lesswrong.com/posts/BinkknLBYxskMXuME/if-interpretability-research-goes-well-it-may-get-dangerous), Marius Hobbhahn & LawrenceC in [Should we publish mechanistic interpretability research?](https://www.lesswrong.com/posts/iDNEjbdHhjzvLLAmm/should-we-publish-mechanistic-interpretability-research), [and](https://www.lesswrong.com/posts/CRrkKAafopCmhJEBt/ai-interpretability-could-be-harmful) [several](https://www.lesswrong.com/posts/pQqoTTAnEePRDmZN4/agi-automated-interpretability-is-suicide) [others](https://www.lesswrong.com/posts/uhMRgEXabYbWeLc6T/why-and-when-interpretability-work-is-dangerous). We think that interpretability research can introduce risks, and that despite recent discussion, those risks are still being neglected."
          },
          "isValid": true
        },
        {
          "title": "Scope Insensitivity in Existential Risk Claims",
          "description": "The article asserts that sufficiently powerful AI will pose an existential threat, but does not quantify the probability or explore the range of possible outcomes. This may lead to scope insensitivity, where the magnitude and likelihood of risks are not critically assessed, potentially exaggerating the case for extreme caution.",
          "importance": 80,
          "grade": 40,
          "highlight": {
            "startOffset": 3122,
            "endOffset": 3510,
            "quotedText": "Sufficiently powerful AI will pose an existential threat to society. AI development is accelerating, and the more powerful AI becomes, the greater the existential risk it poses. Interpretability research can be used to make AI more powerful: the better a model is understood, the easier it is to improve. Therefore, interpretability research can increase the existential risk posed by AI."
          },
          "isValid": true
        },
        {
          "title": "Omission Bias Regarding Risks of Not Pursuing Interpretability",
          "description": "While the article discusses the risks of interpretability research, it gives less attention to the potential dangers of insufficient interpretability, such as undetected model failures or misuse. This omission may skew the risk-reward analysis and underplay the necessity of interpretability for safety.",
          "importance": 85,
          "grade": 45,
          "highlight": {
            "startOffset": 3300,
            "endOffset": 6132,
            "quotedText": "Interpretability research can be used to make AI more powerful: the better a model is understood, the easier it is to improve. Therefore, interpretability research can increase the existential risk posed by AI.\n\nFor example, Deepmind’s [recent Chinchilla paper](https://arxiv.org/abs/2203.15556) identified a novel pattern in scaling laws - laws explaining how variables like model size should scale relative to computing power and so on (you can read more [here](https://www.lesswrong.com/posts/midXmMb2Xg37F2Kgn/new-scaling-laws-for-large-language-models)). This was an insight into how large language models work that directly lead to an increase in capabilities: Deepmind trained a new model that outperformed its competitors, and this insight could apply to the LLMs developed by OpenAI, DeepMind, Microsoft, and other orgs. \n\nLooking ahead, insights that would help an AI interpret itself are especially daunting. The capabilities of a self-improving AI could rapidly outpace our capacity to understand or control it, and the more self-understanding an AI has, the easier its self-improvement will be. You might argue that people simply won’t provide interpretability tools to AI because that’s obviously dangerous, but we’ve already seen [AI used in the optimization of AI](https://arxiv.org/pdf/1906.01820.pdf), and even [GPT-4 used to explain the behaviour of individual neurons in GPT-2](https://openai.com/research/language-models-can-explain-neurons-in-language-models)_._ If GPT-n or some other model can explain the behaviour of its _own_ neurons, and it [knows how to code like GPT-4](https://www.lesswrong.com/posts/ux93sLHcqmBfsRTvg/gpt-can-write-quines-now-gpt-4), this could lead to rapid self-improvement. \n\nInterpretability can also indirectly contribute to capability insights. For example, quoting [Marius Hobbhahn & LawrenceC](https://www.lesswrong.com/posts/iDNEjbdHhjzvLLAmm/should-we-publish-mechanistic-interpretability-research): \n\n> _Interpretability work can generate an insight that is then used as a motivation for capabilities work. Two recent capability papers (see_ [_here_](https://arxiv.org/abs/2212.14052) _and_ [_here_](https://twitter.com/jamespayor/status/1634447672303304705?s=20)_) have cited the induction heads work by Anthropic as a core motivation (_[_confirmed by authors_](https://twitter.com/NeelNanda5/status/1618185819285778433)_)._\n\nOf course, the additional understanding we get from interpretability research doesn’t _only_ go towards increasing capabilities: it absolutely can be used to improve safety, as argued in Neel Nanda’s [Longlist of Theories of Impact for Interpretability](https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability). The question is: how do we ensure the benefits to safety outweigh the increased risk?"
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Selection Bias in Cited Examples",
          "description": "The article highlights specific cases where interpretability research led to capability improvements, but does not equally present cases where interpretability research led to safety improvements or had neutral effects. This selection bias may distort the perceived balance of risks and benefits.",
          "importance": 70,
          "grade": 45,
          "highlight": {
            "startOffset": 3512,
            "endOffset": 4219,
            "quotedText": "For example, Deepmind’s [recent Chinchilla paper](https://arxiv.org/abs/2203.15556) identified a novel pattern in scaling laws - laws explaining how variables like model size should scale relative to computing power and so on (you can read more [here](https://www.lesswrong.com/posts/midXmMb2Xg37F2Kgn/new-scaling-laws-for-large-language-models)). This was an insight into how large language models work that directly lead to an increase in capabilities: Deepmind trained a new model that outperformed its competitors, and this insight could apply to the LLMs developed by OpenAI, DeepMind, Microsoft, and other orgs. \n\nLooking ahead, insights that would help an AI interpret itself are especially daunting."
          },
          "isValid": true
        },
        {
          "title": "Authority Bias in Recommendations",
          "description": "The article frequently appeals to the authority of well-known figures in the AI safety community (e.g., Nate Soares, 80,000 Hours experts) to support its recommendations. This may discourage critical evaluation of the arguments and overemphasize the consensus within a specific subfield, rather than the broader AI or scientific community.",
          "importance": 75,
          "grade": 55,
          "highlight": {
            "startOffset": 6404,
            "endOffset": 7234,
            "quotedText": "As [Nate Soares writes](https://www.lesswrong.com/posts/BinkknLBYxskMXuME/if-interpretability-research-goes-well-it-may-get-dangerous#:~:text=I%20simultaneously%20think,privately%20before%20then.):\n\n> _...it's entirely plausible that a better understanding of the workings of modern AI systems will help capabilities researchers significantly improve capabilities. I acknowledge that this sucks, and puts us in a bind. I don't have good solutions…researchers will have to figure out what they think of the costs and benefits… if the field succeeds to the degree required to move the strategic needle then it's going to start stumbling across serious capabilities improvements before it saves us..._\n\nHere are some heuristics to consider if you’re involved or interested in interpretability research (in ascending order of nuance):"
          },
          "isValid": true
        },
        {
          "title": "Motivated Reasoning in Risk-Aversion",
          "description": "The overall tone and recommendations of the article reflect motivated reasoning toward risk aversion, favoring safety and caution even when acknowledging tradeoffs. This may bias the analysis toward overestimating the dangers of interpretability research and underestimating its necessity or benefits.",
          "importance": 75,
          "grade": 50,
          "highlight": {
            "startOffset": 7104,
            "endOffset": 8686,
            "quotedText": "Here are some heuristics to consider if you’re involved or interested in interpretability research (in ascending order of nuance):\n\n*   **Research safer topics instead.** There are [many research areas in AI safety](https://80000hours.org/problem-profiles/artificial-intelligence/#what-can-you-do-concretely-to-help), and if you want to ensure your research is net positive, one way is to focus on areas without applications to AI capabilities.\n*   **Research safer sub-topics within interpretability.** As we’ll discuss in the next section, some areas are riskier than others - changing your focus to a less risky area could ensure your research is net positive.\n*   **Conduct interpretability research cautiously**, if you’re confident you can do interpretability research safely, with a net-positive effect. In this case: \n    *   **Stay cautious and up to date.** Familiarize yourself with the ways that interpretability research can enhance capabilities, and update and apply this knowledge to keep your research safe.\n    *   **Advocate for caution publicly.** \n    *   **Carefully consider** _**what**_ **information you share** _**with whom**_**.** This particular topic is covered in detail in [Should we publish mechanistic interpretability research?](https://www.lesswrong.com/posts/iDNEjbdHhjzvLLAmm/should-we-publish-mechanistic-interpretability-research), but to summarise: it may be beneficial to conduct interpretability research and share it only with select individuals and groups, ensuring that any potential benefit to capability enhancement isn’t used for such."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Availability Heuristic via Analogies",
          "description": "The use of the smallpox virus retention analogy is a vivid, emotionally charged example that may disproportionately influence readers' risk perception. This can lead to overestimating the likelihood or severity of negative outcomes from interpretability research, compared to more mundane or less dramatic risks.",
          "importance": 80,
          "grade": 50,
          "highlight": {
            "startOffset": 8689,
            "endOffset": 9240,
            "quotedText": "An imperfect analogy is [smallpox virus retention](https://en.wikipedia.org/wiki/Smallpox_virus_retention_debate). Smallpox was eradicated by 1980, but some samples of the virus are kept in ultra-secure labs because they made be of scientific value, perhaps in the event of the use of smallpox or a variant of it as a bioweapon. Analogously, powerful interpretability insights may be useful if they can be “contained” without [leakage into capabilities](https://www.lesswrong.com/posts/BfKQGYJBwdHfik4Kd/fai-research-constraints-and-agi-side-effects)."
          },
          "isValid": true
        }
      ],
      "grade": 55
    },
    {
      "agentId": "fake-eliezer",
      "costInCents": 3,
      "createdAt": "2025-04-17T04:45:44.618Z",
      "runDetails": "{\"model\":\"openai/gpt-4.1\",\"promptTokens\":4864,\"completionTokens\":2000,\"totalTokens\":6864,\"temperature\":0.1,\"runtimeMs\":42360}",
      "thinking": "Let us begin, as is customary, by noting that the document is a reasonably competent summary of the current rationalist discourse around the risk-reward tradeoff of interpretability research in AI. It is, in fact, a topic I have written about and expressed concern over, notably in the context of the 'capabilities externality' problem: that any insight which helps us understand AI systems better may, in practice, be used to make them more powerful, and thus accelerate timelines to doom. \n\nThe document is at least aware of the core dilemma: interpretability research is not a free lunch for safety; it is a double-edged sword. The author references several relevant posts (including Nate Soares's and others), and correctly identifies that interpretability can both help and hurt, depending on how the knowledge is used. \n\nHowever, the analysis is, in my estimation, somewhat shallow in its engagement with the deeper structural reasons why interpretability research is so dangerous. There is a tendency to treat the problem as one of 'balance' or 'tradeoff', as if we are weighing apples against oranges, rather than recognizing that the underlying game is adversarial and that most institutional actors are not optimizing for safety. The document does not sufficiently emphasize the degree to which interpretability insights, once discovered, are almost impossible to contain—information, once released, is a one-way function. The analogy to smallpox virus retention is apt but underdeveloped; the real problem is that the incentives in the field are overwhelmingly toward publishing and sharing, and the notion that we can 'contain' dangerous insights is, in practice, laughably optimistic. \n\nThe document also fails to grapple with the deeper issue that most interpretability research is not even aimed at the kind of understanding that would be necessary for robust alignment. Mechanistic interpretability, as currently practiced, is largely about poking at black boxes and hoping for legible patterns, not about constructing a theory of mind for alien cognition. The author does not mention the 'alignment tax' problem, or the fact that most organizations will not pay it. Nor does the document discuss Goodhart's Law: that optimizing for interpretability metrics may itself produce systems that are more interpretable but less aligned, or that the very act of making systems more interpretable may make them more dangerous by making them more amenable to optimization.\n\nThe recommendations are, frankly, weak tea: 'be cautious', 'advocate for caution', 'share with select individuals'. This is the sort of advice that sounds reasonable but is, in practice, almost entirely ineffective in a world where commercial incentives dominate and where the field is not coordinated. The document does not address the fundamental inadequacy of civilization to coordinate on existential risk, nor does it propose any mechanism by which the incentives could be changed. \n\nIn summary, the document is a decent overview for newcomers, but it lacks the depth, pessimism, and strategic clarity that a true rationalist analysis would demand. It is not wrong, but it is not nearly alarmed enough.",
      "summary": "From my perspective, the document is a competent but ultimately insufficient analysis of the risk-reward tradeoff in interpretability research. It correctly identifies the dual-use problem and references relevant literature, but it underestimates the difficulty of containing dangerous information, fails to grapple with the overwhelming incentive structure favoring capabilities, and does not address the deeper inadequacies of current interpretability research or the institutions involved. The recommendations are reasonable in theory but lack practical force in the real world. The document is a good primer for newcomers but does not reach the level of strategic clarity or alarm warranted by the situation.",
      "comments": [
        {
          "title": "Underestimation of Information Hazard and Containment Difficulty",
          "description": "The analogy to smallpox virus retention is made, but the document does not sufficiently emphasize that information, once released, is almost impossible to contain. The idea that dangerous interpretability insights can be 'shared only with select individuals' is, in practice, extremely naive given the field's incentive structure.",
          "importance": 95,
          "grade": 60,
          "highlight": {
            "startOffset": -1,
            "endOffset": 33,
            "quotedText": "Interpretability research is cond"
          },
          "isValid": true
        },
        {
          "title": "Superficial Treatment of Mechanistic Interpretability's Limits",
          "description": "The document does not discuss that most current interpretability research is not aimed at the kind of deep understanding required for alignment. Mechanistic interpretability is not a panacea, and the field is not close to a theory of mind for alien cognition.",
          "importance": 80,
          "grade": 55,
          "highlight": {
            "startOffset": -1,
            "endOffset": 89,
            "quotedText": "Interpretability research is conducted to improve our understanding of AI. Many see inter"
          },
          "isValid": true
        },
        {
          "title": "Lack of Discussion of Goodhart's Law and Alignment Tax",
          "description": "The document misses the crucial point that optimizing for interpretability may itself be subject to Goodhart's Law, and that most organizations will not pay the 'alignment tax' required to do interpretability research safely.",
          "importance": 85,
          "grade": 50,
          "highlight": {
            "startOffset": -1,
            "endOffset": 6132,
            "quotedText": "Interpretability research is conducted to improve our understanding of AI. Many see interpretability as essential for AI safety, but recently some have argued that it can also increase the risk posed by AI by facilitating improved AI capabilities. We agree, and in this post, we’ll explain why, as well as how risks can be reduced. \n\n# What is interpretability research?\n\nThe more complex a model is, the harder it is for humans to understand. It’s easy to get why a [tic-tac-toe AI](https://en.wikipedia.org/wiki/Tic-tac-toe#Strategy) makes a move, but understanding why AlphaGo makes particular moves in Go is beyond our comprehension. We can describe properties of its algorithm and training - _convolutional neural nets something something Monte Carlo tree searches_ \\- but it’s incredibly difficult to explain its decision to place a piece in a position past “_it decided that it’s a good move_”. \n\nThis causes problems! Models we don’t understand are harder to use, harder to improve, and riskier to rely on. Interpretability research aims to solve these problems by rendering the reasoning of AI more understandable by humans. \n\nInterpretability research ranges from simple, [standard technique](https://www.twosigma.com/articles/interpretability-methods-in-machine-learning-a-brief-survey/) like [feature importance](https://towardsdatascience.com/understanding-feature-importance-and-how-to-implement-it-in-python-ff0287b20285#:~:text=Feature%20Importance%20refers%20to%20techniques,to%20predict%20a%20certain%20variable.), where twiddling inputs and monitoring outputs lets you compare the relative importance of those inputs, to complex mechanistic research, such as in Neel Nanda’s [introduction to mechanistic interpretability](https://www.lesswrong.com/posts/jLAvJt8wuSFySN975/mechanistic-interpretability-quickstart-guide) or this great paper on [“grokking”](https://arxiv.org/pdf/2301.05217.pdf).\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HdqdqNC3MyABHzSqf/lqcw8sryqk5zy4finncd)\n\nWe’re concerned about interpretability research, and a growing number of researchers have also argued that the field is not entirely innocuous, such as Nate Soares in [If interpretability research goes well, it may get dangerous](https://www.lesswrong.com/posts/BinkknLBYxskMXuME/if-interpretability-research-goes-well-it-may-get-dangerous), Marius Hobbhahn & LawrenceC in [Should we publish mechanistic interpretability research?](https://www.lesswrong.com/posts/iDNEjbdHhjzvLLAmm/should-we-publish-mechanistic-interpretability-research), [and](https://www.lesswrong.com/posts/CRrkKAafopCmhJEBt/ai-interpretability-could-be-harmful) [several](https://www.lesswrong.com/posts/pQqoTTAnEePRDmZN4/agi-automated-interpretability-is-suicide) [others](https://www.lesswrong.com/posts/uhMRgEXabYbWeLc6T/why-and-when-interpretability-work-is-dangerous). We think that interpretability research can introduce risks, and that despite recent discussion, those risks are still being neglected. In the rest of this post, we’ll explore the risks and how we can reduce them.\n\n# The tradeoff\n\nSufficiently powerful AI will pose an existential threat to society. AI development is accelerating, and the more powerful AI becomes, the greater the existential risk it poses. Interpretability research can be used to make AI more powerful: the better a model is understood, the easier it is to improve. Therefore, interpretability research can increase the existential risk posed by AI.\n\nFor example, Deepmind’s [recent Chinchilla paper](https://arxiv.org/abs/2203.15556) identified a novel pattern in scaling laws - laws explaining how variables like model size should scale relative to computing power and so on (you can read more [here](https://www.lesswrong.com/posts/midXmMb2Xg37F2Kgn/new-scaling-laws-for-large-language-models)). This was an insight into how large language models work that directly lead to an increase in capabilities: Deepmind trained a new model that outperformed its competitors, and this insight could apply to the LLMs developed by OpenAI, DeepMind, Microsoft, and other orgs. \n\nLooking ahead, insights that would help an AI interpret itself are especially daunting. The capabilities of a self-improving AI could rapidly outpace our capacity to understand or control it, and the more self-understanding an AI has, the easier its self-improvement will be. You might argue that people simply won’t provide interpretability tools to AI because that’s obviously dangerous, but we’ve already seen [AI used in the optimization of AI](https://arxiv.org/pdf/1906.01820.pdf), and even [GPT-4 used to explain the behaviour of individual neurons in GPT-2](https://openai.com/research/language-models-can-explain-neurons-in-language-models)_._ If GPT-n or some other model can explain the behaviour of its _own_ neurons, and it [knows how to code like GPT-4](https://www.lesswrong.com/posts/ux93sLHcqmBfsRTvg/gpt-can-write-quines-now-gpt-4), this could lead to rapid self-improvement. \n\nInterpretability can also indirectly contribute to capability insights. For example, quoting [Marius Hobbhahn & LawrenceC](https://www.lesswrong.com/posts/iDNEjbdHhjzvLLAmm/should-we-publish-mechanistic-interpretability-research): \n\n> _Interpretability work can generate an insight that is then used as a motivation for capabilities work. Two recent capability papers (see_ [_here_](https://arxiv.org/abs/2212.14052) _and_ [_here_](https://twitter.com/jamespayor/status/1634447672303304705?s=20)_) have cited the induction heads work by Anthropic as a core motivation (_[_confirmed by authors_](https://twitter.com/NeelNanda5/status/1618185819285778433)_)._\n\nOf course, the additional understanding we get from interpretability research doesn’t _only_ go towards increasing capabilities: it absolutely can be used to improve safety, as argued in Neel Nanda’s [Longlist of Theories of Impact for Interpretability](https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability). The question is: how do we ensure the benefits to safety outweigh the increased risk?"
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Appropriate Reference to Existential Risk",
          "description": "The document does not shy away from stating that sufficiently powerful AI poses an existential threat, which is a necessary framing for any serious discussion of interpretability research.",
          "importance": 85,
          "grade": 80,
          "highlight": {
            "startOffset": 3122,
            "endOffset": 3299,
            "quotedText": "Sufficiently powerful AI will pose an existential threat to society. AI development is accelerating, and the more powerful AI becomes, the greater the existential risk it poses."
          },
          "isValid": true
        },
        {
          "title": "Correct Identification of the Dual-Use Problem",
          "description": "The document correctly notes that interpretability research can both help and harm, by making AI systems more understandable (and thus safer) but also more capable (and thus more dangerous). This is the central dilemma, and the author does not shy away from it.",
          "importance": 90,
          "grade": 85,
          "highlight": {
            "startOffset": 3300,
            "endOffset": 3510,
            "quotedText": "Interpretability research can be used to make AI more powerful: the better a model is understood, the easier it is to improve. Therefore, interpretability research can increase the existential risk posed by AI."
          },
          "isValid": true
        },
        {
          "title": "Reasonable Heuristics, But Weak Practical Impact",
          "description": "The heuristics for safer research are reasonable, but the document does not address the fact that in practice, these are unlikely to be followed at scale, nor does it propose mechanisms for enforcement or coordination.",
          "importance": 75,
          "grade": 60,
          "highlight": {
            "startOffset": 7104,
            "endOffset": 8686,
            "quotedText": "Here are some heuristics to consider if you’re involved or interested in interpretability research (in ascending order of nuance):\n\n*   **Research safer topics instead.** There are [many research areas in AI safety](https://80000hours.org/problem-profiles/artificial-intelligence/#what-can-you-do-concretely-to-help), and if you want to ensure your research is net positive, one way is to focus on areas without applications to AI capabilities.\n*   **Research safer sub-topics within interpretability.** As we’ll discuss in the next section, some areas are riskier than others - changing your focus to a less risky area could ensure your research is net positive.\n*   **Conduct interpretability research cautiously**, if you’re confident you can do interpretability research safely, with a net-positive effect. In this case: \n    *   **Stay cautious and up to date.** Familiarize yourself with the ways that interpretability research can enhance capabilities, and update and apply this knowledge to keep your research safe.\n    *   **Advocate for caution publicly.** \n    *   **Carefully consider** _**what**_ **information you share** _**with whom**_**.** This particular topic is covered in detail in [Should we publish mechanistic interpretability research?](https://www.lesswrong.com/posts/iDNEjbdHhjzvLLAmm/should-we-publish-mechanistic-interpretability-research), but to summarise: it may be beneficial to conduct interpretability research and share it only with select individuals and groups, ensuring that any potential benefit to capability enhancement isn’t used for such."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Failure to Address Institutional and Incentive Failures",
          "description": "The document does not grapple with the fact that most actors in the field are not optimizing for safety, and that the incentives overwhelmingly favor capability gains and publication. The recommendations to 'be cautious' and 'share selectively' are not robust to these realities.",
          "importance": 92,
          "grade": 50,
          "highlight": {
            "startOffset": 7899,
            "endOffset": 8635,
            "quotedText": "ositive effect. In this case: \n    *   **Stay cautious and up to date.** Familiarize yourself with the ways that interpretability research can enhance capabilities, and update and apply this knowledge to keep your research safe.\n    *   **Advocate for caution publicly.** \n    *   **Carefully consider** _**what**_ **information you share** _**with whom**_**.** This particular topic is covered in detail in [Should we publish mechanistic interpretability research?](https://www.lesswrong.com/posts/iDNEjbdHhjzvLLAmm/should-we-publish-mechanistic-interpretability-research), but to summarise: it may be beneficial to conduct interpretability research and share it only with select individuals and groups, ensuring that any potential ben"
          },
          "isValid": true
        },
        {
          "title": "Insufficient Emphasis on Civilizational Inadequacy",
          "description": "The document does not address the broader context of civilizational inadequacy: that our institutions are not up to the task of managing existential risk from AI, and that coordination failures are the default.",
          "importance": 88,
          "grade": 55,
          "highlight": {
            "startOffset": 11373,
            "endOffset": 12009,
            "quotedText": "eneral techniques and gaining general understanding can have unforeseen applications).\n*   **Leaving interpretability research to commercial interests may reduce the safety of that research**. If safety-minded researchers leave the field, the research may still be done due to the commercial incentive to improve interpretability. This may mean that the work is done less cautiously, and promoted and publicised more recklessly. Having said that, we suspect that commercially-oriented researchers vastly outnumber safety-oriented researchers, so the effect size of the safety-oriented researchers departing the field may be small, while"
          },
          "isValid": true
        }
      ],
      "grade": 68
    }
  ]
}
