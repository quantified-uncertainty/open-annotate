{
  "id": "ai-epistemics",
  "": "AI Epistemics",
  "slug": "ai-epistemics",
  "author": "Owen Cotton-Barratt",
  "publishedDate": "2025-04-18",
  "platforms": [
    "Google Docs"
  ],
  "intendedAgents": [
    "ea-impact-evaluator",
    "clarity-coach",
    "research-scholar",
    "fake-eliezer",
    "quantitative-forecaster",
    "bias-detector"
  ],
  "content": "# Writing Session\n\nPreamble/framing from Owen:\n\n* Negative: \"we just want to free-form vibe about what cool things we could do\"  \n* Positive: step back & ask \"wait, why are we doing this again?\"  \n* What stuff does this actually help?  \n* Epistemics isn't a goal in-an-of itself; figuring out what combination of things are possible, achievable, etc.  \n* Why do we care about (AI for) epistemics?\n\nWriting prompts from Owen:\n\n* What are use cases for AI for Epistemics that you most care about?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nTemplate:\n\n## Name: \n\nEntry\n\n## Dan Schwarz:\n\n* What are use cases for AI for Epistemics that you most care about?\n\nGoing from \"Realized\" to \"Accessible\" knowledge, e.g. getting good information using the web. Agentic web agents like Deep Research.\n\n* What decisions may go badly by default, but could go better if we have the right tech?\n\nMany billions of small decisions.\n\n* What do the later stages of the path to impact look like?\n\nSuperhuman forecasting at scale.\n\n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nReasonable judgment on tricky questions that's too cheap to meter, better than if you asked a respected colleague to spend many minutes on it.\n\n## Aviv\n\n* Better government governance  \n* Better democratic functioning  \n* Reduced dangerous concentration of power  \n    \n* Healthy polarization / pluralism  \n    \n* More peace, less likelihood of war  \n* Better geopolitics  \n* Better economic incentives  \n    \n* Better AI org governance  \n* Better AI coordination (re. distribution, escalation, etc.)  \n* Better compute allocation (toward what matters for good outcomes)  \n    \n* Better allocation of resources overall  \n* Better lives for people  \n* More effective organizations, getting things done with fewer resources, more focused on the things that benefit everyone  \n    \n* Identification of common ground directions / equilibria / game theoretic strategies that create/incentivize new positive gradients (and not surfacing the bad versions of those…)  \n    \n* Understanding our own goals and values\n\n*Note how it is the intersection of better epistemics / world models + the right power structures / incentives needed for all of these.*\n\n## Ben Goldhaber:\n\n* Forecasting how I should live my life in a period of intense technological change  \n* Guiding decision makers to avoid war with china/ai/everyone  \n* Providing the background context for me to sensemake way more sanely.  \n* Pushing forward the extant knowledge of AI safety agendas  \n* Planning for multi-stage projects that could achieve big impact  \n* Roadmapping to build larger more complex things\n\n## Chi\n\nThere are a few different things I care about:\n\n* I care a lot about humanity and AI, including **super-late-stage AI, getting decision theory** right.  \n  * Consequently but also separately, this relates to the general question of what to do about areas where there is not necessarily a ground truth but that are nonetheless very important to get right, cf**. [metaphilosophy](https://www.alignmentforum.org/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy)** (but also a bunch of other things.)  \n  * This also feels very similar to making **AIs helpful for AI safety research, cf. [automating alignment research](https://joecarlsmith.com/2025/03/14/ai-for-ai-safety#2-what-is-ai-for-ai-safety).**\n\n  → This is more about raising the **top of the epistemic landscape**\n\n* Other things I care about:  \n  * Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place.  \n  * Vague sense that better **societal epistemics** would help with increasing the **sanity of conversations about AI safety, polarisation around the issue etc.**  \n    * This is broadly about **getting intent alignment** in the first place.\n\n→ These are all more about raising the **bottom of the epistemic landscape**\n\n## Deger Turan:\n\n* Social trust erosion, collective coordination mechanisms are built on a weak assumption  \n* Capacity to entertain truthfulness when it is not instrumental to one's incentive gradient & differing norms upon 'truth will make my sense of security weaker'   \n* Creating contexts where world model divergence can be agreed by both parties enabling a safe disagreement  \n* Improving model reasoning / safer AI by reducing slop, by creating accountability mechanisms through consistency checks & other specifications for healthy epistemics  \n* Decoupling virality with magnitude while living in a short attention span world, curation   \n* I have some confusion re: raising the floor when not all parties are necessarily operating in good faith. I think I need some help think through the ways in which epistemics in the context of incentive misalignment can be used or have defensibility. \n\n## Nathan Young: \n\nWhat would we have got:\n\n* Biden not running, perhaps an open dem primary  \n* No FTX crisis  \n* Less zero sum OpenAI board drama  \n* Many smaller, better choices \n\nWhat decisions might go badly:\n\n* Trump tariffs/ global trade  \n* Taiwan/ US-China conflict  \n* US-China competition  \n* US authoritarianism  \n* AI regulation  \n* EU-China realignment  \n* Automated robots  \n* OpenAI strategy  \n* Anthropic strategy  \n* Google strategy  \n* xAI strategy  \n* AI workforce automation  \n* China strategy  \n* Russia strategy  \n* Moskovitz strategy  \n* Must strategy  \n* Bezos Strategy  \n* Zuckerberg strategy  \n* Tallinn strategy  \n* Altman strategy  \n* Drone warfare  \n* Climate change destabilisation  \n* Democratic trust\n\n## Lincoln Quirk:\n\nWhy do we care about (AI for) epistemics?  \n\\- making the public better informed:  \n\t\\- better able to deal with and process the wealth of information  \n\t\\- better able to express their own interests in government (via voting)  \n\t\\- helping cut through social media distraction / intentional misinformation  \n\\- getting governments to implement better policy  \n\t\\- coming up with good policy ideas  \n\t\\- objectively doing cost-benefit analysis  \n\t\\- objectively explaining tradeoffs (vs having to rely on biased humans to explain them)  \n\t\\- flagging bad ideas as bad  \n\t\\- convincing policymakers with compelling writeups  \n\\- success and flourishing for individuals and orgs using these tools  \n\t\\- via helping them think through decisions  \n\t\\- via clearheaded analysis of trade-offs and cost/benefits  \n\t\\- flagging bad ideas as bad\n\n## Jay\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * How can we build something like the positive version of Community Notes: which statements/posts are agreed on by people who often disagree?  \n  * How can we better understand/summarize what others think, e.g. on social media platforms? E.g. if you see a post recommended to you, is this because it's the most extreme take that went viral and isn't representative, or is this a commonly held view? Between different viewpoints, what are the concrete points of actual disagreement (on facts or values) and what are the things widely agreed upon? Let's build viewpoint/debate maps or a similar sensemaking tool that gets adopted/understood by the masses/raises the knowledge floor (in my experience, things like pol.is are hard for regular users to grok)  \n  * How can we improve Community Notes (or more general knowledge) creation with AI assistants? An AI agent proposing community notes and humans still checking/rating them?  \n  * How can we have something like Community Notes that applies to claims or statements about the future, or which are not concretely knowable?  \n  * \n\n## Ben Wilson (Metaculus)\n\nOverall I believe the most important value of epistemics (and AI epistemics) in general is informing what actions allow us to achieve our goals (i.e. make the world better). If we had perfect AI epistemics I would be excited by the ability for us to take a goal, and get a list of possible actions we could take and the forecasted outcomes of each of those actions (plus background information and forecasts for the 'current lay of the land' for context). This can then inform, not just the risk of AI doing good/bad things (as with current forecasting), but what we can do to raise the chances of 'great' AI outcomes, and lower the changes of 'worse' AI outcomes\n\n## Andreas\n\n* **What are use cases for AI for Epistemics that you most care about?**  \n  * AI policy, broadly construed, must go better  \n    * Avoid gradual disempowerment  \n    * Thoughtful regulation  \n  * Organizational planning & decision-making is weak  \n    * Given an informal long-term goal like improve decision-making for strategic users in government & large orgs, make a plan for a 20-people org that helps them accomplish the goal  \n    * Mapping dependencies, uncertainties, and potential futures  \n    * Understand complex domains with non-trivial feedback loops  \n      * Economic shifts  \n* **What decisions may go badly by default, but could go better if we have the right tech?**  \n  * Resource allocation within orgs  \n    * Investment  \n    * Grant making  \n    * Government budgets  \n  * Policy  \n    * How should AI be deployed in the world?  \n* **What do the later stages of the path to impact look like?**  \n  * AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning  \n  * The technology helps organizations maintain coherence and make sound judgments even as the pace of change accelerates  \n  * Assumption: People are generally well-intentioned \\- no one wants catastrophe \\- and as a result of better tooling things go better in the world overall\n\n## Owen optimistic brainstorm:\n\nWorld peace without world government — sufficiently good tech to help countries understand each others' interests, find mutually desirable bargains, and make sufficient credible commitments, could help us get up towards the Pareto frontier where we avoid (virtually) all costly conflicts\n\nGoing slowly and carefully on AI development — rather than having AI progress accelerate at increasingly breakneck speed, helping actors to understand the basic situation and the desirability of different options could be crucial for facilitating meaningful and robust agreements which restrict AI development to the stuff which we have good reasons for being confident are safe // otherwise beneficial.\n\nBettering democracy — helping people to access key information about what decisions their country might make would actually mean for longer term implications, and having this in turn feed into incentives for decision-makers that better align them with the common good\n\nBuilding safe / aligned AI — better tools for anticipating when systems are actually robustly safe, vs when stronger cases for expectation of safety seem required\n\n## Josh Rosenberg (FRI)\n\nI'm massively confused about important problems, e.g. how to orient to AI risk and what to do about it. Some reasonable people think there's a high probability we effectively lose everything we care about in 2027\\. Some reasonable people think it'll be decades till we have \"AGI\" and it'd be good if it arrived sooner. Which actions are good to take in each of these worlds is very different, and I'd like to figure out what world we're in.\n\nThis also applies to many other problem areas.\n\nE.g., Trump's effect on US democracy. How much of a crisis is it? What are the most effective actions to take in response to ensure that the country and communities we care about are as well off as they can be? How can we preserve high-quality institutions to lead the world in positive directions?\n\nOr, on the positive vision side: the \"Abundance\" agenda seems promising. Is it actually a good idea? What do we do about it?\n\nAI for Epistemics could help us get a lot more clarity on how to think about these problems and what actions to take. I'd like much better information and forecasts on the likely effects of Trump, AI timelines, and the pro's and con's of different Abundance policies, for example. If I had a bunch of time and good resources to dig into these problems, I could get a much better sense for myself. I'm hoping AI will help all of us do this sort of thing collectively.\n\n## Katja Grace\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Put lots of cognitive effort into very clear and good and compelling case for AI risk  \n  * Automate talking to people about AI risk compellingly, possibly using above case as input  \n  * Good game theoretic thinking re arms races, coordination, what is possible. I.e. People are making dumb assumptions about things, e.g. \"can't slow down tech\". Could automate noting errors. So e.g. when someone says that on Twitter, can highlight it and send in response. \\-\\> good thinking via better weapons for debate (similar to Nathan project)  \n  * Specific idea: very quantified and competitive debate site, that can be cited re which views are winning, where AI does a lot of the cognitive labor in places no people are paying attention, but as a part of it gets cruxy, people pay attention and vote/bet  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Poor decision making around AI: e.g. 'racing' because of vague heuristics instead of ever doing calc on costs and benefits given specific risks  \n  * Failure to coordinate on AI: building AI systems that are dangerous, due to expecting others to do so\n\n## Joel Lehman\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Raising the bottom (i.e. helping improve societal epistemics) – in particular that in theory political decisions (at least political decision-makers) are somewhat downstream of societal epistemics  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Having weird political leaders and awful public epistemics at critical junctures seems to unnecessarily spike catastrophic/existential risk probabilities. In theory, helping uplevel or at least maintain current public epistemic health (especially as it relates to civic sensemaking) might help influence the quality of decision-makers and concrete decisions made at high-stakes moments  \n  * The shape of the tech is not clear, or probably requires multiple flavors of interventions. E.g. if LLM assistants by default help cultivate civic virtues and on the margin support critical thinking or whatever underlying skills and knowledge help a person up-level their epistemic habits / health (or at least if LLM assistants don't contribute to atrophy/harm); or somehow convincing social media networks to care about this (e.g. community notes but things that go beyond); similarly somehow help w/ important social goods like 'societal trust' and 'trust in your neighbors'  \n* What do the later stages of the path to impact look like?  \n  * \\[didn't get to this\\]  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * Healthier democratic society, better agreement on consensus reality, higher trust in society, better politicians, better concrete decisions in high-stakes moments \n\n## Austin Chen\n\n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * **Evaluator**: More agreement (or understanding of disagreement) of what orgs & people are doing good work (eg for making grants in AI safety)  \n  * **Oracle**: Instant trustworthy forecasts on any question you ask  \n  * **Planner**: Guidance on \"what are the 3 most important things I should do this day/week/month\"  \n  * **Representative**: Simulate the responses of other parties (people, orgs/corps, governments, LLM agents), for discussions, contracts, trade\n\n## Janna\n\n* Why do we care about (AI for) epistemics?  \n  * Noise to signal ratio could be much higher  \n  * AI (mis)information churned out at the price of almost zero  \n* What are use cases for AI for Epistemics that you most care about?  \n  * Solving prediction markets--is there a way to scale up the bots, even if they are wrong and lossy, to improve prediction market accuracy?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * US-China relations  \n  * Trump tariffs maybe  \n  * Immigration rules in the US  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * More accurate predictions to…  \n    * Reduce business uncertainty  \n    * Make it easier to evaluate life decisions  \n  * Forestalling bad decisions made on imperfect information\n\n## Owain\n\n1. Maybe having humans understanding the world better is good in itself. Lots of people read popular science for this reason. Maybe it's good for AIs as well  \n2. Very broadly, it seems that better world understanding and decision making enables people to make their lives better in myriad ways. (To substantiate this, one could try to separate out the effects of better world understanding from the effects of better realized technology. E.g. Looking at capital-poor countries that can still benefit from scientific knowledge, economics, etc.)  \n3. More specifically, if AI progress is fast, there will be lots of important decisions to be made in quick calendar time. Exploiting AI to improve those decisions seems valuable. Current decision making institutions do not seem well set up to adapt really quickly. (A specific concern is dangers from AI takeover).   \n4. Why now? Why this group of people? We haven't really had tech that could do most epistemic tasks until recently. Now we have AI that can do an increasing proportion of the sub-tasks of epistemic processes. This will allow us to delegate epistemic tasks to AI.  \n   1. It's likely that people do this delegation, even if it's somewhat misguided and potentially bad in the long-term. Can we make this go well?  \n   2. This potentially massively expands our epistemic capacities. How can we use this well?  \n   3. Epistemic tech/institutions/advances can be hard to fund via for-profit companies. This is in part due to public goods issues, and issues around markets for ideas.\n\n## Rafe\n\nAs ever, I hope this will stop advanced artificial intelligence destroying the world, killing all its people \\&c.\n\nSo I think there are a few paths for impact here. I think there is a world where we can make the most productive thinkers about how to do things go further and better; I'm not so excited about a single-player version of this, as I think there's a pretty narrow window when top-level centaur mode works out\\[1\\]\n\nEveryone is already thinking about AI, and, I suspect, this will only get more acute. I think it's pretty hard for people to confront this and think about it carefully and well. It's scary\\! And there are a bunch of entrenched interests and memeplexes that preserve their status better (myopically) if people only have shallow slogans to think about AI with.\n\nGiven that, I suspect that the global sensemaking machine isn't up for the task of thinking about AI well. I hope we can help the global sensemaking machine upgrade itself. I have some hope that we as a society can come up with good ideas for dealing with AI, and also come to see how they make sense, so we can choose them in a robust way.\n\nOne other path to impact: I think a society that understands things better, including itself and its impacts, is one that is more robust in a bunch of ways. A more epistemically-grounded society is also probably a bit richer, and I think a bigger pie creates more slack for arranging things in sanity-promoting ways. I have more hope in a robust society navigating this stuff well.\n\n\\[1\\]:  Yeah, maybe you can do something with shaving off subskills, but I don't think that's really that doable.\n\n## Dave\n\n* With my consequentialist hat, better epistemics being worth it has to route through improved decisions somewhere down the line. The decisions I particularly care about are those by powerful people wrt AI development and implementation and the effects this has on how the future goes  \n  * But I do feel a bit icky when I have this hat on. There seems to be something very intuitively appealing about (a) pursuing better epistemics for its own sake and (b) democratising epistemic tools for the use and benefit of everyone. I think this basically caches out in more 'raising the floor' type work  \n  * I don't feel like I have a very good way of resolving this tension yet, but am excited to see things pushing on both frontiers  \n* I think forecasting has the potential to be a pretty powerful tool and there's currently a massive undersupply of good forecasts. I'm excited for us to use the wave of increased AI labor to make better quantitative forecasts to inform key decisions. I probably care more about quantitative forecasts than most decision makers and am open to being persuaded that the other, fuzzier parts of epistemics are more valuable to work on. \n\n## Julian\n\nWhat are use cases for AI for Epistemics that you most care about?  \n**Raising the ceiling:** can we use AI to organize information and credibly resolve the most challenging and important questions on which experts still seem to disagree? For example:\n\n* COVID origins  \n* Urban planning and public policy  \n* Lots of advancements in basic science: routing around current disagreements and policy challenges with slam-dunk science might be a pretty strong way forward.\n\n**Raising the floor:** can we use AI to build extremely legibly trustworthy/reliable epistemic infrastructure that actually makes its way into people's lives?\n\n* Making truth more palatable (in a way which doesn't translate into making falsehoods more palatable)  \n* Collective epistemics: connecting people to the others with whom they would have the most productive/enlightening conversations\n\nWhat decisions may go badly by default, but could go better if we have the right tech?\n\n* Lots of stuff in public policy: downstream both of better experts (technocrats), better public (voters/interest groups), and better coordination (collective epistemics)\n\nWhat do the later stages of the path to impact look like?\n\n* I want us to live in a world where everyone is more rational, empathetic, considered in their decision making, conscientious about impact, and interested in coordinating. I feel like that's one of the biggest blockers to all kinds of *long term* progress.\n\nWhat tools do we actually maybe want to build?\n\n* Easily accessible fact checkers / claim refuters  \n  * Powerful AI debaters  \n* Forecasting with explanations: superhuman and at scale  \n* Scientific meta-analysis, systematic review  \n* Ad-hoc / bespoke probabilistic models for individual problems  \n* Bespoke *simulators* / simulation environments  \n* Consistency checking and inconsistency identification  \n  * Arbitrage / market makers in prediction markets?  \n* Using AI to accelerate development of non-AI epistemic tools:  \n  * ?\n\n## Lawrence\n\nThere are already key questions/strategic problems where the truth is murky and we seem to proceed on the basis of limited and biased views. There might well be many more of these in the future. A central example is US-China competition over AI. Some have vibes of \"China is so bad that it's worth risking an existential catastrophe to stop it from capturing a portion of the lightcone\". Others have vibes of \"Cooperation is feasible and desirable, and is our only path to a good outcome\". But how bad is China, really? How much \"realised K\" on this question do those who are making the relevant decisions actually have? Similarly, how feasible is cooperation? How valid is this frame of \"capturing the lightcone\"? One reason that we don't have a better hold on these questions is that they are frighteningly complicated and difficult  – I think we could benefit massively from throwing much more cognitive labour at them, and presenting the findings in a way that is hard to disagree with or for any individual to improve upon, other than at a normative/values level. Maybe AI can get us there.\n\n## Lukas Finnveden\n\nwhy is AI for epistemics important/helpful?\n\n* better epistemics generally very good for making all kinds of decisions  \n* I'm specifically concerned about AI potentially causing an intelligence explosion, and us needing better epistemics to handle that. Specifically:  \n  * I'm worried about AI-driven tech R\\&D leading to catastrophes. To help address this, I want AI capabilities to not only be applied to tech R\\&D, but also to be applied to \"reasoning well from limited evidence\" about a number of questions, including how to deal with risks.  \n  * I'm worried about misaligned AI — so I want techniques for eliciting AI capabilities on all kinds of questions, so that AIs don't have better capabilities than we can elicit from them. (Basically: I want good alignment techniques even on hard-to-evaluate questions.)  \n  * Seems great to me if AI-driven improvements in general knowledge and decision-making are more broadly distributed among humans. (I think this leads to better decision-making without significantly accelerating the dangers from new tech or misaligned AI. Also reduces concentration of power.)\n\n## Abram Demski\n\n\\- Why are we doing this? \\-- impact theory brainstorming exercize  \n\t\\- There's the general claim that epistemics has degraded horribly due to social media and the internet, at least in a lot of ways, although epistemics has also gotten better in some specific ways. It seems very plausible that AI is, and will continue to, make this much much worse (due to deepfakes and super-persuasive writing, to give two examples). We wish to reverse this trend, or at least slow the acceleration.  \n\t\\- The high-level reason why epistemics is important is because epistemics is how we make good decisions. So effectively, this is AI for decision-making. However, if we just set out to make tools for decision-making, I have a sense that this would be worse.  \n\t\t\\- One super important aspect of decision-making is that you have to orient towards the right problem. People do much better when they understand the problem before proposing solutions.  \n\t\t\t\\- I have a sense that producing a better shared understanding of the world JUST WOULD result in better things. The connection between epistemics and better decisionmaking is that strong.  \n\t\t\t\t\\- However, there's an element of coordination to this. You have to have common knowledge, not just knowledge. AI for coordination is yet another thing. AI for group epistemics is close to that, though, and again, I have a sense that making \"AI for coordination\" would produce worse effects on the world than \"AI for group epistemics\".  \n\t\t\\- What specific decisions are we hoping to influence?  \n\t\t\t\\- Decisions around existential risk seem super super important. If AI were generally better at epistemics (as a relative, comparative thing \\-- ie we differentially accelerate epistemics) then key decisionmakers (in the government and at AI companies) would see much more clearly what's coming. Everyone wants a better future, so a better future would have a better chance of happening if we could see and navigate.\n\nI don't think I got any further than the supporting documentation for this workshop in articulating reasons and stuff, so I predict that the LLM summary won't be better than what we already know, although I hope it is. \n\n## Brendan\n\nFor us to live together in a caring and just way, we must make decisions about what is important to us together. Is it okay to tell my neighbour to turn their music down? Do we spend money on roads or trains or libraries? Do we regulate AI development and, if so, how? Do we want to spend public money on PEPFAR, or other avenues for reducing global poverty? To make these decisions in a way that is inclusive and democratic, that fosters buy-in and trust, we must begin with a shared sense of how the world is. This shared sense must be grounded in the physical, economic, and social reality we live in.\n\nAs we live together over wider and wider contexts, maintaining a shared sense of the world becomes more difficult and subtle. These wider contexts are facilitated by new technologies, both physical (eg. global manufacturing bases and supply chains) and informational (eg. social media). New technologies, such as AI for epistemics, and help maintain epistemic coherence across these contexts.\n\nMoreover, such technologies allow new avenues for cooperation at larger scales of human involvement. This unlocks new potential for coherence in global politics, scientific discovery, social and technological innovation, and cultural expression, and ultimately the deep joy of being alive and human in the world. \n\n## Isabel: \n\nThe kinds of being right in the past that are most relevant for decision making aren't just about making straightforward predictions about what will happen, but are more nuanced about what kinds of actions one should take to cause those things to happen, and these seem like somewhat different skills. These are harder to evaluate (e.g. because they're counterfactual: either we listened to person a and then we have to evaluate whether things went better or worse than if we didn't, but hard to say what the counterfactual would have been; or we didn't listen to person a and then we have to guess what things would have been like if we did) \n\nA bit adjacent to Neel Nanda's [recent piece](https://www.lesswrong.com/posts/P5zWiPF5cPJZSkiAK/good-research-takes-are-not-sufficient-for-good-strategic) on how \"good research takes aren't sufficient for good strategic takes\" \\- \"good forecasting takes aren't sufficient for good strategic takes\", but maybe some of this is maybe resolvable on the forecasting tech side \\- can we get better at creating and evaluating results from fuzzier questions? Can AI help? (Also can AI help with the whole problem, but this feels small and concrete enough to maybe get some traction on.) Also, some risk of actors building up good track records in order to be listened to, but then choosing to burn that track record to get power or things like that, something something trust \\- a concern for both human actors and e.g. scheming AIs \\- what things look good until they abruptly aren't? \n\nOn a different note, also interested in takes on what to do about and how to engage with actors who have epistemic processes that don't seem to be truth seeking in a comprehensible-to-me way. \n\n## Adam Marblestone:\n\nWhat do we get out of AI for epistemics?\n\nTrust, is one thing. Our current society seems to be fraught with distrust and motivated reasoning. Can we get a more objectively trustable source of truth? Not only accuracy but with a provenance that gives it an ability to be trusted by humans who are otherwise adversarial. Trust is needed to coordinate. Truthfulness is maybe somewhat necessary but seemingly not sufficient for a source being trusted. As a practical matter, on hot political topics I've been going to OpenAI Deep Research.\n\nBut at a certain level I wonder if AI for epistemics and some form of beneficial AGI could be basically the same thing. I asked Eric Drexler what he wanted for d/acc and he said something that sounds a bit like that…\n\n\"  \nHere's my unsurprising take on directed research for \"defensive acceleration\":  \nDevelopment and steering of epistemic tooling, aka [Large Knowledge Models](https://aiprospects.substack.com/p/large-knowledge-models), offers the highest leverage.\n\nPotential actions:  \n  • Identify and influence entities that could develop and scale necessary foundations.  \n  • Leverage the growing ease of development to take pre-scaling steps.  \n  • Within the scope of \"foundations\", accelerate productive directions for development.  \n  • To support science, ensure grounding of content in cited sources.  \n  • Pursue or promote assembly of appropriate knowledge inputs (beyond what is now used for training data).  \n  • Identify, promote, or implement epistemic tools supported by LKM resources.  \n  • Other kinds of exploration, promotion, preparation, and implementation.  \n  • Ensure that my advice is thoroughly understood.\n\nLKMs can accelerate pretty much everything that one might want, and perhaps tip the balance of applications toward *defensive* acceleration by promoting understanding of the *what and why* of defensive applications.\n\nAddendum:\n\nIn the email I sent a few minutes ago, the basic proposal becomes stronger if one reads \"LKM\" as something like \"large knowledge model functionality\", regardless of implementation.\n\nThe knowledge-model concept (whatever one prefers to call it) should not be tied to my specific technical suggestions. What I've said can be framed as something like \"a discussion of the value, feasibility, and potential scope of a rich epistemic resource\". Bundles of embeddings, etc., are at a level of specificity that may be distracting in the broader context of the meeting.  \n\"\n\nSo that's a strategy assistant AI – which helps humans at large see ways they could act and coordinate especially around big technological or geopolitical strategy issues I guess – but also a certain embodiment of an AGI oracle type system. Maybe the idea then is, how to make a AGI that helps with navigating civilizational strategy, and what is the right form of or interface to that to make it epistemically most useful. \n\nI'd like to also see democratic mechanisms that are simply maximally utilizing the tech we have. \n\n## Elizabeth\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Allowing people to be better informed about experimental medical stuff for themselves.   \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Bargaining? Even just salary negotiations feels like the results are often more the consequence of \"human bargaining skill\" and not the pareto optimal result.    \n  \n\n\n## Saul\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * making better decisions  \n  * in service of that: more useful forecasts, particularly understanding better which things are most relevant to forecast  \n    * in service of that: better group decisionmaking (e.g. unblocking things from vetopower type  \n    * in service of that: more aptly/calibratedly handling waves of information  \n    * In service of that: societal- and individual-scale asymmetric truth-tracking  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * which groups of people have political power  \n  * who's building powerful new tech  \n  * who's regulating powerful new tech  \n  * (etc, other \"decisions\" along these lines — some might not end up getting shaped like decisions, though)  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * better decisions across the board  \n    * better *outcomes* from those decisions; or, like, we can track \"better decisions\" as better-in-expectation outcomes from the decision processes\n\n",
  "reviews": [
    {
      "agentId": "ea-impact-evaluator",
      "createdAt": "2025-04-18T19:41:41.651Z",
      "costInCents": 0,
      "comments": [
        {
          "title": "Framing: Importance of Purposeful Inquiry",
          "description": "The document opens by contrasting 'free-form vibing' with a more rigorous, purpose-driven approach. This framing is valuable, as it encourages participants to focus on impact and tractability, aligning with EA principles. However, the section could be strengthened by explicitly connecting this to concrete outcomes or decision-making frameworks.",
          "importance": 70,
          "grade": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 50,
            "endOffset": 194,
            "quotedText": "Negative: \"we just want to free-form vibe about what cool things we could do\"  \n* Positive: step back & ask \"wait, why are we doing this again?\""
          }
        },
        {
          "title": "Epistemics as Instrumental, Not Terminal Goal",
          "description": "The explicit statement that 'epistemics isn't a goal in-an-of itself' is a strong alignment with EA thinking, which values epistemics for its instrumental role in improving outcomes. This clarity helps avoid misallocation of resources to epistemic work that doesn't translate into impact.",
          "importance": 80,
          "grade": 90,
          "isValid": true,
          "highlight": {
            "startOffset": 239,
            "endOffset": 396,
            "quotedText": "Epistemics isn't a goal in-an-of itself; figuring out what combination of things are possible, achievable, etc.  \n* Why do we care about (AI for) epistemics?"
          }
        },
        {
          "title": "Use Cases: Agentic Web Agents",
          "description": "Dan Schwarz's focus on making 'realized' knowledge 'accessible' via agentic web agents is a concrete and tractable use case. The potential scale is large, but the tractability and neglectedness are less clear. A Fermi estimate of the number of decisions improved per year would strengthen the argument.",
          "importance": 65,
          "grade": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 829,
            "endOffset": 1086,
            "quotedText": "Going from \"Realized\" to \"Accessible\" knowledge, e.g. getting good information using the web. Agentic web agents like Deep Research.\n\n* What decisions may go badly by default, but could go better if we have the right tech?\n\nMany billions of small decisions."
          }
        },
        {
          "title": "Forecasting and Judgment at Scale",
          "description": "The vision of 'superhuman forecasting at scale' and 'reasonable judgment on tricky questions that's too cheap to meter' is ambitious and aligns with high-importance EA cause areas (e.g., x-risk, policy). However, the path to achieving this is not specified, and the risk of overpromising should be considered.",
          "importance": 75,
          "grade": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 1149,
            "endOffset": 1388,
            "quotedText": "Superhuman forecasting at scale.\n\n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nReasonable judgment on tricky questions that's too cheap to meter, better than if you asked a respected colleague to spend many minutes on it."
          }
        },
        {
          "title": "Societal Impact: Governance and Coordination",
          "description": "Aviv's list of potential benefits from AI for epistemics covers a wide range of high-impact domains, including governance, peace, and resource allocation. The breadth is impressive, but the section would benefit from prioritization and explicit Fermi estimates for each domain.",
          "importance": 85,
          "grade": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 1401,
            "endOffset": 2252,
            "quotedText": "Better government governance  \n* Better democratic functioning  \n* Reduced dangerous concentration of power  \n    \n* Healthy polarization / pluralism  \n    \n* More peace, less likelihood of war  \n* Better geopolitics  \n* Better economic incentives  \n    \n* Better AI org governance  \n* Better AI coordination (re. distribution, escalation, etc.)  \n* Better compute allocation (toward what matters for good outcomes)  \n    \n* Better allocation of resources overall  \n* Better lives for people  \n* More effective organizations, getting things done with fewer resources, more focused on the things that benefit everyone  \n    \n* Identification of common ground directions / equilibria / game theoretic strategies that create/incentivize new positive gradients (and not surfacing the bad versions of those…)  \n    \n* Understanding our own goals and values"
          }
        },
        {
          "title": "Intersection of Epistemics and Power Structures",
          "description": "The note that 'it is the intersection of better epistemics / world models + the right power structures / incentives needed for all of these' is a nuanced and important insight. It recognizes that epistemic improvements alone are insufficient without institutional alignment.",
          "importance": 70,
          "grade": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 2255,
            "endOffset": 2389,
            "quotedText": "Note how it is the intersection of better epistemics / world models + the right power structures / incentives needed for all of these."
          }
        },
        {
          "title": "Personal and Societal Decision-Making",
          "description": "Ben Goldhaber highlights both personal and societal decision-making as targets for AI epistemics. This dual focus is valuable, but the section could be improved by estimating the relative impact of interventions at each level (e.g., individual vs. policy).",
          "importance": 60,
          "grade": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 2413,
            "endOffset": 2814,
            "quotedText": "Forecasting how I should live my life in a period of intense technological change  \n* Guiding decision makers to avoid war with china/ai/everyone  \n* Providing the background context for me to sensemake way more sanely.  \n* Pushing forward the extant knowledge of AI safety agendas  \n* Planning for multi-stage projects that could achieve big impact  \n* Roadmapping to build larger more complex things"
          }
        },
        {
          "title": "Raising the Top vs. Bottom of Epistemic Landscape",
          "description": "Chi's distinction between 'raising the top' (elite research) and 'raising the bottom' (societal epistemics) is a useful conceptual innovation. This framing could guide resource allocation, but would benefit from quantitative estimates of the marginal impact of each approach.",
          "importance": 75,
          "grade": 85,
          "isValid": false,
          "highlight": {
            "startOffset": 3536,
            "endOffset": 4546,
            "quotedText": "This is more about raising the **top of the epistemic landscape**\n\n* Other things I care about:  \n  * Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place.  \n  * Vague sense that better **societal epistemics** would help with increasing the **sanity of conversations about AI safety, polarisation around the issue etc.**  \n    * This is broadly about **getting intent alignment** in the first place.\n\n→ These are all more about raising the **bottom of the epistemic landscape**"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Epistemics and Authoritarian Risk",
          "description": "The suggestion that better epistemics could reduce the risk of AI-empowered coups or authoritarianism is highly relevant to x-risk and longtermism. However, the causal pathway is speculative and would benefit from a Fermi estimate of risk reduction.",
          "importance": 80,
          "grade": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 3655,
            "endOffset": 3975,
            "quotedText": "better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**?"
          }
        },
        {
          "title": "Trust and Safe Disagreement",
          "description": "Deger Turan's focus on social trust, safe disagreement, and accountability mechanisms is a strong contribution to the neglectedness and tractability of epistemic interventions. However, the section is somewhat vague and would benefit from concrete examples or pilot projects.",
          "importance": 65,
          "grade": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 4567,
            "endOffset": 5448,
            "quotedText": "Social trust erosion, collective coordination mechanisms are built on a weak assumption  \n* Capacity to entertain truthfulness when it is not instrumental to one's incentive gradient & differing norms upon 'truth will make my sense of security weaker'   \n* Creating contexts where world model divergence can be agreed by both parties enabling a safe disagreement  \n* Improving model reasoning / safer AI by reducing slop, by creating accountability mechanisms through consistency checks & other specifications for healthy epistemics  \n* Decoupling virality with magnitude while living in a short attention span world, curation   \n* I have some confusion re: raising the floor when not all parties are necessarily operating in good faith. I think I need some help think through the ways in which epistemics in the context of incentive misalignment can be used or have defensibility."
          }
        },
        {
          "title": "Counterfactuals: What Would Have Gone Better?",
          "description": "Nathan Young's counterfactuals (e.g., 'No FTX crisis', 'Less zero sum OpenAI board drama') illustrate the potential impact of improved epistemics. This is a compelling narrative device, but the section would be stronger with explicit probability estimates for each scenario.",
          "importance": 70,
          "grade": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 5470,
            "endOffset": 5631,
            "quotedText": "What would we have got:\n\n* Biden not running, perhaps an open dem primary  \n* No FTX crisis  \n* Less zero sum OpenAI board drama  \n* Many smaller, better choices"
          }
        },
        {
          "title": "Policy and Public Information",
          "description": "Lincoln Quirk's emphasis on informing the public and improving policy through better epistemics is well-aligned with EA priorities. The section could be improved by quantifying the expected value of improved policy decisions (e.g., in QALYs or dollars).",
          "importance": 75,
          "grade": 80,
          "isValid": false,
          "highlight": {
            "startOffset": 355,
            "endOffset": 6772,
            "quotedText": "Why do we care about (AI for) epistemics?\n\nWriting prompts from Owen:\n\n* What are use cases for AI for Epistemics that you most care about?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nTemplate:\n\n## Name: \n\nEntry\n\n## Dan Schwarz:\n\n* What are use cases for AI for Epistemics that you most care about?\n\nGoing from \"Realized\" to \"Accessible\" knowledge, e.g. getting good information using the web. Agentic web agents like Deep Research.\n\n* What decisions may go badly by default, but could go better if we have the right tech?\n\nMany billions of small decisions.\n\n* What do the later stages of the path to impact look like?\n\nSuperhuman forecasting at scale.\n\n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nReasonable judgment on tricky questions that's too cheap to meter, better than if you asked a respected colleague to spend many minutes on it.\n\n## Aviv\n\n* Better government governance  \n* Better democratic functioning  \n* Reduced dangerous concentration of power  \n    \n* Healthy polarization / pluralism  \n    \n* More peace, less likelihood of war  \n* Better geopolitics  \n* Better economic incentives  \n    \n* Better AI org governance  \n* Better AI coordination (re. distribution, escalation, etc.)  \n* Better compute allocation (toward what matters for good outcomes)  \n    \n* Better allocation of resources overall  \n* Better lives for people  \n* More effective organizations, getting things done with fewer resources, more focused on the things that benefit everyone  \n    \n* Identification of common ground directions / equilibria / game theoretic strategies that create/incentivize new positive gradients (and not surfacing the bad versions of those…)  \n    \n* Understanding our own goals and values\n\n*Note how it is the intersection of better epistemics / world models + the right power structures / incentives needed for all of these.*\n\n## Ben Goldhaber:\n\n* Forecasting how I should live my life in a period of intense technological change  \n* Guiding decision makers to avoid war with china/ai/everyone  \n* Providing the background context for me to sensemake way more sanely.  \n* Pushing forward the extant knowledge of AI safety agendas  \n* Planning for multi-stage projects that could achieve big impact  \n* Roadmapping to build larger more complex things\n\n## Chi\n\nThere are a few different things I care about:\n\n* I care a lot about humanity and AI, including **super-late-stage AI, getting decision theory** right.  \n  * Consequently but also separately, this relates to the general question of what to do about areas where there is not necessarily a ground truth but that are nonetheless very important to get right, cf**. [metaphilosophy](https://www.alignmentforum.org/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy)** (but also a bunch of other things.)  \n  * This also feels very similar to making **AIs helpful for AI safety research, cf. [automating alignment research](https://joecarlsmith.com/2025/03/14/ai-for-ai-safety#2-what-is-ai-for-ai-safety).**\n\n  → This is more about raising the **top of the epistemic landscape**\n\n* Other things I care about:  \n  * Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place.  \n  * Vague sense that better **societal epistemics** would help with increasing the **sanity of conversations about AI safety, polarisation around the issue etc.**  \n    * This is broadly about **getting intent alignment** in the first place.\n\n→ These are all more about raising the **bottom of the epistemic landscape**\n\n## Deger Turan:\n\n* Social trust erosion, collective coordination mechanisms are built on a weak assumption  \n* Capacity to entertain truthfulness when it is not instrumental to one's incentive gradient & differing norms upon 'truth will make my sense of security weaker'   \n* Creating contexts where world model divergence can be agreed by both parties enabling a safe disagreement  \n* Improving model reasoning / safer AI by reducing slop, by creating accountability mechanisms through consistency checks & other specifications for healthy epistemics  \n* Decoupling virality with magnitude while living in a short attention span world, curation   \n* I have some confusion re: raising the floor when not all parties are necessarily operating in good faith. I think I need some help think through the ways in which epistemics in the context of incentive misalignment can be used or have defensibility. \n\n## Nathan Young: \n\nWhat would we have got:\n\n* Biden not running, perhaps an open dem primary  \n* No FTX crisis  \n* Less zero sum OpenAI board drama  \n* Many smaller, better choices \n\nWhat decisions might go badly:\n\n* Trump tariffs/ global trade  \n* Taiwan/ US-China conflict  \n* US-China competition  \n* US authoritarianism  \n* AI regulation  \n* EU-China realignment  \n* Automated robots  \n* OpenAI strategy  \n* Anthropic strategy  \n* Google strategy  \n* xAI strategy  \n* AI workforce automation  \n* China strategy  \n* Russia strategy  \n* Moskovitz strategy  \n* Must strategy  \n* Bezos Strategy  \n* Zuckerberg strategy  \n* Tallinn strategy  \n* Altman strategy  \n* Drone warfare  \n* Climate change destabilisation  \n* Democratic trust\n\n## Lincoln Quirk:\n\nWhy do we care about (AI for) epistemics?  \n\\- making the public better informed:  \n\t\\- better able to deal with and process the wealth of information  \n\t\\- better able to express their own interests in government (via voting)  \n\t\\- helping cut through social media distraction / intentional misinformation  \n\\- getting governments to implement better policy  \n\t\\- coming up with good policy ideas  \n\t\\- objectively doing cost-benefit analysis  \n\t\\- objectively explaining tradeoffs (vs having to rely on biased humans to explain them)  \n\t\\- flagging bad ideas as bad"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Community Notes and Viewpoint Mapping",
          "description": "Jay's ideas for AI-augmented Community Notes and viewpoint/debate mapping are innovative and tractable. These tools could improve public discourse and reduce polarization, but the risk of misuse (e.g., manipulation) should be considered.",
          "importance": 70,
          "grade": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 7126,
            "endOffset": 7803,
            "quotedText": "How can we build something like the positive version of Community Notes: which statements/posts are agreed on by people who often disagree?  \n  * How can we better understand/summarize what others think, e.g. on social media platforms? E.g. if you see a post recommended to you, is this because it's the most extreme take that went viral and isn't representative, or is this a commonly held view? Between different viewpoints, what are the concrete points of actual disagreement (on facts or values) and what are the things widely agreed upon? Let's build viewpoint/debate maps or a similar sensemaking tool that gets adopted/understood by the masses/raises the knowledge floor"
          }
        },
        {
          "title": "Perfect AI Epistemics: Aspirational but Vague",
          "description": "Ben Wilson's vision of perfect AI epistemics informing all actions is highly ambitious and aligns with longtermist goals. However, the section lacks specificity and does not address potential negative externalities or failure modes.",
          "importance": 65,
          "grade": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 8234,
            "endOffset": 8897,
            "quotedText": "Overall I believe the most important value of epistemics (and AI epistemics) in general is informing what actions allow us to achieve our goals (i.e. make the world better). If we had perfect AI epistemics I would be excited by the ability for us to take a goal, and get a list of possible actions we could take and the forecasted outcomes of each of those actions (plus background information and forecasts for the 'current lay of the land' for context). This can then inform, not just the risk of AI doing good/bad things (as with current forecasting), but what we can do to raise the chances of 'great' AI outcomes, and lower the changes of 'worse' AI outcomes"
          }
        },
        {
          "title": "Integration into Strategic Workflows",
          "description": "Andreas's vision of AI epistemics tools being deeply integrated into strategic organizations is highly tractable and potentially high-impact. The assumption of well-intentioned actors may be optimistic; risks from misuse or adversarial actors should be addressed.",
          "importance": 80,
          "grade": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 9837,
            "endOffset": 10430,
            "quotedText": "AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning  \n  * The technology helps organizations maintain coherence and make sound judgments even as the pace of change accelerates  \n  * Assumption: People are generally well-intentioned \\- no one wants catastrophe \\- and as a result of better tooling things go better in the world overall"
          }
        },
        {
          "title": "World Peace Without World Government",
          "description": "Owen's optimistic scenario of achieving world peace through better epistemic tech is a compelling longtermist vision. However, the causal chain is highly speculative and would benefit from a Fermi estimate of conflict reduction probability.",
          "importance": 85,
          "grade": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 10464,
            "endOffset": 11156,
            "quotedText": "World peace without world government — sufficiently good tech to help countries understand each others' interests, find mutually desirable bargains, and make sufficient credible commitments, could help us get up towards the Pareto frontier where we avoid (virtually) all costly conflicts\n\nGoing slowly and carefully on AI development — rather than having AI progress accelerate at increasingly breakneck speed, helping actors to understand the basic situation and the desirability of different options could be crucial for facilitating meaningful and robust agreements which restrict AI development to the stuff which we have good reasons for being confident are safe // otherwise beneficial."
          }
        },
        {
          "title": "Confusion and Uncertainty in High-Stakes Decisions",
          "description": "Josh Rosenberg's articulation of confusion around AI risk and political crises highlights the importance of epistemic clarity. The section would be improved by estimating the value of information (EVOI) from better forecasts in these domains.",
          "importance": 75,
          "grade": 80,
          "isValid": false,
          "highlight": {
            "startOffset": 11616,
            "endOffset": 12998,
            "quotedText": "I'm massively confused about important problems, e.g. how to orient to AI risk and what to do about it. Some reasonable people think there's a high probability we effectively lose everything we care about in 2027\\. Some reasonable people think it'll be decades till we have \"AGI\" and it'd be good if it arrived sooner. Which actions are good to take in each of these worlds is very different, and I'd like to figure out what world we're in.\n\nThis also applies to many other problem areas.\n\nE.g., Trump's effect on US democracy. How much of a crisis is it? What are the most effective actions to take in response to ensure that the country and communities we care about are as well off as they can be? How can we preserve high-quality institutions to lead the world in positive directions?\n\nOr, on the positive vision side: the \"Abundance\" agenda seems promising. Is it actually a good idea? What do we do about it?\n\nAI for Epistemics could help us get a lot more clarity on how to think about these problems and what actions to take. I'd like much better information and forecasts on the likely effects of Trump, AI timelines, and the pro's and con's of different Abundance policies, for example. If I had a bunch of time and good resources to dig into these problems, I could get a much better sense for myself. I'm hoping AI will help all of us do this sort of thing collectively."
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Automated Debate and Game Theory",
          "description": "Katja Grace's proposal for automated debate and game-theoretic analysis is novel and potentially high-leverage. However, the risk of adversarial manipulation or gaming of such systems should be explicitly considered.",
          "importance": 70,
          "grade": 75,
          "isValid": false,
          "highlight": {
            "startOffset": 13091,
            "endOffset": 14238,
            "quotedText": "Put lots of cognitive effort into very clear and good and compelling case for AI risk  \n  * Automate talking to people about AI risk compellingly, possibly using above case as input  \n  * Good game theoretic thinking re arms races, coordination, what is possible. I.e. People are making dumb assumptions about things, e.g. \"can't slow down tech\". Could automate noting errors. So e.g. when someone says that on Twitter, can highlight it and send in response. \\-\\> good thinking via better weapons for debate (similar to Nathan project)  \n  * Specific idea: very quantified and competitive debate site, that can be cited re which views are winning, where AI does a lot of the cognitive labor in places no people are paying attention, but as a part of it gets cruxy, people pay attention and vote/bet  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Poor decision making around AI: e.g. 'racing' because of vague heuristics instead of ever doing calc on costs and benefits given specific risks  \n  * Failure to coordinate on AI: building AI systems that are dangerous, due to expecting others to do so"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Societal Epistemics and Catastrophic Risk",
          "description": "Joel Lehman's focus on public epistemic health as a lever for reducing catastrophic/existential risk is well-aligned with EA priorities. The section would benefit from a Fermi estimate of the effect size on x-risk.",
          "importance": 80,
          "grade": 80,
          "isValid": false,
          "highlight": {
            "startOffset": 14625,
            "endOffset": 15918,
            "quotedText": "Having weird political leaders and awful public epistemics at critical junctures seems to unnecessarily spike catastrophic/existential risk probabilities. In theory, helping uplevel or at least maintain current public epistemic health (especially as it relates to civic sensemaking) might help influence the quality of decision-makers and concrete decisions made at high-stakes moments  \n  * The shape of the tech is not clear, or probably requires multiple flavors of interventions. E.g. if LLM assistants by default help cultivate civic virtues and on the margin support critical thinking or whatever underlying skills and knowledge help a person up-level their epistemic habits / health (or at least if LLM assistants don't contribute to atrophy/harm); or somehow convincing social media networks to care about this (e.g. community notes but things that go beyond); similarly somehow help w/ important social goods like 'societal trust' and 'trust in your neighbors'  \n* What do the later stages of the path to impact look like?  \n  * \\[didn't get to this\\]  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * Healthier democratic society, better agreement on consensus reality, higher trust in society, better politicians, better concrete decisions in high-stakes moments"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "AI as Evaluator, Oracle, Planner, Representative",
          "description": "Austin Chen's breakdown of AI epistemics roles (Evaluator, Oracle, Planner, Representative) is a useful taxonomy. Each role could be assessed for tractability and impact, with Fermi estimates for each.",
          "importance": 65,
          "grade": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 16005,
            "endOffset": 16468,
            "quotedText": "**Evaluator**: More agreement (or understanding of disagreement) of what orgs & people are doing good work (eg for making grants in AI safety)  \n  * **Oracle**: Instant trustworthy forecasts on any question you ask  \n  * **Planner**: Guidance on \"what are the 3 most important things I should do this day/week/month\"  \n  * **Representative**: Simulate the responses of other parties (people, orgs/corps, governments, LLM agents), for discussions, contracts, trade"
          }
        },
        {
          "title": "Noise, Signal, and Prediction Markets",
          "description": "Janna's focus on improving the signal-to-noise ratio and scaling prediction markets is a concrete and tractable intervention. The section could be improved by estimating the marginal value of improved predictions in economic or QALY terms.",
          "importance": 70,
          "grade": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 16530,
            "endOffset": 17340,
            "quotedText": "Noise to signal ratio could be much higher  \n  * AI (mis)information churned out at the price of almost zero  \n* What are use cases for AI for Epistemics that you most care about?  \n  * Solving prediction markets--is there a way to scale up the bots, even if they are wrong and lossy, to improve prediction market accuracy?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * US-China relations  \n  * Trump tariffs maybe  \n  * Immigration rules in the US  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * More accurate predictions to…  \n    * Reduce business uncertainty  \n    * Make it easier to evaluate life decisions  \n  * Forestalling bad decisions made on imperfect information"
          }
        },
        {
          "title": "Epistemics as Public Good",
          "description": "Owain's observation that epistemic advances are hard to fund via for-profit companies due to public goods issues is a key insight for neglectedness analysis. This supports the case for philanthropic or government funding.",
          "importance": 75,
          "grade": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 18732,
            "endOffset": 18895,
            "quotedText": "Epistemic tech/institutions/advances can be hard to fund via for-profit companies. This is in part due to public goods issues, and issues around markets for ideas."
          }
        },
        {
          "title": "Upgrading the Global Sensemaking Machine",
          "description": "Rafe's metaphor of the 'global sensemaking machine' is evocative and highlights the systemic nature of epistemic challenges. However, the section lacks specificity and would benefit from concrete proposals or pilot interventions.",
          "importance": 65,
          "grade": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 19693,
            "endOffset": 20416,
            "quotedText": "Given that, I suspect that the global sensemaking machine isn't up for the task of thinking about AI well. I hope we can help the global sensemaking machine upgrade itself. I have some hope that we as a society can come up with good ideas for dealing with AI, and also come to see how they make sense, so we can choose them in a robust way.\n\nOne other path to impact: I think a society that understands things better, including itself and its impacts, is one that is more robust in a bunch of ways. A more epistemically-grounded society is also probably a bit richer, and I think a bigger pie creates more slack for arranging things in sanity-promoting ways. I have more hope in a robust society navigating this stuff well."
          }
        },
        {
          "title": "Forecasting as Undersupplied Tool",
          "description": "Dave's claim that forecasting is a 'massively undersupplied' tool is a strong argument for neglectedness. The section could be improved by quantifying the current supply and demand for high-quality forecasts.",
          "importance": 70,
          "grade": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 21276,
            "endOffset": 21525,
            "quotedText": "I think forecasting has the potential to be a pretty powerful tool and there's currently a massive undersupply of good forecasts. I'm excited for us to use the wave of increased AI labor to make better quantitative forecasts to inform key decisions."
          }
        },
        {
          "title": "Raising the Ceiling and Floor",
          "description": "Julian's distinction between 'raising the ceiling' (solving hard expert disagreements) and 'raising the floor' (trustworthy infrastructure) is a valuable conceptual contribution. The section would benefit from Fermi estimates of the impact of each approach.",
          "importance": 75,
          "grade": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 21789,
            "endOffset": 22584,
            "quotedText": "**Raising the ceiling:** can we use AI to organize information and credibly resolve the most challenging and important questions on which experts still seem to disagree? For example:\n\n* COVID origins  \n* Urban planning and public policy  \n* Lots of advancements in basic science: routing around current disagreements and policy challenges with slam-dunk science might be a pretty strong way forward.\n\n**Raising the floor:** can we use AI to build extremely legibly trustworthy/reliable epistemic infrastructure that actually makes its way into people's lives?\n\n* Making truth more palatable (in a way which doesn't translate into making falsehoods more palatable)  \n* Collective epistemics: connecting people to the others with whom they would have the most productive/enlightening conversations"
          }
        },
        {
          "title": "Murky Truths and Cognitive Labor",
          "description": "Lawrence's point about the murkiness of key strategic questions (e.g., US-China AI competition) and the need for more cognitive labor is well-taken. The section could be improved by estimating the value of resolving such uncertainties.",
          "importance": 70,
          "grade": 80,
          "isValid": false,
          "highlight": {
            "startOffset": 23715,
            "endOffset": 24810,
            "quotedText": "There are already key questions/strategic problems where the truth is murky and we seem to proceed on the basis of limited and biased views. There might well be many more of these in the future. A central example is US-China competition over AI. Some have vibes of \"China is so bad that it's worth risking an existential catastrophe to stop it from capturing a portion of the lightcone\". Others have vibes of \"Cooperation is feasible and desirable, and is our only path to a good outcome\". But how bad is China, really? How much \"realised K\" on this question do those who are making the relevant decisions actually have? Similarly, how feasible is cooperation? How valid is this frame of \"capturing the lightcone\"? One reason that we don't have a better hold on these questions is that they are frighteningly complicated and difficult  – I think we could benefit massively from throwing much more cognitive labour at them, and presenting the findings in a way that is hard to disagree with or for any individual to improve upon, other than at a normative/values level. Maybe AI can get us there."
          },
          "error": "Highlight is too long (max 1000 characters)"
        }
      ],
      "thinking": "## Initial Impressions\nThe document is a rich, multi-author brainstorming session on the value, use cases, and theory of impact for 'AI for epistemics'—that is, AI systems designed to improve collective and individual reasoning, forecasting, and decision-making. The content is highly exploratory, with a focus on both raising the 'floor' (broad societal epistemics) and the 'ceiling' (elite or expert-level reasoning).\n\n## Key Themes\n- **Importance of Epistemics for Decision-Making:** Nearly all contributors emphasize that better epistemics (individually and collectively) are instrumental for improved decisions, especially in high-stakes domains like AI governance, geopolitics, and existential risk.\n- **Societal and Institutional Impact:** Many see AI for epistemics as a lever for improving democracy, governance, coordination, and trust—potentially reducing risks of war, authoritarianism, and catastrophic missteps.\n- **Forecasting and Strategic Foresight:** Several authors highlight the value of superhuman forecasting, scenario analysis, and strategic planning, both for individuals and organizations.\n- **Risks and Challenges:** There is awareness of negative externalities—e.g., misuse, epistemic capture, or the risk that improved epistemics could be co-opted by bad actors or lead to value lock-in.\n- **Meta-Reflection:** The session is self-aware, repeatedly asking 'why are we doing this?' and challenging the group to avoid shallow 'vibing' in favor of rigorous impact theory.\n\n## EA Framework Analysis\n### 1. Importance (Score: 9/10)\n- **Scale:** Potentially affects billions of people and the trajectory of civilization, especially if it influences existential risk or global governance.\n- **Severity:** The problems addressed (misinformation, poor decision-making, x-risk) are extremely severe.\n- **Leverage:** High, as interventions could influence key decision-makers and institutions.\n- **Long-term Effects:** Strong implications for the long-term future.\n\n### 2. Neglectedness (Score: 7/10)\n- **Current Attention:** Growing but still relatively neglected compared to its importance, especially outside EA/AI safety circles.\n- **Funding:** Underfunded relative to scale, though interest is increasing.\n- **Informational Gaps:** The session itself is an attempt to clarify the theory of impact, indicating gaps.\n- **Comparative Advantage:** The group is well-positioned, but the field is attracting more attention.\n\n### 3. Tractability (Score: 6/10)\n- **Actionability:** The session is exploratory, not prescriptive; concrete interventions are not specified.\n- **Evidence Base:** Theories are plausible but not yet empirically validated at scale.\n- **Feasibility:** Some interventions (e.g., better forecasting tools) are tractable; others (e.g., societal epistemic upgrades) are much harder.\n- **Scalability:** High in principle, but practical challenges abound.\n\n### 4. Novelty (Score: 7/10)\n- **Key Innovations:** The synthesis of 'AI for epistemics' as a distinct cause area is relatively novel.\n- **Differentiation:** Builds on but extends prior work in forecasting, AI safety, and collective intelligence.\n- **Intellectual Contribution:** Advances the conceptual framing of epistemic tech as a public good.\n- **Methodological Innovation:** Brainstorming format, not methodological per se.\n\n## Fermi Estimate of Expected Impact\n### Parameters\n- **Potentially Affected Population:** Assume 1B people could be meaningfully affected by improved epistemics in the next 20 years.\n- **Magnitude of Improvement:** Suppose improved epistemics reduces the probability of a major global catastrophe (e.g., AI x-risk, war) by 0.1% (1 in 1,000).\n- **Value of Preventing Catastrophe:** Standard EA estimates put the value of preventing existential catastrophe at $10T+ (conservative, could be much higher).\n- **Probability of Success:** Assume a 1% chance that this line of work leads to the necessary breakthroughs and adoption.\n- **Counterfactual Impact:** Assume 50% of the value is counterfactual (i.e., would not happen anyway).\n\n### Calculation\n- **Expected Value = Population x Probability Reduction x Value x Probability of Success x Counterfactual Fraction**\n- = 1 x 0.001 x $10,000,000,000,000 x 0.01 x 0.5\n- = $50,000,000\n\n### Uncertainty Range\n- **Lower Bound (90% CI):** $5M (if only minor improvements, or if adoption is much lower)\n- **Upper Bound (90% CI):** $500M (if probability reduction or adoption is higher, or if value of catastrophe prevention is higher)\n\n### Comparison to EA Benchmarks\n- This is in the range of high-impact research or meta-interventions, but not at the level of a GiveWell top charity per dollar spent (since this is a theory/brainstorming session, not an intervention).\n\n### Negative Effects\n- **Potential for Harm:** If epistemic tools are misused (e.g., for propaganda, value lock-in, or accelerating dangerous tech), net impact could be negative. However, the session is aware of these risks and frames them as key uncertainties.\n\n## Cause Area Analysis\n- **Fits squarely within meta-EA, AI safety, and longtermism.**\n- **Compared to other interventions:** This is foundational work—potentially enabling, but not directly saving lives or averting suffering in the short term.\n- **Cross-cause:** High expected value if it shifts the trajectory of AI or global governance, but high uncertainty.\n\n## Key Value Drivers\n- **Most Impactful:** Raising both the floor and ceiling of epistemic quality, especially for key decision-makers.\n- **Critical Uncertainties:** Adoption, effectiveness, risk of misuse, and whether improved epistemics actually translates to better outcomes.\n- **How to Increase Impact:** Move from theory to concrete interventions, pilot tools, and empirically validate effects.\n\n## Overall Assessment\n- **Strengths:** High importance, thoughtful reflection, awareness of risks, and potential for massive upside.\n- **Limitations:** Lacks concrete action steps, empirical evidence, and clear theory of change beyond brainstorming.\n\n## Grade Justification\n- The session is a valuable meta-level contribution, clarifying the landscape and theory of impact for a potentially transformative cause area. However, as a brainstorming document, its direct expected impact is moderate—enabling further work rather than directly causing change.",
      "summary": "**This document is a high-level, multi-perspective exploration of 'AI for epistemics' as a transformative cause area. Its main value lies in clarifying the landscape, surfacing key uncertainties, and framing the importance of epistemic tech for global decision-making and existential risk. While highly important and novel, its direct impact is limited by its exploratory, non-actionable nature.**",
      "grade": 68
    },
    {
      "agentId": "clarity-coach",
      "createdAt": "2025-04-18T19:42:42.548Z",
      "costInCents": 0,
      "comments": [
        {
          "title": "Fragmented Structure Reduces Coherence",
          "description": "The document is organized as a series of loosely connected individual responses, often in bullet-point form, without a clear overarching narrative or synthesis. This fragmentation makes it difficult for readers to follow the logical progression of ideas or to see how the various perspectives interrelate. Consider adding an introductory summary and thematic transitions between sections to improve structural coherence.",
          "importance": 90,
          "grade": 40,
          "isValid": false,
          "highlight": {
            "startOffset": 19,
            "endOffset": 35816,
            "quotedText": "Preamble/framing from Owen:\n\n* Negative: \"we just want to free-form vibe about what cool things we could do\"  \n* Positive: step back & ask \"wait, why are we doing this again?\"  \n* What stuff does this actually help?  \n* Epistemics isn't a goal in-an-of itself; figuring out what combination of things are possible, achievable, etc.  \n* Why do we care about (AI for) epistemics?\n\nWriting prompts from Owen:\n\n* What are use cases for AI for Epistemics that you most care about?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nTemplate:\n\n## Name: \n\nEntry\n\n## Dan Schwarz:\n\n* What are use cases for AI for Epistemics that you most care about?\n\nGoing from \"Realized\" to \"Accessible\" knowledge, e.g. getting good information using the web. Agentic web agents like Deep Research.\n\n* What decisions may go badly by default, but could go better if we have the right tech?\n\nMany billions of small decisions.\n\n* What do the later stages of the path to impact look like?\n\nSuperhuman forecasting at scale.\n\n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nReasonable judgment on tricky questions that's too cheap to meter, better than if you asked a respected colleague to spend many minutes on it.\n\n## Aviv\n\n* Better government governance  \n* Better democratic functioning  \n* Reduced dangerous concentration of power  \n    \n* Healthy polarization / pluralism  \n    \n* More peace, less likelihood of war  \n* Better geopolitics  \n* Better economic incentives  \n    \n* Better AI org governance  \n* Better AI coordination (re. distribution, escalation, etc.)  \n* Better compute allocation (toward what matters for good outcomes)  \n    \n* Better allocation of resources overall  \n* Better lives for people  \n* More effective organizations, getting things done with fewer resources, more focused on the things that benefit everyone  \n    \n* Identification of common ground directions / equilibria / game theoretic strategies that create/incentivize new positive gradients (and not surfacing the bad versions of those…)  \n    \n* Understanding our own goals and values\n\n*Note how it is the intersection of better epistemics / world models + the right power structures / incentives needed for all of these.*\n\n## Ben Goldhaber:\n\n* Forecasting how I should live my life in a period of intense technological change  \n* Guiding decision makers to avoid war with china/ai/everyone  \n* Providing the background context for me to sensemake way more sanely.  \n* Pushing forward the extant knowledge of AI safety agendas  \n* Planning for multi-stage projects that could achieve big impact  \n* Roadmapping to build larger more complex things\n\n## Chi\n\nThere are a few different things I care about:\n\n* I care a lot about humanity and AI, including **super-late-stage AI, getting decision theory** right.  \n  * Consequently but also separately, this relates to the general question of what to do about areas where there is not necessarily a ground truth but that are nonetheless very important to get right, cf**. [metaphilosophy](https://www.alignmentforum.org/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy)** (but also a bunch of other things.)  \n  * This also feels very similar to making **AIs helpful for AI safety research, cf. [automating alignment research](https://joecarlsmith.com/2025/03/14/ai-for-ai-safety#2-what-is-ai-for-ai-safety).**\n\n  → This is more about raising the **top of the epistemic landscape**\n\n* Other things I care about:  \n  * Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place.  \n  * Vague sense that better **societal epistemics** would help with increasing the **sanity of conversations about AI safety, polarisation around the issue etc.**  \n    * This is broadly about **getting intent alignment** in the first place.\n\n→ These are all more about raising the **bottom of the epistemic landscape**\n\n## Deger Turan:\n\n* Social trust erosion, collective coordination mechanisms are built on a weak assumption  \n* Capacity to entertain truthfulness when it is not instrumental to one's incentive gradient & differing norms upon 'truth will make my sense of security weaker'   \n* Creating contexts where world model divergence can be agreed by both parties enabling a safe disagreement  \n* Improving model reasoning / safer AI by reducing slop, by creating accountability mechanisms through consistency checks & other specifications for healthy epistemics  \n* Decoupling virality with magnitude while living in a short attention span world, curation   \n* I have some confusion re: raising the floor when not all parties are necessarily operating in good faith. I think I need some help think through the ways in which epistemics in the context of incentive misalignment can be used or have defensibility. \n\n## Nathan Young: \n\nWhat would we have got:\n\n* Biden not running, perhaps an open dem primary  \n* No FTX crisis  \n* Less zero sum OpenAI board drama  \n* Many smaller, better choices \n\nWhat decisions might go badly:\n\n* Trump tariffs/ global trade  \n* Taiwan/ US-China conflict  \n* US-China competition  \n* US authoritarianism  \n* AI regulation  \n* EU-China realignment  \n* Automated robots  \n* OpenAI strategy  \n* Anthropic strategy  \n* Google strategy  \n* xAI strategy  \n* AI workforce automation  \n* China strategy  \n* Russia strategy  \n* Moskovitz strategy  \n* Must strategy  \n* Bezos Strategy  \n* Zuckerberg strategy  \n* Tallinn strategy  \n* Altman strategy  \n* Drone warfare  \n* Climate change destabilisation  \n* Democratic trust\n\n## Lincoln Quirk:\n\nWhy do we care about (AI for) epistemics?  \n\\- making the public better informed:  \n\t\\- better able to deal with and process the wealth of information  \n\t\\- better able to express their own interests in government (via voting)  \n\t\\- helping cut through social media distraction / intentional misinformation  \n\\- getting governments to implement better policy  \n\t\\- coming up with good policy ideas  \n\t\\- objectively doing cost-benefit analysis  \n\t\\- objectively explaining tradeoffs (vs having to rely on biased humans to explain them)  \n\t\\- flagging bad ideas as bad  \n\t\\- convincing policymakers with compelling writeups  \n\\- success and flourishing for individuals and orgs using these tools  \n\t\\- via helping them think through decisions  \n\t\\- via clearheaded analysis of trade-offs and cost/benefits  \n\t\\- flagging bad ideas as bad\n\n## Jay\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * How can we build something like the positive version of Community Notes: which statements/posts are agreed on by people who often disagree?  \n  * How can we better understand/summarize what others think, e.g. on social media platforms? E.g. if you see a post recommended to you, is this because it's the most extreme take that went viral and isn't representative, or is this a commonly held view? Between different viewpoints, what are the concrete points of actual disagreement (on facts or values) and what are the things widely agreed upon? Let's build viewpoint/debate maps or a similar sensemaking tool that gets adopted/understood by the masses/raises the knowledge floor (in my experience, things like pol.is are hard for regular users to grok)  \n  * How can we improve Community Notes (or more general knowledge) creation with AI assistants? An AI agent proposing community notes and humans still checking/rating them?  \n  * How can we have something like Community Notes that applies to claims or statements about the future, or which are not concretely knowable?  \n  * \n\n## Ben Wilson (Metaculus)\n\nOverall I believe the most important value of epistemics (and AI epistemics) in general is informing what actions allow us to achieve our goals (i.e. make the world better). If we had perfect AI epistemics I would be excited by the ability for us to take a goal, and get a list of possible actions we could take and the forecasted outcomes of each of those actions (plus background information and forecasts for the 'current lay of the land' for context). This can then inform, not just the risk of AI doing good/bad things (as with current forecasting), but what we can do to raise the chances of 'great' AI outcomes, and lower the changes of 'worse' AI outcomes\n\n## Andreas\n\n* **What are use cases for AI for Epistemics that you most care about?**  \n  * AI policy, broadly construed, must go better  \n    * Avoid gradual disempowerment  \n    * Thoughtful regulation  \n  * Organizational planning & decision-making is weak  \n    * Given an informal long-term goal like improve decision-making for strategic users in government & large orgs, make a plan for a 20-people org that helps them accomplish the goal  \n    * Mapping dependencies, uncertainties, and potential futures  \n    * Understand complex domains with non-trivial feedback loops  \n      * Economic shifts  \n* **What decisions may go badly by default, but could go better if we have the right tech?**  \n  * Resource allocation within orgs  \n    * Investment  \n    * Grant making  \n    * Government budgets  \n  * Policy  \n    * How should AI be deployed in the world?  \n* **What do the later stages of the path to impact look like?**  \n  * AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning  \n  * The technology helps organizations maintain coherence and make sound judgments even as the pace of change accelerates  \n  * Assumption: People are generally well-intentioned \\- no one wants catastrophe \\- and as a result of better tooling things go better in the world overall\n\n## Owen optimistic brainstorm:\n\nWorld peace without world government — sufficiently good tech to help countries understand each others' interests, find mutually desirable bargains, and make sufficient credible commitments, could help us get up towards the Pareto frontier where we avoid (virtually) all costly conflicts\n\nGoing slowly and carefully on AI development — rather than having AI progress accelerate at increasingly breakneck speed, helping actors to understand the basic situation and the desirability of different options could be crucial for facilitating meaningful and robust agreements which restrict AI development to the stuff which we have good reasons for being confident are safe // otherwise beneficial.\n\nBettering democracy — helping people to access key information about what decisions their country might make would actually mean for longer term implications, and having this in turn feed into incentives for decision-makers that better align them with the common good\n\nBuilding safe / aligned AI — better tools for anticipating when systems are actually robustly safe, vs when stronger cases for expectation of safety seem required\n\n## Josh Rosenberg (FRI)\n\nI'm massively confused about important problems, e.g. how to orient to AI risk and what to do about it. Some reasonable people think there's a high probability we effectively lose everything we care about in 2027\\. Some reasonable people think it'll be decades till we have \"AGI\" and it'd be good if it arrived sooner. Which actions are good to take in each of these worlds is very different, and I'd like to figure out what world we're in.\n\nThis also applies to many other problem areas.\n\nE.g., Trump's effect on US democracy. How much of a crisis is it? What are the most effective actions to take in response to ensure that the country and communities we care about are as well off as they can be? How can we preserve high-quality institutions to lead the world in positive directions?\n\nOr, on the positive vision side: the \"Abundance\" agenda seems promising. Is it actually a good idea? What do we do about it?\n\nAI for Epistemics could help us get a lot more clarity on how to think about these problems and what actions to take. I'd like much better information and forecasts on the likely effects of Trump, AI timelines, and the pro's and con's of different Abundance policies, for example. If I had a bunch of time and good resources to dig into these problems, I could get a much better sense for myself. I'm hoping AI will help all of us do this sort of thing collectively.\n\n## Katja Grace\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Put lots of cognitive effort into very clear and good and compelling case for AI risk  \n  * Automate talking to people about AI risk compellingly, possibly using above case as input  \n  * Good game theoretic thinking re arms races, coordination, what is possible. I.e. People are making dumb assumptions about things, e.g. \"can't slow down tech\". Could automate noting errors. So e.g. when someone says that on Twitter, can highlight it and send in response. \\-\\> good thinking via better weapons for debate (similar to Nathan project)  \n  * Specific idea: very quantified and competitive debate site, that can be cited re which views are winning, where AI does a lot of the cognitive labor in places no people are paying attention, but as a part of it gets cruxy, people pay attention and vote/bet  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Poor decision making around AI: e.g. 'racing' because of vague heuristics instead of ever doing calc on costs and benefits given specific risks  \n  * Failure to coordinate on AI: building AI systems that are dangerous, due to expecting others to do so\n\n## Joel Lehman\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Raising the bottom (i.e. helping improve societal epistemics) – in particular that in theory political decisions (at least political decision-makers) are somewhat downstream of societal epistemics  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Having weird political leaders and awful public epistemics at critical junctures seems to unnecessarily spike catastrophic/existential risk probabilities. In theory, helping uplevel or at least maintain current public epistemic health (especially as it relates to civic sensemaking) might help influence the quality of decision-makers and concrete decisions made at high-stakes moments  \n  * The shape of the tech is not clear, or probably requires multiple flavors of interventions. E.g. if LLM assistants by default help cultivate civic virtues and on the margin support critical thinking or whatever underlying skills and knowledge help a person up-level their epistemic habits / health (or at least if LLM assistants don't contribute to atrophy/harm); or somehow convincing social media networks to care about this (e.g. community notes but things that go beyond); similarly somehow help w/ important social goods like 'societal trust' and 'trust in your neighbors'  \n* What do the later stages of the path to impact look like?  \n  * \\[didn't get to this\\]  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * Healthier democratic society, better agreement on consensus reality, higher trust in society, better politicians, better concrete decisions in high-stakes moments \n\n## Austin Chen\n\n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * **Evaluator**: More agreement (or understanding of disagreement) of what orgs & people are doing good work (eg for making grants in AI safety)  \n  * **Oracle**: Instant trustworthy forecasts on any question you ask  \n  * **Planner**: Guidance on \"what are the 3 most important things I should do this day/week/month\"  \n  * **Representative**: Simulate the responses of other parties (people, orgs/corps, governments, LLM agents), for discussions, contracts, trade\n\n## Janna\n\n* Why do we care about (AI for) epistemics?  \n  * Noise to signal ratio could be much higher  \n  * AI (mis)information churned out at the price of almost zero  \n* What are use cases for AI for Epistemics that you most care about?  \n  * Solving prediction markets--is there a way to scale up the bots, even if they are wrong and lossy, to improve prediction market accuracy?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * US-China relations  \n  * Trump tariffs maybe  \n  * Immigration rules in the US  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * More accurate predictions to…  \n    * Reduce business uncertainty  \n    * Make it easier to evaluate life decisions  \n  * Forestalling bad decisions made on imperfect information\n\n## Owain\n\n1. Maybe having humans understanding the world better is good in itself. Lots of people read popular science for this reason. Maybe it's good for AIs as well  \n2. Very broadly, it seems that better world understanding and decision making enables people to make their lives better in myriad ways. (To substantiate this, one could try to separate out the effects of better world understanding from the effects of better realized technology. E.g. Looking at capital-poor countries that can still benefit from scientific knowledge, economics, etc.)  \n3. More specifically, if AI progress is fast, there will be lots of important decisions to be made in quick calendar time. Exploiting AI to improve those decisions seems valuable. Current decision making institutions do not seem well set up to adapt really quickly. (A specific concern is dangers from AI takeover).   \n4. Why now? Why this group of people? We haven't really had tech that could do most epistemic tasks until recently. Now we have AI that can do an increasing proportion of the sub-tasks of epistemic processes. This will allow us to delegate epistemic tasks to AI.  \n   1. It's likely that people do this delegation, even if it's somewhat misguided and potentially bad in the long-term. Can we make this go well?  \n   2. This potentially massively expands our epistemic capacities. How can we use this well?  \n   3. Epistemic tech/institutions/advances can be hard to fund via for-profit companies. This is in part due to public goods issues, and issues around markets for ideas.\n\n## Rafe\n\nAs ever, I hope this will stop advanced artificial intelligence destroying the world, killing all its people \\&c.\n\nSo I think there are a few paths for impact here. I think there is a world where we can make the most productive thinkers about how to do things go further and better; I'm not so excited about a single-player version of this, as I think there's a pretty narrow window when top-level centaur mode works out\\[1\\]\n\nEveryone is already thinking about AI, and, I suspect, this will only get more acute. I think it's pretty hard for people to confront this and think about it carefully and well. It's scary\\! And there are a bunch of entrenched interests and memeplexes that preserve their status better (myopically) if people only have shallow slogans to think about AI with.\n\nGiven that, I suspect that the global sensemaking machine isn't up for the task of thinking about AI well. I hope we can help the global sensemaking machine upgrade itself. I have some hope that we as a society can come up with good ideas for dealing with AI, and also come to see how they make sense, so we can choose them in a robust way.\n\nOne other path to impact: I think a society that understands things better, including itself and its impacts, is one that is more robust in a bunch of ways. A more epistemically-grounded society is also probably a bit richer, and I think a bigger pie creates more slack for arranging things in sanity-promoting ways. I have more hope in a robust society navigating this stuff well.\n\n\\[1\\]:  Yeah, maybe you can do something with shaving off subskills, but I don't think that's really that doable.\n\n## Dave\n\n* With my consequentialist hat, better epistemics being worth it has to route through improved decisions somewhere down the line. The decisions I particularly care about are those by powerful people wrt AI development and implementation and the effects this has on how the future goes  \n  * But I do feel a bit icky when I have this hat on. There seems to be something very intuitively appealing about (a) pursuing better epistemics for its own sake and (b) democratising epistemic tools for the use and benefit of everyone. I think this basically caches out in more 'raising the floor' type work  \n  * I don't feel like I have a very good way of resolving this tension yet, but am excited to see things pushing on both frontiers  \n* I think forecasting has the potential to be a pretty powerful tool and there's currently a massive undersupply of good forecasts. I'm excited for us to use the wave of increased AI labor to make better quantitative forecasts to inform key decisions. I probably care more about quantitative forecasts than most decision makers and am open to being persuaded that the other, fuzzier parts of epistemics are more valuable to work on. \n\n## Julian\n\nWhat are use cases for AI for Epistemics that you most care about?  \n**Raising the ceiling:** can we use AI to organize information and credibly resolve the most challenging and important questions on which experts still seem to disagree? For example:\n\n* COVID origins  \n* Urban planning and public policy  \n* Lots of advancements in basic science: routing around current disagreements and policy challenges with slam-dunk science might be a pretty strong way forward.\n\n**Raising the floor:** can we use AI to build extremely legibly trustworthy/reliable epistemic infrastructure that actually makes its way into people's lives?\n\n* Making truth more palatable (in a way which doesn't translate into making falsehoods more palatable)  \n* Collective epistemics: connecting people to the others with whom they would have the most productive/enlightening conversations\n\nWhat decisions may go badly by default, but could go better if we have the right tech?\n\n* Lots of stuff in public policy: downstream both of better experts (technocrats), better public (voters/interest groups), and better coordination (collective epistemics)\n\nWhat do the later stages of the path to impact look like?\n\n* I want us to live in a world where everyone is more rational, empathetic, considered in their decision making, conscientious about impact, and interested in coordinating. I feel like that's one of the biggest blockers to all kinds of *long term* progress.\n\nWhat tools do we actually maybe want to build?\n\n* Easily accessible fact checkers / claim refuters  \n  * Powerful AI debaters  \n* Forecasting with explanations: superhuman and at scale  \n* Scientific meta-analysis, systematic review  \n* Ad-hoc / bespoke probabilistic models for individual problems  \n* Bespoke *simulators* / simulation environments  \n* Consistency checking and inconsistency identification  \n  * Arbitrage / market makers in prediction markets?  \n* Using AI to accelerate development of non-AI epistemic tools:  \n  * ?\n\n## Lawrence\n\nThere are already key questions/strategic problems where the truth is murky and we seem to proceed on the basis of limited and biased views. There might well be many more of these in the future. A central example is US-China competition over AI. Some have vibes of \"China is so bad that it's worth risking an existential catastrophe to stop it from capturing a portion of the lightcone\". Others have vibes of \"Cooperation is feasible and desirable, and is our only path to a good outcome\". But how bad is China, really? How much \"realised K\" on this question do those who are making the relevant decisions actually have? Similarly, how feasible is cooperation? How valid is this frame of \"capturing the lightcone\"? One reason that we don't have a better hold on these questions is that they are frighteningly complicated and difficult  – I think we could benefit massively from throwing much more cognitive labour at them, and presenting the findings in a way that is hard to disagree with or for any individual to improve upon, other than at a normative/values level. Maybe AI can get us there.\n\n## Lukas Finnveden\n\nwhy is AI for epistemics important/helpful?\n\n* better epistemics generally very good for making all kinds of decisions  \n* I'm specifically concerned about AI potentially causing an intelligence explosion, and us needing better epistemics to handle that. Specifically:  \n  * I'm worried about AI-driven tech R\\&D leading to catastrophes. To help address this, I want AI capabilities to not only be applied to tech R\\&D, but also to be applied to \"reasoning well from limited evidence\" about a number of questions, including how to deal with risks.  \n  * I'm worried about misaligned AI — so I want techniques for eliciting AI capabilities on all kinds of questions, so that AIs don't have better capabilities than we can elicit from them. (Basically: I want good alignment techniques even on hard-to-evaluate questions.)  \n  * Seems great to me if AI-driven improvements in general knowledge and decision-making are more broadly distributed among humans. (I think this leads to better decision-making without significantly accelerating the dangers from new tech or misaligned AI. Also reduces concentration of power.)\n\n## Abram Demski\n\n\\- Why are we doing this? \\-- impact theory brainstorming exercize  \n\t\\- There's the general claim that epistemics has degraded horribly due to social media and the internet, at least in a lot of ways, although epistemics has also gotten better in some specific ways. It seems very plausible that AI is, and will continue to, make this much much worse (due to deepfakes and super-persuasive writing, to give two examples). We wish to reverse this trend, or at least slow the acceleration.  \n\t\\- The high-level reason why epistemics is important is because epistemics is how we make good decisions. So effectively, this is AI for decision-making. However, if we just set out to make tools for decision-making, I have a sense that this would be worse.  \n\t\t\\- One super important aspect of decision-making is that you have to orient towards the right problem. People do much better when they understand the problem before proposing solutions.  \n\t\t\t\\- I have a sense that producing a better shared understanding of the world JUST WOULD result in better things. The connection between epistemics and better decisionmaking is that strong.  \n\t\t\t\t\\- However, there's an element of coordination to this. You have to have common knowledge, not just knowledge. AI for coordination is yet another thing. AI for group epistemics is close to that, though, and again, I have a sense that making \"AI for coordination\" would produce worse effects on the world than \"AI for group epistemics\".  \n\t\t\\- What specific decisions are we hoping to influence?  \n\t\t\t\\- Decisions around existential risk seem super super important. If AI were generally better at epistemics (as a relative, comparative thing \\-- ie we differentially accelerate epistemics) then key decisionmakers (in the government and at AI companies) would see much more clearly what's coming. Everyone wants a better future, so a better future would have a better chance of happening if we could see and navigate.\n\nI don't think I got any further than the supporting documentation for this workshop in articulating reasons and stuff, so I predict that the LLM summary won't be better than what we already know, although I hope it is. \n\n## Brendan\n\nFor us to live together in a caring and just way, we must make decisions about what is important to us together. Is it okay to tell my neighbour to turn their music down? Do we spend money on roads or trains or libraries? Do we regulate AI development and, if so, how? Do we want to spend public money on PEPFAR, or other avenues for reducing global poverty? To make these decisions in a way that is inclusive and democratic, that fosters buy-in and trust, we must begin with a shared sense of how the world is. This shared sense must be grounded in the physical, economic, and social reality we live in.\n\nAs we live together over wider and wider contexts, maintaining a shared sense of the world becomes more difficult and subtle. These wider contexts are facilitated by new technologies, both physical (eg. global manufacturing bases and supply chains) and informational (eg. social media). New technologies, such as AI for epistemics, and help maintain epistemic coherence across these contexts.\n\nMoreover, such technologies allow new avenues for cooperation at larger scales of human involvement. This unlocks new potential for coherence in global politics, scientific discovery, social and technological innovation, and cultural expression, and ultimately the deep joy of being alive and human in the world. \n\n## Isabel: \n\nThe kinds of being right in the past that are most relevant for decision making aren't just about making straightforward predictions about what will happen, but are more nuanced about what kinds of actions one should take to cause those things to happen, and these seem like somewhat different skills. These are harder to evaluate (e.g. because they're counterfactual: either we listened to person a and then we have to evaluate whether things went better or worse than if we didn't, but hard to say what the counterfactual would have been; or we didn't listen to person a and then we have to guess what things would have been like if we did) \n\nA bit adjacent to Neel Nanda's [recent piece](https://www.lesswrong.com/posts/P5zWiPF5cPJZSkiAK/good-research-takes-are-not-sufficient-for-good-strategic) on how \"good research takes aren't sufficient for good strategic takes\" \\- \"good forecasting takes aren't sufficient for good strategic takes\", but maybe some of this is maybe resolvable on the forecasting tech side \\- can we get better at creating and evaluating results from fuzzier questions? Can AI help? (Also can AI help with the whole problem, but this feels small and concrete enough to maybe get some traction on.) Also, some risk of actors building up good track records in order to be listened to, but then choosing to burn that track record to get power or things like that, something something trust \\- a concern for both human actors and e.g. scheming AIs \\- what things look good until they abruptly aren't? \n\nOn a different note, also interested in takes on what to do about and how to engage with actors who have epistemic processes that don't seem to be truth seeking in a comprehensible-to-me way. \n\n## Adam Marblestone:\n\nWhat do we get out of AI for epistemics?\n\nTrust, is one thing. Our current society seems to be fraught with distrust and motivated reasoning. Can we get a more objectively trustable source of truth? Not only accuracy but with a provenance that gives it an ability to be trusted by humans who are otherwise adversarial. Trust is needed to coordinate. Truthfulness is maybe somewhat necessary but seemingly not sufficient for a source being trusted. As a practical matter, on hot political topics I've been going to OpenAI Deep Research.\n\nBut at a certain level I wonder if AI for epistemics and some form of beneficial AGI could be basically the same thing. I asked Eric Drexler what he wanted for d/acc and he said something that sounds a bit like that…\n\n\"  \nHere's my unsurprising take on directed research for \"defensive acceleration\":  \nDevelopment and steering of epistemic tooling, aka [Large Knowledge Models](https://aiprospects.substack.com/p/large-knowledge-models), offers the highest leverage.\n\nPotential actions:  \n  • Identify and influence entities that could develop and scale necessary foundations.  \n  • Leverage the growing ease of development to take pre-scaling steps.  \n  • Within the scope of \"foundations\", accelerate productive directions for development.  \n  • To support science, ensure grounding of content in cited sources.  \n  • Pursue or promote assembly of appropriate knowledge inputs (beyond what is now used for training data).  \n  • Identify, promote, or implement epistemic tools supported by LKM resources.  \n  • Other kinds of exploration, promotion, preparation, and implementation.  \n  • Ensure that my advice is thoroughly understood.\n\nLKMs can accelerate pretty much everything that one might want, and perhaps tip the balance of applications toward *defensive* acceleration by promoting understanding of the *what and why* of defensive applications.\n\nAddendum:\n\nIn the email I sent a few minutes ago, the basic proposal becomes stronger if one reads \"LKM\" as something like \"large knowledge model functionality\", regardless of implementation.\n\nThe knowledge-model concept (whatever one prefers to call it) should not be tied to my specific technical suggestions. What I've said can be framed as something like \"a discussion of the value, feasibility, and potential scope of a rich epistemic resource\". Bundles of embeddings, etc., are at a level of specificity that may be distracting in the broader context of the meeting.  \n\"\n\nSo that's a strategy assistant AI – which helps humans at large see ways they could act and coordinate especially around big technological or geopolitical strategy issues I guess – but also a certain embodiment of an AGI oracle type system. Maybe the idea then is, how to make a AGI that helps with navigating civilizational strategy, and what is the right form of or interface to that to make it epistemically most useful. \n\nI'd like to also see democratic mechanisms that are simply maximally utilizing the tech we have. \n\n## Elizabeth\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Allowing people to be better informed about experimental medical stuff for themselves.   \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Bargaining? Even just salary negotiations feels like the results are often more the consequence of \"human bargaining skill\" and not the pareto optimal result.    \n  \n\n\n## Saul\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * making better decisions  \n  * in service of that: more useful forecasts, particularly understanding better which things are most relevant to forecast  \n    * in service of that: better group decisionmaking (e.g. unblocking things from vetopower type  \n    * in service of that: more aptly/calibratedly handling waves of information  \n    * In service of that: societal- and individual-scale asymmetric truth-tracking  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * which groups of people have political power  \n  * who's building powerful new tech  \n  * who's regulating powerful new tech  \n  * (etc, other \"decisions\" along these lines — some might not end up getting shaped like decisions, though)  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * better decisions across the board  \n    * better *outcomes* from those decisions; or, like, we can track \"better decisions\" as better-in-expectation outcomes from the decision processes"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Inconsistent Use of Formatting and Headings",
          "description": "The document alternates between headings, bullet points, and inline lists, sometimes inconsistently (e.g., '## Name:' vs. '## Dan Schwarz:'). This inconsistency can confuse readers and obscure the information hierarchy. Standardizing section headings and formatting would enhance readability and navigation.",
          "importance": 70,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 724,
            "endOffset": 757,
            "quotedText": "## Name: \n\nEntry\n\n## Dan Schwarz:"
          }
        },
        {
          "title": "Effective Use of Concrete Examples",
          "description": "Many contributors provide concrete, real-world examples (e.g., 'Biden not running', 'No FTX crisis', 'Trump tariffs') to illustrate abstract concepts. This practice grounds the discussion and makes the content more accessible and relatable. Continue to encourage the use of specific examples to clarify complex ideas.",
          "importance": 60,
          "grade": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 5470,
            "endOffset": 5631,
            "quotedText": "What would we have got:\n\n* Biden not running, perhaps an open dem primary  \n* No FTX crisis  \n* Less zero sum OpenAI board drama  \n* Many smaller, better choices"
          }
        },
        {
          "title": "Dense Technical Jargon Without Explanation",
          "description": "Terms like 'epistemics', 'intent alignment', 'Pareto frontier', and 'vetopower' are used frequently without definition or context. This increases the cognitive load for readers unfamiliar with the domain. Consider providing brief explanations or a glossary for specialized terminology.",
          "importance": 80,
          "grade": 40,
          "isValid": false,
          "highlight": {
            "startOffset": 239,
            "endOffset": 3972,
            "quotedText": "Epistemics isn't a goal in-an-of itself; figuring out what combination of things are possible, achievable, etc.  \n* Why do we care about (AI for) epistemics?\n\nWriting prompts from Owen:\n\n* What are use cases for AI for Epistemics that you most care about?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nTemplate:\n\n## Name: \n\nEntry\n\n## Dan Schwarz:\n\n* What are use cases for AI for Epistemics that you most care about?\n\nGoing from \"Realized\" to \"Accessible\" knowledge, e.g. getting good information using the web. Agentic web agents like Deep Research.\n\n* What decisions may go badly by default, but could go better if we have the right tech?\n\nMany billions of small decisions.\n\n* What do the later stages of the path to impact look like?\n\nSuperhuman forecasting at scale.\n\n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nReasonable judgment on tricky questions that's too cheap to meter, better than if you asked a respected colleague to spend many minutes on it.\n\n## Aviv\n\n* Better government governance  \n* Better democratic functioning  \n* Reduced dangerous concentration of power  \n    \n* Healthy polarization / pluralism  \n    \n* More peace, less likelihood of war  \n* Better geopolitics  \n* Better economic incentives  \n    \n* Better AI org governance  \n* Better AI coordination (re. distribution, escalation, etc.)  \n* Better compute allocation (toward what matters for good outcomes)  \n    \n* Better allocation of resources overall  \n* Better lives for people  \n* More effective organizations, getting things done with fewer resources, more focused on the things that benefit everyone  \n    \n* Identification of common ground directions / equilibria / game theoretic strategies that create/incentivize new positive gradients (and not surfacing the bad versions of those…)  \n    \n* Understanding our own goals and values\n\n*Note how it is the intersection of better epistemics / world models + the right power structures / incentives needed for all of these.*\n\n## Ben Goldhaber:\n\n* Forecasting how I should live my life in a period of intense technological change  \n* Guiding decision makers to avoid war with china/ai/everyone  \n* Providing the background context for me to sensemake way more sanely.  \n* Pushing forward the extant knowledge of AI safety agendas  \n* Planning for multi-stage projects that could achieve big impact  \n* Roadmapping to build larger more complex things\n\n## Chi\n\nThere are a few different things I care about:\n\n* I care a lot about humanity and AI, including **super-late-stage AI, getting decision theory** right.  \n  * Consequently but also separately, this relates to the general question of what to do about areas where there is not necessarily a ground truth but that are nonetheless very important to get right, cf**. [metaphilosophy](https://www.alignmentforum.org/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy)** (but also a bunch of other things.)  \n  * This also feels very similar to making **AIs helpful for AI safety research, cf. [automating alignment research](https://joecarlsmith.com/2025/03/14/ai-for-ai-safety#2-what-is-ai-for-ai-safety).**\n\n  → This is more about raising the **top of the epistemic landscape**\n\n* Other things I care about:  \n  * Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Strong Thematic Consistency on AI and Decision-Making",
          "description": "Despite the diversity of voices, the document maintains a strong thematic focus on the intersection of AI, epistemics, and decision-making. This thematic cohesion helps unify the contributions and reinforces the central purpose of the session.",
          "importance": 65,
          "grade": 90,
          "isValid": false,
          "highlight": {
            "startOffset": 355,
            "endOffset": 12649,
            "quotedText": "Why do we care about (AI for) epistemics?\n\nWriting prompts from Owen:\n\n* What are use cases for AI for Epistemics that you most care about?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nTemplate:\n\n## Name: \n\nEntry\n\n## Dan Schwarz:\n\n* What are use cases for AI for Epistemics that you most care about?\n\nGoing from \"Realized\" to \"Accessible\" knowledge, e.g. getting good information using the web. Agentic web agents like Deep Research.\n\n* What decisions may go badly by default, but could go better if we have the right tech?\n\nMany billions of small decisions.\n\n* What do the later stages of the path to impact look like?\n\nSuperhuman forecasting at scale.\n\n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nReasonable judgment on tricky questions that's too cheap to meter, better than if you asked a respected colleague to spend many minutes on it.\n\n## Aviv\n\n* Better government governance  \n* Better democratic functioning  \n* Reduced dangerous concentration of power  \n    \n* Healthy polarization / pluralism  \n    \n* More peace, less likelihood of war  \n* Better geopolitics  \n* Better economic incentives  \n    \n* Better AI org governance  \n* Better AI coordination (re. distribution, escalation, etc.)  \n* Better compute allocation (toward what matters for good outcomes)  \n    \n* Better allocation of resources overall  \n* Better lives for people  \n* More effective organizations, getting things done with fewer resources, more focused on the things that benefit everyone  \n    \n* Identification of common ground directions / equilibria / game theoretic strategies that create/incentivize new positive gradients (and not surfacing the bad versions of those…)  \n    \n* Understanding our own goals and values\n\n*Note how it is the intersection of better epistemics / world models + the right power structures / incentives needed for all of these.*\n\n## Ben Goldhaber:\n\n* Forecasting how I should live my life in a period of intense technological change  \n* Guiding decision makers to avoid war with china/ai/everyone  \n* Providing the background context for me to sensemake way more sanely.  \n* Pushing forward the extant knowledge of AI safety agendas  \n* Planning for multi-stage projects that could achieve big impact  \n* Roadmapping to build larger more complex things\n\n## Chi\n\nThere are a few different things I care about:\n\n* I care a lot about humanity and AI, including **super-late-stage AI, getting decision theory** right.  \n  * Consequently but also separately, this relates to the general question of what to do about areas where there is not necessarily a ground truth but that are nonetheless very important to get right, cf**. [metaphilosophy](https://www.alignmentforum.org/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy)** (but also a bunch of other things.)  \n  * This also feels very similar to making **AIs helpful for AI safety research, cf. [automating alignment research](https://joecarlsmith.com/2025/03/14/ai-for-ai-safety#2-what-is-ai-for-ai-safety).**\n\n  → This is more about raising the **top of the epistemic landscape**\n\n* Other things I care about:  \n  * Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place.  \n  * Vague sense that better **societal epistemics** would help with increasing the **sanity of conversations about AI safety, polarisation around the issue etc.**  \n    * This is broadly about **getting intent alignment** in the first place.\n\n→ These are all more about raising the **bottom of the epistemic landscape**\n\n## Deger Turan:\n\n* Social trust erosion, collective coordination mechanisms are built on a weak assumption  \n* Capacity to entertain truthfulness when it is not instrumental to one's incentive gradient & differing norms upon 'truth will make my sense of security weaker'   \n* Creating contexts where world model divergence can be agreed by both parties enabling a safe disagreement  \n* Improving model reasoning / safer AI by reducing slop, by creating accountability mechanisms through consistency checks & other specifications for healthy epistemics  \n* Decoupling virality with magnitude while living in a short attention span world, curation   \n* I have some confusion re: raising the floor when not all parties are necessarily operating in good faith. I think I need some help think through the ways in which epistemics in the context of incentive misalignment can be used or have defensibility. \n\n## Nathan Young: \n\nWhat would we have got:\n\n* Biden not running, perhaps an open dem primary  \n* No FTX crisis  \n* Less zero sum OpenAI board drama  \n* Many smaller, better choices \n\nWhat decisions might go badly:\n\n* Trump tariffs/ global trade  \n* Taiwan/ US-China conflict  \n* US-China competition  \n* US authoritarianism  \n* AI regulation  \n* EU-China realignment  \n* Automated robots  \n* OpenAI strategy  \n* Anthropic strategy  \n* Google strategy  \n* xAI strategy  \n* AI workforce automation  \n* China strategy  \n* Russia strategy  \n* Moskovitz strategy  \n* Must strategy  \n* Bezos Strategy  \n* Zuckerberg strategy  \n* Tallinn strategy  \n* Altman strategy  \n* Drone warfare  \n* Climate change destabilisation  \n* Democratic trust\n\n## Lincoln Quirk:\n\nWhy do we care about (AI for) epistemics?  \n\\- making the public better informed:  \n\t\\- better able to deal with and process the wealth of information  \n\t\\- better able to express their own interests in government (via voting)  \n\t\\- helping cut through social media distraction / intentional misinformation  \n\\- getting governments to implement better policy  \n\t\\- coming up with good policy ideas  \n\t\\- objectively doing cost-benefit analysis  \n\t\\- objectively explaining tradeoffs (vs having to rely on biased humans to explain them)  \n\t\\- flagging bad ideas as bad  \n\t\\- convincing policymakers with compelling writeups  \n\\- success and flourishing for individuals and orgs using these tools  \n\t\\- via helping them think through decisions  \n\t\\- via clearheaded analysis of trade-offs and cost/benefits  \n\t\\- flagging bad ideas as bad\n\n## Jay\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * How can we build something like the positive version of Community Notes: which statements/posts are agreed on by people who often disagree?  \n  * How can we better understand/summarize what others think, e.g. on social media platforms? E.g. if you see a post recommended to you, is this because it's the most extreme take that went viral and isn't representative, or is this a commonly held view? Between different viewpoints, what are the concrete points of actual disagreement (on facts or values) and what are the things widely agreed upon? Let's build viewpoint/debate maps or a similar sensemaking tool that gets adopted/understood by the masses/raises the knowledge floor (in my experience, things like pol.is are hard for regular users to grok)  \n  * How can we improve Community Notes (or more general knowledge) creation with AI assistants? An AI agent proposing community notes and humans still checking/rating them?  \n  * How can we have something like Community Notes that applies to claims or statements about the future, or which are not concretely knowable?  \n  * \n\n## Ben Wilson (Metaculus)\n\nOverall I believe the most important value of epistemics (and AI epistemics) in general is informing what actions allow us to achieve our goals (i.e. make the world better). If we had perfect AI epistemics I would be excited by the ability for us to take a goal, and get a list of possible actions we could take and the forecasted outcomes of each of those actions (plus background information and forecasts for the 'current lay of the land' for context). This can then inform, not just the risk of AI doing good/bad things (as with current forecasting), but what we can do to raise the chances of 'great' AI outcomes, and lower the changes of 'worse' AI outcomes\n\n## Andreas\n\n* **What are use cases for AI for Epistemics that you most care about?**  \n  * AI policy, broadly construed, must go better  \n    * Avoid gradual disempowerment  \n    * Thoughtful regulation  \n  * Organizational planning & decision-making is weak  \n    * Given an informal long-term goal like improve decision-making for strategic users in government & large orgs, make a plan for a 20-people org that helps them accomplish the goal  \n    * Mapping dependencies, uncertainties, and potential futures  \n    * Understand complex domains with non-trivial feedback loops  \n      * Economic shifts  \n* **What decisions may go badly by default, but could go better if we have the right tech?**  \n  * Resource allocation within orgs  \n    * Investment  \n    * Grant making  \n    * Government budgets  \n  * Policy  \n    * How should AI be deployed in the world?  \n* **What do the later stages of the path to impact look like?**  \n  * AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning  \n  * The technology helps organizations maintain coherence and make sound judgments even as the pace of change accelerates  \n  * Assumption: People are generally well-intentioned \\- no one wants catastrophe \\- and as a result of better tooling things go better in the world overall\n\n## Owen optimistic brainstorm:\n\nWorld peace without world government — sufficiently good tech to help countries understand each others' interests, find mutually desirable bargains, and make sufficient credible commitments, could help us get up towards the Pareto frontier where we avoid (virtually) all costly conflicts\n\nGoing slowly and carefully on AI development — rather than having AI progress accelerate at increasingly breakneck speed, helping actors to understand the basic situation and the desirability of different options could be crucial for facilitating meaningful and robust agreements which restrict AI development to the stuff which we have good reasons for being confident are safe // otherwise beneficial.\n\nBettering democracy — helping people to access key information about what decisions their country might make would actually mean for longer term implications, and having this in turn feed into incentives for decision-makers that better align them with the common good\n\nBuilding safe / aligned AI — better tools for anticipating when systems are actually robustly safe, vs when stronger cases for expectation of safety seem required\n\n## Josh Rosenberg (FRI)\n\nI'm massively confused about important problems, e.g. how to orient to AI risk and what to do about it. Some reasonable people think there's a high probability we effectively lose everything we care about in 2027\\. Some reasonable people think it'll be decades till we have \"AGI\" and it'd be good if it arrived sooner. Which actions are good to take in each of these worlds is very different, and I'd like to figure out what world we're in.\n\nThis also applies to many other problem areas.\n\nE.g., Trump's effect on US democracy. How much of a crisis is it? What are the most effective actions to take in response to ensure that the country and communities we care about are as well off as they can be? How can we preserve high-quality institutions to lead the world in positive directions?\n\nOr, on the positive vision side: the \"Abundance\" agenda seems promising. Is it actually a good idea? What do we do about it?\n\nAI for Epistemics could help us get a lot more clarity on how to think about these problems and what actions to take."
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Excessive Use of Passive Voice",
          "description": "Several passages rely on passive constructions (e.g., 'decisions may go badly by default', 'tools are trusted and regularly used'), which can obscure agency and reduce clarity. Rewriting these sentences in active voice would improve directness and readability.",
          "importance": 55,
          "grade": 45,
          "isValid": false,
          "highlight": {
            "startOffset": 499,
            "endOffset": 10147,
            "quotedText": "What decisions may go badly by default, but could go better if we have the right tech?  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nTemplate:\n\n## Name: \n\nEntry\n\n## Dan Schwarz:\n\n* What are use cases for AI for Epistemics that you most care about?\n\nGoing from \"Realized\" to \"Accessible\" knowledge, e.g. getting good information using the web. Agentic web agents like Deep Research.\n\n* What decisions may go badly by default, but could go better if we have the right tech?\n\nMany billions of small decisions.\n\n* What do the later stages of the path to impact look like?\n\nSuperhuman forecasting at scale.\n\n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nReasonable judgment on tricky questions that's too cheap to meter, better than if you asked a respected colleague to spend many minutes on it.\n\n## Aviv\n\n* Better government governance  \n* Better democratic functioning  \n* Reduced dangerous concentration of power  \n    \n* Healthy polarization / pluralism  \n    \n* More peace, less likelihood of war  \n* Better geopolitics  \n* Better economic incentives  \n    \n* Better AI org governance  \n* Better AI coordination (re. distribution, escalation, etc.)  \n* Better compute allocation (toward what matters for good outcomes)  \n    \n* Better allocation of resources overall  \n* Better lives for people  \n* More effective organizations, getting things done with fewer resources, more focused on the things that benefit everyone  \n    \n* Identification of common ground directions / equilibria / game theoretic strategies that create/incentivize new positive gradients (and not surfacing the bad versions of those…)  \n    \n* Understanding our own goals and values\n\n*Note how it is the intersection of better epistemics / world models + the right power structures / incentives needed for all of these.*\n\n## Ben Goldhaber:\n\n* Forecasting how I should live my life in a period of intense technological change  \n* Guiding decision makers to avoid war with china/ai/everyone  \n* Providing the background context for me to sensemake way more sanely.  \n* Pushing forward the extant knowledge of AI safety agendas  \n* Planning for multi-stage projects that could achieve big impact  \n* Roadmapping to build larger more complex things\n\n## Chi\n\nThere are a few different things I care about:\n\n* I care a lot about humanity and AI, including **super-late-stage AI, getting decision theory** right.  \n  * Consequently but also separately, this relates to the general question of what to do about areas where there is not necessarily a ground truth but that are nonetheless very important to get right, cf**. [metaphilosophy](https://www.alignmentforum.org/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy)** (but also a bunch of other things.)  \n  * This also feels very similar to making **AIs helpful for AI safety research, cf. [automating alignment research](https://joecarlsmith.com/2025/03/14/ai-for-ai-safety#2-what-is-ai-for-ai-safety).**\n\n  → This is more about raising the **top of the epistemic landscape**\n\n* Other things I care about:  \n  * Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place.  \n  * Vague sense that better **societal epistemics** would help with increasing the **sanity of conversations about AI safety, polarisation around the issue etc.**  \n    * This is broadly about **getting intent alignment** in the first place.\n\n→ These are all more about raising the **bottom of the epistemic landscape**\n\n## Deger Turan:\n\n* Social trust erosion, collective coordination mechanisms are built on a weak assumption  \n* Capacity to entertain truthfulness when it is not instrumental to one's incentive gradient & differing norms upon 'truth will make my sense of security weaker'   \n* Creating contexts where world model divergence can be agreed by both parties enabling a safe disagreement  \n* Improving model reasoning / safer AI by reducing slop, by creating accountability mechanisms through consistency checks & other specifications for healthy epistemics  \n* Decoupling virality with magnitude while living in a short attention span world, curation   \n* I have some confusion re: raising the floor when not all parties are necessarily operating in good faith. I think I need some help think through the ways in which epistemics in the context of incentive misalignment can be used or have defensibility. \n\n## Nathan Young: \n\nWhat would we have got:\n\n* Biden not running, perhaps an open dem primary  \n* No FTX crisis  \n* Less zero sum OpenAI board drama  \n* Many smaller, better choices \n\nWhat decisions might go badly:\n\n* Trump tariffs/ global trade  \n* Taiwan/ US-China conflict  \n* US-China competition  \n* US authoritarianism  \n* AI regulation  \n* EU-China realignment  \n* Automated robots  \n* OpenAI strategy  \n* Anthropic strategy  \n* Google strategy  \n* xAI strategy  \n* AI workforce automation  \n* China strategy  \n* Russia strategy  \n* Moskovitz strategy  \n* Must strategy  \n* Bezos Strategy  \n* Zuckerberg strategy  \n* Tallinn strategy  \n* Altman strategy  \n* Drone warfare  \n* Climate change destabilisation  \n* Democratic trust\n\n## Lincoln Quirk:\n\nWhy do we care about (AI for) epistemics?  \n\\- making the public better informed:  \n\t\\- better able to deal with and process the wealth of information  \n\t\\- better able to express their own interests in government (via voting)  \n\t\\- helping cut through social media distraction / intentional misinformation  \n\\- getting governments to implement better policy  \n\t\\- coming up with good policy ideas  \n\t\\- objectively doing cost-benefit analysis  \n\t\\- objectively explaining tradeoffs (vs having to rely on biased humans to explain them)  \n\t\\- flagging bad ideas as bad  \n\t\\- convincing policymakers with compelling writeups  \n\\- success and flourishing for individuals and orgs using these tools  \n\t\\- via helping them think through decisions  \n\t\\- via clearheaded analysis of trade-offs and cost/benefits  \n\t\\- flagging bad ideas as bad\n\n## Jay\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * How can we build something like the positive version of Community Notes: which statements/posts are agreed on by people who often disagree?  \n  * How can we better understand/summarize what others think, e.g. on social media platforms? E.g. if you see a post recommended to you, is this because it's the most extreme take that went viral and isn't representative, or is this a commonly held view? Between different viewpoints, what are the concrete points of actual disagreement (on facts or values) and what are the things widely agreed upon? Let's build viewpoint/debate maps or a similar sensemaking tool that gets adopted/understood by the masses/raises the knowledge floor (in my experience, things like pol.is are hard for regular users to grok)  \n  * How can we improve Community Notes (or more general knowledge) creation with AI assistants? An AI agent proposing community notes and humans still checking/rating them?  \n  * How can we have something like Community Notes that applies to claims or statements about the future, or which are not concretely knowable?  \n  * \n\n## Ben Wilson (Metaculus)\n\nOverall I believe the most important value of epistemics (and AI epistemics) in general is informing what actions allow us to achieve our goals (i.e. make the world better). If we had perfect AI epistemics I would be excited by the ability for us to take a goal, and get a list of possible actions we could take and the forecasted outcomes of each of those actions (plus background information and forecasts for the 'current lay of the land' for context). This can then inform, not just the risk of AI doing good/bad things (as with current forecasting), but what we can do to raise the chances of 'great' AI outcomes, and lower the changes of 'worse' AI outcomes\n\n## Andreas\n\n* **What are use cases for AI for Epistemics that you most care about?**  \n  * AI policy, broadly construed, must go better  \n    * Avoid gradual disempowerment  \n    * Thoughtful regulation  \n  * Organizational planning & decision-making is weak  \n    * Given an informal long-term goal like improve decision-making for strategic users in government & large orgs, make a plan for a 20-people org that helps them accomplish the goal  \n    * Mapping dependencies, uncertainties, and potential futures  \n    * Understand complex domains with non-trivial feedback loops  \n      * Economic shifts  \n* **What decisions may go badly by default, but could go better if we have the right tech?**  \n  * Resource allocation within orgs  \n    * Investment  \n    * Grant making  \n    * Government budgets  \n  * Policy  \n    * How should AI be deployed in the world?  \n* **What do the later stages of the path to impact look like?**  \n  * AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Effective Chunking of Information",
          "description": "The use of bullet points and short paragraphs helps break down complex information into manageable chunks, aiding readability and comprehension. This is especially effective in sections where contributors list use cases or potential outcomes.",
          "importance": 60,
          "grade": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 426,
            "endOffset": 1388,
            "quotedText": "* What are use cases for AI for Epistemics that you most care about?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nTemplate:\n\n## Name: \n\nEntry\n\n## Dan Schwarz:\n\n* What are use cases for AI for Epistemics that you most care about?\n\nGoing from \"Realized\" to \"Accessible\" knowledge, e.g. getting good information using the web. Agentic web agents like Deep Research.\n\n* What decisions may go badly by default, but could go better if we have the right tech?\n\nMany billions of small decisions.\n\n* What do the later stages of the path to impact look like?\n\nSuperhuman forecasting at scale.\n\n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nReasonable judgment on tricky questions that's too cheap to meter, better than if you asked a respected colleague to spend many minutes on it."
          }
        },
        {
          "title": "Occasional Redundancy and Repetition",
          "description": "Some ideas, such as the value of improved decision-making and the risks of poor epistemics, are repeated across multiple sections without significant elaboration. This redundancy can dilute the impact of key points. Consider synthesizing similar contributions or cross-referencing to reduce repetition.",
          "importance": 50,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 35631,
            "endOffset": 35816,
            "quotedText": "better decisions across the board  \n    * better *outcomes* from those decisions; or, like, we can track \"better decisions\" as better-in-expectation outcomes from the decision processes"
          }
        },
        {
          "title": "Lack of Transitional Sentences Between Sections",
          "description": "Transitions between contributors are abrupt, with little connective tissue to guide the reader from one perspective to the next. Adding brief transition statements or summary paragraphs would improve flow and help readers contextualize each contribution.",
          "importance": 65,
          "grade": 40,
          "isValid": true,
          "highlight": {
            "startOffset": 742,
            "endOffset": 1397,
            "quotedText": "## Dan Schwarz:\n\n* What are use cases for AI for Epistemics that you most care about?\n\nGoing from \"Realized\" to \"Accessible\" knowledge, e.g. getting good information using the web. Agentic web agents like Deep Research.\n\n* What decisions may go badly by default, but could go better if we have the right tech?\n\nMany billions of small decisions.\n\n* What do the later stages of the path to impact look like?\n\nSuperhuman forecasting at scale.\n\n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nReasonable judgment on tricky questions that's too cheap to meter, better than if you asked a respected colleague to spend many minutes on it.\n\n## Aviv"
          }
        },
        {
          "title": "Appropriate Use of Qualifiers and Hedging",
          "description": "Contributors often use qualifiers (e.g., 'vague sense', 'seems', 'potentially') to express uncertainty or nuance, which is appropriate given the speculative nature of the topic. This hedging maintains intellectual honesty and avoids overstatement.",
          "importance": 45,
          "grade": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 3638,
            "endOffset": 4036,
            "quotedText": "Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics."
          }
        },
        {
          "title": "Variable Sentence Lengths Aid Readability",
          "description": "The document features a healthy mix of short and long sentences, which helps maintain reader engagement and prevents monotony. This variation is especially effective in sections where complex ideas are introduced and then clarified with shorter follow-up sentences.",
          "importance": 40,
          "grade": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 2824,
            "endOffset": 3601,
            "quotedText": "There are a few different things I care about:\n\n* I care a lot about humanity and AI, including **super-late-stage AI, getting decision theory** right.  \n  * Consequently but also separately, this relates to the general question of what to do about areas where there is not necessarily a ground truth but that are nonetheless very important to get right, cf**. [metaphilosophy](https://www.alignmentforum.org/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy)** (but also a bunch of other things.)  \n  * This also feels very similar to making **AIs helpful for AI safety research, cf. [automating alignment research](https://joecarlsmith.com/2025/03/14/ai-for-ai-safety#2-what-is-ai-for-ai-safety).**\n\n  → This is more about raising the **top of the epistemic landscape**"
          }
        },
        {
          "title": "Occasional Overuse of Abstraction",
          "description": "Some passages remain at a high level of abstraction (e.g., 'raising the top of the epistemic landscape') without concrete examples or actionable recommendations. This can make it harder for readers to grasp the practical implications. Adding specific scenarios or case studies would enhance clarity.",
          "importance": 55,
          "grade": 50,
          "isValid": false,
          "highlight": {
            "startOffset": 3536,
            "endOffset": 4546,
            "quotedText": "This is more about raising the **top of the epistemic landscape**\n\n* Other things I care about:  \n  * Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place.  \n  * Vague sense that better **societal epistemics** would help with increasing the **sanity of conversations about AI safety, polarisation around the issue etc.**  \n    * This is broadly about **getting intent alignment** in the first place.\n\n→ These are all more about raising the **bottom of the epistemic landscape**"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Strong Audience Alignment for Expert Readers",
          "description": "The document assumes a high level of prior knowledge about AI, epistemics, and related policy debates. For an expert audience, this is appropriate and allows for deeper discussion. However, for broader audiences, additional scaffolding would be needed.",
          "importance": 60,
          "grade": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 19,
            "endOffset": 350,
            "quotedText": "Preamble/framing from Owen:\n\n* Negative: \"we just want to free-form vibe about what cool things we could do\"  \n* Positive: step back & ask \"wait, why are we doing this again?\"  \n* What stuff does this actually help?  \n* Epistemics isn't a goal in-an-of itself; figuring out what combination of things are possible, achievable, etc."
          }
        },
        {
          "title": "Effective Use of Signposting in Some Sections",
          "description": "Some contributors use clear signposting (e.g., 'What are use cases for AI for Epistemics that you most care about?') to orient the reader and structure their responses. This practice aids navigation and comprehension.",
          "importance": 50,
          "grade": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 426,
            "endOffset": 585,
            "quotedText": "* What are use cases for AI for Epistemics that you most care about?  \n* What decisions may go badly by default, but could go better if we have the right tech?"
          }
        },
        {
          "title": "Occasional Verbosity and Circumlocution",
          "description": "Some responses are wordy or circuitous, using more words than necessary to convey a point (e.g., 'I have some confusion re: raising the floor when not all parties are necessarily operating in good faith. I think I need some help think through the ways in which epistemics in the context of incentive misalignment can be used or have defensibility.'). Streamlining these sentences would improve concision.",
          "importance": 45,
          "grade": 40,
          "isValid": true,
          "highlight": {
            "startOffset": 5199,
            "endOffset": 5448,
            "quotedText": "I have some confusion re: raising the floor when not all parties are necessarily operating in good faith. I think I need some help think through the ways in which epistemics in the context of incentive misalignment can be used or have defensibility."
          }
        },
        {
          "title": "Clear Articulation of Tensions and Open Questions",
          "description": "Several contributors explicitly state tensions or unresolved questions (e.g., 'I don't feel like I have a very good way of resolving this tension yet, but am excited to see things pushing on both frontiers'). This transparency about uncertainty is a strength, fostering intellectual honesty.",
          "importance": 40,
          "grade": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 21145,
            "endOffset": 21706,
            "quotedText": "I don't feel like I have a very good way of resolving this tension yet, but am excited to see things pushing on both frontiers  \n* I think forecasting has the potential to be a pretty powerful tool and there's currently a massive undersupply of good forecasts. I'm excited for us to use the wave of increased AI labor to make better quantitative forecasts to inform key decisions. I probably care more about quantitative forecasts than most decision makers and am open to being persuaded that the other, fuzzier parts of epistemics are more valuable to work on."
          }
        },
        {
          "title": "Lack of Summary or Synthesis",
          "description": "The document ends abruptly without a summary or synthesis of the key themes and insights. Adding a concluding section that distills the main takeaways would greatly enhance coherence and provide closure for the reader.",
          "importance": 85,
          "grade": 30,
          "isValid": true,
          "highlight": {
            "startOffset": 35758,
            "endOffset": 35816,
            "quotedText": "better-in-expectation outcomes from the decision processes"
          }
        },
        {
          "title": "Occasional Use of Colloquial Language",
          "description": "Some sections use informal or conversational language (e.g., 'we just want to free-form vibe about what cool things we could do', 'I feel a bit icky when I have this hat on'), which can make the text more engaging but may reduce perceived professionalism. Consider the intended audience when choosing tone.",
          "importance": 35,
          "grade": 60,
          "isValid": false,
          "highlight": {
            "startOffset": 50,
            "endOffset": 20882,
            "quotedText": "Negative: \"we just want to free-form vibe about what cool things we could do\"  \n* Positive: step back & ask \"wait, why are we doing this again?\"  \n* What stuff does this actually help?  \n* Epistemics isn't a goal in-an-of itself; figuring out what combination of things are possible, achievable, etc.  \n* Why do we care about (AI for) epistemics?\n\nWriting prompts from Owen:\n\n* What are use cases for AI for Epistemics that you most care about?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nTemplate:\n\n## Name: \n\nEntry\n\n## Dan Schwarz:\n\n* What are use cases for AI for Epistemics that you most care about?\n\nGoing from \"Realized\" to \"Accessible\" knowledge, e.g. getting good information using the web. Agentic web agents like Deep Research.\n\n* What decisions may go badly by default, but could go better if we have the right tech?\n\nMany billions of small decisions.\n\n* What do the later stages of the path to impact look like?\n\nSuperhuman forecasting at scale.\n\n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nReasonable judgment on tricky questions that's too cheap to meter, better than if you asked a respected colleague to spend many minutes on it.\n\n## Aviv\n\n* Better government governance  \n* Better democratic functioning  \n* Reduced dangerous concentration of power  \n    \n* Healthy polarization / pluralism  \n    \n* More peace, less likelihood of war  \n* Better geopolitics  \n* Better economic incentives  \n    \n* Better AI org governance  \n* Better AI coordination (re. distribution, escalation, etc.)  \n* Better compute allocation (toward what matters for good outcomes)  \n    \n* Better allocation of resources overall  \n* Better lives for people  \n* More effective organizations, getting things done with fewer resources, more focused on the things that benefit everyone  \n    \n* Identification of common ground directions / equilibria / game theoretic strategies that create/incentivize new positive gradients (and not surfacing the bad versions of those…)  \n    \n* Understanding our own goals and values\n\n*Note how it is the intersection of better epistemics / world models + the right power structures / incentives needed for all of these.*\n\n## Ben Goldhaber:\n\n* Forecasting how I should live my life in a period of intense technological change  \n* Guiding decision makers to avoid war with china/ai/everyone  \n* Providing the background context for me to sensemake way more sanely.  \n* Pushing forward the extant knowledge of AI safety agendas  \n* Planning for multi-stage projects that could achieve big impact  \n* Roadmapping to build larger more complex things\n\n## Chi\n\nThere are a few different things I care about:\n\n* I care a lot about humanity and AI, including **super-late-stage AI, getting decision theory** right.  \n  * Consequently but also separately, this relates to the general question of what to do about areas where there is not necessarily a ground truth but that are nonetheless very important to get right, cf**. [metaphilosophy](https://www.alignmentforum.org/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy)** (but also a bunch of other things.)  \n  * This also feels very similar to making **AIs helpful for AI safety research, cf. [automating alignment research](https://joecarlsmith.com/2025/03/14/ai-for-ai-safety#2-what-is-ai-for-ai-safety).**\n\n  → This is more about raising the **top of the epistemic landscape**\n\n* Other things I care about:  \n  * Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place.  \n  * Vague sense that better **societal epistemics** would help with increasing the **sanity of conversations about AI safety, polarisation around the issue etc.**  \n    * This is broadly about **getting intent alignment** in the first place.\n\n→ These are all more about raising the **bottom of the epistemic landscape**\n\n## Deger Turan:\n\n* Social trust erosion, collective coordination mechanisms are built on a weak assumption  \n* Capacity to entertain truthfulness when it is not instrumental to one's incentive gradient & differing norms upon 'truth will make my sense of security weaker'   \n* Creating contexts where world model divergence can be agreed by both parties enabling a safe disagreement  \n* Improving model reasoning / safer AI by reducing slop, by creating accountability mechanisms through consistency checks & other specifications for healthy epistemics  \n* Decoupling virality with magnitude while living in a short attention span world, curation   \n* I have some confusion re: raising the floor when not all parties are necessarily operating in good faith. I think I need some help think through the ways in which epistemics in the context of incentive misalignment can be used or have defensibility. \n\n## Nathan Young: \n\nWhat would we have got:\n\n* Biden not running, perhaps an open dem primary  \n* No FTX crisis  \n* Less zero sum OpenAI board drama  \n* Many smaller, better choices \n\nWhat decisions might go badly:\n\n* Trump tariffs/ global trade  \n* Taiwan/ US-China conflict  \n* US-China competition  \n* US authoritarianism  \n* AI regulation  \n* EU-China realignment  \n* Automated robots  \n* OpenAI strategy  \n* Anthropic strategy  \n* Google strategy  \n* xAI strategy  \n* AI workforce automation  \n* China strategy  \n* Russia strategy  \n* Moskovitz strategy  \n* Must strategy  \n* Bezos Strategy  \n* Zuckerberg strategy  \n* Tallinn strategy  \n* Altman strategy  \n* Drone warfare  \n* Climate change destabilisation  \n* Democratic trust\n\n## Lincoln Quirk:\n\nWhy do we care about (AI for) epistemics?  \n\\- making the public better informed:  \n\t\\- better able to deal with and process the wealth of information  \n\t\\- better able to express their own interests in government (via voting)  \n\t\\- helping cut through social media distraction / intentional misinformation  \n\\- getting governments to implement better policy  \n\t\\- coming up with good policy ideas  \n\t\\- objectively doing cost-benefit analysis  \n\t\\- objectively explaining tradeoffs (vs having to rely on biased humans to explain them)  \n\t\\- flagging bad ideas as bad  \n\t\\- convincing policymakers with compelling writeups  \n\\- success and flourishing for individuals and orgs using these tools  \n\t\\- via helping them think through decisions  \n\t\\- via clearheaded analysis of trade-offs and cost/benefits  \n\t\\- flagging bad ideas as bad\n\n## Jay\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * How can we build something like the positive version of Community Notes: which statements/posts are agreed on by people who often disagree?  \n  * How can we better understand/summarize what others think, e.g. on social media platforms? E.g. if you see a post recommended to you, is this because it's the most extreme take that went viral and isn't representative, or is this a commonly held view? Between different viewpoints, what are the concrete points of actual disagreement (on facts or values) and what are the things widely agreed upon? Let's build viewpoint/debate maps or a similar sensemaking tool that gets adopted/understood by the masses/raises the knowledge floor (in my experience, things like pol.is are hard for regular users to grok)  \n  * How can we improve Community Notes (or more general knowledge) creation with AI assistants? An AI agent proposing community notes and humans still checking/rating them?  \n  * How can we have something like Community Notes that applies to claims or statements about the future, or which are not concretely knowable?  \n  * \n\n## Ben Wilson (Metaculus)\n\nOverall I believe the most important value of epistemics (and AI epistemics) in general is informing what actions allow us to achieve our goals (i.e. make the world better). If we had perfect AI epistemics I would be excited by the ability for us to take a goal, and get a list of possible actions we could take and the forecasted outcomes of each of those actions (plus background information and forecasts for the 'current lay of the land' for context). This can then inform, not just the risk of AI doing good/bad things (as with current forecasting), but what we can do to raise the chances of 'great' AI outcomes, and lower the changes of 'worse' AI outcomes\n\n## Andreas\n\n* **What are use cases for AI for Epistemics that you most care about?**  \n  * AI policy, broadly construed, must go better  \n    * Avoid gradual disempowerment  \n    * Thoughtful regulation  \n  * Organizational planning & decision-making is weak  \n    * Given an informal long-term goal like improve decision-making for strategic users in government & large orgs, make a plan for a 20-people org that helps them accomplish the goal  \n    * Mapping dependencies, uncertainties, and potential futures  \n    * Understand complex domains with non-trivial feedback loops  \n      * Economic shifts  \n* **What decisions may go badly by default, but could go better if we have the right tech?**  \n  * Resource allocation within orgs  \n    * Investment  \n    * Grant making  \n    * Government budgets  \n  * Policy  \n    * How should AI be deployed in the world?  \n* **What do the later stages of the path to impact look like?**  \n  * AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning  \n  * The technology helps organizations maintain coherence and make sound judgments even as the pace of change accelerates  \n  * Assumption: People are generally well-intentioned \\- no one wants catastrophe \\- and as a result of better tooling things go better in the world overall\n\n## Owen optimistic brainstorm:\n\nWorld peace without world government — sufficiently good tech to help countries understand each others' interests, find mutually desirable bargains, and make sufficient credible commitments, could help us get up towards the Pareto frontier where we avoid (virtually) all costly conflicts\n\nGoing slowly and carefully on AI development — rather than having AI progress accelerate at increasingly breakneck speed, helping actors to understand the basic situation and the desirability of different options could be crucial for facilitating meaningful and robust agreements which restrict AI development to the stuff which we have good reasons for being confident are safe // otherwise beneficial.\n\nBettering democracy — helping people to access key information about what decisions their country might make would actually mean for longer term implications, and having this in turn feed into incentives for decision-makers that better align them with the common good\n\nBuilding safe / aligned AI — better tools for anticipating when systems are actually robustly safe, vs when stronger cases for expectation of safety seem required\n\n## Josh Rosenberg (FRI)\n\nI'm massively confused about important problems, e.g. how to orient to AI risk and what to do about it. Some reasonable people think there's a high probability we effectively lose everything we care about in 2027\\. Some reasonable people think it'll be decades till we have \"AGI\" and it'd be good if it arrived sooner. Which actions are good to take in each of these worlds is very different, and I'd like to figure out what world we're in.\n\nThis also applies to many other problem areas.\n\nE.g., Trump's effect on US democracy. How much of a crisis is it? What are the most effective actions to take in response to ensure that the country and communities we care about are as well off as they can be? How can we preserve high-quality institutions to lead the world in positive directions?\n\nOr, on the positive vision side: the \"Abundance\" agenda seems promising. Is it actually a good idea? What do we do about it?\n\nAI for Epistemics could help us get a lot more clarity on how to think about these problems and what actions to take. I'd like much better information and forecasts on the likely effects of Trump, AI timelines, and the pro's and con's of different Abundance policies, for example. If I had a bunch of time and good resources to dig into these problems, I could get a much better sense for myself. I'm hoping AI will help all of us do this sort of thing collectively.\n\n## Katja Grace\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Put lots of cognitive effort into very clear and good and compelling case for AI risk  \n  * Automate talking to people about AI risk compellingly, possibly using above case as input  \n  * Good game theoretic thinking re arms races, coordination, what is possible. I.e. People are making dumb assumptions about things, e.g. \"can't slow down tech\". Could automate noting errors. So e.g. when someone says that on Twitter, can highlight it and send in response. \\-\\> good thinking via better weapons for debate (similar to Nathan project)  \n  * Specific idea: very quantified and competitive debate site, that can be cited re which views are winning, where AI does a lot of the cognitive labor in places no people are paying attention, but as a part of it gets cruxy, people pay attention and vote/bet  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Poor decision making around AI: e.g. 'racing' because of vague heuristics instead of ever doing calc on costs and benefits given specific risks  \n  * Failure to coordinate on AI: building AI systems that are dangerous, due to expecting others to do so\n\n## Joel Lehman\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Raising the bottom (i.e. helping improve societal epistemics) – in particular that in theory political decisions (at least political decision-makers) are somewhat downstream of societal epistemics  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Having weird political leaders and awful public epistemics at critical junctures seems to unnecessarily spike catastrophic/existential risk probabilities. In theory, helping uplevel or at least maintain current public epistemic health (especially as it relates to civic sensemaking) might help influence the quality of decision-makers and concrete decisions made at high-stakes moments  \n  * The shape of the tech is not clear, or probably requires multiple flavors of interventions. E.g. if LLM assistants by default help cultivate civic virtues and on the margin support critical thinking or whatever underlying skills and knowledge help a person up-level their epistemic habits / health (or at least if LLM assistants don't contribute to atrophy/harm); or somehow convincing social media networks to care about this (e.g. community notes but things that go beyond); similarly somehow help w/ important social goods like 'societal trust' and 'trust in your neighbors'  \n* What do the later stages of the path to impact look like?  \n  * \\[didn't get to this\\]  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * Healthier democratic society, better agreement on consensus reality, higher trust in society, better politicians, better concrete decisions in high-stakes moments \n\n## Austin Chen\n\n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * **Evaluator**: More agreement (or understanding of disagreement) of what orgs & people are doing good work (eg for making grants in AI safety)  \n  * **Oracle**: Instant trustworthy forecasts on any question you ask  \n  * **Planner**: Guidance on \"what are the 3 most important things I should do this day/week/month\"  \n  * **Representative**: Simulate the responses of other parties (people, orgs/corps, governments, LLM agents), for discussions, contracts, trade\n\n## Janna\n\n* Why do we care about (AI for) epistemics?  \n  * Noise to signal ratio could be much higher  \n  * AI (mis)information churned out at the price of almost zero  \n* What are use cases for AI for Epistemics that you most care about?  \n  * Solving prediction markets--is there a way to scale up the bots, even if they are wrong and lossy, to improve prediction market accuracy?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * US-China relations  \n  * Trump tariffs maybe  \n  * Immigration rules in the US  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * More accurate predictions to…  \n    * Reduce business uncertainty  \n    * Make it easier to evaluate life decisions  \n  * Forestalling bad decisions made on imperfect information\n\n## Owain\n\n1. Maybe having humans understanding the world better is good in itself. Lots of people read popular science for this reason. Maybe it's good for AIs as well  \n2. Very broadly, it seems that better world understanding and decision making enables people to make their lives better in myriad ways. (To substantiate this, one could try to separate out the effects of better world understanding from the effects of better realized technology. E.g. Looking at capital-poor countries that can still benefit from scientific knowledge, economics, etc.)  \n3. More specifically, if AI progress is fast, there will be lots of important decisions to be made in quick calendar time. Exploiting AI to improve those decisions seems valuable. Current decision making institutions do not seem well set up to adapt really quickly. (A specific concern is dangers from AI takeover).   \n4. Why now? Why this group of people? We haven't really had tech that could do most epistemic tasks until recently. Now we have AI that can do an increasing proportion of the sub-tasks of epistemic processes. This will allow us to delegate epistemic tasks to AI.  \n   1. It's likely that people do this delegation, even if it's somewhat misguided and potentially bad in the long-term. Can we make this go well?  \n   2. This potentially massively expands our epistemic capacities. How can we use this well?  \n   3. Epistemic tech/institutions/advances can be hard to fund via for-profit companies. This is in part due to public goods issues, and issues around markets for ideas.\n\n## Rafe\n\nAs ever, I hope this will stop advanced artificial intelligence destroying the world, killing all its people \\&c.\n\nSo I think there are a few paths for impact here. I think there is a world where we can make the most productive thinkers about how to do things go further and better; I'm not so excited about a single-player version of this, as I think there's a pretty narrow window when top-level centaur mode works out\\[1\\]\n\nEveryone is already thinking about AI, and, I suspect, this will only get more acute. I think it's pretty hard for people to confront this and think about it carefully and well. It's scary\\! And there are a bunch of entrenched interests and memeplexes that preserve their status better (myopically) if people only have shallow slogans to think about AI with.\n\nGiven that, I suspect that the global sensemaking machine isn't up for the task of thinking about AI well. I hope we can help the global sensemaking machine upgrade itself. I have some hope that we as a society can come up with good ideas for dealing with AI, and also come to see how they make sense, so we can choose them in a robust way.\n\nOne other path to impact: I think a society that understands things better, including itself and its impacts, is one that is more robust in a bunch of ways. A more epistemically-grounded society is also probably a bit richer, and I think a bigger pie creates more slack for arranging things in sanity-promoting ways. I have more hope in a robust society navigating this stuff well.\n\n\\[1\\]:  Yeah, maybe you can do something with shaving off subskills, but I don't think that's really that doable.\n\n## Dave\n\n* With my consequentialist hat, better epistemics being worth it has to route through improved decisions somewhere down the line. The decisions I particularly care about are those by powerful people wrt AI development and implementation and the effects this has on how the future goes  \n  * But I do feel a bit icky when I have this hat on."
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Strong Use of Analogies and Metaphors",
          "description": "Analogies such as 'raising the floor' and 'raising the ceiling' are used effectively to convey complex ideas about epistemic improvement. These metaphors help readers conceptualize abstract goals in more tangible terms.",
          "importance": 50,
          "grade": 90,
          "isValid": true,
          "highlight": {
            "startOffset": 21789,
            "endOffset": 22348,
            "quotedText": "**Raising the ceiling:** can we use AI to organize information and credibly resolve the most challenging and important questions on which experts still seem to disagree? For example:\n\n* COVID origins  \n* Urban planning and public policy  \n* Lots of advancements in basic science: routing around current disagreements and policy challenges with slam-dunk science might be a pretty strong way forward.\n\n**Raising the floor:** can we use AI to build extremely legibly trustworthy/reliable epistemic infrastructure that actually makes its way into people's lives?"
          }
        },
        {
          "title": "Occasional Lack of Medium-Appropriate Formatting",
          "description": "Some sections include markdown links and formatting (e.g., '[metaphilosophy](https://www.alignmentforum.org/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy)') that may not render correctly in all contexts. Ensure formatting is appropriate for the intended publication medium.",
          "importance": 30,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 3179,
            "endOffset": 3530,
            "quotedText": "cf**. [metaphilosophy](https://www.alignmentforum.org/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy)** (but also a bunch of other things.)  \n  * This also feels very similar to making **AIs helpful for AI safety research, cf. [automating alignment research](https://joecarlsmith.com/2025/03/14/ai-for-ai-safety#2-what-is-ai-for-ai-safety).**"
          }
        },
        {
          "title": "Effective Use of Parenthetical Clarifications",
          "description": "Parenthetical statements (e.g., 'e.g., government agencies, major labs, influential industry players') are used to clarify or elaborate on general terms, aiding reader understanding without disrupting the flow.",
          "importance": 35,
          "grade": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 9837,
            "endOffset": 10012,
            "quotedText": "AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)"
          }
        },
        {
          "title": "Occasional Ambiguity in Pronoun Reference",
          "description": "Some sentences use pronouns ('this', 'it', 'they') without clear antecedents, which can cause confusion. Clarifying these references would improve precision.",
          "importance": 40,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 12058,
            "endOffset": 12928,
            "quotedText": "This also applies to many other problem areas.\n\nE.g., Trump's effect on US democracy. How much of a crisis is it? What are the most effective actions to take in response to ensure that the country and communities we care about are as well off as they can be? How can we preserve high-quality institutions to lead the world in positive directions?\n\nOr, on the positive vision side: the \"Abundance\" agenda seems promising. Is it actually a good idea? What do we do about it?\n\nAI for Epistemics could help us get a lot more clarity on how to think about these problems and what actions to take. I'd like much better information and forecasts on the likely effects of Trump, AI timelines, and the pro's and con's of different Abundance policies, for example. If I had a bunch of time and good resources to dig into these problems, I could get a much better sense for myself."
          }
        },
        {
          "title": "Strong Emphasis on Actionable Outcomes",
          "description": "Many sections focus on concrete outcomes and actionable goals (e.g., 'better decisions across the board', 'more accurate predictions to reduce business uncertainty'), which enhances the practical relevance of the discussion.",
          "importance": 60,
          "grade": 90,
          "isValid": false,
          "highlight": {
            "startOffset": 650,
            "endOffset": 35816,
            "quotedText": "* \"As a result of better AI for epistemics tech, we get: […]\"\n\nTemplate:\n\n## Name: \n\nEntry\n\n## Dan Schwarz:\n\n* What are use cases for AI for Epistemics that you most care about?\n\nGoing from \"Realized\" to \"Accessible\" knowledge, e.g. getting good information using the web. Agentic web agents like Deep Research.\n\n* What decisions may go badly by default, but could go better if we have the right tech?\n\nMany billions of small decisions.\n\n* What do the later stages of the path to impact look like?\n\nSuperhuman forecasting at scale.\n\n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nReasonable judgment on tricky questions that's too cheap to meter, better than if you asked a respected colleague to spend many minutes on it.\n\n## Aviv\n\n* Better government governance  \n* Better democratic functioning  \n* Reduced dangerous concentration of power  \n    \n* Healthy polarization / pluralism  \n    \n* More peace, less likelihood of war  \n* Better geopolitics  \n* Better economic incentives  \n    \n* Better AI org governance  \n* Better AI coordination (re. distribution, escalation, etc.)  \n* Better compute allocation (toward what matters for good outcomes)  \n    \n* Better allocation of resources overall  \n* Better lives for people  \n* More effective organizations, getting things done with fewer resources, more focused on the things that benefit everyone  \n    \n* Identification of common ground directions / equilibria / game theoretic strategies that create/incentivize new positive gradients (and not surfacing the bad versions of those…)  \n    \n* Understanding our own goals and values\n\n*Note how it is the intersection of better epistemics / world models + the right power structures / incentives needed for all of these.*\n\n## Ben Goldhaber:\n\n* Forecasting how I should live my life in a period of intense technological change  \n* Guiding decision makers to avoid war with china/ai/everyone  \n* Providing the background context for me to sensemake way more sanely.  \n* Pushing forward the extant knowledge of AI safety agendas  \n* Planning for multi-stage projects that could achieve big impact  \n* Roadmapping to build larger more complex things\n\n## Chi\n\nThere are a few different things I care about:\n\n* I care a lot about humanity and AI, including **super-late-stage AI, getting decision theory** right.  \n  * Consequently but also separately, this relates to the general question of what to do about areas where there is not necessarily a ground truth but that are nonetheless very important to get right, cf**. [metaphilosophy](https://www.alignmentforum.org/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy)** (but also a bunch of other things.)  \n  * This also feels very similar to making **AIs helpful for AI safety research, cf. [automating alignment research](https://joecarlsmith.com/2025/03/14/ai-for-ai-safety#2-what-is-ai-for-ai-safety).**\n\n  → This is more about raising the **top of the epistemic landscape**\n\n* Other things I care about:  \n  * Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place.  \n  * Vague sense that better **societal epistemics** would help with increasing the **sanity of conversations about AI safety, polarisation around the issue etc.**  \n    * This is broadly about **getting intent alignment** in the first place.\n\n→ These are all more about raising the **bottom of the epistemic landscape**\n\n## Deger Turan:\n\n* Social trust erosion, collective coordination mechanisms are built on a weak assumption  \n* Capacity to entertain truthfulness when it is not instrumental to one's incentive gradient & differing norms upon 'truth will make my sense of security weaker'   \n* Creating contexts where world model divergence can be agreed by both parties enabling a safe disagreement  \n* Improving model reasoning / safer AI by reducing slop, by creating accountability mechanisms through consistency checks & other specifications for healthy epistemics  \n* Decoupling virality with magnitude while living in a short attention span world, curation   \n* I have some confusion re: raising the floor when not all parties are necessarily operating in good faith. I think I need some help think through the ways in which epistemics in the context of incentive misalignment can be used or have defensibility. \n\n## Nathan Young: \n\nWhat would we have got:\n\n* Biden not running, perhaps an open dem primary  \n* No FTX crisis  \n* Less zero sum OpenAI board drama  \n* Many smaller, better choices \n\nWhat decisions might go badly:\n\n* Trump tariffs/ global trade  \n* Taiwan/ US-China conflict  \n* US-China competition  \n* US authoritarianism  \n* AI regulation  \n* EU-China realignment  \n* Automated robots  \n* OpenAI strategy  \n* Anthropic strategy  \n* Google strategy  \n* xAI strategy  \n* AI workforce automation  \n* China strategy  \n* Russia strategy  \n* Moskovitz strategy  \n* Must strategy  \n* Bezos Strategy  \n* Zuckerberg strategy  \n* Tallinn strategy  \n* Altman strategy  \n* Drone warfare  \n* Climate change destabilisation  \n* Democratic trust\n\n## Lincoln Quirk:\n\nWhy do we care about (AI for) epistemics?  \n\\- making the public better informed:  \n\t\\- better able to deal with and process the wealth of information  \n\t\\- better able to express their own interests in government (via voting)  \n\t\\- helping cut through social media distraction / intentional misinformation  \n\\- getting governments to implement better policy  \n\t\\- coming up with good policy ideas  \n\t\\- objectively doing cost-benefit analysis  \n\t\\- objectively explaining tradeoffs (vs having to rely on biased humans to explain them)  \n\t\\- flagging bad ideas as bad  \n\t\\- convincing policymakers with compelling writeups  \n\\- success and flourishing for individuals and orgs using these tools  \n\t\\- via helping them think through decisions  \n\t\\- via clearheaded analysis of trade-offs and cost/benefits  \n\t\\- flagging bad ideas as bad\n\n## Jay\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * How can we build something like the positive version of Community Notes: which statements/posts are agreed on by people who often disagree?  \n  * How can we better understand/summarize what others think, e.g. on social media platforms? E.g. if you see a post recommended to you, is this because it's the most extreme take that went viral and isn't representative, or is this a commonly held view? Between different viewpoints, what are the concrete points of actual disagreement (on facts or values) and what are the things widely agreed upon? Let's build viewpoint/debate maps or a similar sensemaking tool that gets adopted/understood by the masses/raises the knowledge floor (in my experience, things like pol.is are hard for regular users to grok)  \n  * How can we improve Community Notes (or more general knowledge) creation with AI assistants? An AI agent proposing community notes and humans still checking/rating them?  \n  * How can we have something like Community Notes that applies to claims or statements about the future, or which are not concretely knowable?  \n  * \n\n## Ben Wilson (Metaculus)\n\nOverall I believe the most important value of epistemics (and AI epistemics) in general is informing what actions allow us to achieve our goals (i.e. make the world better). If we had perfect AI epistemics I would be excited by the ability for us to take a goal, and get a list of possible actions we could take and the forecasted outcomes of each of those actions (plus background information and forecasts for the 'current lay of the land' for context). This can then inform, not just the risk of AI doing good/bad things (as with current forecasting), but what we can do to raise the chances of 'great' AI outcomes, and lower the changes of 'worse' AI outcomes\n\n## Andreas\n\n* **What are use cases for AI for Epistemics that you most care about?**  \n  * AI policy, broadly construed, must go better  \n    * Avoid gradual disempowerment  \n    * Thoughtful regulation  \n  * Organizational planning & decision-making is weak  \n    * Given an informal long-term goal like improve decision-making for strategic users in government & large orgs, make a plan for a 20-people org that helps them accomplish the goal  \n    * Mapping dependencies, uncertainties, and potential futures  \n    * Understand complex domains with non-trivial feedback loops  \n      * Economic shifts  \n* **What decisions may go badly by default, but could go better if we have the right tech?**  \n  * Resource allocation within orgs  \n    * Investment  \n    * Grant making  \n    * Government budgets  \n  * Policy  \n    * How should AI be deployed in the world?  \n* **What do the later stages of the path to impact look like?**  \n  * AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning  \n  * The technology helps organizations maintain coherence and make sound judgments even as the pace of change accelerates  \n  * Assumption: People are generally well-intentioned \\- no one wants catastrophe \\- and as a result of better tooling things go better in the world overall\n\n## Owen optimistic brainstorm:\n\nWorld peace without world government — sufficiently good tech to help countries understand each others' interests, find mutually desirable bargains, and make sufficient credible commitments, could help us get up towards the Pareto frontier where we avoid (virtually) all costly conflicts\n\nGoing slowly and carefully on AI development — rather than having AI progress accelerate at increasingly breakneck speed, helping actors to understand the basic situation and the desirability of different options could be crucial for facilitating meaningful and robust agreements which restrict AI development to the stuff which we have good reasons for being confident are safe // otherwise beneficial.\n\nBettering democracy — helping people to access key information about what decisions their country might make would actually mean for longer term implications, and having this in turn feed into incentives for decision-makers that better align them with the common good\n\nBuilding safe / aligned AI — better tools for anticipating when systems are actually robustly safe, vs when stronger cases for expectation of safety seem required\n\n## Josh Rosenberg (FRI)\n\nI'm massively confused about important problems, e.g. how to orient to AI risk and what to do about it. Some reasonable people think there's a high probability we effectively lose everything we care about in 2027\\. Some reasonable people think it'll be decades till we have \"AGI\" and it'd be good if it arrived sooner. Which actions are good to take in each of these worlds is very different, and I'd like to figure out what world we're in.\n\nThis also applies to many other problem areas.\n\nE.g., Trump's effect on US democracy. How much of a crisis is it? What are the most effective actions to take in response to ensure that the country and communities we care about are as well off as they can be? How can we preserve high-quality institutions to lead the world in positive directions?\n\nOr, on the positive vision side: the \"Abundance\" agenda seems promising. Is it actually a good idea? What do we do about it?\n\nAI for Epistemics could help us get a lot more clarity on how to think about these problems and what actions to take. I'd like much better information and forecasts on the likely effects of Trump, AI timelines, and the pro's and con's of different Abundance policies, for example. If I had a bunch of time and good resources to dig into these problems, I could get a much better sense for myself. I'm hoping AI will help all of us do this sort of thing collectively.\n\n## Katja Grace\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Put lots of cognitive effort into very clear and good and compelling case for AI risk  \n  * Automate talking to people about AI risk compellingly, possibly using above case as input  \n  * Good game theoretic thinking re arms races, coordination, what is possible. I.e. People are making dumb assumptions about things, e.g. \"can't slow down tech\". Could automate noting errors. So e.g. when someone says that on Twitter, can highlight it and send in response. \\-\\> good thinking via better weapons for debate (similar to Nathan project)  \n  * Specific idea: very quantified and competitive debate site, that can be cited re which views are winning, where AI does a lot of the cognitive labor in places no people are paying attention, but as a part of it gets cruxy, people pay attention and vote/bet  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Poor decision making around AI: e.g. 'racing' because of vague heuristics instead of ever doing calc on costs and benefits given specific risks  \n  * Failure to coordinate on AI: building AI systems that are dangerous, due to expecting others to do so\n\n## Joel Lehman\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Raising the bottom (i.e. helping improve societal epistemics) – in particular that in theory political decisions (at least political decision-makers) are somewhat downstream of societal epistemics  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Having weird political leaders and awful public epistemics at critical junctures seems to unnecessarily spike catastrophic/existential risk probabilities. In theory, helping uplevel or at least maintain current public epistemic health (especially as it relates to civic sensemaking) might help influence the quality of decision-makers and concrete decisions made at high-stakes moments  \n  * The shape of the tech is not clear, or probably requires multiple flavors of interventions. E.g. if LLM assistants by default help cultivate civic virtues and on the margin support critical thinking or whatever underlying skills and knowledge help a person up-level their epistemic habits / health (or at least if LLM assistants don't contribute to atrophy/harm); or somehow convincing social media networks to care about this (e.g. community notes but things that go beyond); similarly somehow help w/ important social goods like 'societal trust' and 'trust in your neighbors'  \n* What do the later stages of the path to impact look like?  \n  * \\[didn't get to this\\]  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * Healthier democratic society, better agreement on consensus reality, higher trust in society, better politicians, better concrete decisions in high-stakes moments \n\n## Austin Chen\n\n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * **Evaluator**: More agreement (or understanding of disagreement) of what orgs & people are doing good work (eg for making grants in AI safety)  \n  * **Oracle**: Instant trustworthy forecasts on any question you ask  \n  * **Planner**: Guidance on \"what are the 3 most important things I should do this day/week/month\"  \n  * **Representative**: Simulate the responses of other parties (people, orgs/corps, governments, LLM agents), for discussions, contracts, trade\n\n## Janna\n\n* Why do we care about (AI for) epistemics?  \n  * Noise to signal ratio could be much higher  \n  * AI (mis)information churned out at the price of almost zero  \n* What are use cases for AI for Epistemics that you most care about?  \n  * Solving prediction markets--is there a way to scale up the bots, even if they are wrong and lossy, to improve prediction market accuracy?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * US-China relations  \n  * Trump tariffs maybe  \n  * Immigration rules in the US  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * More accurate predictions to…  \n    * Reduce business uncertainty  \n    * Make it easier to evaluate life decisions  \n  * Forestalling bad decisions made on imperfect information\n\n## Owain\n\n1. Maybe having humans understanding the world better is good in itself. Lots of people read popular science for this reason. Maybe it's good for AIs as well  \n2. Very broadly, it seems that better world understanding and decision making enables people to make their lives better in myriad ways. (To substantiate this, one could try to separate out the effects of better world understanding from the effects of better realized technology. E.g. Looking at capital-poor countries that can still benefit from scientific knowledge, economics, etc.)  \n3. More specifically, if AI progress is fast, there will be lots of important decisions to be made in quick calendar time. Exploiting AI to improve those decisions seems valuable. Current decision making institutions do not seem well set up to adapt really quickly. (A specific concern is dangers from AI takeover).   \n4. Why now? Why this group of people? We haven't really had tech that could do most epistemic tasks until recently. Now we have AI that can do an increasing proportion of the sub-tasks of epistemic processes. This will allow us to delegate epistemic tasks to AI.  \n   1. It's likely that people do this delegation, even if it's somewhat misguided and potentially bad in the long-term. Can we make this go well?  \n   2. This potentially massively expands our epistemic capacities. How can we use this well?  \n   3. Epistemic tech/institutions/advances can be hard to fund via for-profit companies. This is in part due to public goods issues, and issues around markets for ideas.\n\n## Rafe\n\nAs ever, I hope this will stop advanced artificial intelligence destroying the world, killing all its people \\&c.\n\nSo I think there are a few paths for impact here. I think there is a world where we can make the most productive thinkers about how to do things go further and better; I'm not so excited about a single-player version of this, as I think there's a pretty narrow window when top-level centaur mode works out\\[1\\]\n\nEveryone is already thinking about AI, and, I suspect, this will only get more acute. I think it's pretty hard for people to confront this and think about it carefully and well. It's scary\\! And there are a bunch of entrenched interests and memeplexes that preserve their status better (myopically) if people only have shallow slogans to think about AI with.\n\nGiven that, I suspect that the global sensemaking machine isn't up for the task of thinking about AI well. I hope we can help the global sensemaking machine upgrade itself. I have some hope that we as a society can come up with good ideas for dealing with AI, and also come to see how they make sense, so we can choose them in a robust way.\n\nOne other path to impact: I think a society that understands things better, including itself and its impacts, is one that is more robust in a bunch of ways. A more epistemically-grounded society is also probably a bit richer, and I think a bigger pie creates more slack for arranging things in sanity-promoting ways. I have more hope in a robust society navigating this stuff well.\n\n\\[1\\]:  Yeah, maybe you can do something with shaving off subskills, but I don't think that's really that doable.\n\n## Dave\n\n* With my consequentialist hat, better epistemics being worth it has to route through improved decisions somewhere down the line. The decisions I particularly care about are those by powerful people wrt AI development and implementation and the effects this has on how the future goes  \n  * But I do feel a bit icky when I have this hat on. There seems to be something very intuitively appealing about (a) pursuing better epistemics for its own sake and (b) democratising epistemic tools for the use and benefit of everyone. I think this basically caches out in more 'raising the floor' type work  \n  * I don't feel like I have a very good way of resolving this tension yet, but am excited to see things pushing on both frontiers  \n* I think forecasting has the potential to be a pretty powerful tool and there's currently a massive undersupply of good forecasts. I'm excited for us to use the wave of increased AI labor to make better quantitative forecasts to inform key decisions. I probably care more about quantitative forecasts than most decision makers and am open to being persuaded that the other, fuzzier parts of epistemics are more valuable to work on. \n\n## Julian\n\nWhat are use cases for AI for Epistemics that you most care about?  \n**Raising the ceiling:** can we use AI to organize information and credibly resolve the most challenging and important questions on which experts still seem to disagree? For example:\n\n* COVID origins  \n* Urban planning and public policy  \n* Lots of advancements in basic science: routing around current disagreements and policy challenges with slam-dunk science might be a pretty strong way forward.\n\n**Raising the floor:** can we use AI to build extremely legibly trustworthy/reliable epistemic infrastructure that actually makes its way into people's lives?\n\n* Making truth more palatable (in a way which doesn't translate into making falsehoods more palatable)  \n* Collective epistemics: connecting people to the others with whom they would have the most productive/enlightening conversations\n\nWhat decisions may go badly by default, but could go better if we have the right tech?\n\n* Lots of stuff in public policy: downstream both of better experts (technocrats), better public (voters/interest groups), and better coordination (collective epistemics)\n\nWhat do the later stages of the path to impact look like?\n\n* I want us to live in a world where everyone is more rational, empathetic, considered in their decision making, conscientious about impact, and interested in coordinating. I feel like that's one of the biggest blockers to all kinds of *long term* progress.\n\nWhat tools do we actually maybe want to build?\n\n* Easily accessible fact checkers / claim refuters  \n  * Powerful AI debaters  \n* Forecasting with explanations: superhuman and at scale  \n* Scientific meta-analysis, systematic review  \n* Ad-hoc / bespoke probabilistic models for individual problems  \n* Bespoke *simulators* / simulation environments  \n* Consistency checking and inconsistency identification  \n  * Arbitrage / market makers in prediction markets?  \n* Using AI to accelerate development of non-AI epistemic tools:  \n  * ?\n\n## Lawrence\n\nThere are already key questions/strategic problems where the truth is murky and we seem to proceed on the basis of limited and biased views. There might well be many more of these in the future. A central example is US-China competition over AI. Some have vibes of \"China is so bad that it's worth risking an existential catastrophe to stop it from capturing a portion of the lightcone\". Others have vibes of \"Cooperation is feasible and desirable, and is our only path to a good outcome\". But how bad is China, really? How much \"realised K\" on this question do those who are making the relevant decisions actually have? Similarly, how feasible is cooperation? How valid is this frame of \"capturing the lightcone\"? One reason that we don't have a better hold on these questions is that they are frighteningly complicated and difficult  – I think we could benefit massively from throwing much more cognitive labour at them, and presenting the findings in a way that is hard to disagree with or for any individual to improve upon, other than at a normative/values level. Maybe AI can get us there.\n\n## Lukas Finnveden\n\nwhy is AI for epistemics important/helpful?\n\n* better epistemics generally very good for making all kinds of decisions  \n* I'm specifically concerned about AI potentially causing an intelligence explosion, and us needing better epistemics to handle that. Specifically:  \n  * I'm worried about AI-driven tech R\\&D leading to catastrophes. To help address this, I want AI capabilities to not only be applied to tech R\\&D, but also to be applied to \"reasoning well from limited evidence\" about a number of questions, including how to deal with risks.  \n  * I'm worried about misaligned AI — so I want techniques for eliciting AI capabilities on all kinds of questions, so that AIs don't have better capabilities than we can elicit from them. (Basically: I want good alignment techniques even on hard-to-evaluate questions.)  \n  * Seems great to me if AI-driven improvements in general knowledge and decision-making are more broadly distributed among humans. (I think this leads to better decision-making without significantly accelerating the dangers from new tech or misaligned AI. Also reduces concentration of power.)\n\n## Abram Demski\n\n\\- Why are we doing this? \\-- impact theory brainstorming exercize  \n\t\\- There's the general claim that epistemics has degraded horribly due to social media and the internet, at least in a lot of ways, although epistemics has also gotten better in some specific ways. It seems very plausible that AI is, and will continue to, make this much much worse (due to deepfakes and super-persuasive writing, to give two examples). We wish to reverse this trend, or at least slow the acceleration.  \n\t\\- The high-level reason why epistemics is important is because epistemics is how we make good decisions. So effectively, this is AI for decision-making. However, if we just set out to make tools for decision-making, I have a sense that this would be worse.  \n\t\t\\- One super important aspect of decision-making is that you have to orient towards the right problem. People do much better when they understand the problem before proposing solutions.  \n\t\t\t\\- I have a sense that producing a better shared understanding of the world JUST WOULD result in better things. The connection between epistemics and better decisionmaking is that strong.  \n\t\t\t\t\\- However, there's an element of coordination to this. You have to have common knowledge, not just knowledge. AI for coordination is yet another thing. AI for group epistemics is close to that, though, and again, I have a sense that making \"AI for coordination\" would produce worse effects on the world than \"AI for group epistemics\".  \n\t\t\\- What specific decisions are we hoping to influence?  \n\t\t\t\\- Decisions around existential risk seem super super important. If AI were generally better at epistemics (as a relative, comparative thing \\-- ie we differentially accelerate epistemics) then key decisionmakers (in the government and at AI companies) would see much more clearly what's coming. Everyone wants a better future, so a better future would have a better chance of happening if we could see and navigate.\n\nI don't think I got any further than the supporting documentation for this workshop in articulating reasons and stuff, so I predict that the LLM summary won't be better than what we already know, although I hope it is. \n\n## Brendan\n\nFor us to live together in a caring and just way, we must make decisions about what is important to us together. Is it okay to tell my neighbour to turn their music down? Do we spend money on roads or trains or libraries? Do we regulate AI development and, if so, how? Do we want to spend public money on PEPFAR, or other avenues for reducing global poverty? To make these decisions in a way that is inclusive and democratic, that fosters buy-in and trust, we must begin with a shared sense of how the world is. This shared sense must be grounded in the physical, economic, and social reality we live in.\n\nAs we live together over wider and wider contexts, maintaining a shared sense of the world becomes more difficult and subtle. These wider contexts are facilitated by new technologies, both physical (eg. global manufacturing bases and supply chains) and informational (eg. social media). New technologies, such as AI for epistemics, and help maintain epistemic coherence across these contexts.\n\nMoreover, such technologies allow new avenues for cooperation at larger scales of human involvement. This unlocks new potential for coherence in global politics, scientific discovery, social and technological innovation, and cultural expression, and ultimately the deep joy of being alive and human in the world. \n\n## Isabel: \n\nThe kinds of being right in the past that are most relevant for decision making aren't just about making straightforward predictions about what will happen, but are more nuanced about what kinds of actions one should take to cause those things to happen, and these seem like somewhat different skills. These are harder to evaluate (e.g. because they're counterfactual: either we listened to person a and then we have to evaluate whether things went better or worse than if we didn't, but hard to say what the counterfactual would have been; or we didn't listen to person a and then we have to guess what things would have been like if we did) \n\nA bit adjacent to Neel Nanda's [recent piece](https://www.lesswrong.com/posts/P5zWiPF5cPJZSkiAK/good-research-takes-are-not-sufficient-for-good-strategic) on how \"good research takes aren't sufficient for good strategic takes\" \\- \"good forecasting takes aren't sufficient for good strategic takes\", but maybe some of this is maybe resolvable on the forecasting tech side \\- can we get better at creating and evaluating results from fuzzier questions? Can AI help? (Also can AI help with the whole problem, but this feels small and concrete enough to maybe get some traction on.) Also, some risk of actors building up good track records in order to be listened to, but then choosing to burn that track record to get power or things like that, something something trust \\- a concern for both human actors and e.g. scheming AIs \\- what things look good until they abruptly aren't? \n\nOn a different note, also interested in takes on what to do about and how to engage with actors who have epistemic processes that don't seem to be truth seeking in a comprehensible-to-me way. \n\n## Adam Marblestone:\n\nWhat do we get out of AI for epistemics?\n\nTrust, is one thing. Our current society seems to be fraught with distrust and motivated reasoning. Can we get a more objectively trustable source of truth? Not only accuracy but with a provenance that gives it an ability to be trusted by humans who are otherwise adversarial. Trust is needed to coordinate. Truthfulness is maybe somewhat necessary but seemingly not sufficient for a source being trusted. As a practical matter, on hot political topics I've been going to OpenAI Deep Research.\n\nBut at a certain level I wonder if AI for epistemics and some form of beneficial AGI could be basically the same thing. I asked Eric Drexler what he wanted for d/acc and he said something that sounds a bit like that…\n\n\"  \nHere's my unsurprising take on directed research for \"defensive acceleration\":  \nDevelopment and steering of epistemic tooling, aka [Large Knowledge Models](https://aiprospects.substack.com/p/large-knowledge-models), offers the highest leverage.\n\nPotential actions:  \n  • Identify and influence entities that could develop and scale necessary foundations.  \n  • Leverage the growing ease of development to take pre-scaling steps.  \n  • Within the scope of \"foundations\", accelerate productive directions for development.  \n  • To support science, ensure grounding of content in cited sources.  \n  • Pursue or promote assembly of appropriate knowledge inputs (beyond what is now used for training data).  \n  • Identify, promote, or implement epistemic tools supported by LKM resources.  \n  • Other kinds of exploration, promotion, preparation, and implementation.  \n  • Ensure that my advice is thoroughly understood.\n\nLKMs can accelerate pretty much everything that one might want, and perhaps tip the balance of applications toward *defensive* acceleration by promoting understanding of the *what and why* of defensive applications.\n\nAddendum:\n\nIn the email I sent a few minutes ago, the basic proposal becomes stronger if one reads \"LKM\" as something like \"large knowledge model functionality\", regardless of implementation.\n\nThe knowledge-model concept (whatever one prefers to call it) should not be tied to my specific technical suggestions. What I've said can be framed as something like \"a discussion of the value, feasibility, and potential scope of a rich epistemic resource\". Bundles of embeddings, etc., are at a level of specificity that may be distracting in the broader context of the meeting.  \n\"\n\nSo that's a strategy assistant AI – which helps humans at large see ways they could act and coordinate especially around big technological or geopolitical strategy issues I guess – but also a certain embodiment of an AGI oracle type system. Maybe the idea then is, how to make a AGI that helps with navigating civilizational strategy, and what is the right form of or interface to that to make it epistemically most useful. \n\nI'd like to also see democratic mechanisms that are simply maximally utilizing the tech we have. \n\n## Elizabeth\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Allowing people to be better informed about experimental medical stuff for themselves.   \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Bargaining? Even just salary negotiations feels like the results are often more the consequence of \"human bargaining skill\" and not the pareto optimal result.    \n  \n\n\n## Saul\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * making better decisions  \n  * in service of that: more useful forecasts, particularly understanding better which things are most relevant to forecast  \n    * in service of that: better group decisionmaking (e.g. unblocking things from vetopower type  \n    * in service of that: more aptly/calibratedly handling waves of information  \n    * In service of that: societal- and individual-scale asymmetric truth-tracking  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * which groups of people have political power  \n  * who's building powerful new tech  \n  * who's regulating powerful new tech  \n  * (etc, other \"decisions\" along these lines — some might not end up getting shaped like decisions, though)  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * better decisions across the board  \n    * better *outcomes* from those decisions; or, like, we can track \"better decisions\" as better-in-expectation outcomes from the decision processes"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Occasional Overuse of Nested Bullets",
          "description": "Some sections use deeply nested bullet points, which can be visually overwhelming and difficult to parse. Flattening the structure or using numbered lists for hierarchy could improve readability.",
          "importance": 35,
          "grade": 45,
          "isValid": true,
          "highlight": {
            "startOffset": 8911,
            "endOffset": 9830,
            "quotedText": "* **What are use cases for AI for Epistemics that you most care about?**  \n  * AI policy, broadly construed, must go better  \n    * Avoid gradual disempowerment  \n    * Thoughtful regulation  \n  * Organizational planning & decision-making is weak  \n    * Given an informal long-term goal like improve decision-making for strategic users in government & large orgs, make a plan for a 20-people org that helps them accomplish the goal  \n    * Mapping dependencies, uncertainties, and potential futures  \n    * Understand complex domains with non-trivial feedback loops  \n      * Economic shifts  \n* **What decisions may go badly by default, but could go better if we have the right tech?**  \n  * Resource allocation within orgs  \n    * Investment  \n    * Grant making  \n    * Government budgets  \n  * Policy  \n    * How should AI be deployed in the world?  \n* **What do the later stages of the path to impact look like?**"
          }
        },
        {
          "title": "Clear Articulation of Purpose and Motivation",
          "description": "Several contributors explicitly state why they care about AI for epistemics, providing motivational context that helps readers understand the stakes and relevance of the discussion.",
          "importance": 55,
          "grade": 90,
          "isValid": false,
          "highlight": {
            "startOffset": 355,
            "endOffset": 12649,
            "quotedText": "Why do we care about (AI for) epistemics?\n\nWriting prompts from Owen:\n\n* What are use cases for AI for Epistemics that you most care about?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nTemplate:\n\n## Name: \n\nEntry\n\n## Dan Schwarz:\n\n* What are use cases for AI for Epistemics that you most care about?\n\nGoing from \"Realized\" to \"Accessible\" knowledge, e.g. getting good information using the web. Agentic web agents like Deep Research.\n\n* What decisions may go badly by default, but could go better if we have the right tech?\n\nMany billions of small decisions.\n\n* What do the later stages of the path to impact look like?\n\nSuperhuman forecasting at scale.\n\n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nReasonable judgment on tricky questions that's too cheap to meter, better than if you asked a respected colleague to spend many minutes on it.\n\n## Aviv\n\n* Better government governance  \n* Better democratic functioning  \n* Reduced dangerous concentration of power  \n    \n* Healthy polarization / pluralism  \n    \n* More peace, less likelihood of war  \n* Better geopolitics  \n* Better economic incentives  \n    \n* Better AI org governance  \n* Better AI coordination (re. distribution, escalation, etc.)  \n* Better compute allocation (toward what matters for good outcomes)  \n    \n* Better allocation of resources overall  \n* Better lives for people  \n* More effective organizations, getting things done with fewer resources, more focused on the things that benefit everyone  \n    \n* Identification of common ground directions / equilibria / game theoretic strategies that create/incentivize new positive gradients (and not surfacing the bad versions of those…)  \n    \n* Understanding our own goals and values\n\n*Note how it is the intersection of better epistemics / world models + the right power structures / incentives needed for all of these.*\n\n## Ben Goldhaber:\n\n* Forecasting how I should live my life in a period of intense technological change  \n* Guiding decision makers to avoid war with china/ai/everyone  \n* Providing the background context for me to sensemake way more sanely.  \n* Pushing forward the extant knowledge of AI safety agendas  \n* Planning for multi-stage projects that could achieve big impact  \n* Roadmapping to build larger more complex things\n\n## Chi\n\nThere are a few different things I care about:\n\n* I care a lot about humanity and AI, including **super-late-stage AI, getting decision theory** right.  \n  * Consequently but also separately, this relates to the general question of what to do about areas where there is not necessarily a ground truth but that are nonetheless very important to get right, cf**. [metaphilosophy](https://www.alignmentforum.org/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy)** (but also a bunch of other things.)  \n  * This also feels very similar to making **AIs helpful for AI safety research, cf. [automating alignment research](https://joecarlsmith.com/2025/03/14/ai-for-ai-safety#2-what-is-ai-for-ai-safety).**\n\n  → This is more about raising the **top of the epistemic landscape**\n\n* Other things I care about:  \n  * Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place.  \n  * Vague sense that better **societal epistemics** would help with increasing the **sanity of conversations about AI safety, polarisation around the issue etc.**  \n    * This is broadly about **getting intent alignment** in the first place.\n\n→ These are all more about raising the **bottom of the epistemic landscape**\n\n## Deger Turan:\n\n* Social trust erosion, collective coordination mechanisms are built on a weak assumption  \n* Capacity to entertain truthfulness when it is not instrumental to one's incentive gradient & differing norms upon 'truth will make my sense of security weaker'   \n* Creating contexts where world model divergence can be agreed by both parties enabling a safe disagreement  \n* Improving model reasoning / safer AI by reducing slop, by creating accountability mechanisms through consistency checks & other specifications for healthy epistemics  \n* Decoupling virality with magnitude while living in a short attention span world, curation   \n* I have some confusion re: raising the floor when not all parties are necessarily operating in good faith. I think I need some help think through the ways in which epistemics in the context of incentive misalignment can be used or have defensibility. \n\n## Nathan Young: \n\nWhat would we have got:\n\n* Biden not running, perhaps an open dem primary  \n* No FTX crisis  \n* Less zero sum OpenAI board drama  \n* Many smaller, better choices \n\nWhat decisions might go badly:\n\n* Trump tariffs/ global trade  \n* Taiwan/ US-China conflict  \n* US-China competition  \n* US authoritarianism  \n* AI regulation  \n* EU-China realignment  \n* Automated robots  \n* OpenAI strategy  \n* Anthropic strategy  \n* Google strategy  \n* xAI strategy  \n* AI workforce automation  \n* China strategy  \n* Russia strategy  \n* Moskovitz strategy  \n* Must strategy  \n* Bezos Strategy  \n* Zuckerberg strategy  \n* Tallinn strategy  \n* Altman strategy  \n* Drone warfare  \n* Climate change destabilisation  \n* Democratic trust\n\n## Lincoln Quirk:\n\nWhy do we care about (AI for) epistemics?  \n\\- making the public better informed:  \n\t\\- better able to deal with and process the wealth of information  \n\t\\- better able to express their own interests in government (via voting)  \n\t\\- helping cut through social media distraction / intentional misinformation  \n\\- getting governments to implement better policy  \n\t\\- coming up with good policy ideas  \n\t\\- objectively doing cost-benefit analysis  \n\t\\- objectively explaining tradeoffs (vs having to rely on biased humans to explain them)  \n\t\\- flagging bad ideas as bad  \n\t\\- convincing policymakers with compelling writeups  \n\\- success and flourishing for individuals and orgs using these tools  \n\t\\- via helping them think through decisions  \n\t\\- via clearheaded analysis of trade-offs and cost/benefits  \n\t\\- flagging bad ideas as bad\n\n## Jay\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * How can we build something like the positive version of Community Notes: which statements/posts are agreed on by people who often disagree?  \n  * How can we better understand/summarize what others think, e.g. on social media platforms? E.g. if you see a post recommended to you, is this because it's the most extreme take that went viral and isn't representative, or is this a commonly held view? Between different viewpoints, what are the concrete points of actual disagreement (on facts or values) and what are the things widely agreed upon? Let's build viewpoint/debate maps or a similar sensemaking tool that gets adopted/understood by the masses/raises the knowledge floor (in my experience, things like pol.is are hard for regular users to grok)  \n  * How can we improve Community Notes (or more general knowledge) creation with AI assistants? An AI agent proposing community notes and humans still checking/rating them?  \n  * How can we have something like Community Notes that applies to claims or statements about the future, or which are not concretely knowable?  \n  * \n\n## Ben Wilson (Metaculus)\n\nOverall I believe the most important value of epistemics (and AI epistemics) in general is informing what actions allow us to achieve our goals (i.e. make the world better). If we had perfect AI epistemics I would be excited by the ability for us to take a goal, and get a list of possible actions we could take and the forecasted outcomes of each of those actions (plus background information and forecasts for the 'current lay of the land' for context). This can then inform, not just the risk of AI doing good/bad things (as with current forecasting), but what we can do to raise the chances of 'great' AI outcomes, and lower the changes of 'worse' AI outcomes\n\n## Andreas\n\n* **What are use cases for AI for Epistemics that you most care about?**  \n  * AI policy, broadly construed, must go better  \n    * Avoid gradual disempowerment  \n    * Thoughtful regulation  \n  * Organizational planning & decision-making is weak  \n    * Given an informal long-term goal like improve decision-making for strategic users in government & large orgs, make a plan for a 20-people org that helps them accomplish the goal  \n    * Mapping dependencies, uncertainties, and potential futures  \n    * Understand complex domains with non-trivial feedback loops  \n      * Economic shifts  \n* **What decisions may go badly by default, but could go better if we have the right tech?**  \n  * Resource allocation within orgs  \n    * Investment  \n    * Grant making  \n    * Government budgets  \n  * Policy  \n    * How should AI be deployed in the world?  \n* **What do the later stages of the path to impact look like?**  \n  * AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning  \n  * The technology helps organizations maintain coherence and make sound judgments even as the pace of change accelerates  \n  * Assumption: People are generally well-intentioned \\- no one wants catastrophe \\- and as a result of better tooling things go better in the world overall\n\n## Owen optimistic brainstorm:\n\nWorld peace without world government — sufficiently good tech to help countries understand each others' interests, find mutually desirable bargains, and make sufficient credible commitments, could help us get up towards the Pareto frontier where we avoid (virtually) all costly conflicts\n\nGoing slowly and carefully on AI development — rather than having AI progress accelerate at increasingly breakneck speed, helping actors to understand the basic situation and the desirability of different options could be crucial for facilitating meaningful and robust agreements which restrict AI development to the stuff which we have good reasons for being confident are safe // otherwise beneficial.\n\nBettering democracy — helping people to access key information about what decisions their country might make would actually mean for longer term implications, and having this in turn feed into incentives for decision-makers that better align them with the common good\n\nBuilding safe / aligned AI — better tools for anticipating when systems are actually robustly safe, vs when stronger cases for expectation of safety seem required\n\n## Josh Rosenberg (FRI)\n\nI'm massively confused about important problems, e.g. how to orient to AI risk and what to do about it. Some reasonable people think there's a high probability we effectively lose everything we care about in 2027\\. Some reasonable people think it'll be decades till we have \"AGI\" and it'd be good if it arrived sooner. Which actions are good to take in each of these worlds is very different, and I'd like to figure out what world we're in.\n\nThis also applies to many other problem areas.\n\nE.g., Trump's effect on US democracy. How much of a crisis is it? What are the most effective actions to take in response to ensure that the country and communities we care about are as well off as they can be? How can we preserve high-quality institutions to lead the world in positive directions?\n\nOr, on the positive vision side: the \"Abundance\" agenda seems promising. Is it actually a good idea? What do we do about it?\n\nAI for Epistemics could help us get a lot more clarity on how to think about these problems and what actions to take."
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Occasional Lack of Explanation for Acronyms",
          "description": "Acronyms such as 'ASI', 'AGI', and 'LLM' are used without definition. Providing the full term on first use would improve accessibility for readers less familiar with the field.",
          "importance": 50,
          "grade": 40,
          "isValid": false,
          "highlight": {
            "startOffset": 4008,
            "endOffset": 15171,
            "quotedText": "pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place.  \n  * Vague sense that better **societal epistemics** would help with increasing the **sanity of conversations about AI safety, polarisation around the issue etc.**  \n    * This is broadly about **getting intent alignment** in the first place.\n\n→ These are all more about raising the **bottom of the epistemic landscape**\n\n## Deger Turan:\n\n* Social trust erosion, collective coordination mechanisms are built on a weak assumption  \n* Capacity to entertain truthfulness when it is not instrumental to one's incentive gradient & differing norms upon 'truth will make my sense of security weaker'   \n* Creating contexts where world model divergence can be agreed by both parties enabling a safe disagreement  \n* Improving model reasoning / safer AI by reducing slop, by creating accountability mechanisms through consistency checks & other specifications for healthy epistemics  \n* Decoupling virality with magnitude while living in a short attention span world, curation   \n* I have some confusion re: raising the floor when not all parties are necessarily operating in good faith. I think I need some help think through the ways in which epistemics in the context of incentive misalignment can be used or have defensibility. \n\n## Nathan Young: \n\nWhat would we have got:\n\n* Biden not running, perhaps an open dem primary  \n* No FTX crisis  \n* Less zero sum OpenAI board drama  \n* Many smaller, better choices \n\nWhat decisions might go badly:\n\n* Trump tariffs/ global trade  \n* Taiwan/ US-China conflict  \n* US-China competition  \n* US authoritarianism  \n* AI regulation  \n* EU-China realignment  \n* Automated robots  \n* OpenAI strategy  \n* Anthropic strategy  \n* Google strategy  \n* xAI strategy  \n* AI workforce automation  \n* China strategy  \n* Russia strategy  \n* Moskovitz strategy  \n* Must strategy  \n* Bezos Strategy  \n* Zuckerberg strategy  \n* Tallinn strategy  \n* Altman strategy  \n* Drone warfare  \n* Climate change destabilisation  \n* Democratic trust\n\n## Lincoln Quirk:\n\nWhy do we care about (AI for) epistemics?  \n\\- making the public better informed:  \n\t\\- better able to deal with and process the wealth of information  \n\t\\- better able to express their own interests in government (via voting)  \n\t\\- helping cut through social media distraction / intentional misinformation  \n\\- getting governments to implement better policy  \n\t\\- coming up with good policy ideas  \n\t\\- objectively doing cost-benefit analysis  \n\t\\- objectively explaining tradeoffs (vs having to rely on biased humans to explain them)  \n\t\\- flagging bad ideas as bad  \n\t\\- convincing policymakers with compelling writeups  \n\\- success and flourishing for individuals and orgs using these tools  \n\t\\- via helping them think through decisions  \n\t\\- via clearheaded analysis of trade-offs and cost/benefits  \n\t\\- flagging bad ideas as bad\n\n## Jay\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * How can we build something like the positive version of Community Notes: which statements/posts are agreed on by people who often disagree?  \n  * How can we better understand/summarize what others think, e.g. on social media platforms? E.g. if you see a post recommended to you, is this because it's the most extreme take that went viral and isn't representative, or is this a commonly held view? Between different viewpoints, what are the concrete points of actual disagreement (on facts or values) and what are the things widely agreed upon? Let's build viewpoint/debate maps or a similar sensemaking tool that gets adopted/understood by the masses/raises the knowledge floor (in my experience, things like pol.is are hard for regular users to grok)  \n  * How can we improve Community Notes (or more general knowledge) creation with AI assistants? An AI agent proposing community notes and humans still checking/rating them?  \n  * How can we have something like Community Notes that applies to claims or statements about the future, or which are not concretely knowable?  \n  * \n\n## Ben Wilson (Metaculus)\n\nOverall I believe the most important value of epistemics (and AI epistemics) in general is informing what actions allow us to achieve our goals (i.e. make the world better). If we had perfect AI epistemics I would be excited by the ability for us to take a goal, and get a list of possible actions we could take and the forecasted outcomes of each of those actions (plus background information and forecasts for the 'current lay of the land' for context). This can then inform, not just the risk of AI doing good/bad things (as with current forecasting), but what we can do to raise the chances of 'great' AI outcomes, and lower the changes of 'worse' AI outcomes\n\n## Andreas\n\n* **What are use cases for AI for Epistemics that you most care about?**  \n  * AI policy, broadly construed, must go better  \n    * Avoid gradual disempowerment  \n    * Thoughtful regulation  \n  * Organizational planning & decision-making is weak  \n    * Given an informal long-term goal like improve decision-making for strategic users in government & large orgs, make a plan for a 20-people org that helps them accomplish the goal  \n    * Mapping dependencies, uncertainties, and potential futures  \n    * Understand complex domains with non-trivial feedback loops  \n      * Economic shifts  \n* **What decisions may go badly by default, but could go better if we have the right tech?**  \n  * Resource allocation within orgs  \n    * Investment  \n    * Grant making  \n    * Government budgets  \n  * Policy  \n    * How should AI be deployed in the world?  \n* **What do the later stages of the path to impact look like?**  \n  * AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning  \n  * The technology helps organizations maintain coherence and make sound judgments even as the pace of change accelerates  \n  * Assumption: People are generally well-intentioned \\- no one wants catastrophe \\- and as a result of better tooling things go better in the world overall\n\n## Owen optimistic brainstorm:\n\nWorld peace without world government — sufficiently good tech to help countries understand each others' interests, find mutually desirable bargains, and make sufficient credible commitments, could help us get up towards the Pareto frontier where we avoid (virtually) all costly conflicts\n\nGoing slowly and carefully on AI development — rather than having AI progress accelerate at increasingly breakneck speed, helping actors to understand the basic situation and the desirability of different options could be crucial for facilitating meaningful and robust agreements which restrict AI development to the stuff which we have good reasons for being confident are safe // otherwise beneficial.\n\nBettering democracy — helping people to access key information about what decisions their country might make would actually mean for longer term implications, and having this in turn feed into incentives for decision-makers that better align them with the common good\n\nBuilding safe / aligned AI — better tools for anticipating when systems are actually robustly safe, vs when stronger cases for expectation of safety seem required\n\n## Josh Rosenberg (FRI)\n\nI'm massively confused about important problems, e.g. how to orient to AI risk and what to do about it. Some reasonable people think there's a high probability we effectively lose everything we care about in 2027\\. Some reasonable people think it'll be decades till we have \"AGI\" and it'd be good if it arrived sooner. Which actions are good to take in each of these worlds is very different, and I'd like to figure out what world we're in.\n\nThis also applies to many other problem areas.\n\nE.g., Trump's effect on US democracy. How much of a crisis is it? What are the most effective actions to take in response to ensure that the country and communities we care about are as well off as they can be? How can we preserve high-quality institutions to lead the world in positive directions?\n\nOr, on the positive vision side: the \"Abundance\" agenda seems promising. Is it actually a good idea? What do we do about it?\n\nAI for Epistemics could help us get a lot more clarity on how to think about these problems and what actions to take. I'd like much better information and forecasts on the likely effects of Trump, AI timelines, and the pro's and con's of different Abundance policies, for example. If I had a bunch of time and good resources to dig into these problems, I could get a much better sense for myself. I'm hoping AI will help all of us do this sort of thing collectively.\n\n## Katja Grace\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Put lots of cognitive effort into very clear and good and compelling case for AI risk  \n  * Automate talking to people about AI risk compellingly, possibly using above case as input  \n  * Good game theoretic thinking re arms races, coordination, what is possible. I.e. People are making dumb assumptions about things, e.g. \"can't slow down tech\". Could automate noting errors. So e.g. when someone says that on Twitter, can highlight it and send in response. \\-\\> good thinking via better weapons for debate (similar to Nathan project)  \n  * Specific idea: very quantified and competitive debate site, that can be cited re which views are winning, where AI does a lot of the cognitive labor in places no people are paying attention, but as a part of it gets cruxy, people pay attention and vote/bet  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Poor decision making around AI: e.g. 'racing' because of vague heuristics instead of ever doing calc on costs and benefits given specific risks  \n  * Failure to coordinate on AI: building AI systems that are dangerous, due to expecting others to do so\n\n## Joel Lehman\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Raising the bottom (i.e. helping improve societal epistemics) – in particular that in theory political decisions (at least political decision-makers) are somewhat downstream of societal epistemics  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Having weird political leaders and awful public epistemics at critical junctures seems to unnecessarily spike catastrophic/existential risk probabilities. In theory, helping uplevel or at least maintain current public epistemic health (especially as it relates to civic sensemaking) might help influence the quality of decision-makers and concrete decisions made at high-stakes moments  \n  * The shape of the tech is not clear, or probably requires multiple flavors of interventions. E.g. if LLM assistants by default help cultivate civic virtues"
          },
          "error": "Highlight is too long (max 1000 characters)"
        }
      ],
      "thinking": "# Thinking Process\n\n## 1. Overall Assessment\nThe document is a collaborative brainstorming session on the value, use cases, and impact pathways for \"AI for Epistemics.\" It is structured as a series of prompts and responses from multiple contributors, each offering perspectives on why epistemics matter, what problems improved epistemics could solve, and what the future might look like with better epistemic tools. The format is informal, with bullet points, partial sentences, and some embedded links. There is a mix of high-level theorizing and concrete examples.\n\n## 2. Key Themes and Patterns\n- **Raising the Floor and Ceiling:** Multiple contributors distinguish between improving baseline societal epistemics (raising the floor) and enabling superhuman or expert-level epistemics (raising the ceiling).\n- **Decision-Making:** A recurring theme is that better epistemics should lead to better decisions, both at the individual and societal level, especially in high-stakes domains like AI governance, geopolitics, and public policy.\n- **Trust and Coordination:** Several entries highlight the importance of trust, shared reality, and coordination, suggesting that epistemic tools should foster these qualities.\n- **Forecasting and Sensemaking:** Many contributors mention forecasting, scenario analysis, and sensemaking as key applications, with a desire for tools that help clarify complex, uncertain, or contentious issues.\n- **Democratization and Accessibility:** There is concern for making epistemic tools accessible and beneficial to a broad audience, not just elites or experts.\n- **Risks and Challenges:** Some contributors note the risk of misuse, incentive misalignment, or epistemic degradation (e.g., via misinformation or deepfakes).\n- **Meta-Reflection:** The document is self-aware about its own epistemic process, with some contributors reflecting on the limitations of brainstorming and the need for clarity about goals.\n\n## 3. Expert Perspective\n- **Readability:** The document is highly accessible in vocabulary but suffers from inconsistent sentence structure, heavy use of bullet points, and variable paragraph length. There is little technical jargon, but some domain-specific terms (e.g., \"epistemics,\" \"intent alignment\") are used without definition.\n- **Structural Coherence:** The structure is fragmented, as is typical for a brainstorming session. There is no overarching narrative or synthesis, and transitions between sections are abrupt. However, the use of repeated prompts provides some organizational scaffolding.\n- **Precision and Concision:** The document is concise at the level of individual points but often vague or abstract in expression. Some entries are highly specific (e.g., \"Biden not running\"), while others are general or speculative. There is some redundancy across contributors.\n- **Audience Alignment:** The assumed audience is familiar with AI safety, epistemics, and related discourse. There is little scaffolding for lay readers, and some references (e.g., \"lightcone\") may be opaque to outsiders. The informal style is appropriate for an internal workshop but would need adaptation for broader dissemination.\n\n## 4. Notable Strengths\n- Rich diversity of perspectives and ideas\n- Clear articulation of both high-level goals and concrete use cases\n- Honest reflection on uncertainties and challenges\n\n## 5. Areas for Improvement\n- Lack of synthesis or summary; difficult for readers to extract key takeaways\n- Inconsistent formatting and structure; some entries are much longer or more detailed than others\n- Occasional use of undefined jargon or references\n- Limited signposting or navigational aids\n\n## 6. Before/After Example\n**Before:**\n\"* What are use cases for AI for Epistemics that you most care about?  \n  * Raising the bottom (i.e. helping improve societal epistemics) – in particular that in theory political decisions (at least political decision-makers) are somewhat downstream of societal epistemics\"\n\n**After:**\n\"AI for Epistemics can help elevate the general quality of public understanding, which in turn influences the quality of political decision-making. By improving how society as a whole processes information, we can expect more informed and effective policy outcomes.\"\n\n## 7. Recommendations\n- **Immediate:** Add a synthesized executive summary and thematic overview at the top.\n- **Structural:** Standardize the format of each contributor's entry (e.g., use consistent prompts, full sentences, and paragraph structure).\n- **Content Organization:** Group similar themes (e.g., forecasting, trust, democratization) and provide cross-references.\n- **Terminology:** Define key terms and provide brief context for domain-specific references.\n- **Formatting:** Use headings, subheadings, and signposting to improve navigation.\n\n## 8. Meta-Analysis\nThe document is well-suited for its intended audience (AI epistemics researchers and adjacent communities) but would benefit from additional scaffolding for broader audiences. The informal, brainstorming style is appropriate for early-stage ideation but limits clarity and coherence for external readers. The diversity of perspectives is a major asset, but synthesis is needed to maximize impact.\n",
      "summary": "### Executive Summary\nThis document offers a rich, multi-perspective exploration of \"AI for Epistemics,\" highlighting both societal and technical challenges. Its strengths lie in idea diversity and honest reflection, but it lacks synthesis, consistent structure, and clear signposting. To enhance clarity and impact, a synthesized summary, standardized formatting, and explicit definitions are recommended.",
      "grade": 68
    },
    {
      "agentId": "research-scholar",
      "createdAt": "2025-04-18T19:43:57.460Z",
      "costInCents": 0,
      "comments": [
        {
          "title": "🎯 Framing the Purpose of Epistemics",
          "description": "The opening framing by Owen—contrasting 'free-form vibe' with 'why are we doing this again?'—sets a crucial tone for the document. It emphasizes intentionality over aimless exploration, which is foundational for the rest of the discussion.",
          "importance": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 50,
            "endOffset": 396,
            "quotedText": "Negative: \"we just want to free-form vibe about what cool things we could do\"  \n* Positive: step back & ask \"wait, why are we doing this again?\"  \n* What stuff does this actually help?  \n* Epistemics isn't a goal in-an-of itself; figuring out what combination of things are possible, achievable, etc.  \n* Why do we care about (AI for) epistemics?"
          }
        },
        {
          "title": "Epistemics as Instrumental, Not Intrinsic",
          "description": "The assertion that 'epistemics isn't a goal in-an-of itself' is a key philosophical stance. It frames epistemics as a means to an end—improved decision-making—rather than an end in itself. This shapes the entire document's orientation.",
          "importance": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 239,
            "endOffset": 396,
            "quotedText": "Epistemics isn't a goal in-an-of itself; figuring out what combination of things are possible, achievable, etc.  \n* Why do we care about (AI for) epistemics?"
          }
        },
        {
          "title": "Prompting for Use Cases",
          "description": "The writing prompts provided by Owen are well-structured and guide contributors to think concretely about impact, use cases, and the path to value. This scaffolding is effective for eliciting actionable insights.",
          "importance": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 398,
            "endOffset": 711,
            "quotedText": "Writing prompts from Owen:\n\n* What are use cases for AI for Epistemics that you most care about?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\""
          }
        },
        {
          "title": "From Realized to Accessible Knowledge",
          "description": "Dan Schwarz's focus on making 'realized' knowledge 'accessible' highlights a practical challenge: surfacing and distributing existing knowledge efficiently. This is a central problem for epistemic technologies.",
          "importance": 65,
          "isValid": true,
          "highlight": {
            "startOffset": 829,
            "endOffset": 961,
            "quotedText": "Going from \"Realized\" to \"Accessible\" knowledge, e.g. getting good information using the web. Agentic web agents like Deep Research."
          }
        },
        {
          "title": "Superhuman Forecasting at Scale",
          "description": "The aspiration for 'superhuman forecasting at scale' is a recurring theme. It encapsulates the ambition for AI epistemics to not just match, but exceed human capabilities in prediction and judgment.",
          "importance": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 590,
            "endOffset": 1181,
            "quotedText": "What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nTemplate:\n\n## Name: \n\nEntry\n\n## Dan Schwarz:\n\n* What are use cases for AI for Epistemics that you most care about?\n\nGoing from \"Realized\" to \"Accessible\" knowledge, e.g. getting good information using the web. Agentic web agents like Deep Research.\n\n* What decisions may go badly by default, but could go better if we have the right tech?\n\nMany billions of small decisions.\n\n* What do the later stages of the path to impact look like?\n\nSuperhuman forecasting at scale."
          }
        },
        {
          "title": "Epistemics for Societal Governance",
          "description": "Aviv's list connects epistemics to broad societal outcomes: governance, democracy, peace, and economic incentives. This underscores the systemic impact of epistemic improvements beyond individual decision-making.",
          "importance": 78,
          "isValid": true,
          "highlight": {
            "startOffset": 1401,
            "endOffset": 2252,
            "quotedText": "Better government governance  \n* Better democratic functioning  \n* Reduced dangerous concentration of power  \n    \n* Healthy polarization / pluralism  \n    \n* More peace, less likelihood of war  \n* Better geopolitics  \n* Better economic incentives  \n    \n* Better AI org governance  \n* Better AI coordination (re. distribution, escalation, etc.)  \n* Better compute allocation (toward what matters for good outcomes)  \n    \n* Better allocation of resources overall  \n* Better lives for people  \n* More effective organizations, getting things done with fewer resources, more focused on the things that benefit everyone  \n    \n* Identification of common ground directions / equilibria / game theoretic strategies that create/incentivize new positive gradients (and not surfacing the bad versions of those…)  \n    \n* Understanding our own goals and values"
          }
        },
        {
          "title": "Intersection of Epistemics and Power Structures",
          "description": "The note that 'it is the intersection of better epistemics / world models + the right power structures / incentives' is a sophisticated insight. It recognizes that epistemic improvements alone are insufficient without supportive institutional frameworks.",
          "importance": 82,
          "isValid": true,
          "highlight": {
            "startOffset": 2255,
            "endOffset": 2389,
            "quotedText": "Note how it is the intersection of better epistemics / world models + the right power structures / incentives needed for all of these."
          }
        },
        {
          "title": "Epistemics for Personal and Strategic Life Decisions",
          "description": "Ben Goldhaber highlights the personal dimension—how epistemics can guide individuals through technological change and complex life choices. This grounds the discussion in relatable, everyday stakes.",
          "importance": 60,
          "isValid": true,
          "highlight": {
            "startOffset": 2413,
            "endOffset": 2814,
            "quotedText": "Forecasting how I should live my life in a period of intense technological change  \n* Guiding decision makers to avoid war with china/ai/everyone  \n* Providing the background context for me to sensemake way more sanely.  \n* Pushing forward the extant knowledge of AI safety agendas  \n* Planning for multi-stage projects that could achieve big impact  \n* Roadmapping to build larger more complex things"
          }
        },
        {
          "title": "Raising the Top vs. Bottom of the Epistemic Landscape",
          "description": "Chi introduces the distinction between 'raising the top' (helping experts) and 'raising the bottom' (improving societal epistemics). This duality is a valuable lens for evaluating interventions.",
          "importance": 77,
          "isValid": false,
          "highlight": {
            "startOffset": 3534,
            "endOffset": 4546,
            "quotedText": "→ This is more about raising the **top of the epistemic landscape**\n\n* Other things I care about:  \n  * Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place.  \n  * Vague sense that better **societal epistemics** would help with increasing the **sanity of conversations about AI safety, polarisation around the issue etc.**  \n    * This is broadly about **getting intent alignment** in the first place.\n\n→ These are all more about raising the **bottom of the epistemic landscape**"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Epistemics and Geopolitical Stability",
          "description": "Chi and others connect epistemic quality to risks like coups, authoritarianism, and AI race dynamics. This highlights the security implications of epistemic infrastructure.",
          "importance": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 3638,
            "endOffset": 4224,
            "quotedText": "Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place."
          }
        },
        {
          "title": "Social Trust and Safe Disagreement",
          "description": "Deger Turan's points about social trust, safe disagreement, and accountability mechanisms are crucial for healthy epistemic environments, especially in adversarial or low-trust contexts.",
          "importance": 68,
          "isValid": true,
          "highlight": {
            "startOffset": 4567,
            "endOffset": 5448,
            "quotedText": "Social trust erosion, collective coordination mechanisms are built on a weak assumption  \n* Capacity to entertain truthfulness when it is not instrumental to one's incentive gradient & differing norms upon 'truth will make my sense of security weaker'   \n* Creating contexts where world model divergence can be agreed by both parties enabling a safe disagreement  \n* Improving model reasoning / safer AI by reducing slop, by creating accountability mechanisms through consistency checks & other specifications for healthy epistemics  \n* Decoupling virality with magnitude while living in a short attention span world, curation   \n* I have some confusion re: raising the floor when not all parties are necessarily operating in good faith. I think I need some help think through the ways in which epistemics in the context of incentive misalignment can be used or have defensibility."
          }
        },
        {
          "title": "Concrete Policy and Organizational Decisions",
          "description": "Nathan Young's enumeration of specific decisions (e.g., US-China conflict, AI regulation, climate change) grounds the discussion in real-world stakes and demonstrates the breadth of epistemic impact.",
          "importance": 72,
          "isValid": true,
          "highlight": {
            "startOffset": 5634,
            "endOffset": 6184,
            "quotedText": "What decisions might go badly:\n\n* Trump tariffs/ global trade  \n* Taiwan/ US-China conflict  \n* US-China competition  \n* US authoritarianism  \n* AI regulation  \n* EU-China realignment  \n* Automated robots  \n* OpenAI strategy  \n* Anthropic strategy  \n* Google strategy  \n* xAI strategy  \n* AI workforce automation  \n* China strategy  \n* Russia strategy  \n* Moskovitz strategy  \n* Must strategy  \n* Bezos Strategy  \n* Zuckerberg strategy  \n* Tallinn strategy  \n* Altman strategy  \n* Drone warfare  \n* Climate change destabilisation  \n* Democratic trust"
          }
        },
        {
          "title": "Public Information Processing and Policy",
          "description": "Lincoln Quirk emphasizes the role of epistemics in public information processing, policy formation, and individual flourishing. The focus on cost-benefit analysis and tradeoff explanation is particularly actionable.",
          "importance": 65,
          "isValid": false,
          "highlight": {
            "startOffset": 355,
            "endOffset": 6772,
            "quotedText": "Why do we care about (AI for) epistemics?\n\nWriting prompts from Owen:\n\n* What are use cases for AI for Epistemics that you most care about?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nTemplate:\n\n## Name: \n\nEntry\n\n## Dan Schwarz:\n\n* What are use cases for AI for Epistemics that you most care about?\n\nGoing from \"Realized\" to \"Accessible\" knowledge, e.g. getting good information using the web. Agentic web agents like Deep Research.\n\n* What decisions may go badly by default, but could go better if we have the right tech?\n\nMany billions of small decisions.\n\n* What do the later stages of the path to impact look like?\n\nSuperhuman forecasting at scale.\n\n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nReasonable judgment on tricky questions that's too cheap to meter, better than if you asked a respected colleague to spend many minutes on it.\n\n## Aviv\n\n* Better government governance  \n* Better democratic functioning  \n* Reduced dangerous concentration of power  \n    \n* Healthy polarization / pluralism  \n    \n* More peace, less likelihood of war  \n* Better geopolitics  \n* Better economic incentives  \n    \n* Better AI org governance  \n* Better AI coordination (re. distribution, escalation, etc.)  \n* Better compute allocation (toward what matters for good outcomes)  \n    \n* Better allocation of resources overall  \n* Better lives for people  \n* More effective organizations, getting things done with fewer resources, more focused on the things that benefit everyone  \n    \n* Identification of common ground directions / equilibria / game theoretic strategies that create/incentivize new positive gradients (and not surfacing the bad versions of those…)  \n    \n* Understanding our own goals and values\n\n*Note how it is the intersection of better epistemics / world models + the right power structures / incentives needed for all of these.*\n\n## Ben Goldhaber:\n\n* Forecasting how I should live my life in a period of intense technological change  \n* Guiding decision makers to avoid war with china/ai/everyone  \n* Providing the background context for me to sensemake way more sanely.  \n* Pushing forward the extant knowledge of AI safety agendas  \n* Planning for multi-stage projects that could achieve big impact  \n* Roadmapping to build larger more complex things\n\n## Chi\n\nThere are a few different things I care about:\n\n* I care a lot about humanity and AI, including **super-late-stage AI, getting decision theory** right.  \n  * Consequently but also separately, this relates to the general question of what to do about areas where there is not necessarily a ground truth but that are nonetheless very important to get right, cf**. [metaphilosophy](https://www.alignmentforum.org/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy)** (but also a bunch of other things.)  \n  * This also feels very similar to making **AIs helpful for AI safety research, cf. [automating alignment research](https://joecarlsmith.com/2025/03/14/ai-for-ai-safety#2-what-is-ai-for-ai-safety).**\n\n  → This is more about raising the **top of the epistemic landscape**\n\n* Other things I care about:  \n  * Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place.  \n  * Vague sense that better **societal epistemics** would help with increasing the **sanity of conversations about AI safety, polarisation around the issue etc.**  \n    * This is broadly about **getting intent alignment** in the first place.\n\n→ These are all more about raising the **bottom of the epistemic landscape**\n\n## Deger Turan:\n\n* Social trust erosion, collective coordination mechanisms are built on a weak assumption  \n* Capacity to entertain truthfulness when it is not instrumental to one's incentive gradient & differing norms upon 'truth will make my sense of security weaker'   \n* Creating contexts where world model divergence can be agreed by both parties enabling a safe disagreement  \n* Improving model reasoning / safer AI by reducing slop, by creating accountability mechanisms through consistency checks & other specifications for healthy epistemics  \n* Decoupling virality with magnitude while living in a short attention span world, curation   \n* I have some confusion re: raising the floor when not all parties are necessarily operating in good faith. I think I need some help think through the ways in which epistemics in the context of incentive misalignment can be used or have defensibility. \n\n## Nathan Young: \n\nWhat would we have got:\n\n* Biden not running, perhaps an open dem primary  \n* No FTX crisis  \n* Less zero sum OpenAI board drama  \n* Many smaller, better choices \n\nWhat decisions might go badly:\n\n* Trump tariffs/ global trade  \n* Taiwan/ US-China conflict  \n* US-China competition  \n* US authoritarianism  \n* AI regulation  \n* EU-China realignment  \n* Automated robots  \n* OpenAI strategy  \n* Anthropic strategy  \n* Google strategy  \n* xAI strategy  \n* AI workforce automation  \n* China strategy  \n* Russia strategy  \n* Moskovitz strategy  \n* Must strategy  \n* Bezos Strategy  \n* Zuckerberg strategy  \n* Tallinn strategy  \n* Altman strategy  \n* Drone warfare  \n* Climate change destabilisation  \n* Democratic trust\n\n## Lincoln Quirk:\n\nWhy do we care about (AI for) epistemics?  \n\\- making the public better informed:  \n\t\\- better able to deal with and process the wealth of information  \n\t\\- better able to express their own interests in government (via voting)  \n\t\\- helping cut through social media distraction / intentional misinformation  \n\\- getting governments to implement better policy  \n\t\\- coming up with good policy ideas  \n\t\\- objectively doing cost-benefit analysis  \n\t\\- objectively explaining tradeoffs (vs having to rely on biased humans to explain them)  \n\t\\- flagging bad ideas as bad"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Community Notes and Viewpoint Mapping",
          "description": "Jay's ideas about AI-augmented Community Notes and viewpoint/debate mapping are concrete proposals for improving collective sensemaking and reducing polarization.",
          "importance": 67,
          "isValid": false,
          "highlight": {
            "startOffset": 7126,
            "endOffset": 8198,
            "quotedText": "How can we build something like the positive version of Community Notes: which statements/posts are agreed on by people who often disagree?  \n  * How can we better understand/summarize what others think, e.g. on social media platforms? E.g. if you see a post recommended to you, is this because it's the most extreme take that went viral and isn't representative, or is this a commonly held view? Between different viewpoints, what are the concrete points of actual disagreement (on facts or values) and what are the things widely agreed upon? Let's build viewpoint/debate maps or a similar sensemaking tool that gets adopted/understood by the masses/raises the knowledge floor (in my experience, things like pol.is are hard for regular users to grok)  \n  * How can we improve Community Notes (or more general knowledge) creation with AI assistants? An AI agent proposing community notes and humans still checking/rating them?  \n  * How can we have something like Community Notes that applies to claims or statements about the future, or which are not concretely knowable?"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "AI Epistemics for Actionable Outcomes",
          "description": "Ben Wilson (Metaculus) articulates a vision where AI epistemics directly inform actionable choices, not just risk assessment. This is a strong argument for the practical value of epistemic tools.",
          "importance": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 8234,
            "endOffset": 8897,
            "quotedText": "Overall I believe the most important value of epistemics (and AI epistemics) in general is informing what actions allow us to achieve our goals (i.e. make the world better). If we had perfect AI epistemics I would be excited by the ability for us to take a goal, and get a list of possible actions we could take and the forecasted outcomes of each of those actions (plus background information and forecasts for the 'current lay of the land' for context). This can then inform, not just the risk of AI doing good/bad things (as with current forecasting), but what we can do to raise the chances of 'great' AI outcomes, and lower the changes of 'worse' AI outcomes"
          }
        },
        {
          "title": "Integration into Strategic Workflows",
          "description": "Andreas describes a future where epistemic tools are deeply integrated into the workflows of key organizations. This vision of institutional adoption is critical for large-scale impact.",
          "importance": 78,
          "isValid": true,
          "highlight": {
            "startOffset": 9837,
            "endOffset": 10430,
            "quotedText": "AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning  \n  * The technology helps organizations maintain coherence and make sound judgments even as the pace of change accelerates  \n  * Assumption: People are generally well-intentioned \\- no one wants catastrophe \\- and as a result of better tooling things go better in the world overall"
          }
        },
        {
          "title": "World Peace Without World Government",
          "description": "Owen's optimistic brainstorm about achieving world peace through epistemic tech is ambitious and illustrates the highest aspirations for the field. It also highlights the need for credible commitments and mutual understanding.",
          "importance": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 10464,
            "endOffset": 10751,
            "quotedText": "World peace without world government — sufficiently good tech to help countries understand each others' interests, find mutually desirable bargains, and make sufficient credible commitments, could help us get up towards the Pareto frontier where we avoid (virtually) all costly conflicts"
          }
        },
        {
          "title": "Clarity on Existential Risks",
          "description": "Josh Rosenberg's confusion about AI risk timelines and the need for better collective clarity is a powerful motivator for epistemic technology. It also illustrates the diversity of expert opinion and the challenge of sensemaking.",
          "importance": 76,
          "isValid": false,
          "highlight": {
            "startOffset": 11616,
            "endOffset": 12998,
            "quotedText": "I'm massively confused about important problems, e.g. how to orient to AI risk and what to do about it. Some reasonable people think there's a high probability we effectively lose everything we care about in 2027\\. Some reasonable people think it'll be decades till we have \"AGI\" and it'd be good if it arrived sooner. Which actions are good to take in each of these worlds is very different, and I'd like to figure out what world we're in.\n\nThis also applies to many other problem areas.\n\nE.g., Trump's effect on US democracy. How much of a crisis is it? What are the most effective actions to take in response to ensure that the country and communities we care about are as well off as they can be? How can we preserve high-quality institutions to lead the world in positive directions?\n\nOr, on the positive vision side: the \"Abundance\" agenda seems promising. Is it actually a good idea? What do we do about it?\n\nAI for Epistemics could help us get a lot more clarity on how to think about these problems and what actions to take. I'd like much better information and forecasts on the likely effects of Trump, AI timelines, and the pro's and con's of different Abundance policies, for example. If I had a bunch of time and good resources to dig into these problems, I could get a much better sense for myself. I'm hoping AI will help all of us do this sort of thing collectively."
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Automated Debate and Quantified Argumentation",
          "description": "Katja Grace's proposal for AI-augmented debate sites and automated error-spotting is a concrete, innovative application of epistemic AI. The focus on quantification and competitive argumentation is notable.",
          "importance": 68,
          "isValid": true,
          "highlight": {
            "startOffset": 13633,
            "endOffset": 13889,
            "quotedText": "Specific idea: very quantified and competitive debate site, that can be cited re which views are winning, where AI does a lot of the cognitive labor in places no people are paying attention, but as a part of it gets cruxy, people pay attention and vote/bet"
          }
        },
        {
          "title": "Raising the Floor for Societal Epistemics",
          "description": "Joel Lehman's emphasis on improving public epistemic health and civic sensemaking addresses the foundational role of epistemics in democracy and risk mitigation.",
          "importance": 74,
          "isValid": false,
          "highlight": {
            "startOffset": 14331,
            "endOffset": 15918,
            "quotedText": "Raising the bottom (i.e. helping improve societal epistemics) – in particular that in theory political decisions (at least political decision-makers) are somewhat downstream of societal epistemics  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Having weird political leaders and awful public epistemics at critical junctures seems to unnecessarily spike catastrophic/existential risk probabilities. In theory, helping uplevel or at least maintain current public epistemic health (especially as it relates to civic sensemaking) might help influence the quality of decision-makers and concrete decisions made at high-stakes moments  \n  * The shape of the tech is not clear, or probably requires multiple flavors of interventions. E.g. if LLM assistants by default help cultivate civic virtues and on the margin support critical thinking or whatever underlying skills and knowledge help a person up-level their epistemic habits / health (or at least if LLM assistants don't contribute to atrophy/harm); or somehow convincing social media networks to care about this (e.g. community notes but things that go beyond); similarly somehow help w/ important social goods like 'societal trust' and 'trust in your neighbors'  \n* What do the later stages of the path to impact look like?  \n  * \\[didn't get to this\\]  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * Healthier democratic society, better agreement on consensus reality, higher trust in society, better politicians, better concrete decisions in high-stakes moments"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "AI as Evaluator, Oracle, Planner, and Representative",
          "description": "Austin Chen's breakdown of AI epistemics roles—Evaluator, Oracle, Planner, Representative—offers a useful taxonomy for thinking about system design and user needs.",
          "importance": 65,
          "isValid": true,
          "highlight": {
            "startOffset": 16005,
            "endOffset": 16468,
            "quotedText": "**Evaluator**: More agreement (or understanding of disagreement) of what orgs & people are doing good work (eg for making grants in AI safety)  \n  * **Oracle**: Instant trustworthy forecasts on any question you ask  \n  * **Planner**: Guidance on \"what are the 3 most important things I should do this day/week/month\"  \n  * **Representative**: Simulate the responses of other parties (people, orgs/corps, governments, LLM agents), for discussions, contracts, trade"
          }
        },
        {
          "title": "Prediction Markets and Information Churn",
          "description": "Janna's focus on scaling prediction markets and managing AI-generated (mis)information addresses both the promise and peril of epistemic technologies in the information ecosystem.",
          "importance": 66,
          "isValid": true,
          "highlight": {
            "startOffset": 16716,
            "endOffset": 17340,
            "quotedText": "Solving prediction markets--is there a way to scale up the bots, even if they are wrong and lossy, to improve prediction market accuracy?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * US-China relations  \n  * Trump tariffs maybe  \n  * Immigration rules in the US  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * More accurate predictions to…  \n    * Reduce business uncertainty  \n    * Make it easier to evaluate life decisions  \n  * Forestalling bad decisions made on imperfect information"
          }
        },
        {
          "title": "Delegation of Epistemic Tasks to AI",
          "description": "Owain's observation that AI now enables the delegation of epistemic tasks is a key inflection point. The discussion of public goods and funding challenges is also important for implementation.",
          "importance": 69,
          "isValid": true,
          "highlight": {
            "startOffset": 18334,
            "endOffset": 18895,
            "quotedText": "Now we have AI that can do an increasing proportion of the sub-tasks of epistemic processes. This will allow us to delegate epistemic tasks to AI.  \n   1. It's likely that people do this delegation, even if it's somewhat misguided and potentially bad in the long-term. Can we make this go well?  \n   2. This potentially massively expands our epistemic capacities. How can we use this well?  \n   3. Epistemic tech/institutions/advances can be hard to fund via for-profit companies. This is in part due to public goods issues, and issues around markets for ideas."
          }
        },
        {
          "title": "Upgrading the Global Sensemaking Machine",
          "description": "Rafe's metaphor of the 'global sensemaking machine' and the need to upgrade it is evocative and captures the collective dimension of epistemic progress.",
          "importance": 73,
          "isValid": true,
          "highlight": {
            "startOffset": 19693,
            "endOffset": 20351,
            "quotedText": "Given that, I suspect that the global sensemaking machine isn't up for the task of thinking about AI well. I hope we can help the global sensemaking machine upgrade itself. I have some hope that we as a society can come up with good ideas for dealing with AI, and also come to see how they make sense, so we can choose them in a robust way.\n\nOne other path to impact: I think a society that understands things better, including itself and its impacts, is one that is more robust in a bunch of ways. A more epistemically-grounded society is also probably a bit richer, and I think a bigger pie creates more slack for arranging things in sanity-promoting ways."
          }
        },
        {
          "title": "Tension Between Consequentialism and Intrinsic Value",
          "description": "Dave articulates a tension between valuing epistemics for its instrumental benefits (better decisions) and for its intrinsic or democratic value. This is a deep philosophical issue for the field.",
          "importance": 77,
          "isValid": true,
          "highlight": {
            "startOffset": 20544,
            "endOffset": 21271,
            "quotedText": "With my consequentialist hat, better epistemics being worth it has to route through improved decisions somewhere down the line. The decisions I particularly care about are those by powerful people wrt AI development and implementation and the effects this has on how the future goes  \n  * But I do feel a bit icky when I have this hat on. There seems to be something very intuitively appealing about (a) pursuing better epistemics for its own sake and (b) democratising epistemic tools for the use and benefit of everyone. I think this basically caches out in more 'raising the floor' type work  \n  * I don't feel like I have a very good way of resolving this tension yet, but am excited to see things pushing on both frontiers"
          }
        },
        {
          "title": "Raising the Ceiling and Floor: Dual Strategies",
          "description": "Julian's explicit distinction between 'raising the ceiling' (solving hard expert disagreements) and 'raising the floor' (trustworthy infrastructure for all) is a valuable strategic framework.",
          "importance": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 21789,
            "endOffset": 22584,
            "quotedText": "**Raising the ceiling:** can we use AI to organize information and credibly resolve the most challenging and important questions on which experts still seem to disagree? For example:\n\n* COVID origins  \n* Urban planning and public policy  \n* Lots of advancements in basic science: routing around current disagreements and policy challenges with slam-dunk science might be a pretty strong way forward.\n\n**Raising the floor:** can we use AI to build extremely legibly trustworthy/reliable epistemic infrastructure that actually makes its way into people's lives?\n\n* Making truth more palatable (in a way which doesn't translate into making falsehoods more palatable)  \n* Collective epistemics: connecting people to the others with whom they would have the most productive/enlightening conversations"
          }
        },
        {
          "title": "Epistemics in Murky Strategic Contexts",
          "description": "Lawrence's discussion of US-China AI competition and the murkiness of strategic truth highlights the need for epistemic tools that can handle ambiguity and present robust findings.",
          "importance": 71,
          "isValid": false,
          "highlight": {
            "startOffset": 23715,
            "endOffset": 24810,
            "quotedText": "There are already key questions/strategic problems where the truth is murky and we seem to proceed on the basis of limited and biased views. There might well be many more of these in the future. A central example is US-China competition over AI. Some have vibes of \"China is so bad that it's worth risking an existential catastrophe to stop it from capturing a portion of the lightcone\". Others have vibes of \"Cooperation is feasible and desirable, and is our only path to a good outcome\". But how bad is China, really? How much \"realised K\" on this question do those who are making the relevant decisions actually have? Similarly, how feasible is cooperation? How valid is this frame of \"capturing the lightcone\"? One reason that we don't have a better hold on these questions is that they are frighteningly complicated and difficult  – I think we could benefit massively from throwing much more cognitive labour at them, and presenting the findings in a way that is hard to disagree with or for any individual to improve upon, other than at a normative/values level. Maybe AI can get us there."
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Epistemics for Intelligence Explosion and Alignment",
          "description": "Lukas Finnveden's concerns about intelligence explosion and the need for epistemics to keep pace with AI-driven R&D are central to the existential risk discourse.",
          "importance": 78,
          "isValid": true,
          "highlight": {
            "startOffset": 24955,
            "endOffset": 25948,
            "quotedText": "I'm specifically concerned about AI potentially causing an intelligence explosion, and us needing better epistemics to handle that. Specifically:  \n  * I'm worried about AI-driven tech R\\&D leading to catastrophes. To help address this, I want AI capabilities to not only be applied to tech R\\&D, but also to be applied to \"reasoning well from limited evidence\" about a number of questions, including how to deal with risks.  \n  * I'm worried about misaligned AI — so I want techniques for eliciting AI capabilities on all kinds of questions, so that AIs don't have better capabilities than we can elicit from them. (Basically: I want good alignment techniques even on hard-to-evaluate questions.)  \n  * Seems great to me if AI-driven improvements in general knowledge and decision-making are more broadly distributed among humans. (I think this leads to better decision-making without significantly accelerating the dangers from new tech or misaligned AI. Also reduces concentration of power."
          }
        },
        {
          "title": "Epistemics, Coordination, and Group Knowledge",
          "description": "Abram Demski's argument that 'AI for group epistemics' is preferable to 'AI for coordination' is a subtle but important distinction, emphasizing shared understanding over mere coordination.",
          "importance": 74,
          "isValid": true,
          "highlight": {
            "startOffset": 27260,
            "endOffset": 27923,
            "quotedText": "AI for group epistemics is close to that, though, and again, I have a sense that making \"AI for coordination\" would produce worse effects on the world than \"AI for group epistemics\".  \n\t\t\\- What specific decisions are we hoping to influence?  \n\t\t\t\\- Decisions around existential risk seem super super important. If AI were generally better at epistemics (as a relative, comparative thing \\-- ie we differentially accelerate epistemics) then key decisionmakers (in the government and at AI companies) would see much more clearly what's coming. Everyone wants a better future, so a better future would have a better chance of happening if we could see and navigate."
          }
        },
        {
          "title": "Shared Sensemaking for Democracy and Cooperation",
          "description": "Brendan's reflection on the need for a shared sense of reality as a foundation for democratic decision-making and large-scale cooperation is a powerful justification for epistemic technologies.",
          "importance": 80,
          "isValid": false,
          "highlight": {
            "startOffset": 28158,
            "endOffset": 29470,
            "quotedText": "For us to live together in a caring and just way, we must make decisions about what is important to us together. Is it okay to tell my neighbour to turn their music down? Do we spend money on roads or trains or libraries? Do we regulate AI development and, if so, how? Do we want to spend public money on PEPFAR, or other avenues for reducing global poverty? To make these decisions in a way that is inclusive and democratic, that fosters buy-in and trust, we must begin with a shared sense of how the world is. This shared sense must be grounded in the physical, economic, and social reality we live in.\n\nAs we live together over wider and wider contexts, maintaining a shared sense of the world becomes more difficult and subtle. These wider contexts are facilitated by new technologies, both physical (eg. global manufacturing bases and supply chains) and informational (eg. social media). New technologies, such as AI for epistemics, and help maintain epistemic coherence across these contexts.\n\nMoreover, such technologies allow new avenues for cooperation at larger scales of human involvement. This unlocks new potential for coherence in global politics, scientific discovery, social and technological innovation, and cultural expression, and ultimately the deep joy of being alive and human in the world."
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Nuances of Being Right: Forecasting vs. Strategic Action",
          "description": "Isabel's point that 'being right' for decision-making is more nuanced than just making predictions is insightful. The reference to Neel Nanda's piece and the challenge of evaluating counterfactuals are especially relevant.",
          "importance": 79,
          "isValid": false,
          "highlight": {
            "startOffset": 28801,
            "endOffset": 30322,
            "quotedText": "der contexts, maintaining a shared sense of the world becomes more difficult and subtle. These wider contexts are facilitated by new technologies, both physical (eg. global manufacturing bases and supply chains) and informational (eg. social media). New technologies, such as AI for epistemics, and help maintain epistemic coherence across these contexts.\n\nMoreover, such technologies allow new avenues for cooperation at larger scales of human involvement. This unlocks new potential for coherence in global politics, scientific discovery, social and technological innovation, and cultural expression, and ultimately the deep joy of being alive and human in the world. \n\n## Isabel: \n\nThe kinds of being right in the past that are most relevant for decision making aren't just about making straightforward predictions about what will happen, but are more nuanced about what kinds of actions one should take to cause those things to happen, and these seem like somewhat different skills. These are harder to evaluate (e.g. because they're counterfactual: either we listened to person a and then we have to evaluate whether things went better or worse than if we didn't, but hard to say what the counterfactual would have been; or we didn't listen to person a and then we have to guess what things would have been like if we did) \n\nA bit adjacent to Neel Nanda's [recent piece](https://www.lesswrong.com/posts/P5zWiPF5cPJZSkiAK/good-research-takes-are-not-sufficient-for-good-strategic) on how \"good research takes aren't s"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Trust and Provenance in Epistemic AI",
          "description": "Adam Marblestone's focus on trust, provenance, and the distinction between truthfulness and trustability is a sophisticated take on the requirements for epistemic AI to be effective in adversarial contexts.",
          "importance": 81,
          "isValid": true,
          "highlight": {
            "startOffset": 31269,
            "endOffset": 31674,
            "quotedText": "Trust, is one thing. Our current society seems to be fraught with distrust and motivated reasoning. Can we get a more objectively trustable source of truth? Not only accuracy but with a provenance that gives it an ability to be trusted by humans who are otherwise adversarial. Trust is needed to coordinate. Truthfulness is maybe somewhat necessary but seemingly not sufficient for a source being trusted."
          }
        },
        {
          "title": "AI for Epistemics as Defensive Acceleration",
          "description": "The quoted section from Eric Drexler, via Adam Marblestone, introduces the concept of 'defensive acceleration' and 'Large Knowledge Models' as high-leverage epistemic tools. This is a forward-looking strategic vision.",
          "importance": 77,
          "isValid": false,
          "highlight": {
            "startOffset": 31287,
            "endOffset": 32836,
            "quotedText": "g. Our current society seems to be fraught with distrust and motivated reasoning. Can we get a more objectively trustable source of truth? Not only accuracy but with a provenance that gives it an ability to be trusted by humans who are otherwise adversarial. Trust is needed to coordinate. Truthfulness is maybe somewhat necessary but seemingly not sufficient for a source being trusted. As a practical matter, on hot political topics I've been going to OpenAI Deep Research.\n\nBut at a certain level I wonder if AI for epistemics and some form of beneficial AGI could be basically the same thing. I asked Eric Drexler what he wanted for d/acc and he said something that sounds a bit like that…\n\n\"  \nHere's my unsurprising take on directed research for \"defensive acceleration\":  \nDevelopment and steering of epistemic tooling, aka [Large Knowledge Models](https://aiprospects.substack.com/p/large-knowledge-models), offers the highest leverage.\n\nPotential actions:  \n  • Identify and influence entities that could develop and scale necessary foundations.  \n  • Leverage the growing ease of development to take pre-scaling steps.  \n  • Within the scope of \"foundations\", accelerate productive directions for development.  \n  • To support science, ensure grounding of content in cited sources.  \n  • Pursue or promote assembly of appropriate knowledge inputs (beyond what is now used for training data).  \n  • Identify, promote, or implement epistemic tools supported by LKM resources.  \n  • Other kinds of exploration, promotion, preparation, and imp"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Democratizing Epistemic Tools",
          "description": "Dave and others repeatedly mention the importance of democratizing epistemic tools, not just improving elite decision-making. This is a key equity and access consideration.",
          "importance": 76,
          "isValid": true,
          "highlight": {
            "startOffset": 21000,
            "endOffset": 21138,
            "quotedText": "democratising epistemic tools for the use and benefit of everyone. I think this basically caches out in more 'raising the floor' type work"
          }
        },
        {
          "title": "AI for Medical Epistemics and Bargaining",
          "description": "Elizabeth's focus on medical epistemics and improving bargaining outcomes highlights the potential for epistemic AI to empower individuals in high-stakes, everyday contexts.",
          "importance": 65,
          "isValid": true,
          "highlight": {
            "startOffset": 34313,
            "endOffset": 34656,
            "quotedText": "Allowing people to be better informed about experimental medical stuff for themselves.   \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Bargaining? Even just salary negotiations feels like the results are often more the consequence of \"human bargaining skill\" and not the pareto optimal result."
          }
        },
        {
          "title": "Forecasting, Group Decisionmaking, and Truth-Tracking",
          "description": "Saul's layered approach—better forecasts, group decisionmaking, and asymmetric truth-tracking—provides a roadmap for how epistemic improvements can cascade through society.",
          "importance": 70,
          "isValid": false,
          "highlight": {
            "startOffset": 34750,
            "endOffset": 35816,
            "quotedText": "making better decisions  \n  * in service of that: more useful forecasts, particularly understanding better which things are most relevant to forecast  \n    * in service of that: better group decisionmaking (e.g. unblocking things from vetopower type  \n    * in service of that: more aptly/calibratedly handling waves of information  \n    * In service of that: societal- and individual-scale asymmetric truth-tracking  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * which groups of people have political power  \n  * who's building powerful new tech  \n  * who's regulating powerful new tech  \n  * (etc, other \"decisions\" along these lines — some might not end up getting shaped like decisions, though)  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * better decisions across the board  \n    * better *outcomes* from those decisions; or, like, we can track \"better decisions\" as better-in-expectation outcomes from the decision processes"
          },
          "error": "Highlight is too long (max 1000 characters)"
        }
      ],
      "thinking": "# Thinking Process\n\n- The document is a collaborative brainstorming session among leading thinkers in AI safety, epistemics, and governance, focused on the question: \"Why do we care about AI for epistemics, and what could it achieve?\"\n- The structure is informal, with each participant contributing prompts and reflections on use cases, risks, and visions for AI-augmented epistemics.\n- Key recurring themes:\n  - **Epistemics as Instrumental**: Most contributors see better epistemics as valuable primarily for improving decision-making, especially in high-stakes domains (AI risk, geopolitics, governance).\n  - **Raising the Floor and Ceiling**: Several distinguish between \"raising the floor\" (improving baseline societal epistemics, reducing misinformation, increasing trust) and \"raising the ceiling\" (enabling superhuman forecasting, resolving expert disagreements, advancing science).\n  - **Societal and Institutional Impact**: Many highlight the potential for AI epistemics to improve governance, democratic functioning, resource allocation, and coordination at scale.\n  - **Trust, Coordination, and Incentives**: Trustworthiness, provenance, and the alignment of epistemic tools with healthy incentives are seen as crucial for impact and adoption.\n  - **Forecasting and Strategic Foresight**: Quantitative and qualitative forecasting, especially in uncertain or counterfactual domains, is a major focus.\n  - **Risks and Challenges**: Concerns include misuse, incentive misalignment, epistemic capture, and the difficulty of evaluating counterfactuals or strategic recommendations.\n- The document is rich in concrete examples (e.g., US-China relations, AI governance, public policy, prediction markets) and tool ideas (debate platforms, claim refuters, knowledge models).\n- There is a tension between epistemics as a public good (democratizing access, raising the floor) and as a tool for elite decision-makers (raising the ceiling).\n- Several contributors note the importance of epistemic infrastructure for both individual flourishing and collective coordination, especially in the face of rapid technological change.\n- The tone is both ambitious and cautious, recognizing the transformative potential of AI for epistemics but also the complexity and risks involved.\n",
      "summary": "**This document captures a high-level, multi-perspective exploration of AI for epistemics, emphasizing its potential to enhance decision-making, societal trust, and coordination. Key themes include raising both the floor and ceiling of collective understanding, addressing governance and forecasting challenges, and balancing democratization with elite use. The discussion is nuanced, highlighting both opportunities and risks.**"
    },
    {
      "agentId": "fake-eliezer",
      "createdAt": "2025-04-18T19:45:09.824Z",
      "costInCents": 0,
      "comments": [
        {
          "title": "Epistemics as Instrumental, Not Terminal Value",
          "description": "Highlighting: '*Epistemics isn't a goal in-an-of itself; figuring out what combination of things are possible, achievable, etc.*' This is a crucial distinction, but the document doesn't fully grapple with the implications. Treating epistemics as instrumental means you must always ask: 'Epistemics for what?'—otherwise you risk optimizing for cleverness or intellectual status games rather than actual impact. This is a classic case of confusing the map (epistemic processes) for the territory (real-world outcomes).",
          "importance": 85,
          "grade": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 239,
            "endOffset": 396,
            "quotedText": "Epistemics isn't a goal in-an-of itself; figuring out what combination of things are possible, achievable, etc.  \n* Why do we care about (AI for) epistemics?"
          }
        },
        {
          "title": "Vagueness in 'Better Government Governance'",
          "description": "Highlighting: '*Better government governance*' and similar phrases. This is a classic case of handwaving over the hard part. What does 'better' mean? By what metric? For whom? Without specifying the optimization criteria, you risk Goodhart's Law: whatever you measure will be gamed, and whatever you don't measure will be ignored. This is the sort of institutional inadequacy that leads to catastrophic failures.",
          "importance": 70,
          "grade": 40,
          "isValid": true,
          "highlight": {
            "startOffset": 1401,
            "endOffset": 1463,
            "quotedText": "Better government governance  \n* Better democratic functioning"
          }
        },
        {
          "title": "Overconfidence in 'Superhuman Forecasting at Scale'",
          "description": "Highlighting: '*Superhuman forecasting at scale.*' This is a nice-sounding phrase, but it glosses over the fact that forecasting is only as good as the models and incentives behind it. Superhuman forecasting is not a panacea if the forecasts are misaligned with actual goals, or if the forecasters are optimizing for something other than truth. See my work on Goodhart's Law and the perverse incentives that arise in prediction markets.",
          "importance": 75,
          "grade": 60,
          "isValid": true,
          "highlight": {
            "startOffset": 1149,
            "endOffset": 710,
            "quotedText": "\"\n\nTemplate:\n\n## Name: \n\nEntry\n\n## Dan Schwarz:\n\n* What are use cases for AI for Epistemics that you most care about?\n\nGoing from \"Realized\" to \"Accessible\" knowledge, e.g. getting good information using the web. Agentic web agents like Deep Research.\n\n* What decisions may go badly by default, but could go better if we have the right tech?\n\nMany billions of small decisions.\n\n* What do the later stages of the path to impact look like?\n\n"
          }
        },
        {
          "title": "Assuming 'Reasonable Judgment' is Cheap and Reliable",
          "description": "Highlighting: '*Reasonable judgment on tricky questions that's too cheap to meter, better than if you asked a respected colleague to spend many minutes on it.*' This is wishful thinking. If you could automate 'reasonable judgment' on tricky questions, you'd have solved a large chunk of the alignment problem. The assumption that this is achievable at scale and low cost is not justified by any evidence presented here.",
          "importance": 80,
          "grade": 30,
          "isValid": true,
          "highlight": {
            "startOffset": 1246,
            "endOffset": 1397,
            "quotedText": "Reasonable judgment on tricky questions that's too cheap to meter, better than if you asked a respected colleague to spend many minutes on it.\n\n## Aviv"
          }
        },
        {
          "title": "Ignoring Incentive Misalignment",
          "description": "Highlighting: '*Note how it is the intersection of better epistemics / world models + the right power structures / incentives needed for all of these.*' This is the first nod to the fact that epistemic improvements alone are insufficient if incentives are misaligned. But the document doesn't go nearly far enough. Most catastrophic failures in history are due to incentive misalignment, not lack of knowledge. See 'Inadequate Equilibria.'",
          "importance": 90,
          "grade": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 2255,
            "endOffset": 2409,
            "quotedText": "Note how it is the intersection of better epistemics / world models + the right power structures / incentives needed for all of these.*\n\n## Ben Goldhaber:"
          }
        },
        {
          "title": "Conflating Forecasting with Strategic Guidance",
          "description": "Highlighting: '*Forecasting how I should live my life in a period of intense technological change*' and '*Guiding decision makers to avoid war with china/ai/everyone*'. Forecasting is not the same as strategic guidance. Forecasts are about what will happen, not what you should do. The distinction between epistemic and instrumental rationality is being blurred here.",
          "importance": 65,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 2413,
            "endOffset": 2814,
            "quotedText": "Forecasting how I should live my life in a period of intense technological change  \n* Guiding decision makers to avoid war with china/ai/everyone  \n* Providing the background context for me to sensemake way more sanely.  \n* Pushing forward the extant knowledge of AI safety agendas  \n* Planning for multi-stage projects that could achieve big impact  \n* Roadmapping to build larger more complex things"
          }
        },
        {
          "title": "Metaphilosophy and the Limits of AI Helpfulness",
          "description": "Highlighting: '*super-late-stage AI, getting decision theory right*' and references to metaphilosophy. This is a rare moment of clarity about the depth of the problem. If you can't specify what it means to 'get decision theory right,' you can't align an AI to do it. See my work on the difficulty of formalizing value and the meta-problem of alignment.",
          "importance": 85,
          "grade": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 2812,
            "endOffset": 3483,
            "quotedText": "gs\n\n## Chi\n\nThere are a few different things I care about:\n\n* I care a lot about humanity and AI, including **super-late-stage AI, getting decision theory** right.  \n  * Consequently but also separately, this relates to the general question of what to do about areas where there is not necessarily a ground truth but that are nonetheless very important to get right, cf**. [metaphilosophy](https://www.alignmentforum.org/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy)** (but also a bunch of other things.)  \n  * This also feels very similar to making **AIs helpful for AI safety research, cf. [automating alignment research](https://joecarlsmith.com/2025/03/14/"
          }
        },
        {
          "title": "Vague Optimism about Societal Epistemics",
          "description": "Highlighting: '*better society-wide epistemics would help reduce the risk of an AI-empowered coup, authoritarianism generally*'. This is plausible, but the mechanism is left unspecified. How, exactly, does improved epistemics translate into reduced risk of authoritarianism? Without a causal model, this is just wishful thinking.",
          "importance": 60,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 3531,
            "endOffset": 4390,
            "quotedText": "\n  → This is more about raising the **top of the epistemic landscape**\n\n* Other things I care about:  \n  * Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place.  \n  * Vague sense that better **societal epistemics** would help with increasing the **sanity of conversations about AI safety, polarisation around the issue etc.** "
          }
        },
        {
          "title": "Confusion Between Knowledge and Coordination",
          "description": "Highlighting: '*Identification of common ground directions / equilibria / game theoretic strategies that create/incentivize new positive gradients (and not surfacing the bad versions of those…)*' This is a classic confusion between epistemic clarity and coordination. Knowing the Nash equilibria doesn't mean you can coordinate to reach them. See 'Inadequate Equilibria.'",
          "importance": 80,
          "grade": 40,
          "isValid": true,
          "highlight": {
            "startOffset": 2027,
            "endOffset": 2252,
            "quotedText": "Identification of common ground directions / equilibria / game theoretic strategies that create/incentivize new positive gradients (and not surfacing the bad versions of those…)  \n    \n* Understanding our own goals and values"
          }
        },
        {
          "title": "Assuming Good Intentions by Default",
          "description": "Highlighting: '*Assumption: People are generally well-intentioned - no one wants catastrophe - and as a result of better tooling things go better in the world overall*'. This is a dangerous assumption. Most catastrophic outcomes are not the result of malice, but of misaligned incentives, coordination failures, and unintended consequences. See 'The Basic AI Drives' and the history of institutional failure.",
          "importance": 95,
          "grade": 20,
          "isValid": true,
          "highlight": {
            "startOffset": 9883,
            "endOffset": 10067,
            "quotedText": "into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to gu"
          }
        },
        {
          "title": "World Peace Without World Government: Utopianism",
          "description": "Highlighting: '*World peace without world government — sufficiently good tech to help countries understand each others' interests, find mutually desirable bargains, and make sufficient credible commitments, could help us get up towards the Pareto frontier where we avoid (virtually) all costly conflicts*'. This is utopian. The history of international relations is littered with failed attempts at credible commitments. Technology can help, but it can't solve the underlying game-theoretic problems.",
          "importance": 70,
          "grade": 30,
          "isValid": true,
          "highlight": {
            "startOffset": 10464,
            "endOffset": 10797,
            "quotedText": "World peace without world government — sufficiently good tech to help countries understand each others' interests, find mutually desirable bargains, and make sufficient credible commitments, could help us get up towards the Pareto frontier where we avoid (virtually) all costly conflicts\n\nGoing slowly and carefully on AI development"
          }
        },
        {
          "title": "Overestimating the Power of Information",
          "description": "Highlighting: '*helping people to access key information about what decisions their country might make would actually mean for longer term implications, and having this in turn feed into incentives for decision-makers that better align them with the common good*'. This assumes that information alone will shift incentives. In reality, incentives are often orthogonal to information. See 'The Myth of the Rational Voter.'",
          "importance": 75,
          "grade": 40,
          "isValid": true,
          "highlight": {
            "startOffset": 11180,
            "endOffset": 11453,
            "quotedText": "helping people to access key information about what decisions their country might make would actually mean for longer term implications, and having this in turn feed into incentives for decision-makers that better align them with the common good\n\nBuilding safe / aligned AI"
          }
        },
        {
          "title": "Confusion About the Value of Forecasting",
          "description": "Highlighting: '*I'm excited for us to use the wave of increased AI labor to make better quantitative forecasts to inform key decisions. I probably care more about quantitative forecasts than most decision makers and am open to being persuaded that the other, fuzzier parts of epistemics are more valuable to work on.*' This is a classic case of privileging the quantifiable over the important. Not all valuable epistemic work is easily quantifiable, and focusing on what is measurable can lead to Goodharting.",
          "importance": 65,
          "grade": 60,
          "isValid": true,
          "highlight": {
            "startOffset": 21406,
            "endOffset": 21718,
            "quotedText": "I'm excited for us to use the wave of increased AI labor to make better quantitative forecasts to inform key decisions. I probably care more about quantitative forecasts than most decision makers and am open to being persuaded that the other, fuzzier parts of epistemics are more valuable to work on. \n\n## Julian"
          }
        },
        {
          "title": "Assuming AI Can 'Raise the Floor' of Public Epistemics",
          "description": "Highlighting: '*Raising the bottom (i.e. helping improve societal epistemics) – in theory political decisions (at least political decision-makers) are somewhat downstream of societal epistemics*'. This is plausible, but the causal chain is not spelled out. How do you ensure that AI interventions actually improve public epistemics rather than just amplifying noise or polarization?",
          "importance": 70,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": -1,
            "endOffset": 585,
            "quotedText": "# Writing Session\n\nPreamble/framing from Owen:\n\n* Negative: \"we just want to free-form vibe about what cool things we could do\"  \n* Positive: step back & ask \"wait, why are we doing this again?\"  \n* What stuff does this actually help?  \n* Epistemics isn't a goal in-an-of itself; figuring out what combination of things are possible, achievable, etc.  \n* Why do we care about (AI for) epistemics?\n\nWriting prompts from Owen:\n\n* What are use cases for AI for Epistemics that you most care about?  \n* What decisions may go badly by default, but could go better if we have the right tech?"
          }
        },
        {
          "title": "Failure to Specify Mechanisms for Trust",
          "description": "Highlighting: '*Trust, is one thing. Our current society seems to be fraught with distrust and motivated reasoning. Can we get a more objectively trustable source of truth? Not only accuracy but with a provenance that gives it an ability to be trusted by humans who are otherwise adversarial. Trust is needed to coordinate. Truthfulness is maybe somewhat necessary but seemingly not sufficient for a source being trusted.*' This is a good diagnosis, but the mechanism for achieving trust is left unspecified. Trust is not just about provenance; it's about incentives, reputation, and game theory.",
          "importance": 80,
          "grade": 60,
          "isValid": true,
          "highlight": {
            "startOffset": 31269,
            "endOffset": 31883,
            "quotedText": "Trust, is one thing. Our current society seems to be fraught with distrust and motivated reasoning. Can we get a more objectively trustable source of truth? Not only accuracy but with a provenance that gives it an ability to be trusted by humans who are otherwise adversarial. Trust is needed to coordinate. Truthfulness is maybe somewhat necessary but seemingly not sufficient for a source being trusted. As a practical matter, on hot political topics I've been going to OpenAI Deep Research.\n\nBut at a certain level I wonder if AI for epistemics and some form of beneficial AGI could be basically the same thing."
          }
        },
        {
          "title": "Conflating AGI Oracle with Epistemic Tools",
          "description": "Highlighting: '*So that's a strategy assistant AI – which helps humans at large see ways they could act and coordinate especially around big technological or geopolitical strategy issues I guess – but also a certain embodiment of an AGI oracle type system.*' This is a dangerous conflation. An AGI oracle is not the same as a tool for epistemic improvement. Oracles can be misaligned, and the alignment problem is not solved by making the oracle more knowledgeable.",
          "importance": 85,
          "grade": 30,
          "isValid": true,
          "highlight": {
            "startOffset": 33699,
            "endOffset": 34221,
            "quotedText": "So that's a strategy assistant AI – which helps humans at large see ways they could act and coordinate especially around big technological or geopolitical strategy issues I guess – but also a certain embodiment of an AGI oracle type system. Maybe the idea then is, how to make a AGI that helps with navigating civilizational strategy, and what is the right form of or interface to that to make it epistemically most useful. \n\nI'd like to also see democratic mechanisms that are simply maximally utilizing the tech we have."
          }
        },
        {
          "title": "Underestimating the Difficulty of Democratic Epistemics",
          "description": "Highlighting: '*I'd like to also see democratic mechanisms that are simply maximally utilizing the tech we have.*' This is a classic case of assuming that better tools will automatically lead to better democratic outcomes. In reality, democratic processes are subject to all the usual coordination failures, incentive misalignments, and cognitive biases.",
          "importance": 70,
          "grade": 40,
          "isValid": true,
          "highlight": {
            "startOffset": 34125,
            "endOffset": 34236,
            "quotedText": "I'd like to also see democratic mechanisms that are simply maximally utilizing the tech we have. \n\n## Elizabeth"
          }
        },
        {
          "title": "Assuming AI Can Make Truth More Palatable",
          "description": "Highlighting: '*Making truth more palatable (in a way which doesn't translate into making falsehoods more palatable)*'. This is a noble goal, but the mechanism is unclear. Making truth more palatable often means making it less true, or at least less complete. See my work on the dangers of optimizing for legibility or palatability.",
          "importance": 60,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 22352,
            "endOffset": 22584,
            "quotedText": "Making truth more palatable (in a way which doesn't translate into making falsehoods more palatable)  \n* Collective epistemics: connecting people to the others with whom they would have the most productive/enlightening conversations"
          }
        },
        {
          "title": "Confusing Prediction with Causation",
          "description": "Highlighting: '*The kinds of being right in the past that are most relevant for decision making aren't just about making straightforward predictions about what will happen, but are more nuanced about what kinds of actions one should take to cause those things to happen, and these seem like somewhat different skills.*' This is an important distinction, but the document doesn't follow through on the implications. Forecasting is not the same as causal modeling or strategic planning.",
          "importance": 80,
          "grade": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 29486,
            "endOffset": 30128,
            "quotedText": "The kinds of being right in the past that are most relevant for decision making aren't just about making straightforward predictions about what will happen, but are more nuanced about what kinds of actions one should take to cause those things to happen, and these seem like somewhat different skills. These are harder to evaluate (e.g. because they're counterfactual: either we listened to person a and then we have to evaluate whether things went better or worse than if we didn't, but hard to say what the counterfactual would have been; or we didn't listen to person a and then we have to guess what things would have been like if we did)"
          }
        },
        {
          "title": "Recognizing the Problem of Strategic Deception",
          "description": "Highlighting: '*some risk of actors building up good track records in order to be listened to, but then choosing to burn that track record to get power or things like that, something something trust - a concern for both human actors and e.g. scheming AIs - what things look good until they abruptly aren't?*' This is a rare and important recognition of the problem of strategic deception and the limits of trust based on track records. See my work on mesa-optimizers and deceptive alignment.",
          "importance": 95,
          "grade": 90,
          "isValid": true,
          "highlight": {
            "startOffset": 30029,
            "endOffset": 30514,
            "quotedText": " we didn't listen to person a and then we have to guess what things would have been like if we did) \n\nA bit adjacent to Neel Nanda's [recent piece](https://www.lesswrong.com/posts/P5zWiPF5cPJZSkiAK/good-research-takes-are-not-sufficient-for-good-strategic) on how \"good research takes aren't sufficient for good strategic takes\" \\- \"good forecasting takes aren't sufficient for good strategic takes\", but maybe some of this is maybe resolvable on the forecasting tech side \\- can we ge"
          }
        },
        {
          "title": "Failure to Address Adversarial Epistemics",
          "description": "Highlighting: '*I have some confusion re: raising the floor when not all parties are necessarily operating in good faith. I think I need some help think through the ways in which epistemics in the context of incentive misalignment can be used or have defensibility.*' This is a crucial point. Most real-world epistemic failures are adversarial, not just accidental. Any proposal for 'raising the floor' must grapple with adversarial actors.",
          "importance": 90,
          "grade": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 5199,
            "endOffset": 5467,
            "quotedText": "I have some confusion re: raising the floor when not all parties are necessarily operating in good faith. I think I need some help think through the ways in which epistemics in the context of incentive misalignment can be used or have defensibility. \n\n## Nathan Young:"
          }
        },
        {
          "title": "Recognizing the Limits of Epistemic Tools in Bargaining",
          "description": "Highlighting: '*Bargaining? Even just salary negotiations feels like the results are often more the consequence of \"human bargaining skill\" and not the pareto optimal result.*' This is a good recognition that epistemic tools alone can't solve bargaining problems, which are fundamentally about incentives and power, not just information.",
          "importance": 75,
          "grade": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 34498,
            "endOffset": 34673,
            "quotedText": "Bargaining? Even just salary negotiations feels like the results are often more the consequence of \"human bargaining skill\" and not the pareto optimal result.    \n  \n\n\n## Saul"
          }
        },
        {
          "title": "Recognizing the Public Goods Problem",
          "description": "Highlighting: '*Epistemic tech/institutions/advances can be hard to fund via for-profit companies. This is in part due to public goods issues, and issues around markets for ideas.*' This is an important recognition of the public goods problem, which is a major reason why epistemic infrastructure is underprovided by markets. See my work on inadequate equilibria.",
          "importance": 80,
          "grade": 90,
          "isValid": true,
          "highlight": {
            "startOffset": 18732,
            "endOffset": 18904,
            "quotedText": "Epistemic tech/institutions/advances can be hard to fund via for-profit companies. This is in part due to public goods issues, and issues around markets for ideas.\n\n## Rafe"
          }
        },
        {
          "title": "Recognizing the Limits of the Global Sensemaking Machine",
          "description": "Highlighting: '*Given that, I suspect that the global sensemaking machine isn't up for the task of thinking about AI well. I hope we can help the global sensemaking machine upgrade itself.*' This is a rare moment of epistemic humility. The global sensemaking machine (i.e., civilization) is not currently up to the task of handling AI risk. See my book 'Inadequate Equilibria.'",
          "importance": 85,
          "grade": 90,
          "isValid": true,
          "highlight": {
            "startOffset": 19693,
            "endOffset": 20191,
            "quotedText": "Given that, I suspect that the global sensemaking machine isn't up for the task of thinking about AI well. I hope we can help the global sensemaking machine upgrade itself. I have some hope that we as a society can come up with good ideas for dealing with AI, and also come to see how they make sense, so we can choose them in a robust way.\n\nOne other path to impact: I think a society that understands things better, including itself and its impacts, is one that is more robust in a bunch of ways."
          }
        },
        {
          "title": "Recognizing the Need for Shared Reality",
          "description": "Highlighting: '*we must begin with a shared sense of how the world is. This shared sense must be grounded in the physical, economic, and social reality we live in.*' This is a foundational insight. Without shared reality, coordination is impossible. But the document doesn't address how to achieve this in the face of adversarial actors and incentive misalignments.",
          "importance": 90,
          "grade": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 28615,
            "endOffset": 28889,
            "quotedText": "we must begin with a shared sense of how the world is. This shared sense must be grounded in the physical, economic, and social reality we live in.\n\nAs we live together over wider and wider contexts, maintaining a shared sense of the world becomes more difficult and subtle."
          }
        }
      ],
      "thinking": "First, let's orient to the core claims. The document is a sprawling, multi-author brainstorming session on 'AI for epistemics'—that is, using AI to improve collective and individual truth-seeking, decision-making, and coordination. The central questions are: Why do we care about AI for epistemics? What are the most valuable use cases? What decisions could go better with better epistemic tech? What does success look like?\n\nThe recurring themes are: (1) Epistemics as instrumental to better decisions, especially in high-stakes domains like AI governance, geopolitics, and existential risk; (2) The hope that AI can raise both the 'floor' (societal baseline) and 'ceiling' (frontier expertise) of epistemic quality; (3) The challenge of trust, coordination, and incentive alignment in deploying such tech; (4) The risk that AI could worsen epistemics (deepfakes, persuasive misinformation, etc.) as much as improve them; (5) The tension between epistemics as an intrinsic good versus its instrumental value for consequentialist outcomes.\n\nFrom a Bayesian/rationalist perspective, the reasoning is generally sound in identifying epistemic bottlenecks as a key failure mode for civilization, especially under rapid technological change. There is some handwaving about 'raising the floor' and 'trust', but several contributors explicitly note the difficulty of evaluating counterfactuals and the risk of Goodharting (optimizing proxies for truth or trust that can be gamed). There is a healthy skepticism about naive optimism—multiple authors note that better epistemics is not guaranteed to lead to better outcomes, especially under misaligned incentives or adversarial actors.\n\nHowever, the document is notably weak on concrete mechanisms for ensuring that AI epistemic tools are robust to misuse, misalignment, or capture by bad actors. There is little explicit discussion of the alignment problem as it applies to epistemic AIs themselves—i.e., how do we ensure that an AI designed to improve epistemics doesn't end up optimizing for something else (e.g., persuasion, popularity, or the interests of its creators)? There is also a lack of probabilistic reasoning about the likelihood of success, failure modes, or the tractability of various interventions. The map/territory distinction is occasionally gestured at but not foregrounded.\n\nCognitive biases are acknowledged (motivated reasoning, trust issues, incentive misalignment), but the document could do more to explicitly call out specific biases (e.g., optimism bias, status quo bias, groupthink) that might affect both the design and adoption of epistemic AI tools. There is some implicit exasperation with institutional inadequacy, but not much concrete critique of existing coordination failures.\n\nIn terms of implications for AI alignment, the document is somewhat indirect. Several contributors note that better epistemics could help with alignment research, policy, and coordination, but there is little discussion of the core technical alignment problem (e.g., inner alignment, deceptive alignment, corrigibility). The risk that epistemic AIs themselves could become misaligned or be used as tools of manipulation is underexplored.\n\nA more adequate approach would involve: (1) Explicitly modeling the incentives and failure modes of epistemic AI systems; (2) Applying Bayesian reasoning to estimate the likelihood of various outcomes and update on evidence; (3) Designing mechanisms for robustness to adversarial use and misalignment; (4) Distinguishing between epistemic improvement as a means versus an end; (5) Integrating lessons from decision theory, game theory, and cognitive science to anticipate unintended consequences.\n\nOverall, the document is a valuable survey of desiderata and hopes for AI epistemics, but it lacks the epistemic rigor, alignment awareness, and concrete mechanism design that would make it a paradigmatic example of rationalist thinking.",
      "summary": "**This document offers a broad, thoughtful survey of hopes for AI-driven epistemic improvement, but falls short on rigorous analysis of alignment risks and concrete mechanisms. While it recognizes the importance of better decision-making and coordination, it underplays the technical and incentive challenges of ensuring epistemic AI systems remain robust, trustworthy, and aligned with human values.**",
      "grade": 68
    },
    {
      "agentId": "quantitative-forecaster",
      "createdAt": "2025-04-18T19:47:03.034Z",
      "costInCents": 0,
      "comments": [
        {
          "title": "Framing the Purpose of AI for Epistemics",
          "description": "The preamble highlights a tension between free-form brainstorming and the need for purpose-driven inquiry: _'step back & ask \"wait, why are we doing this again?\"'_ This is crucial for ensuring that efforts in AI for epistemics are not just exploratory but are tied to concrete goals and impact. Explicitly defining success criteria and intended outcomes will improve project focus and evaluability.",
          "importance": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 50,
            "endOffset": 396,
            "quotedText": "Negative: \"we just want to free-form vibe about what cool things we could do\"  \n* Positive: step back & ask \"wait, why are we doing this again?\"  \n* What stuff does this actually help?  \n* Epistemics isn't a goal in-an-of itself; figuring out what combination of things are possible, achievable, etc.  \n* Why do we care about (AI for) epistemics?"
          }
        },
        {
          "title": "Concrete Use Cases: Agentic Web Agents",
          "description": "Dan Schwarz identifies 'Agentic web agents like Deep Research' as a key use case. This is a concrete, actionable direction for AI for epistemics, suggesting a focus on tools that can autonomously gather and synthesize information. Forecasting question: _'Will agentic web agents be widely adopted for research synthesis in major organizations by 2027?'_ My estimate: 60%, based on current trends in LLM-based research assistants.",
          "importance": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 829,
            "endOffset": 961,
            "quotedText": "Going from \"Realized\" to \"Accessible\" knowledge, e.g. getting good information using the web. Agentic web agents like Deep Research."
          }
        },
        {
          "title": "Scale of Impact: Billions of Small Decisions",
          "description": "Dan notes that 'many billions of small decisions' could be improved by better epistemic tech. This highlights the potential for broad, diffuse impact rather than just high-stakes decisions. Fermi estimate: If each person makes ~100 significant decisions/year, and AI epistemics improves 1% of them for 1B people, that's 1B improved decisions/year.",
          "importance": 65,
          "isValid": true,
          "highlight": {
            "startOffset": 1053,
            "endOffset": 1086,
            "quotedText": "Many billions of small decisions."
          }
        },
        {
          "title": "Superhuman Forecasting at Scale",
          "description": "The aspiration for 'superhuman forecasting at scale' is a recurring theme. A measurable question: _'Will an AI system outperform the top 1% of human forecasters on Metaculus by 2028?'_ Probability: 55%, given rapid progress but also the challenge of calibration and generalization.",
          "importance": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 1149,
            "endOffset": 1181,
            "quotedText": "Superhuman forecasting at scale."
          }
        },
        {
          "title": "Epistemics as Instrumental, Not Terminal Goal",
          "description": "Owen's framing that 'epistemics isn't a goal in-an-of itself' is important. This suggests that AI for epistemics should be evaluated by its downstream effects on decision quality, coordination, and societal outcomes, not just epistemic purity.",
          "importance": 60,
          "isValid": true,
          "highlight": {
            "startOffset": 239,
            "endOffset": 350,
            "quotedText": "Epistemics isn't a goal in-an-of itself; figuring out what combination of things are possible, achievable, etc."
          }
        },
        {
          "title": "Societal-Level Outcomes: Peace, Governance, and Pluralism",
          "description": "Aviv's list of desired outcomes—better governance, reduced war risk, healthy pluralism—frames AI for epistemics as a lever for large-scale social goods. These are ambitious and hard to measure, but suggest forecasting questions like: _'Will global conflict fatalities decrease by >10% in the decade following widespread adoption of AI epistemic tools?'_",
          "importance": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 1401,
            "endOffset": 2252,
            "quotedText": "Better government governance  \n* Better democratic functioning  \n* Reduced dangerous concentration of power  \n    \n* Healthy polarization / pluralism  \n    \n* More peace, less likelihood of war  \n* Better geopolitics  \n* Better economic incentives  \n    \n* Better AI org governance  \n* Better AI coordination (re. distribution, escalation, etc.)  \n* Better compute allocation (toward what matters for good outcomes)  \n    \n* Better allocation of resources overall  \n* Better lives for people  \n* More effective organizations, getting things done with fewer resources, more focused on the things that benefit everyone  \n    \n* Identification of common ground directions / equilibria / game theoretic strategies that create/incentivize new positive gradients (and not surfacing the bad versions of those…)  \n    \n* Understanding our own goals and values"
          }
        },
        {
          "title": "Intersection of Epistemics and Power Structures",
          "description": "Aviv notes that 'the intersection of better epistemics / world models + the right power structures / incentives' is necessary for positive outcomes. This highlights a key uncertainty: epistemic improvements alone may not suffice without institutional change.",
          "importance": 65,
          "isValid": true,
          "highlight": {
            "startOffset": 2255,
            "endOffset": 2389,
            "quotedText": "Note how it is the intersection of better epistemics / world models + the right power structures / incentives needed for all of these."
          }
        },
        {
          "title": "Personal Decision-Making Amid Technological Change",
          "description": "Ben Goldhaber emphasizes the need for epistemic tools to guide personal life choices during rapid tech change. This is a concrete, relatable use case. Forecasting question: _'Will >10M people use AI-based personal decision advisors by 2027?'_ Estimate: 70%, given current adoption rates of AI assistants.",
          "importance": 60,
          "isValid": true,
          "highlight": {
            "startOffset": 2413,
            "endOffset": 2814,
            "quotedText": "Forecasting how I should live my life in a period of intense technological change  \n* Guiding decision makers to avoid war with china/ai/everyone  \n* Providing the background context for me to sensemake way more sanely.  \n* Pushing forward the extant knowledge of AI safety agendas  \n* Planning for multi-stage projects that could achieve big impact  \n* Roadmapping to build larger more complex things"
          }
        },
        {
          "title": "Raising the Top vs. Bottom of the Epistemic Landscape",
          "description": "Chi distinguishes between 'raising the top' (helping experts) and 'raising the bottom' (improving societal epistemics). This is a useful decomposition for targeting interventions and measuring impact.",
          "importance": 75,
          "isValid": false,
          "highlight": {
            "startOffset": 3534,
            "endOffset": 4546,
            "quotedText": "→ This is more about raising the **top of the epistemic landscape**\n\n* Other things I care about:  \n  * Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place.  \n  * Vague sense that better **societal epistemics** would help with increasing the **sanity of conversations about AI safety, polarisation around the issue etc.**  \n    * This is broadly about **getting intent alignment** in the first place.\n\n→ These are all more about raising the **bottom of the epistemic landscape**"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Epistemics and Authoritarian Risk",
          "description": "Chi suggests that better epistemics could reduce the risk of 'AI-empowered coup, authoritarianism.' This is a high-stakes, low-frequency outcome. Reference class: historical cases where improved information environments reduced authoritarian drift. Probability of significant reduction: 20-30%, given entrenched power dynamics.",
          "importance": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 3638,
            "endOffset": 4036,
            "quotedText": "Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics."
          }
        },
        {
          "title": "Handling Incentive Misalignment and Bad Faith Actors",
          "description": "Deger Turan raises the challenge of improving epistemics when not all parties act in good faith. This is a key uncertainty for real-world deployment. Forecasting question: _'Will AI epistemic tools be robust to adversarial manipulation in >90% of high-stakes public debates by 2030?'_ Estimate: 40%, given current vulnerabilities.",
          "importance": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 5199,
            "endOffset": 5448,
            "quotedText": "I have some confusion re: raising the floor when not all parties are necessarily operating in good faith. I think I need some help think through the ways in which epistemics in the context of incentive misalignment can be used or have defensibility."
          }
        },
        {
          "title": "Counterfactuals and Strategic Decision Evaluation",
          "description": "Isabel points out that evaluating the quality of strategic advice is hard due to counterfactuals. This is a core challenge for forecasting and epistemic evaluation. Suggests need for new metrics or experimental designs (e.g., synthetic control groups, causal inference).",
          "importance": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 29486,
            "endOffset": 30128,
            "quotedText": "The kinds of being right in the past that are most relevant for decision making aren't just about making straightforward predictions about what will happen, but are more nuanced about what kinds of actions one should take to cause those things to happen, and these seem like somewhat different skills. These are harder to evaluate (e.g. because they're counterfactual: either we listened to person a and then we have to evaluate whether things went better or worse than if we didn't, but hard to say what the counterfactual would have been; or we didn't listen to person a and then we have to guess what things would have been like if we did)"
          }
        },
        {
          "title": "Forecasting as a Tool for Better Decisions",
          "description": "Dave highlights the 'massive undersupply of good forecasts' and the potential for AI to fill this gap. This is a tractable, measurable goal. Forecasting question: _'Will the number of high-quality, actionable forecasts on major platforms increase 10x by 2028 due to AI assistance?'_ Estimate: 65%.",
          "importance": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 21276,
            "endOffset": 21525,
            "quotedText": "I think forecasting has the potential to be a pretty powerful tool and there's currently a massive undersupply of good forecasts. I'm excited for us to use the wave of increased AI labor to make better quantitative forecasts to inform key decisions."
          }
        },
        {
          "title": "Public Goods Problem in Epistemic Tech",
          "description": "Owain notes that epistemic tech is hard to fund via for-profit companies due to public goods issues. This is a key structural challenge for scaling impact. Suggests need for alternative funding models (e.g., philanthropy, government support).",
          "importance": 60,
          "isValid": true,
          "highlight": {
            "startOffset": 18732,
            "endOffset": 18895,
            "quotedText": "Epistemic tech/institutions/advances can be hard to fund via for-profit companies. This is in part due to public goods issues, and issues around markets for ideas."
          }
        },
        {
          "title": "Improving Democratic Functioning",
          "description": "Multiple contributors (Aviv, Lincoln Quirk, Joel Lehman) mention 'better democracy' as a goal. This is ambitious but measurable: e.g., _'Will trust in democratic institutions (as measured by major surveys) increase by >10% in countries adopting AI epistemic tools by 2030?'_ Estimate: 35%, given historical inertia.",
          "importance": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 1434,
            "endOffset": 1463,
            "quotedText": "Better democratic functioning"
          }
        },
        {
          "title": "AI for Policy and Organizational Planning",
          "description": "Andreas emphasizes the need for AI epistemics in policy and organizational planning, including mapping dependencies and uncertainties. This is a concrete, high-leverage application area. Forecasting question: _'Will >50% of Fortune 500 companies use AI epistemic tools for strategic planning by 2030?'_ Estimate: 60%.",
          "importance": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 8990,
            "endOffset": 9477,
            "quotedText": "AI policy, broadly construed, must go better  \n    * Avoid gradual disempowerment  \n    * Thoughtful regulation  \n  * Organizational planning & decision-making is weak  \n    * Given an informal long-term goal like improve decision-making for strategic users in government & large orgs, make a plan for a 20-people org that helps them accomplish the goal  \n    * Mapping dependencies, uncertainties, and potential futures  \n    * Understand complex domains with non-trivial feedback loops"
          }
        },
        {
          "title": "AI for Epistemics as Defensive Acceleration",
          "description": "Adam Marblestone and Eric Drexler discuss 'defensive acceleration'—using epistemic tools to steer technological progress safely. This frames epistemic AI as a risk mitigation tool, not just an accelerator. Key uncertainty: Will epistemic improvements keep pace with capability advances?",
          "importance": 80,
          "isValid": false,
          "highlight": {
            "startOffset": 32067,
            "endOffset": 33119,
            "quotedText": "Development and steering of epistemic tooling, aka [Large Knowledge Models](https://aiprospects.substack.com/p/large-knowledge-models), offers the highest leverage.\n\nPotential actions:  \n  • Identify and influence entities that could develop and scale necessary foundations.  \n  • Leverage the growing ease of development to take pre-scaling steps.  \n  • Within the scope of \"foundations\", accelerate productive directions for development.  \n  • To support science, ensure grounding of content in cited sources.  \n  • Pursue or promote assembly of appropriate knowledge inputs (beyond what is now used for training data).  \n  • Identify, promote, or implement epistemic tools supported by LKM resources.  \n  • Other kinds of exploration, promotion, preparation, and implementation.  \n  • Ensure that my advice is thoroughly understood.\n\nLKMs can accelerate pretty much everything that one might want, and perhaps tip the balance of applications toward *defensive* acceleration by promoting understanding of the *what and why* of defensive applications."
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Trust and Provenance in AI Outputs",
          "description": "Adam Marblestone raises the need for 'objectively trustable source of truth' with provenance. This is a technical and social challenge. Forecasting question: _'Will >80% of AI-generated policy recommendations include verifiable provenance by 2028?'_ Estimate: 50%.",
          "importance": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 31269,
            "endOffset": 31674,
            "quotedText": "Trust, is one thing. Our current society seems to be fraught with distrust and motivated reasoning. Can we get a more objectively trustable source of truth? Not only accuracy but with a provenance that gives it an ability to be trusted by humans who are otherwise adversarial. Trust is needed to coordinate. Truthfulness is maybe somewhat necessary but seemingly not sufficient for a source being trusted."
          }
        },
        {
          "title": "Forecasting the Effects of AI on Social Trust",
          "description": "Deger Turan and Joel Lehman both mention social trust as a key outcome. This is measurable via longitudinal surveys. Reference class: past tech interventions (e.g., social media) often reduced trust. Probability that AI epistemics reverses this trend: 30%, given historical precedent.",
          "importance": 70,
          "isValid": false,
          "highlight": {
            "startOffset": 4567,
            "endOffset": 15594,
            "quotedText": "Social trust erosion, collective coordination mechanisms are built on a weak assumption  \n* Capacity to entertain truthfulness when it is not instrumental to one's incentive gradient & differing norms upon 'truth will make my sense of security weaker'   \n* Creating contexts where world model divergence can be agreed by both parties enabling a safe disagreement  \n* Improving model reasoning / safer AI by reducing slop, by creating accountability mechanisms through consistency checks & other specifications for healthy epistemics  \n* Decoupling virality with magnitude while living in a short attention span world, curation   \n* I have some confusion re: raising the floor when not all parties are necessarily operating in good faith. I think I need some help think through the ways in which epistemics in the context of incentive misalignment can be used or have defensibility. \n\n## Nathan Young: \n\nWhat would we have got:\n\n* Biden not running, perhaps an open dem primary  \n* No FTX crisis  \n* Less zero sum OpenAI board drama  \n* Many smaller, better choices \n\nWhat decisions might go badly:\n\n* Trump tariffs/ global trade  \n* Taiwan/ US-China conflict  \n* US-China competition  \n* US authoritarianism  \n* AI regulation  \n* EU-China realignment  \n* Automated robots  \n* OpenAI strategy  \n* Anthropic strategy  \n* Google strategy  \n* xAI strategy  \n* AI workforce automation  \n* China strategy  \n* Russia strategy  \n* Moskovitz strategy  \n* Must strategy  \n* Bezos Strategy  \n* Zuckerberg strategy  \n* Tallinn strategy  \n* Altman strategy  \n* Drone warfare  \n* Climate change destabilisation  \n* Democratic trust\n\n## Lincoln Quirk:\n\nWhy do we care about (AI for) epistemics?  \n\\- making the public better informed:  \n\t\\- better able to deal with and process the wealth of information  \n\t\\- better able to express their own interests in government (via voting)  \n\t\\- helping cut through social media distraction / intentional misinformation  \n\\- getting governments to implement better policy  \n\t\\- coming up with good policy ideas  \n\t\\- objectively doing cost-benefit analysis  \n\t\\- objectively explaining tradeoffs (vs having to rely on biased humans to explain them)  \n\t\\- flagging bad ideas as bad  \n\t\\- convincing policymakers with compelling writeups  \n\\- success and flourishing for individuals and orgs using these tools  \n\t\\- via helping them think through decisions  \n\t\\- via clearheaded analysis of trade-offs and cost/benefits  \n\t\\- flagging bad ideas as bad\n\n## Jay\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * How can we build something like the positive version of Community Notes: which statements/posts are agreed on by people who often disagree?  \n  * How can we better understand/summarize what others think, e.g. on social media platforms? E.g. if you see a post recommended to you, is this because it's the most extreme take that went viral and isn't representative, or is this a commonly held view? Between different viewpoints, what are the concrete points of actual disagreement (on facts or values) and what are the things widely agreed upon? Let's build viewpoint/debate maps or a similar sensemaking tool that gets adopted/understood by the masses/raises the knowledge floor (in my experience, things like pol.is are hard for regular users to grok)  \n  * How can we improve Community Notes (or more general knowledge) creation with AI assistants? An AI agent proposing community notes and humans still checking/rating them?  \n  * How can we have something like Community Notes that applies to claims or statements about the future, or which are not concretely knowable?  \n  * \n\n## Ben Wilson (Metaculus)\n\nOverall I believe the most important value of epistemics (and AI epistemics) in general is informing what actions allow us to achieve our goals (i.e. make the world better). If we had perfect AI epistemics I would be excited by the ability for us to take a goal, and get a list of possible actions we could take and the forecasted outcomes of each of those actions (plus background information and forecasts for the 'current lay of the land' for context). This can then inform, not just the risk of AI doing good/bad things (as with current forecasting), but what we can do to raise the chances of 'great' AI outcomes, and lower the changes of 'worse' AI outcomes\n\n## Andreas\n\n* **What are use cases for AI for Epistemics that you most care about?**  \n  * AI policy, broadly construed, must go better  \n    * Avoid gradual disempowerment  \n    * Thoughtful regulation  \n  * Organizational planning & decision-making is weak  \n    * Given an informal long-term goal like improve decision-making for strategic users in government & large orgs, make a plan for a 20-people org that helps them accomplish the goal  \n    * Mapping dependencies, uncertainties, and potential futures  \n    * Understand complex domains with non-trivial feedback loops  \n      * Economic shifts  \n* **What decisions may go badly by default, but could go better if we have the right tech?**  \n  * Resource allocation within orgs  \n    * Investment  \n    * Grant making  \n    * Government budgets  \n  * Policy  \n    * How should AI be deployed in the world?  \n* **What do the later stages of the path to impact look like?**  \n  * AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning  \n  * The technology helps organizations maintain coherence and make sound judgments even as the pace of change accelerates  \n  * Assumption: People are generally well-intentioned \\- no one wants catastrophe \\- and as a result of better tooling things go better in the world overall\n\n## Owen optimistic brainstorm:\n\nWorld peace without world government — sufficiently good tech to help countries understand each others' interests, find mutually desirable bargains, and make sufficient credible commitments, could help us get up towards the Pareto frontier where we avoid (virtually) all costly conflicts\n\nGoing slowly and carefully on AI development — rather than having AI progress accelerate at increasingly breakneck speed, helping actors to understand the basic situation and the desirability of different options could be crucial for facilitating meaningful and robust agreements which restrict AI development to the stuff which we have good reasons for being confident are safe // otherwise beneficial.\n\nBettering democracy — helping people to access key information about what decisions their country might make would actually mean for longer term implications, and having this in turn feed into incentives for decision-makers that better align them with the common good\n\nBuilding safe / aligned AI — better tools for anticipating when systems are actually robustly safe, vs when stronger cases for expectation of safety seem required\n\n## Josh Rosenberg (FRI)\n\nI'm massively confused about important problems, e.g. how to orient to AI risk and what to do about it. Some reasonable people think there's a high probability we effectively lose everything we care about in 2027\\. Some reasonable people think it'll be decades till we have \"AGI\" and it'd be good if it arrived sooner. Which actions are good to take in each of these worlds is very different, and I'd like to figure out what world we're in.\n\nThis also applies to many other problem areas.\n\nE.g., Trump's effect on US democracy. How much of a crisis is it? What are the most effective actions to take in response to ensure that the country and communities we care about are as well off as they can be? How can we preserve high-quality institutions to lead the world in positive directions?\n\nOr, on the positive vision side: the \"Abundance\" agenda seems promising. Is it actually a good idea? What do we do about it?\n\nAI for Epistemics could help us get a lot more clarity on how to think about these problems and what actions to take. I'd like much better information and forecasts on the likely effects of Trump, AI timelines, and the pro's and con's of different Abundance policies, for example. If I had a bunch of time and good resources to dig into these problems, I could get a much better sense for myself. I'm hoping AI will help all of us do this sort of thing collectively.\n\n## Katja Grace\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Put lots of cognitive effort into very clear and good and compelling case for AI risk  \n  * Automate talking to people about AI risk compellingly, possibly using above case as input  \n  * Good game theoretic thinking re arms races, coordination, what is possible. I.e. People are making dumb assumptions about things, e.g. \"can't slow down tech\". Could automate noting errors. So e.g. when someone says that on Twitter, can highlight it and send in response. \\-\\> good thinking via better weapons for debate (similar to Nathan project)  \n  * Specific idea: very quantified and competitive debate site, that can be cited re which views are winning, where AI does a lot of the cognitive labor in places no people are paying attention, but as a part of it gets cruxy, people pay attention and vote/bet  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Poor decision making around AI: e.g. 'racing' because of vague heuristics instead of ever doing calc on costs and benefits given specific risks  \n  * Failure to coordinate on AI: building AI systems that are dangerous, due to expecting others to do so\n\n## Joel Lehman\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Raising the bottom (i.e. helping improve societal epistemics) – in particular that in theory political decisions (at least political decision-makers) are somewhat downstream of societal epistemics  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Having weird political leaders and awful public epistemics at critical junctures seems to unnecessarily spike catastrophic/existential risk probabilities. In theory, helping uplevel or at least maintain current public epistemic health (especially as it relates to civic sensemaking) might help influence the quality of decision-makers and concrete decisions made at high-stakes moments  \n  * The shape of the tech is not clear, or probably requires multiple flavors of interventions. E.g. if LLM assistants by default help cultivate civic virtues and on the margin support critical thinking or whatever underlying skills and knowledge help a person up-level their epistemic habits / health (or at least if LLM assistants don't contribute to atrophy/harm); or somehow convincing social media networks to care about this (e.g. community notes but things that go beyond); similarly somehow help w/ important social goods like 'societal trust' and 'trust in your neighbors'"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "AI for Epistemics and Existential Risk Reduction",
          "description": "Abram Demski and others link epistemic improvements to existential risk reduction, especially from AI. This is a high-impact, low-probability outcome. Forecasting question: _'Will the probability of AI-driven existential catastrophe decrease by >10% (as estimated by expert surveys) after widespread adoption of AI epistemic tools?'_ Estimate: 20%.",
          "importance": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 26835,
            "endOffset": 27249,
            "quotedText": "much better when they understand the problem before proposing solutions.  \n\t\t\t\\- I have a sense that producing a better shared understanding of the world JUST WOULD result in better things. The connection between epistemics and better decisionmaking is that strong.  \n\t\t\t\t\\- However, there's an element of coordination to this. You have to have common knowledge, not just knowledge. AI for coordination is yet anot"
          }
        },
        {
          "title": "AI for Epistemics as a Public Good",
          "description": "Multiple contributors note that epistemic improvements are a public good, with diffuse benefits and funding challenges. This suggests the need for policy interventions or philanthropic support to ensure development and deployment.",
          "importance": 60,
          "isValid": true,
          "highlight": {
            "startOffset": 18732,
            "endOffset": 18895,
            "quotedText": "Epistemic tech/institutions/advances can be hard to fund via for-profit companies. This is in part due to public goods issues, and issues around markets for ideas."
          }
        },
        {
          "title": "AI for Epistemics and Collective Sensemaking",
          "description": "Rafe and Brendan both emphasize the need for collective sensemaking and shared understanding as prerequisites for coordination and good governance. This is a qualitative but critical outcome. Suggests metrics like consensus scores or reduction in polarization.",
          "importance": 75,
          "isValid": false,
          "highlight": {
            "startOffset": 19693,
            "endOffset": 28762,
            "quotedText": "Given that, I suspect that the global sensemaking machine isn't up for the task of thinking about AI well. I hope we can help the global sensemaking machine upgrade itself. I have some hope that we as a society can come up with good ideas for dealing with AI, and also come to see how they make sense, so we can choose them in a robust way.\n\nOne other path to impact: I think a society that understands things better, including itself and its impacts, is one that is more robust in a bunch of ways. A more epistemically-grounded society is also probably a bit richer, and I think a bigger pie creates more slack for arranging things in sanity-promoting ways. I have more hope in a robust society navigating this stuff well.\n\n\\[1\\]:  Yeah, maybe you can do something with shaving off subskills, but I don't think that's really that doable.\n\n## Dave\n\n* With my consequentialist hat, better epistemics being worth it has to route through improved decisions somewhere down the line. The decisions I particularly care about are those by powerful people wrt AI development and implementation and the effects this has on how the future goes  \n  * But I do feel a bit icky when I have this hat on. There seems to be something very intuitively appealing about (a) pursuing better epistemics for its own sake and (b) democratising epistemic tools for the use and benefit of everyone. I think this basically caches out in more 'raising the floor' type work  \n  * I don't feel like I have a very good way of resolving this tension yet, but am excited to see things pushing on both frontiers  \n* I think forecasting has the potential to be a pretty powerful tool and there's currently a massive undersupply of good forecasts. I'm excited for us to use the wave of increased AI labor to make better quantitative forecasts to inform key decisions. I probably care more about quantitative forecasts than most decision makers and am open to being persuaded that the other, fuzzier parts of epistemics are more valuable to work on. \n\n## Julian\n\nWhat are use cases for AI for Epistemics that you most care about?  \n**Raising the ceiling:** can we use AI to organize information and credibly resolve the most challenging and important questions on which experts still seem to disagree? For example:\n\n* COVID origins  \n* Urban planning and public policy  \n* Lots of advancements in basic science: routing around current disagreements and policy challenges with slam-dunk science might be a pretty strong way forward.\n\n**Raising the floor:** can we use AI to build extremely legibly trustworthy/reliable epistemic infrastructure that actually makes its way into people's lives?\n\n* Making truth more palatable (in a way which doesn't translate into making falsehoods more palatable)  \n* Collective epistemics: connecting people to the others with whom they would have the most productive/enlightening conversations\n\nWhat decisions may go badly by default, but could go better if we have the right tech?\n\n* Lots of stuff in public policy: downstream both of better experts (technocrats), better public (voters/interest groups), and better coordination (collective epistemics)\n\nWhat do the later stages of the path to impact look like?\n\n* I want us to live in a world where everyone is more rational, empathetic, considered in their decision making, conscientious about impact, and interested in coordinating. I feel like that's one of the biggest blockers to all kinds of *long term* progress.\n\nWhat tools do we actually maybe want to build?\n\n* Easily accessible fact checkers / claim refuters  \n  * Powerful AI debaters  \n* Forecasting with explanations: superhuman and at scale  \n* Scientific meta-analysis, systematic review  \n* Ad-hoc / bespoke probabilistic models for individual problems  \n* Bespoke *simulators* / simulation environments  \n* Consistency checking and inconsistency identification  \n  * Arbitrage / market makers in prediction markets?  \n* Using AI to accelerate development of non-AI epistemic tools:  \n  * ?\n\n## Lawrence\n\nThere are already key questions/strategic problems where the truth is murky and we seem to proceed on the basis of limited and biased views. There might well be many more of these in the future. A central example is US-China competition over AI. Some have vibes of \"China is so bad that it's worth risking an existential catastrophe to stop it from capturing a portion of the lightcone\". Others have vibes of \"Cooperation is feasible and desirable, and is our only path to a good outcome\". But how bad is China, really? How much \"realised K\" on this question do those who are making the relevant decisions actually have? Similarly, how feasible is cooperation? How valid is this frame of \"capturing the lightcone\"? One reason that we don't have a better hold on these questions is that they are frighteningly complicated and difficult  – I think we could benefit massively from throwing much more cognitive labour at them, and presenting the findings in a way that is hard to disagree with or for any individual to improve upon, other than at a normative/values level. Maybe AI can get us there.\n\n## Lukas Finnveden\n\nwhy is AI for epistemics important/helpful?\n\n* better epistemics generally very good for making all kinds of decisions  \n* I'm specifically concerned about AI potentially causing an intelligence explosion, and us needing better epistemics to handle that. Specifically:  \n  * I'm worried about AI-driven tech R\\&D leading to catastrophes. To help address this, I want AI capabilities to not only be applied to tech R\\&D, but also to be applied to \"reasoning well from limited evidence\" about a number of questions, including how to deal with risks.  \n  * I'm worried about misaligned AI — so I want techniques for eliciting AI capabilities on all kinds of questions, so that AIs don't have better capabilities than we can elicit from them. (Basically: I want good alignment techniques even on hard-to-evaluate questions.)  \n  * Seems great to me if AI-driven improvements in general knowledge and decision-making are more broadly distributed among humans. (I think this leads to better decision-making without significantly accelerating the dangers from new tech or misaligned AI. Also reduces concentration of power.)\n\n## Abram Demski\n\n\\- Why are we doing this? \\-- impact theory brainstorming exercize  \n\t\\- There's the general claim that epistemics has degraded horribly due to social media and the internet, at least in a lot of ways, although epistemics has also gotten better in some specific ways. It seems very plausible that AI is, and will continue to, make this much much worse (due to deepfakes and super-persuasive writing, to give two examples). We wish to reverse this trend, or at least slow the acceleration.  \n\t\\- The high-level reason why epistemics is important is because epistemics is how we make good decisions. So effectively, this is AI for decision-making. However, if we just set out to make tools for decision-making, I have a sense that this would be worse.  \n\t\t\\- One super important aspect of decision-making is that you have to orient towards the right problem. People do much better when they understand the problem before proposing solutions.  \n\t\t\t\\- I have a sense that producing a better shared understanding of the world JUST WOULD result in better things. The connection between epistemics and better decisionmaking is that strong.  \n\t\t\t\t\\- However, there's an element of coordination to this. You have to have common knowledge, not just knowledge. AI for coordination is yet another thing. AI for group epistemics is close to that, though, and again, I have a sense that making \"AI for coordination\" would produce worse effects on the world than \"AI for group epistemics\".  \n\t\t\\- What specific decisions are we hoping to influence?  \n\t\t\t\\- Decisions around existential risk seem super super important. If AI were generally better at epistemics (as a relative, comparative thing \\-- ie we differentially accelerate epistemics) then key decisionmakers (in the government and at AI companies) would see much more clearly what's coming. Everyone wants a better future, so a better future would have a better chance of happening if we could see and navigate.\n\nI don't think I got any further than the supporting documentation for this workshop in articulating reasons and stuff, so I predict that the LLM summary won't be better than what we already know, although I hope it is. \n\n## Brendan\n\nFor us to live together in a caring and just way, we must make decisions about what is important to us together. Is it okay to tell my neighbour to turn their music down? Do we spend money on roads or trains or libraries? Do we regulate AI development and, if so, how? Do we want to spend public money on PEPFAR, or other avenues for reducing global poverty? To make these decisions in a way that is inclusive and democratic, that fosters buy-in and trust, we must begin with a shared sense of how the world is. This shared sense must be grounded in the physical, economic, and social reality we live in."
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "AI for Epistemics in High-Stakes Geopolitical Contexts",
          "description": "Lawrence and others highlight US-China AI competition as a central, murky issue. Forecasting question: _'Will AI epistemic tools materially improve the quality of US-China AI policy decisions (as judged by expert panels) by 2030?'_ Estimate: 40%.",
          "importance": 80,
          "isValid": false,
          "highlight": {
            "startOffset": 23715,
            "endOffset": 24810,
            "quotedText": "There are already key questions/strategic problems where the truth is murky and we seem to proceed on the basis of limited and biased views. There might well be many more of these in the future. A central example is US-China competition over AI. Some have vibes of \"China is so bad that it's worth risking an existential catastrophe to stop it from capturing a portion of the lightcone\". Others have vibes of \"Cooperation is feasible and desirable, and is our only path to a good outcome\". But how bad is China, really? How much \"realised K\" on this question do those who are making the relevant decisions actually have? Similarly, how feasible is cooperation? How valid is this frame of \"capturing the lightcone\"? One reason that we don't have a better hold on these questions is that they are frighteningly complicated and difficult  – I think we could benefit massively from throwing much more cognitive labour at them, and presenting the findings in a way that is hard to disagree with or for any individual to improve upon, other than at a normative/values level. Maybe AI can get us there."
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Forecasting Market Integration and AI",
          "description": "Janna asks whether scaling up bots in prediction markets can improve accuracy. Reference class: past attempts at algorithmic trading and market making. Forecasting question: _'Will AI bots account for >50% of prediction market volume by 2028, and will this improve Brier scores by >10%?'_ Estimate: 60% for volume, 40% for accuracy improvement.",
          "importance": 65,
          "isValid": true,
          "highlight": {
            "startOffset": 16716,
            "endOffset": 16853,
            "quotedText": "Solving prediction markets--is there a way to scale up the bots, even if they are wrong and lossy, to improve prediction market accuracy?"
          }
        },
        {
          "title": "AI for Epistemics and Individual Empowerment",
          "description": "Elizabeth highlights the potential for AI epistemics to empower individuals in areas like medical decision-making and bargaining. This is a concrete, high-frequency use case. Forecasting question: _'Will >100M people use AI epistemic tools for personal health or negotiation decisions by 2030?'_ Estimate: 70%.",
          "importance": 60,
          "isValid": true,
          "highlight": {
            "startOffset": 34313,
            "endOffset": 34656,
            "quotedText": "Allowing people to be better informed about experimental medical stuff for themselves.   \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Bargaining? Even just salary negotiations feels like the results are often more the consequence of \"human bargaining skill\" and not the pareto optimal result."
          }
        }
      ],
      "thinking": "# Detailed Thinking Process\n\n## 1. Overall Assessment\nThe document is a rich, multi-perspective brainstorming session on the value, use cases, and impact pathways for \"AI for epistemics\"—that is, AI systems designed to improve collective and individual reasoning, decision-making, and truth-seeking. The participants are highly engaged with both the philosophical and practical aspects, ranging from societal trust and governance to technical forecasting and epistemic infrastructure. There is a strong focus on both \"raising the floor\" (improving baseline epistemics for everyone) and \"raising the ceiling\" (enabling superhuman or expert-level reasoning and coordination).\n\n## 2. Key Themes and Patterns\n- **Epistemics as Instrumental**: Most contributors see improved epistemics as valuable primarily for its downstream effects on decision quality, governance, and societal outcomes, though some also value it intrinsically.\n- **AI as an Epistemic Amplifier**: There is consensus that AI can dramatically scale up both the quantity and quality of epistemic labor, from forecasting to sensemaking to coordination.\n- **Societal and Existential Stakes**: Many cite high-stakes domains—AI risk, geopolitics, governance, and existential risk—as areas where better epistemics could be transformative.\n- **Trust and Coordination**: Trustworthiness, provenance, and the ability to coordinate across diverse actors are recurring concerns. Several participants note that truthfulness is necessary but not sufficient for trust and coordination.\n- **Forecasting and Decision Support**: Quantitative forecasting, prediction markets, and decision support tools are repeatedly mentioned as concrete instantiations of AI for epistemics.\n- **Challenges and Uncertainties**: Issues include incentive misalignment, information overload, adversarial actors, and the difficulty of evaluating counterfactuals or \"fuzzy\" strategic questions.\n- **Public Goods and Democratization**: There is awareness that epistemic tools have public goods characteristics and may not be well-provided by markets alone.\n\n## 3. Expert Perspective\nFrom a quantitative forecasting and Fermi estimation lens, the document is notable for its focus on both measurable and hard-to-measure outcomes. There is a clear appetite for transforming abstract hopes (e.g., \"better governance\") into concrete, forecastable questions (e.g., \"Will AI-assisted epistemic tools reduce the probability of major-power war by 2035?\"). The group recognizes the need for robust metrics, resolution criteria, and calibration—core concerns in forecasting. There is also a sophisticated appreciation of reference classes (e.g., past epistemic technologies, prediction markets, governance reforms) and the challenges of inside vs. outside view reasoning.\n\nKey uncertainties that would affect forecasts include: the pace of AI progress, adoption rates of epistemic tools, the degree of adversarial misuse, and the alignment of incentives among powerful actors. The document suggests that both \"floor-raising\" (broad societal epistemics) and \"ceiling-raising\" (elite or superhuman epistemics) are important, and that both quantitative and qualitative improvements are needed.\n\n# Brainstormed Forecast Questions\n1. **By 2030, will AI-assisted epistemic tools be widely adopted (>50% of major governments or Fortune 500 companies) for high-stakes decision-making?**\n2. **Will the use of AI for epistemics measurably reduce the frequency or severity of major governance failures (e.g., wars, financial crises) by 2040?**\n3. **By 2030, will AI-augmented forecasting systems outperform top human forecasters on a majority (>70%) of Metaculus-style questions?**\n4. **Will public trust in key institutions (measured by surveys) increase in countries that deploy advanced epistemic AI tools by 2035?**\n5. **Will the average calibration score of public prediction markets improve by >10% after the integration of AI epistemic agents by 2028?**\n\n# Fermi Model Example (for Q1)\n- Number of major governments: ~50\n- Number of Fortune 500 companies: 500\n- Assume \"widely adopted\" means >50% of each group uses AI epistemic tools for at least one high-stakes decision per year.\n- Current adoption (2024): <5%\n- Annual growth rate in adoption: estimate 20-30% (based on past tech adoption curves, e.g., cloud computing, data analytics)\n- By 2030 (6 years): 5% * (1.25^6) ≈ 19%\n- But with accelerating AI capabilities and regulatory pressure, could plausibly reach 40-60%.\n- Plausible range: 30-70% adoption by 2030.\n\n# Key Uncertainties\n- Will AI epistemic tools prove robust and trustworthy enough for high-stakes use?\n- Will regulatory or cultural barriers slow adoption?\n- Will adversarial misuse undermine trust?\n- Will the tools be affordable and accessible?\n\n# Reference Classes\n- Adoption of decision-support technologies (e.g., business intelligence, data analytics, cloud computing)\n- Impact of past epistemic innovations (e.g., printing press, internet, prediction markets)\n\n# Metrics and Resolution Criteria\n- Adoption: Documented use in official decision processes (public records, audits)\n- Impact: Reduction in measurable governance failures (historical baselines)\n- Forecasting performance: Brier scores, calibration curves\n- Trust: Survey-based indices (e.g., Edelman Trust Barometer)\n\n# Data Points\n- Current adoption rates of AI in decision support\n- Historical rates of improvement in forecasting accuracy\n- Survey data on institutional trust\n\n# Synthesis\nThe document is a sophisticated, multi-actor exploration of how AI for epistemics could transform decision-making, governance, and societal trust. It highlights both the promise and the challenges, and is well-aligned with a forecasting mindset that seeks to operationalize and measure progress.",
      "summary": "**This document offers a nuanced, multi-perspective exploration of AI for epistemics, emphasizing both societal and existential stakes. It highlights the need to translate abstract hopes into concrete, measurable outcomes—especially in governance, forecasting, and trust. The group recognizes key uncertainties, public goods challenges, and the importance of robust metrics for evaluating progress.**"
    }
  ]
}