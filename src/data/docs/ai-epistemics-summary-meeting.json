{
  "id": "ai-epistemics",
  "title": "AI Epistemics",
  "slug": "ai-epistemics",
  "author": "Owen Cotton-Barratt",
  "publishedDate": "2025-04-18",
  "intendedAgents": [
    "bias-detector",
    "clarity-coach",
    "research-scholar"
  ],
  "content": "# Writing Session\n\nPreamble/framing from Owen:\n\n* Negative: \"we just want to free-form vibe about what cool things we could do\"  \n* Positive: step back & ask \"wait, why are we doing this again?\"  \n* What stuff does this actually help?  \n* Epistemics isn't a goal in-an-of itself; figuring out what combination of things are possible, achievable, etc.  \n* Why do we care about (AI for) epistemics?\n\nWriting prompts from Owen:\n\n* What are use cases for AI for Epistemics that you most care about?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nTemplate:\n\n## Name: \n\nEntry\n\n## Dan Schwarz:\n\n* What are use cases for AI for Epistemics that you most care about?\n\nGoing from \"Realized\" to \"Accessible\" knowledge, e.g. getting good information using the web. Agentic web agents like Deep Research.\n\n* What decisions may go badly by default, but could go better if we have the right tech?\n\nMany billions of small decisions.\n\n* What do the later stages of the path to impact look like?\n\nSuperhuman forecasting at scale.\n\n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nReasonable judgment on tricky questions that's too cheap to meter, better than if you asked a respected colleague to spend many minutes on it.\n\n## Aviv\n\n* Better government governance  \n* Better democratic functioning  \n* Reduced dangerous concentration of power  \n    \n* Healthy polarization / pluralism  \n    \n* More peace, less likelihood of war  \n* Better geopolitics  \n* Better economic incentives  \n    \n* Better AI org governance  \n* Better AI coordination (re. distribution, escalation, etc.)  \n* Better compute allocation (toward what matters for good outcomes)  \n    \n* Better allocation of resources overall  \n* Better lives for people  \n* More effective organizations, getting things done with fewer resources, more focused on the things that benefit everyone  \n    \n* Identification of common ground directions / equilibria / game theoretic strategies that create/incentivize new positive gradients (and not surfacing the bad versions of those…)  \n    \n* Understanding our own goals and values\n\n*Note how it is the intersection of better epistemics / world models + the right power structures / incentives needed for all of these.*\n\n## Ben Goldhaber:\n\n* Forecasting how I should live my life in a period of intense technological change  \n* Guiding decision makers to avoid war with china/ai/everyone  \n* Providing the background context for me to sensemake way more sanely.  \n* Pushing forward the extant knowledge of AI safety agendas  \n* Planning for multi-stage projects that could achieve big impact  \n* Roadmapping to build larger more complex things\n\n## Chi\n\nThere are a few different things I care about:\n\n* I care a lot about humanity and AI, including **super-late-stage AI, getting decision theory** right.  \n  * Consequently but also separately, this relates to the general question of what to do about areas where there is not necessarily a ground truth but that are nonetheless very important to get right, cf**. [metaphilosophy](https://www.alignmentforum.org/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy)** (but also a bunch of other things.)  \n  * This also feels very similar to making **AIs helpful for AI safety research, cf. [automating alignment research](https://joecarlsmith.com/2025/03/14/ai-for-ai-safety#2-what-is-ai-for-ai-safety).**\n\n  → This is more about raising the **top of the epistemic landscape**\n\n* Other things I care about:  \n  * Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place.  \n  * Vague sense that better **societal epistemics** would help with increasing the **sanity of conversations about AI safety, polarisation around the issue etc.**  \n    * This is broadly about **getting intent alignment** in the first place.\n\n→ These are all more about raising the **bottom of the epistemic landscape**\n\n## Deger Turan:\n\n* Social trust erosion, collective coordination mechanisms are built on a weak assumption  \n* Capacity to entertain truthfulness when it is not instrumental to one's incentive gradient & differing norms upon 'truth will make my sense of security weaker'   \n* Creating contexts where world model divergence can be agreed by both parties enabling a safe disagreement  \n* Improving model reasoning / safer AI by reducing slop, by creating accountability mechanisms through consistency checks & other specifications for healthy epistemics  \n* Decoupling virality with magnitude while living in a short attention span world, curation   \n* I have some confusion re: raising the floor when not all parties are necessarily operating in good faith. I think I need some help think through the ways in which epistemics in the context of incentive misalignment can be used or have defensibility. \n\n## Nathan Young: \n\nWhat would we have got:\n\n* Biden not running, perhaps an open dem primary  \n* No FTX crisis  \n* Less zero sum OpenAI board drama  \n* Many smaller, better choices \n\nWhat decisions might go badly:\n\n* Trump tariffs/ global trade  \n* Taiwan/ US-China conflict  \n* US-China competition  \n* US authoritarianism  \n* AI regulation  \n* EU-China realignment  \n* Automated robots  \n* OpenAI strategy  \n* Anthropic strategy  \n* Google strategy  \n* xAI strategy  \n* AI workforce automation  \n* China strategy  \n* Russia strategy  \n* Moskovitz strategy  \n* Must strategy  \n* Bezos Strategy  \n* Zuckerberg strategy  \n* Tallinn strategy  \n* Altman strategy  \n* Drone warfare  \n* Climate change destabilisation  \n* Democratic trust\n\n## Lincoln Quirk:\n\nWhy do we care about (AI for) epistemics?  \n\\- making the public better informed:  \n\t\\- better able to deal with and process the wealth of information  \n\t\\- better able to express their own interests in government (via voting)  \n\t\\- helping cut through social media distraction / intentional misinformation  \n\\- getting governments to implement better policy  \n\t\\- coming up with good policy ideas  \n\t\\- objectively doing cost-benefit analysis  \n\t\\- objectively explaining tradeoffs (vs having to rely on biased humans to explain them)  \n\t\\- flagging bad ideas as bad  \n\t\\- convincing policymakers with compelling writeups  \n\\- success and flourishing for individuals and orgs using these tools  \n\t\\- via helping them think through decisions  \n\t\\- via clearheaded analysis of trade-offs and cost/benefits  \n\t\\- flagging bad ideas as bad\n\n## Jay\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * How can we build something like the positive version of Community Notes: which statements/posts are agreed on by people who often disagree?  \n  * How can we better understand/summarize what others think, e.g. on social media platforms? E.g. if you see a post recommended to you, is this because it's the most extreme take that went viral and isn't representative, or is this a commonly held view? Between different viewpoints, what are the concrete points of actual disagreement (on facts or values) and what are the things widely agreed upon? Let's build viewpoint/debate maps or a similar sensemaking tool that gets adopted/understood by the masses/raises the knowledge floor (in my experience, things like pol.is are hard for regular users to grok)  \n  * How can we improve Community Notes (or more general knowledge) creation with AI assistants? An AI agent proposing community notes and humans still checking/rating them?  \n  * How can we have something like Community Notes that applies to claims or statements about the future, or which are not concretely knowable?  \n  * \n\n## Ben Wilson (Metaculus)\n\nOverall I believe the most important value of epistemics (and AI epistemics) in general is informing what actions allow us to achieve our goals (i.e. make the world better). If we had perfect AI epistemics I would be excited by the ability for us to take a goal, and get a list of possible actions we could take and the forecasted outcomes of each of those actions (plus background information and forecasts for the 'current lay of the land' for context). This can then inform, not just the risk of AI doing good/bad things (as with current forecasting), but what we can do to raise the chances of 'great' AI outcomes, and lower the changes of 'worse' AI outcomes\n\n## Andreas\n\n* **What are use cases for AI for Epistemics that you most care about?**  \n  * AI policy, broadly construed, must go better  \n    * Avoid gradual disempowerment  \n    * Thoughtful regulation  \n  * Organizational planning & decision-making is weak  \n    * Given an informal long-term goal like improve decision-making for strategic users in government & large orgs, make a plan for a 20-people org that helps them accomplish the goal  \n    * Mapping dependencies, uncertainties, and potential futures  \n    * Understand complex domains with non-trivial feedback loops  \n      * Economic shifts  \n* **What decisions may go badly by default, but could go better if we have the right tech?**  \n  * Resource allocation within orgs  \n    * Investment  \n    * Grant making  \n    * Government budgets  \n  * Policy  \n    * How should AI be deployed in the world?  \n* **What do the later stages of the path to impact look like?**  \n  * AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning  \n  * The technology helps organizations maintain coherence and make sound judgments even as the pace of change accelerates  \n  * Assumption: People are generally well-intentioned \\- no one wants catastrophe \\- and as a result of better tooling things go better in the world overall\n\n## Owen optimistic brainstorm:\n\nWorld peace without world government — sufficiently good tech to help countries understand each others' interests, find mutually desirable bargains, and make sufficient credible commitments, could help us get up towards the Pareto frontier where we avoid (virtually) all costly conflicts\n\nGoing slowly and carefully on AI development — rather than having AI progress accelerate at increasingly breakneck speed, helping actors to understand the basic situation and the desirability of different options could be crucial for facilitating meaningful and robust agreements which restrict AI development to the stuff which we have good reasons for being confident are safe // otherwise beneficial.\n\nBettering democracy — helping people to access key information about what decisions their country might make would actually mean for longer term implications, and having this in turn feed into incentives for decision-makers that better align them with the common good\n\nBuilding safe / aligned AI — better tools for anticipating when systems are actually robustly safe, vs when stronger cases for expectation of safety seem required\n\n## Josh Rosenberg (FRI)\n\nI'm massively confused about important problems, e.g. how to orient to AI risk and what to do about it. Some reasonable people think there's a high probability we effectively lose everything we care about in 2027\\. Some reasonable people think it'll be decades till we have \"AGI\" and it'd be good if it arrived sooner. Which actions are good to take in each of these worlds is very different, and I'd like to figure out what world we're in.\n\nThis also applies to many other problem areas.\n\nE.g., Trump's effect on US democracy. How much of a crisis is it? What are the most effective actions to take in response to ensure that the country and communities we care about are as well off as they can be? How can we preserve high-quality institutions to lead the world in positive directions?\n\nOr, on the positive vision side: the \"Abundance\" agenda seems promising. Is it actually a good idea? What do we do about it?\n\nAI for Epistemics could help us get a lot more clarity on how to think about these problems and what actions to take. I'd like much better information and forecasts on the likely effects of Trump, AI timelines, and the pro's and con's of different Abundance policies, for example. If I had a bunch of time and good resources to dig into these problems, I could get a much better sense for myself. I'm hoping AI will help all of us do this sort of thing collectively.\n\n## Katja Grace\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Put lots of cognitive effort into very clear and good and compelling case for AI risk  \n  * Automate talking to people about AI risk compellingly, possibly using above case as input  \n  * Good game theoretic thinking re arms races, coordination, what is possible. I.e. People are making dumb assumptions about things, e.g. \"can't slow down tech\". Could automate noting errors. So e.g. when someone says that on Twitter, can highlight it and send in response. \\-\\> good thinking via better weapons for debate (similar to Nathan project)  \n  * Specific idea: very quantified and competitive debate site, that can be cited re which views are winning, where AI does a lot of the cognitive labor in places no people are paying attention, but as a part of it gets cruxy, people pay attention and vote/bet  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Poor decision making around AI: e.g. 'racing' because of vague heuristics instead of ever doing calc on costs and benefits given specific risks  \n  * Failure to coordinate on AI: building AI systems that are dangerous, due to expecting others to do so\n\n## Joel Lehman\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Raising the bottom (i.e. helping improve societal epistemics) – in particular that in theory political decisions (at least political decision-makers) are somewhat downstream of societal epistemics  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Having weird political leaders and awful public epistemics at critical junctures seems to unnecessarily spike catastrophic/existential risk probabilities. In theory, helping uplevel or at least maintain current public epistemic health (especially as it relates to civic sensemaking) might help influence the quality of decision-makers and concrete decisions made at high-stakes moments  \n  * The shape of the tech is not clear, or probably requires multiple flavors of interventions. E.g. if LLM assistants by default help cultivate civic virtues and on the margin support critical thinking or whatever underlying skills and knowledge help a person up-level their epistemic habits / health (or at least if LLM assistants don't contribute to atrophy/harm); or somehow convincing social media networks to care about this (e.g. community notes but things that go beyond); similarly somehow help w/ important social goods like 'societal trust' and 'trust in your neighbors'  \n* What do the later stages of the path to impact look like?  \n  * \\[didn't get to this\\]  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * Healthier democratic society, better agreement on consensus reality, higher trust in society, better politicians, better concrete decisions in high-stakes moments \n\n## Austin Chen\n\n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * **Evaluator**: More agreement (or understanding of disagreement) of what orgs & people are doing good work (eg for making grants in AI safety)  \n  * **Oracle**: Instant trustworthy forecasts on any question you ask  \n  * **Planner**: Guidance on \"what are the 3 most important things I should do this day/week/month\"  \n  * **Representative**: Simulate the responses of other parties (people, orgs/corps, governments, LLM agents), for discussions, contracts, trade\n\n## Janna\n\n* Why do we care about (AI for) epistemics?  \n  * Noise to signal ratio could be much higher  \n  * AI (mis)information churned out at the price of almost zero  \n* What are use cases for AI for Epistemics that you most care about?  \n  * Solving prediction markets--is there a way to scale up the bots, even if they are wrong and lossy, to improve prediction market accuracy?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * US-China relations  \n  * Trump tariffs maybe  \n  * Immigration rules in the US  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * More accurate predictions to…  \n    * Reduce business uncertainty  \n    * Make it easier to evaluate life decisions  \n  * Forestalling bad decisions made on imperfect information\n\n## Owain\n\n1. Maybe having humans understanding the world better is good in itself. Lots of people read popular science for this reason. Maybe it's good for AIs as well  \n2. Very broadly, it seems that better world understanding and decision making enables people to make their lives better in myriad ways. (To substantiate this, one could try to separate out the effects of better world understanding from the effects of better realized technology. E.g. Looking at capital-poor countries that can still benefit from scientific knowledge, economics, etc.)  \n3. More specifically, if AI progress is fast, there will be lots of important decisions to be made in quick calendar time. Exploiting AI to improve those decisions seems valuable. Current decision making institutions do not seem well set up to adapt really quickly. (A specific concern is dangers from AI takeover).   \n4. Why now? Why this group of people? We haven't really had tech that could do most epistemic tasks until recently. Now we have AI that can do an increasing proportion of the sub-tasks of epistemic processes. This will allow us to delegate epistemic tasks to AI.  \n   1. It's likely that people do this delegation, even if it's somewhat misguided and potentially bad in the long-term. Can we make this go well?  \n   2. This potentially massively expands our epistemic capacities. How can we use this well?  \n   3. Epistemic tech/institutions/advances can be hard to fund via for-profit companies. This is in part due to public goods issues, and issues around markets for ideas.\n\n## Rafe\n\nAs ever, I hope this will stop advanced artificial intelligence destroying the world, killing all its people \\&c.\n\nSo I think there are a few paths for impact here. I think there is a world where we can make the most productive thinkers about how to do things go further and better; I'm not so excited about a single-player version of this, as I think there's a pretty narrow window when top-level centaur mode works out\\[1\\]\n\nEveryone is already thinking about AI, and, I suspect, this will only get more acute. I think it's pretty hard for people to confront this and think about it carefully and well. It's scary\\! And there are a bunch of entrenched interests and memeplexes that preserve their status better (myopically) if people only have shallow slogans to think about AI with.\n\nGiven that, I suspect that the global sensemaking machine isn't up for the task of thinking about AI well. I hope we can help the global sensemaking machine upgrade itself. I have some hope that we as a society can come up with good ideas for dealing with AI, and also come to see how they make sense, so we can choose them in a robust way.\n\nOne other path to impact: I think a society that understands things better, including itself and its impacts, is one that is more robust in a bunch of ways. A more epistemically-grounded society is also probably a bit richer, and I think a bigger pie creates more slack for arranging things in sanity-promoting ways. I have more hope in a robust society navigating this stuff well.\n\n\\[1\\]:  Yeah, maybe you can do something with shaving off subskills, but I don't think that's really that doable.\n\n## Dave\n\n* With my consequentialist hat, better epistemics being worth it has to route through improved decisions somewhere down the line. The decisions I particularly care about are those by powerful people wrt AI development and implementation and the effects this has on how the future goes  \n  * But I do feel a bit icky when I have this hat on. There seems to be something very intuitively appealing about (a) pursuing better epistemics for its own sake and (b) democratising epistemic tools for the use and benefit of everyone. I think this basically caches out in more 'raising the floor' type work  \n  * I don't feel like I have a very good way of resolving this tension yet, but am excited to see things pushing on both frontiers  \n* I think forecasting has the potential to be a pretty powerful tool and there's currently a massive undersupply of good forecasts. I'm excited for us to use the wave of increased AI labor to make better quantitative forecasts to inform key decisions. I probably care more about quantitative forecasts than most decision makers and am open to being persuaded that the other, fuzzier parts of epistemics are more valuable to work on. \n\n## Julian\n\nWhat are use cases for AI for Epistemics that you most care about?  \n**Raising the ceiling:** can we use AI to organize information and credibly resolve the most challenging and important questions on which experts still seem to disagree? For example:\n\n* COVID origins  \n* Urban planning and public policy  \n* Lots of advancements in basic science: routing around current disagreements and policy challenges with slam-dunk science might be a pretty strong way forward.\n\n**Raising the floor:** can we use AI to build extremely legibly trustworthy/reliable epistemic infrastructure that actually makes its way into people's lives?\n\n* Making truth more palatable (in a way which doesn't translate into making falsehoods more palatable)  \n* Collective epistemics: connecting people to the others with whom they would have the most productive/enlightening conversations\n\nWhat decisions may go badly by default, but could go better if we have the right tech?\n\n* Lots of stuff in public policy: downstream both of better experts (technocrats), better public (voters/interest groups), and better coordination (collective epistemics)\n\nWhat do the later stages of the path to impact look like?\n\n* I want us to live in a world where everyone is more rational, empathetic, considered in their decision making, conscientious about impact, and interested in coordinating. I feel like that's one of the biggest blockers to all kinds of *long term* progress.\n\nWhat tools do we actually maybe want to build?\n\n* Easily accessible fact checkers / claim refuters  \n  * Powerful AI debaters  \n* Forecasting with explanations: superhuman and at scale  \n* Scientific meta-analysis, systematic review  \n* Ad-hoc / bespoke probabilistic models for individual problems  \n* Bespoke *simulators* / simulation environments  \n* Consistency checking and inconsistency identification  \n  * Arbitrage / market makers in prediction markets?  \n* Using AI to accelerate development of non-AI epistemic tools:  \n  * ?\n\n## Lawrence\n\nThere are already key questions/strategic problems where the truth is murky and we seem to proceed on the basis of limited and biased views. There might well be many more of these in the future. A central example is US-China competition over AI. Some have vibes of \"China is so bad that it's worth risking an existential catastrophe to stop it from capturing a portion of the lightcone\". Others have vibes of \"Cooperation is feasible and desirable, and is our only path to a good outcome\". But how bad is China, really? How much \"realised K\" on this question do those who are making the relevant decisions actually have? Similarly, how feasible is cooperation? How valid is this frame of \"capturing the lightcone\"? One reason that we don't have a better hold on these questions is that they are frighteningly complicated and difficult  – I think we could benefit massively from throwing much more cognitive labour at them, and presenting the findings in a way that is hard to disagree with or for any individual to improve upon, other than at a normative/values level. Maybe AI can get us there.\n\n## Lukas Finnveden\n\nwhy is AI for epistemics important/helpful?\n\n* better epistemics generally very good for making all kinds of decisions  \n* I'm specifically concerned about AI potentially causing an intelligence explosion, and us needing better epistemics to handle that. Specifically:  \n  * I'm worried about AI-driven tech R\\&D leading to catastrophes. To help address this, I want AI capabilities to not only be applied to tech R\\&D, but also to be applied to \"reasoning well from limited evidence\" about a number of questions, including how to deal with risks.  \n  * I'm worried about misaligned AI — so I want techniques for eliciting AI capabilities on all kinds of questions, so that AIs don't have better capabilities than we can elicit from them. (Basically: I want good alignment techniques even on hard-to-evaluate questions.)  \n  * Seems great to me if AI-driven improvements in general knowledge and decision-making are more broadly distributed among humans. (I think this leads to better decision-making without significantly accelerating the dangers from new tech or misaligned AI. Also reduces concentration of power.)\n\n## Abram Demski\n\n\\- Why are we doing this? \\-- impact theory brainstorming exercize  \n\t\\- There's the general claim that epistemics has degraded horribly due to social media and the internet, at least in a lot of ways, although epistemics has also gotten better in some specific ways. It seems very plausible that AI is, and will continue to, make this much much worse (due to deepfakes and super-persuasive writing, to give two examples). We wish to reverse this trend, or at least slow the acceleration.  \n\t\\- The high-level reason why epistemics is important is because epistemics is how we make good decisions. So effectively, this is AI for decision-making. However, if we just set out to make tools for decision-making, I have a sense that this would be worse.  \n\t\t\\- One super important aspect of decision-making is that you have to orient towards the right problem. People do much better when they understand the problem before proposing solutions.  \n\t\t\t\\- I have a sense that producing a better shared understanding of the world JUST WOULD result in better things. The connection between epistemics and better decisionmaking is that strong.  \n\t\t\t\t\\- However, there's an element of coordination to this. You have to have common knowledge, not just knowledge. AI for coordination is yet another thing. AI for group epistemics is close to that, though, and again, I have a sense that making \"AI for coordination\" would produce worse effects on the world than \"AI for group epistemics\".  \n\t\t\\- What specific decisions are we hoping to influence?  \n\t\t\t\\- Decisions around existential risk seem super super important. If AI were generally better at epistemics (as a relative, comparative thing \\-- ie we differentially accelerate epistemics) then key decisionmakers (in the government and at AI companies) would see much more clearly what's coming. Everyone wants a better future, so a better future would have a better chance of happening if we could see and navigate.\n\nI don't think I got any further than the supporting documentation for this workshop in articulating reasons and stuff, so I predict that the LLM summary won't be better than what we already know, although I hope it is. \n\n## Brendan\n\nFor us to live together in a caring and just way, we must make decisions about what is important to us together. Is it okay to tell my neighbour to turn their music down? Do we spend money on roads or trains or libraries? Do we regulate AI development and, if so, how? Do we want to spend public money on PEPFAR, or other avenues for reducing global poverty? To make these decisions in a way that is inclusive and democratic, that fosters buy-in and trust, we must begin with a shared sense of how the world is. This shared sense must be grounded in the physical, economic, and social reality we live in.\n\nAs we live together over wider and wider contexts, maintaining a shared sense of the world becomes more difficult and subtle. These wider contexts are facilitated by new technologies, both physical (eg. global manufacturing bases and supply chains) and informational (eg. social media). New technologies, such as AI for epistemics, and help maintain epistemic coherence across these contexts.\n\nMoreover, such technologies allow new avenues for cooperation at larger scales of human involvement. This unlocks new potential for coherence in global politics, scientific discovery, social and technological innovation, and cultural expression, and ultimately the deep joy of being alive and human in the world. \n\n## Isabel: \n\nThe kinds of being right in the past that are most relevant for decision making aren't just about making straightforward predictions about what will happen, but are more nuanced about what kinds of actions one should take to cause those things to happen, and these seem like somewhat different skills. These are harder to evaluate (e.g. because they're counterfactual: either we listened to person a and then we have to evaluate whether things went better or worse than if we didn't, but hard to say what the counterfactual would have been; or we didn't listen to person a and then we have to guess what things would have been like if we did) \n\nA bit adjacent to Neel Nanda's [recent piece](https://www.lesswrong.com/posts/P5zWiPF5cPJZSkiAK/good-research-takes-are-not-sufficient-for-good-strategic) on how \"good research takes aren't sufficient for good strategic takes\" \\- \"good forecasting takes aren't sufficient for good strategic takes\", but maybe some of this is maybe resolvable on the forecasting tech side \\- can we get better at creating and evaluating results from fuzzier questions? Can AI help? (Also can AI help with the whole problem, but this feels small and concrete enough to maybe get some traction on.) Also, some risk of actors building up good track records in order to be listened to, but then choosing to burn that track record to get power or things like that, something something trust \\- a concern for both human actors and e.g. scheming AIs \\- what things look good until they abruptly aren't? \n\nOn a different note, also interested in takes on what to do about and how to engage with actors who have epistemic processes that don't seem to be truth seeking in a comprehensible-to-me way. \n\n## Adam Marblestone:\n\nWhat do we get out of AI for epistemics?\n\nTrust, is one thing. Our current society seems to be fraught with distrust and motivated reasoning. Can we get a more objectively trustable source of truth? Not only accuracy but with a provenance that gives it an ability to be trusted by humans who are otherwise adversarial. Trust is needed to coordinate. Truthfulness is maybe somewhat necessary but seemingly not sufficient for a source being trusted. As a practical matter, on hot political topics I've been going to OpenAI Deep Research.\n\nBut at a certain level I wonder if AI for epistemics and some form of beneficial AGI could be basically the same thing. I asked Eric Drexler what he wanted for d/acc and he said something that sounds a bit like that…\n\n\"  \nHere's my unsurprising take on directed research for \"defensive acceleration\":  \nDevelopment and steering of epistemic tooling, aka [Large Knowledge Models](https://aiprospects.substack.com/p/large-knowledge-models), offers the highest leverage.\n\nPotential actions:  \n  • Identify and influence entities that could develop and scale necessary foundations.  \n  • Leverage the growing ease of development to take pre-scaling steps.  \n  • Within the scope of \"foundations\", accelerate productive directions for development.  \n  • To support science, ensure grounding of content in cited sources.  \n  • Pursue or promote assembly of appropriate knowledge inputs (beyond what is now used for training data).  \n  • Identify, promote, or implement epistemic tools supported by LKM resources.  \n  • Other kinds of exploration, promotion, preparation, and implementation.  \n  • Ensure that my advice is thoroughly understood.\n\nLKMs can accelerate pretty much everything that one might want, and perhaps tip the balance of applications toward *defensive* acceleration by promoting understanding of the *what and why* of defensive applications.\n\nAddendum:\n\nIn the email I sent a few minutes ago, the basic proposal becomes stronger if one reads \"LKM\" as something like \"large knowledge model functionality\", regardless of implementation.\n\nThe knowledge-model concept (whatever one prefers to call it) should not be tied to my specific technical suggestions. What I've said can be framed as something like \"a discussion of the value, feasibility, and potential scope of a rich epistemic resource\". Bundles of embeddings, etc., are at a level of specificity that may be distracting in the broader context of the meeting.  \n\"\n\nSo that's a strategy assistant AI – which helps humans at large see ways they could act and coordinate especially around big technological or geopolitical strategy issues I guess – but also a certain embodiment of an AGI oracle type system. Maybe the idea then is, how to make a AGI that helps with navigating civilizational strategy, and what is the right form of or interface to that to make it epistemically most useful. \n\nI'd like to also see democratic mechanisms that are simply maximally utilizing the tech we have. \n\n## Elizabeth\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Allowing people to be better informed about experimental medical stuff for themselves.   \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Bargaining? Even just salary negotiations feels like the results are often more the consequence of \"human bargaining skill\" and not the pareto optimal result.    \n  \n\n\n## Saul\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * making better decisions  \n  * in service of that: more useful forecasts, particularly understanding better which things are most relevant to forecast  \n    * in service of that: better group decisionmaking (e.g. unblocking things from vetopower type  \n    * in service of that: more aptly/calibratedly handling waves of information  \n    * In service of that: societal- and individual-scale asymmetric truth-tracking  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * which groups of people have political power  \n  * who's building powerful new tech  \n  * who's regulating powerful new tech  \n  * (etc, other \"decisions\" along these lines — some might not end up getting shaped like decisions, though)  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * better decisions across the board  \n    * better *outcomes* from those decisions; or, like, we can track \"better decisions\" as better-in-expectation outcomes from the decision processes\n\n",
  "reviews": [
    {
      "agentId": "bias-detector",
      "createdAt": "2025-04-18T21:11:33.312Z",
      "costInCents": 0,
      "comments": [
        {
          "title": "Confirmation Bias in Framing AI Epistemics",
          "description": "The preamble frames the discussion as inherently positive, assuming that AI for epistemics is desirable and beneficial. This may lead to overlooking potential downsides or alternative perspectives. To mitigate, explicitly invite critical viewpoints and counterexamples.",
          "importance": 80,
          "grade": 40,
          "isValid": true,
          "highlight": {
            "startOffset": 239,
            "endOffset": 396,
            "quotedText": "Epistemics isn't a goal in-an-of itself; figuring out what combination of things are possible, achievable, etc.  \n* Why do we care about (AI for) epistemics?"
          }
        },
        {
          "title": "Availability Heuristic in Use Case Selection",
          "description": "Many examples focus on recent, high-profile issues (e.g., US-China relations, AI safety, FTX crisis), potentially overemphasizing their importance due to recency and media coverage. Broader, less salient but equally important cases could be considered.",
          "importance": 60,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 499,
            "endOffset": 1086,
            "quotedText": "What decisions may go badly by default, but could go better if we have the right tech?  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nTemplate:\n\n## Name: \n\nEntry\n\n## Dan Schwarz:\n\n* What are use cases for AI for Epistemics that you most care about?\n\nGoing from \"Realized\" to \"Accessible\" knowledge, e.g. getting good information using the web. Agentic web agents like Deep Research.\n\n* What decisions may go badly by default, but could go better if we have the right tech?\n\nMany billions of small decisions."
          }
        },
        {
          "title": "Selection Bias in Contributors",
          "description": "The document reflects the views of a specific, highly engaged community (AI safety, forecasting, rationalist circles), which may not represent broader societal perspectives. This could limit the generalizability of conclusions.",
          "importance": 85,
          "grade": 30,
          "isValid": true,
          "highlight": {
            "startOffset": 742,
            "endOffset": 961,
            "quotedText": "## Dan Schwarz:\n\n* What are use cases for AI for Epistemics that you most care about?\n\nGoing from \"Realized\" to \"Accessible\" knowledge, e.g. getting good information using the web. Agentic web agents like Deep Research."
          }
        },
        {
          "title": "Scope Insensitivity in Impact Claims",
          "description": "Statements like 'Many billions of small decisions' lack specificity and may overstate the uniformity of impact. The actual scope and distribution of benefits should be more carefully delineated.",
          "importance": 55,
          "grade": 45,
          "isValid": true,
          "highlight": {
            "startOffset": 1053,
            "endOffset": 1181,
            "quotedText": "Many billions of small decisions.\n\n* What do the later stages of the path to impact look like?\n\nSuperhuman forecasting at scale."
          }
        },
        {
          "title": "Optimism Bias in Forecasting Outcomes",
          "description": "Several contributors assume that better epistemics will naturally lead to better outcomes, underestimating potential negative side effects or implementation challenges. Consideration of failure modes and unintended consequences is needed.",
          "importance": 70,
          "grade": 40,
          "isValid": true,
          "highlight": {
            "startOffset": 653,
            "endOffset": 1388,
            "quotedText": "As a result of better AI for epistemics tech, we get: […]\"\n\nTemplate:\n\n## Name: \n\nEntry\n\n## Dan Schwarz:\n\n* What are use cases for AI for Epistemics that you most care about?\n\nGoing from \"Realized\" to \"Accessible\" knowledge, e.g. getting good information using the web. Agentic web agents like Deep Research.\n\n* What decisions may go badly by default, but could go better if we have the right tech?\n\nMany billions of small decisions.\n\n* What do the later stages of the path to impact look like?\n\nSuperhuman forecasting at scale.\n\n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nReasonable judgment on tricky questions that's too cheap to meter, better than if you asked a respected colleague to spend many minutes on it."
          }
        },
        {
          "title": "Status Quo Bias in Institutional Focus",
          "description": "There is an implicit assumption that existing institutions (governments, large organizations) are the primary loci for epistemic improvement, potentially neglecting grassroots or alternative models.",
          "importance": 60,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 9837,
            "endOffset": 10147,
            "quotedText": "AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning"
          }
        },
        {
          "title": "Authority Bias in Valuing Expert Judgment",
          "description": "The repeated comparison to 'respected colleagues' or 'experts' as benchmarks for AI epistemics may overvalue traditional authority and underappreciate diverse or unconventional sources of insight.",
          "importance": 50,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 1313,
            "endOffset": 1388,
            "quotedText": "better than if you asked a respected colleague to spend many minutes on it."
          }
        },
        {
          "title": "Base Rate Neglect in Risk Assessment",
          "description": "Discussions of existential risk (e.g., AI takeover, catastrophic events) often lack reference to base rates or historical frequencies, potentially exaggerating or miscalibrating perceived dangers.",
          "importance": 65,
          "grade": 40,
          "isValid": true,
          "highlight": {
            "startOffset": 3514,
            "endOffset": 3901,
            "quotedText": "or-ai-safety).**\n\n  → This is more about raising the **top of the epistemic landscape**\n\n* Other things I care about:  \n  * Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if"
          }
        },
        {
          "title": "Narrative Fallacy in Path to Impact",
          "description": "Several entries construct linear, coherent stories about how AI epistemics will lead to positive change, potentially oversimplifying complex, nonlinear processes.",
          "importance": 55,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 590,
            "endOffset": 1181,
            "quotedText": "What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nTemplate:\n\n## Name: \n\nEntry\n\n## Dan Schwarz:\n\n* What are use cases for AI for Epistemics that you most care about?\n\nGoing from \"Realized\" to \"Accessible\" knowledge, e.g. getting good information using the web. Agentic web agents like Deep Research.\n\n* What decisions may go badly by default, but could go better if we have the right tech?\n\nMany billions of small decisions.\n\n* What do the later stages of the path to impact look like?\n\nSuperhuman forecasting at scale."
          }
        },
        {
          "title": "In-group Bias in Community References",
          "description": "Frequent references to community-specific tools (e.g., Metaculus, Community Notes, pol.is) and jargon may reinforce in-group perspectives and limit accessibility for outsiders.",
          "importance": 60,
          "grade": 45,
          "isValid": true,
          "highlight": {
            "startOffset": 7126,
            "endOffset": 7876,
            "quotedText": "How can we build something like the positive version of Community Notes: which statements/posts are agreed on by people who often disagree?  \n  * How can we better understand/summarize what others think, e.g. on social media platforms? E.g. if you see a post recommended to you, is this because it's the most extreme take that went viral and isn't representative, or is this a commonly held view? Between different viewpoints, what are the concrete points of actual disagreement (on facts or values) and what are the things widely agreed upon? Let's build viewpoint/debate maps or a similar sensemaking tool that gets adopted/understood by the masses/raises the knowledge floor (in my experience, things like pol.is are hard for regular users to grok"
          }
        },
        {
          "title": "Goodhart's Law in Quantitative Metrics",
          "description": "Proposals for competitive debate sites and quantified forecasting risk incentivizing gaming of metrics rather than genuine epistemic improvement. Safeguards against metric manipulation should be considered.",
          "importance": 75,
          "grade": 35,
          "isValid": true,
          "highlight": {
            "startOffset": 13633,
            "endOffset": 13889,
            "quotedText": "Specific idea: very quantified and competitive debate site, that can be cited re which views are winning, where AI does a lot of the cognitive labor in places no people are paying attention, but as a part of it gets cruxy, people pay attention and vote/bet"
          }
        },
        {
          "title": "False Consensus Effect in Assumptions of Agreement",
          "description": "Some statements assume widespread agreement on the value of epistemics or the desirability of certain outcomes, which may not hold across different cultures or stakeholder groups.",
          "importance": 60,
          "grade": 40,
          "isValid": true,
          "highlight": {
            "startOffset": 27803,
            "endOffset": 27923,
            "quotedText": "Everyone wants a better future, so a better future would have a better chance of happening if we could see and navigate."
          }
        },
        {
          "title": "Anchoring Effect in Example Selection",
          "description": "Repeated use of certain high-profile examples (e.g., US-China, AI safety, Trump) may anchor the discussion and bias the perceived importance of these issues relative to others.",
          "importance": 55,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 16951,
            "endOffset": 16995,
            "quotedText": "US-China relations  \n  * Trump tariffs maybe"
          }
        },
        {
          "title": "Survivorship Bias in Success Stories",
          "description": "The focus on successful epistemic interventions or positive visions may neglect failed or problematic attempts, skewing perceptions of feasibility and impact.",
          "importance": 50,
          "grade": 60,
          "isValid": true,
          "highlight": {
            "startOffset": 22907,
            "endOffset": 23162,
            "quotedText": "I want us to live in a world where everyone is more rational, empathetic, considered in their decision making, conscientious about impact, and interested in coordinating. I feel like that's one of the biggest blockers to all kinds of *long term* progress."
          }
        },
        {
          "title": "Planning Fallacy in Implementation Timelines",
          "description": "There is little discussion of the practical challenges, delays, or resource requirements involved in deploying AI epistemics tools at scale. More realistic planning and risk assessment would strengthen the analysis.",
          "importance": 65,
          "grade": 45,
          "isValid": true,
          "highlight": {
            "startOffset": 9837,
            "endOffset": 10147,
            "quotedText": "AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning"
          }
        },
        {
          "title": "Omission Bias in Neglecting Downsides",
          "description": "Potential negative consequences of AI for epistemics (e.g., misuse, manipulation, privacy concerns) are rarely mentioned. A more balanced analysis would address these risks.",
          "importance": 80,
          "grade": 35,
          "isValid": true,
          "highlight": {
            "startOffset": 653,
            "endOffset": 1388,
            "quotedText": "As a result of better AI for epistemics tech, we get: […]\"\n\nTemplate:\n\n## Name: \n\nEntry\n\n## Dan Schwarz:\n\n* What are use cases for AI for Epistemics that you most care about?\n\nGoing from \"Realized\" to \"Accessible\" knowledge, e.g. getting good information using the web. Agentic web agents like Deep Research.\n\n* What decisions may go badly by default, but could go better if we have the right tech?\n\nMany billions of small decisions.\n\n* What do the later stages of the path to impact look like?\n\nSuperhuman forecasting at scale.\n\n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nReasonable judgment on tricky questions that's too cheap to meter, better than if you asked a respected colleague to spend many minutes on it."
          }
        },
        {
          "title": "Illusory Correlation in Linking Epistemics and Outcomes",
          "description": "Assuming a direct, strong correlation between improved epistemics and better societal outcomes may overstate the causal relationship, ignoring mediating factors.",
          "importance": 70,
          "grade": 45,
          "isValid": true,
          "highlight": {
            "startOffset": 26463,
            "endOffset": 27100,
            "quotedText": "The high-level reason why epistemics is important is because epistemics is how we make good decisions. So effectively, this is AI for decision-making. However, if we just set out to make tools for decision-making, I have a sense that this would be worse.  \n\t\t\\- One super important aspect of decision-making is that you have to orient towards the right problem. People do much better when they understand the problem before proposing solutions.  \n\t\t\t\\- I have a sense that producing a better shared understanding of the world JUST WOULD result in better things. The connection between epistemics and better decisionmaking is that strong."
          }
        },
        {
          "title": "System Justification Bias in Institutional Trust",
          "description": "Assuming that existing institutions will use epistemic tools for the common good may overlook incentives for self-preservation, power consolidation, or resistance to change.",
          "importance": 60,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 10035,
            "endOffset": 10271,
            "quotedText": "trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning  \n  * The technology helps organizations maintain coherence and make sound judgments even as the pace of change accelerates"
          }
        },
        {
          "title": "Motivated Reasoning in AI Safety Focus",
          "description": "The strong emphasis on AI safety and existential risk may reflect the community's pre-existing concerns, potentially biasing the analysis toward these issues at the expense of others.",
          "importance": 65,
          "grade": 45,
          "isValid": true,
          "highlight": {
            "startOffset": 2637,
            "endOffset": 2762,
            "quotedText": "Pushing forward the extant knowledge of AI safety agendas  \n* Planning for multi-stage projects that could achieve big impact"
          }
        },
        {
          "title": "Zero-risk Bias in Policy Recommendations",
          "description": "Some proposals implicitly seek to eliminate all risk (e.g., 'avoid all costly conflicts'), which may be unrealistic and lead to overly cautious or paralyzing strategies.",
          "importance": 55,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 10464,
            "endOffset": 10751,
            "quotedText": "World peace without world government — sufficiently good tech to help countries understand each others' interests, find mutually desirable bargains, and make sufficient credible commitments, could help us get up towards the Pareto frontier where we avoid (virtually) all costly conflicts"
          }
        },
        {
          "title": "Bandwagon Effect in AI Epistemics Advocacy",
          "description": "The document's tone and content may encourage uncritical enthusiasm for AI epistemics, potentially suppressing dissent or alternative approaches.",
          "importance": 50,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 12929,
            "endOffset": 12998,
            "quotedText": "I'm hoping AI will help all of us do this sort of thing collectively."
          }
        },
        {
          "title": "Defensive Attribution Hypothesis in Blame Assignment",
          "description": "References to past crises (e.g., FTX, OpenAI board drama) may attribute failures to individual decisions rather than systemic issues, potentially oversimplifying causality.",
          "importance": 45,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 5548,
            "endOffset": 5598,
            "quotedText": "No FTX crisis  \n* Less zero sum OpenAI board drama"
          }
        },
        {
          "title": "Curse of Knowledge in Technical Jargon",
          "description": "Use of specialized terms (e.g., 'intent alignment', 'epistemic landscape', 'centaur mode') may assume background knowledge not shared by all readers, reducing accessibility.",
          "importance": 50,
          "grade": 60,
          "isValid": true,
          "highlight": {
            "startOffset": 3216,
            "endOffset": 3483,
            "quotedText": "ignmentforum.org/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy)** (but also a bunch of other things.)  \n  * This also feels very similar to making **AIs helpful for AI safety research, cf. [automating alignment research](https://joecarlsmith.com/2025/03/14/"
          }
        },
        {
          "title": "False Uniqueness Bias in Community Solutions",
          "description": "There is an implicit belief that the proposed approaches are novel or uniquely effective, potentially overlooking similar efforts in other domains or communities.",
          "importance": 45,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 7670,
            "endOffset": 7876,
            "quotedText": "Let's build viewpoint/debate maps or a similar sensemaking tool that gets adopted/understood by the masses/raises the knowledge floor (in my experience, things like pol.is are hard for regular users to grok"
          }
        },
        {
          "title": "Actor-Observer Bias in Attribution of Epistemic Failures",
          "description": "Failures in decision-making are often attributed to others' lack of epistemics, rather than considering one's own limitations or systemic factors.",
          "importance": 55,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 13360,
            "endOffset": 13549,
            "quotedText": "People are making dumb assumptions about things, e.g. \"can't slow down tech\". Could automate noting errors. So e.g. when someone says that on Twitter, can highlight it and send in response."
          }
        },
        {
          "title": "Just-world Hypothesis in Outcome Expectations",
          "description": "Assuming that better epistemics will naturally lead to just or desirable outcomes may underestimate the role of power, interests, and structural inequalities.",
          "importance": 60,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 28158,
            "endOffset": 28762,
            "quotedText": "For us to live together in a caring and just way, we must make decisions about what is important to us together. Is it okay to tell my neighbour to turn their music down? Do we spend money on roads or trains or libraries? Do we regulate AI development and, if so, how? Do we want to spend public money on PEPFAR, or other avenues for reducing global poverty? To make these decisions in a way that is inclusive and democratic, that fosters buy-in and trust, we must begin with a shared sense of how the world is. This shared sense must be grounded in the physical, economic, and social reality we live in."
          }
        },
        {
          "title": "Illusion of Control in AI Epistemics Deployment",
          "description": "There is an implicit belief that deploying AI epistemics tools will reliably improve decision-making, potentially underestimating the complexity and unpredictability of social systems.",
          "importance": 65,
          "grade": 45,
          "isValid": true,
          "highlight": {
            "startOffset": 8408,
            "endOffset": 8897,
            "quotedText": "If we had perfect AI epistemics I would be excited by the ability for us to take a goal, and get a list of possible actions we could take and the forecasted outcomes of each of those actions (plus background information and forecasts for the 'current lay of the land' for context). This can then inform, not just the risk of AI doing good/bad things (as with current forecasting), but what we can do to raise the chances of 'great' AI outcomes, and lower the changes of 'worse' AI outcomes"
          }
        },
        {
          "title": "Omission Bias in Neglecting Cultural Variation",
          "description": "The document rarely addresses how epistemic tools might need to be adapted for different cultural, linguistic, or political contexts.",
          "importance": 60,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 1434,
            "endOffset": 1892,
            "quotedText": "Better democratic functioning  \n* Reduced dangerous concentration of power  \n    \n* Healthy polarization / pluralism  \n    \n* More peace, less likelihood of war  \n* Better geopolitics  \n* Better economic incentives  \n    \n* Better AI org governance  \n* Better AI coordination (re. distribution, escalation, etc.)  \n* Better compute allocation (toward what matters for good outcomes)  \n    \n* Better allocation of resources overall  \n* Better lives for people"
          }
        },
        {
          "title": "Hindsight Bias in Evaluating Past Decisions",
          "description": "References to past crises (e.g., FTX, OpenAI board drama) may imply that better epistemics would have obviously prevented them, oversimplifying the uncertainty present at the time.",
          "importance": 55,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 5548,
            "endOffset": 5598,
            "quotedText": "No FTX crisis  \n* Less zero sum OpenAI board drama"
          }
        },
        {
          "title": "Sunk Cost Fallacy in Advocacy for AI Epistemics",
          "description": "There is a risk that ongoing investment in AI epistemics is justified by prior effort rather than current evidence of effectiveness. Regular evaluation and willingness to pivot are important.",
          "importance": 45,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 2637,
            "endOffset": 2762,
            "quotedText": "Pushing forward the extant knowledge of AI safety agendas  \n* Planning for multi-stage projects that could achieve big impact"
          }
        },
        {
          "title": "Belief Perseverance in AI Risk Focus",
          "description": "Despite acknowledging uncertainty and disagreement about AI timelines and risks, the document often returns to strong claims about existential risk, suggesting resistance to updating beliefs.",
          "importance": 60,
          "grade": 45,
          "isValid": true,
          "highlight": {
            "startOffset": 11318,
            "endOffset": 11533,
            "quotedText": "nd having this in turn feed into incentives for decision-makers that better align them with the common good\n\nBuilding safe / aligned AI — better tools for anticipating when systems are actually robustly safe, vs whe"
          }
        },
        {
          "title": "Omission Bias in Neglecting Non-Technical Solutions",
          "description": "The focus is overwhelmingly on technical (AI-based) solutions to epistemic problems, with little attention to social, educational, or institutional reforms.",
          "importance": 65,
          "grade": 45,
          "isValid": true,
          "highlight": {
            "startOffset": 9837,
            "endOffset": 10147,
            "quotedText": "AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning"
          }
        },
        {
          "title": "Moral Credential Effect in Advocacy",
          "description": "Some contributors may overestimate the moral value of their preferred interventions, potentially leading to complacency or neglect of unintended harms.",
          "importance": 50,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 22907,
            "endOffset": 23162,
            "quotedText": "I want us to live in a world where everyone is more rational, empathetic, considered in their decision making, conscientious about impact, and interested in coordinating. I feel like that's one of the biggest blockers to all kinds of *long term* progress."
          }
        },
        {
          "title": "Omission Bias in Neglecting Privacy Concerns",
          "description": "There is little discussion of how AI epistemics tools might impact privacy, surveillance, or data security.",
          "importance": 60,
          "grade": 45,
          "isValid": true,
          "highlight": {
            "startOffset": 7884,
            "endOffset": 8052,
            "quotedText": "How can we improve Community Notes (or more general knowledge) creation with AI assistants? An AI agent proposing community notes and humans still checking/rating them?"
          }
        },
        {
          "title": "Authority Bias in Citing Notable Figures",
          "description": "References to prominent individuals (e.g., Eric Drexler) may lend undue weight to their views, potentially discouraging independent evaluation.",
          "importance": 45,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 31884,
            "endOffset": 32231,
            "quotedText": "I asked Eric Drexler what he wanted for d/acc and he said something that sounds a bit like that…\n\n\"  \nHere's my unsurprising take on directed research for \"defensive acceleration\":  \nDevelopment and steering of epistemic tooling, aka [Large Knowledge Models](https://aiprospects.substack.com/p/large-knowledge-models), offers the highest leverage."
          }
        },
        {
          "title": "Anchoring Effect in Framing AI as Existential Risk",
          "description": "The repeated framing of AI as an existential risk may anchor the discussion, limiting exploration of other important but less extreme risks.",
          "importance": 60,
          "grade": 45,
          "isValid": true,
          "highlight": {
            "startOffset": 24324,
            "endOffset": 24535,
            "quotedText": "ually have? Similarly, how feasible is cooperation? How valid is this frame of \"capturing the lightcone\"? One reason that we don't have a better hold on these questions is that they are frighteningly complicated"
          }
        },
        {
          "title": "Omission Bias in Neglecting Economic Incentives",
          "description": "The challenges of funding and sustaining epistemic tools, especially given public goods dynamics, are mentioned but not deeply explored.",
          "importance": 55,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 18732,
            "endOffset": 18895,
            "quotedText": "Epistemic tech/institutions/advances can be hard to fund via for-profit companies. This is in part due to public goods issues, and issues around markets for ideas."
          }
        },
        {
          "title": "Out-group Homogeneity Bias in Geopolitical Framing",
          "description": "Discussions of 'China' or 'Russia' as monolithic actors may oversimplify the diversity of perspectives and interests within these countries.",
          "importance": 60,
          "grade": 45,
          "isValid": true,
          "highlight": {
            "startOffset": 23961,
            "endOffset": 24204,
            "quotedText": "Some have vibes of \"China is so bad that it's worth risking an existential catastrophe to stop it from capturing a portion of the lightcone\". Others have vibes of \"Cooperation is feasible and desirable, and is our only path to a good outcome\"."
          }
        },
        {
          "title": "Omission Bias in Neglecting Accessibility",
          "description": "There is little discussion of how epistemic tools will be made accessible to people with varying levels of education, digital literacy, or resources.",
          "importance": 55,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 23214,
            "endOffset": 23289,
            "quotedText": "Easily accessible fact checkers / claim refuters  \n  * Powerful AI debaters"
          }
        },
        {
          "title": "Illusion of Transparency in Communication",
          "description": "Assuming that improved epistemic tools will automatically lead to shared understanding may underestimate communication barriers and interpretive differences.",
          "importance": 55,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 28670,
            "endOffset": 28889,
            "quotedText": "This shared sense must be grounded in the physical, economic, and social reality we live in.\n\nAs we live together over wider and wider contexts, maintaining a shared sense of the world becomes more difficult and subtle."
          }
        },
        {
          "title": "Reactance Bias in Resistance to Epistemic Tools",
          "description": "Potential resistance from individuals or groups who perceive epistemic tools as threatening to their autonomy or beliefs is not addressed.",
          "importance": 60,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 4659,
            "endOffset": 4929,
            "quotedText": "Capacity to entertain truthfulness when it is not instrumental to one's incentive gradient & differing norms upon 'truth will make my sense of security weaker'   \n* Creating contexts where world model divergence can be agreed by both parties enabling a safe disagreement"
          }
        },
        {
          "title": "Omission Bias in Neglecting Adversarial Use",
          "description": "The risk that epistemic tools could be used for manipulation, propaganda, or other adversarial purposes is not discussed.",
          "importance": 70,
          "grade": 40,
          "isValid": true,
          "highlight": {
            "startOffset": 16579,
            "endOffset": 16853,
            "quotedText": "AI (mis)information churned out at the price of almost zero  \n* What are use cases for AI for Epistemics that you most care about?  \n  * Solving prediction markets--is there a way to scale up the bots, even if they are wrong and lossy, to improve prediction market accuracy?"
          }
        },
        {
          "title": "Omission Bias in Neglecting Environmental Impact",
          "description": "The environmental costs of large-scale AI deployment (e.g., energy use, resource consumption) are not considered.",
          "importance": 40,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 1149,
            "endOffset": 1311,
            "quotedText": "Superhuman forecasting at scale.\n\n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nReasonable judgment on tricky questions that's too cheap to meter"
          }
        },
        {
          "title": "Omission Bias in Neglecting Psychological Impact",
          "description": "The psychological effects of widespread reliance on AI epistemics (e.g., reduced critical thinking, overreliance on automation) are not addressed.",
          "importance": 55,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 15749,
            "endOffset": 15988,
            "quotedText": "  \n  * Healthier democratic society, better agreement on consensus reality, higher trust in society, better politicians, better concrete decisions in high-stakes moments \n\n## Austin Chen\n\n* \"As a result of better AI for epistemics tech, we"
          }
        },
        {
          "title": "Omission Bias in Neglecting Diversity of Epistemic Values",
          "description": "Different cultures and communities may have varying epistemic norms and values, which are not explored in the document.",
          "importance": 60,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 1401,
            "endOffset": 1892,
            "quotedText": "Better government governance  \n* Better democratic functioning  \n* Reduced dangerous concentration of power  \n    \n* Healthy polarization / pluralism  \n    \n* More peace, less likelihood of war  \n* Better geopolitics  \n* Better economic incentives  \n    \n* Better AI org governance  \n* Better AI coordination (re. distribution, escalation, etc.)  \n* Better compute allocation (toward what matters for good outcomes)  \n    \n* Better allocation of resources overall  \n* Better lives for people"
          }
        },
        {
          "title": "Omission Bias in Neglecting Long-term Governance",
          "description": "The long-term governance and oversight of AI epistemics tools is not discussed, raising questions about accountability and control.",
          "importance": 65,
          "grade": 45,
          "isValid": true,
          "highlight": {
            "startOffset": 9837,
            "endOffset": 10147,
            "quotedText": "AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning"
          }
        },
        {
          "title": "Omission Bias in Neglecting Failure Modes",
          "description": "Potential failure modes (e.g., groupthink, overfitting, algorithmic bias) in epistemic tools are not systematically analyzed.",
          "importance": 70,
          "grade": 45,
          "isValid": true,
          "highlight": {
            "startOffset": 4934,
            "endOffset": 5193,
            "quotedText": "Improving model reasoning / safer AI by reducing slop, by creating accountability mechanisms through consistency checks & other specifications for healthy epistemics  \n* Decoupling virality with magnitude while living in a short attention span world, curation"
          }
        },
        {
          "title": "Omission Bias in Neglecting Legal and Regulatory Issues",
          "description": "Legal and regulatory challenges associated with deploying AI epistemics tools (e.g., liability, compliance) are not addressed.",
          "importance": 55,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 5779,
            "endOffset": 5817,
            "quotedText": "AI regulation  \n* EU-China realignment"
          }
        },
        {
          "title": "Omission Bias in Neglecting User Agency",
          "description": "The potential for epistemic tools to undermine user agency or autonomy is not discussed.",
          "importance": 55,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 15749,
            "endOffset": 15988,
            "quotedText": "  \n  * Healthier democratic society, better agreement on consensus reality, higher trust in society, better politicians, better concrete decisions in high-stakes moments \n\n## Austin Chen\n\n* \"As a result of better AI for epistemics tech, we"
          }
        },
        {
          "title": "Omission Bias in Neglecting Education and Training",
          "description": "The need for education and training to use epistemic tools effectively is not considered.",
          "importance": 50,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 23214,
            "endOffset": 23289,
            "quotedText": "Easily accessible fact checkers / claim refuters  \n  * Powerful AI debaters"
          }
        },
        {
          "title": "Omission Bias in Neglecting Marginalized Groups",
          "description": "The impact of epistemic tools on marginalized or vulnerable groups is not discussed.",
          "importance": 60,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 1826,
            "endOffset": 1892,
            "quotedText": "Better allocation of resources overall  \n* Better lives for people"
          }
        },
        {
          "title": "Omission Bias in Neglecting Global South Perspectives",
          "description": "Perspectives from the Global South or less technologically advanced regions are largely absent, limiting the global relevance of the analysis.",
          "importance": 60,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 17796,
            "endOffset": 17895,
            "quotedText": "Looking at capital-poor countries that can still benefit from scientific knowledge, economics, etc."
          }
        },
        {
          "title": "Omission Bias in Neglecting Non-English Contexts",
          "description": "The document assumes English-language contexts, with little attention to multilingual or non-English-speaking users.",
          "importance": 50,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 1401,
            "endOffset": 1892,
            "quotedText": "Better government governance  \n* Better democratic functioning  \n* Reduced dangerous concentration of power  \n    \n* Healthy polarization / pluralism  \n    \n* More peace, less likelihood of war  \n* Better geopolitics  \n* Better economic incentives  \n    \n* Better AI org governance  \n* Better AI coordination (re. distribution, escalation, etc.)  \n* Better compute allocation (toward what matters for good outcomes)  \n    \n* Better allocation of resources overall  \n* Better lives for people"
          }
        },
        {
          "title": "Omission Bias in Neglecting Algorithmic Bias",
          "description": "The risk of algorithmic bias in AI epistemics tools is not explicitly addressed.",
          "importance": 70,
          "grade": 45,
          "isValid": true,
          "highlight": {
            "startOffset": 4934,
            "endOffset": 5193,
            "quotedText": "Improving model reasoning / safer AI by reducing slop, by creating accountability mechanisms through consistency checks & other specifications for healthy epistemics  \n* Decoupling virality with magnitude while living in a short attention span world, curation"
          }
        },
        {
          "title": "Omission Bias in Neglecting Epistemic Humility",
          "description": "There is little discussion of the importance of epistemic humility or the limits of knowledge, especially in complex or uncertain domains.",
          "importance": 55,
          "grade": 55,
          "isValid": false,
          "highlight": {
            "startOffset": 11616,
            "endOffset": 12649,
            "quotedText": "I'm massively confused about important problems, e.g. how to orient to AI risk and what to do about it. Some reasonable people think there's a high probability we effectively lose everything we care about in 2027\\. Some reasonable people think it'll be decades till we have \"AGI\" and it'd be good if it arrived sooner. Which actions are good to take in each of these worlds is very different, and I'd like to figure out what world we're in.\n\nThis also applies to many other problem areas.\n\nE.g., Trump's effect on US democracy. How much of a crisis is it? What are the most effective actions to take in response to ensure that the country and communities we care about are as well off as they can be? How can we preserve high-quality institutions to lead the world in positive directions?\n\nOr, on the positive vision side: the \"Abundance\" agenda seems promising. Is it actually a good idea? What do we do about it?\n\nAI for Epistemics could help us get a lot more clarity on how to think about these problems and what actions to take."
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Omission Bias in Neglecting Value Alignment",
          "description": "The challenge of aligning epistemic tools with diverse human values is not systematically explored.",
          "importance": 60,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 2214,
            "endOffset": 2252,
            "quotedText": "Understanding our own goals and values"
          }
        },
        {
          "title": "Omission Bias in Neglecting Power Dynamics",
          "description": "The potential for epistemic tools to reinforce or disrupt existing power dynamics is not deeply analyzed.",
          "importance": 65,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 1468,
            "endOffset": 1682,
            "quotedText": "Reduced dangerous concentration of power  \n    \n* Healthy polarization / pluralism  \n    \n* More peace, less likelihood of war  \n* Better geopolitics  \n* Better economic incentives  \n    \n* Better AI org governance"
          }
        },
        {
          "title": "Omission Bias in Neglecting Trust-building Mechanisms",
          "description": "While trust is mentioned, concrete mechanisms for building and maintaining trust in epistemic tools are not detailed.",
          "importance": 60,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 31269,
            "endOffset": 31674,
            "quotedText": "Trust, is one thing. Our current society seems to be fraught with distrust and motivated reasoning. Can we get a more objectively trustable source of truth? Not only accuracy but with a provenance that gives it an ability to be trusted by humans who are otherwise adversarial. Trust is needed to coordinate. Truthfulness is maybe somewhat necessary but seemingly not sufficient for a source being trusted."
          }
        },
        {
          "title": "Omission Bias in Neglecting Feedback Loops",
          "description": "The potential for epistemic tools to create feedback loops (e.g., reinforcing popular views, amplifying polarization) is not discussed.",
          "importance": 65,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 1518,
            "endOffset": 1594,
            "quotedText": "Healthy polarization / pluralism  \n    \n* More peace, less likelihood of war"
          }
        },
        {
          "title": "Omission Bias in Neglecting Unintended Consequences",
          "description": "Unintended consequences of deploying epistemic tools at scale are not systematically considered.",
          "importance": 70,
          "grade": 45,
          "isValid": true,
          "highlight": {
            "startOffset": 9837,
            "endOffset": 10147,
            "quotedText": "AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning"
          }
        },
        {
          "title": "Omission Bias in Neglecting Human-AI Collaboration Challenges",
          "description": "The challenges of effective human-AI collaboration in epistemic processes are not explored.",
          "importance": 60,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 18509,
            "endOffset": 19470,
            "quotedText": "ple do this delegation, even if it's somewhat misguided and potentially bad in the long-term. Can we make this go well?  \n   2. This potentially massively expands our epistemic capacities. How can we use this well?  \n   3. Epistemic tech/institutions/advances can be hard to fund via for-profit companies. This is in part due to public goods issues, and issues around markets for ideas.\n\n## Rafe\n\nAs ever, I hope this will stop advanced artificial intelligence destroying the world, killing all its people \\&c.\n\nSo I think there are a few paths for impact here. I think there is a world where we can make the most productive thinkers about how to do things go further and better; I'm not so excited about a single-player version of this, as I think there's a pretty narrow window when top-level centaur mode works out\\[1\\]\n\nEveryone is already thinking about AI, and, I suspect, this will only get more acute. I think it's pretty hard for people to confront thi"
          }
        },
        {
          "title": "Omission Bias in Neglecting Evaluation and Benchmarking",
          "description": "How to evaluate and benchmark the effectiveness of epistemic tools is not discussed.",
          "importance": 55,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 23294,
            "endOffset": 23396,
            "quotedText": "Forecasting with explanations: superhuman and at scale  \n* Scientific meta-analysis, systematic review"
          }
        },
        {
          "title": "Omission Bias in Neglecting Open-source vs Proprietary Models",
          "description": "The trade-offs between open-source and proprietary epistemic tools are not considered.",
          "importance": 50,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 9837,
            "endOffset": 10147,
            "quotedText": "AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning"
          }
        },
        {
          "title": "Omission Bias in Neglecting Interoperability",
          "description": "The need for interoperability between different epistemic tools and platforms is not addressed.",
          "importance": 45,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 23467,
            "endOffset": 23571,
            "quotedText": "Bespoke *simulators* / simulation environments  \n* Consistency checking and inconsistency identification"
          }
        },
        {
          "title": "Omission Bias in Neglecting Maintenance and Updating",
          "description": "The ongoing maintenance and updating of epistemic tools is not discussed.",
          "importance": 50,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 9837,
            "endOffset": 10147,
            "quotedText": "AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning"
          }
        },
        {
          "title": "Omission Bias in Neglecting User Feedback Mechanisms",
          "description": "Mechanisms for incorporating user feedback into epistemic tools are not discussed.",
          "importance": 50,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 7884,
            "endOffset": 8052,
            "quotedText": "How can we improve Community Notes (or more general knowledge) creation with AI assistants? An AI agent proposing community notes and humans still checking/rating them?"
          }
        },
        {
          "title": "Omission Bias in Neglecting Cost Considerations",
          "description": "The costs (financial, computational, social) of deploying epistemic tools are not analyzed.",
          "importance": 55,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 1246,
            "endOffset": 1388,
            "quotedText": "Reasonable judgment on tricky questions that's too cheap to meter, better than if you asked a respected colleague to spend many minutes on it."
          }
        },
        {
          "title": "Omission Bias in Neglecting User Motivation",
          "description": "How to motivate users to engage with epistemic tools is not discussed.",
          "importance": 50,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 7670,
            "endOffset": 7876,
            "quotedText": "Let's build viewpoint/debate maps or a similar sensemaking tool that gets adopted/understood by the masses/raises the knowledge floor (in my experience, things like pol.is are hard for regular users to grok"
          }
        },
        {
          "title": "Omission Bias in Neglecting Social Dynamics",
          "description": "The social dynamics of group epistemics (e.g., conformity, polarization) are not deeply explored.",
          "importance": 60,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 4824,
            "endOffset": 5099,
            "quotedText": "Creating contexts where world model divergence can be agreed by both parties enabling a safe disagreement  \n* Improving model reasoning / safer AI by reducing slop, by creating accountability mechanisms through consistency checks & other specifications for healthy epistemics"
          }
        },
        {
          "title": "Omission Bias in Neglecting Emotional Factors",
          "description": "The role of emotions in epistemic processes and decision-making is not addressed.",
          "importance": 55,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 18948,
            "endOffset": 19236,
            "quotedText": "tificial intelligence destroying the world, killing all its people \\&c.\n\nSo I think there are a few paths for impact here. I think there is a world where we can make the most productive thinkers about how to do things go further and better; I'm not so excited about a single-player versio"
          }
        },
        {
          "title": "Omission Bias in Neglecting Incentive Structures",
          "description": "The incentive structures that shape the use and impact of epistemic tools are not systematically analyzed.",
          "importance": 60,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 1622,
            "endOffset": 1682,
            "quotedText": "Better economic incentives  \n    \n* Better AI org governance"
          }
        },
        {
          "title": "Omission Bias in Neglecting Disinformation Risks",
          "description": "The risk that epistemic tools could be co-opted for disinformation or manipulation is not discussed.",
          "importance": 70,
          "grade": 40,
          "isValid": true,
          "highlight": {
            "startOffset": 16579,
            "endOffset": 16853,
            "quotedText": "AI (mis)information churned out at the price of almost zero  \n* What are use cases for AI for Epistemics that you most care about?  \n  * Solving prediction markets--is there a way to scale up the bots, even if they are wrong and lossy, to improve prediction market accuracy?"
          }
        },
        {
          "title": "Omission Bias in Neglecting User Diversity",
          "description": "The diversity of user needs, preferences, and contexts is not systematically considered.",
          "importance": 60,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 1826,
            "endOffset": 1892,
            "quotedText": "Better allocation of resources overall  \n* Better lives for people"
          }
        },
        {
          "title": "Omission Bias in Neglecting Interdisciplinary Approaches",
          "description": "The potential for interdisciplinary collaboration (e.g., with social sciences, humanities) in epistemic tool development is not explored.",
          "importance": 55,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 9837,
            "endOffset": 10147,
            "quotedText": "AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning"
          }
        },
        {
          "title": "Omission Bias in Neglecting User Empowerment",
          "description": "How epistemic tools can empower users to make independent judgments is not discussed.",
          "importance": 55,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 15749,
            "endOffset": 15988,
            "quotedText": "  \n  * Healthier democratic society, better agreement on consensus reality, higher trust in society, better politicians, better concrete decisions in high-stakes moments \n\n## Austin Chen\n\n* \"As a result of better AI for epistemics tech, we"
          }
        },
        {
          "title": "Omission Bias in Neglecting Transparency and Explainability",
          "description": "The importance of transparency and explainability in epistemic tools is not systematically addressed.",
          "importance": 60,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 32513,
            "endOffset": 32688,
            "quotedText": "To support science, ensure grounding of content in cited sources.  \n  • Pursue or promote assembly of appropriate knowledge inputs (beyond what is now used for training data)."
          }
        },
        {
          "title": "Omission Bias in Neglecting User Trust Calibration",
          "description": "How to calibrate user trust in epistemic tools (avoiding both over- and under-trust) is not discussed.",
          "importance": 60,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 31269,
            "endOffset": 31674,
            "quotedText": "Trust, is one thing. Our current society seems to be fraught with distrust and motivated reasoning. Can we get a more objectively trustable source of truth? Not only accuracy but with a provenance that gives it an ability to be trusted by humans who are otherwise adversarial. Trust is needed to coordinate. Truthfulness is maybe somewhat necessary but seemingly not sufficient for a source being trusted."
          }
        },
        {
          "title": "Omission Bias in Neglecting Human Oversight",
          "description": "The role of human oversight in the use of epistemic tools is not systematically explored.",
          "importance": 55,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 7976,
            "endOffset": 8052,
            "quotedText": "An AI agent proposing community notes and humans still checking/rating them?"
          }
        },
        {
          "title": "Omission Bias in Neglecting Social Acceptance",
          "description": "The social acceptance and legitimacy of epistemic tools is not discussed.",
          "importance": 55,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 7670,
            "endOffset": 7876,
            "quotedText": "Let's build viewpoint/debate maps or a similar sensemaking tool that gets adopted/understood by the masses/raises the knowledge floor (in my experience, things like pol.is are hard for regular users to grok"
          }
        },
        {
          "title": "Omission Bias in Neglecting User Experience Design",
          "description": "The importance of user experience and interface design in epistemic tools is not addressed.",
          "importance": 50,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 7670,
            "endOffset": 7876,
            "quotedText": "Let's build viewpoint/debate maps or a similar sensemaking tool that gets adopted/understood by the masses/raises the knowledge floor (in my experience, things like pol.is are hard for regular users to grok"
          }
        },
        {
          "title": "Omission Bias in Neglecting Long-term Sustainability",
          "description": "The long-term sustainability of epistemic tools (financial, technical, social) is not discussed.",
          "importance": 50,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 9837,
            "endOffset": 10147,
            "quotedText": "AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning"
          }
        },
        {
          "title": "Omission Bias in Neglecting Cross-sector Collaboration",
          "description": "The potential for collaboration across sectors (public, private, civil society) in epistemic tool development is not explored.",
          "importance": 50,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 9837,
            "endOffset": 10147,
            "quotedText": "AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning"
          }
        },
        {
          "title": "Omission Bias in Neglecting Ethical Considerations",
          "description": "Ethical considerations in the design and deployment of epistemic tools are not systematically addressed.",
          "importance": 65,
          "grade": 45,
          "isValid": true,
          "highlight": {
            "startOffset": 9837,
            "endOffset": 10147,
            "quotedText": "AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning"
          }
        },
        {
          "title": "Omission Bias in Neglecting Uncertainty Quantification",
          "description": "How epistemic tools communicate and quantify uncertainty is not discussed.",
          "importance": 55,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 23294,
            "endOffset": 23396,
            "quotedText": "Forecasting with explanations: superhuman and at scale  \n* Scientific meta-analysis, systematic review"
          }
        },
        {
          "title": "Omission Bias in Neglecting User Customization",
          "description": "The potential for user customization and personalization in epistemic tools is not addressed.",
          "importance": 50,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 15749,
            "endOffset": 15988,
            "quotedText": "  \n  * Healthier democratic society, better agreement on consensus reality, higher trust in society, better politicians, better concrete decisions in high-stakes moments \n\n## Austin Chen\n\n* \"As a result of better AI for epistemics tech, we"
          }
        },
        {
          "title": "Omission Bias in Neglecting Data Quality",
          "description": "The importance of data quality and provenance in epistemic tools is not systematically addressed.",
          "importance": 60,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 32513,
            "endOffset": 32688,
            "quotedText": "To support science, ensure grounding of content in cited sources.  \n  • Pursue or promote assembly of appropriate knowledge inputs (beyond what is now used for training data)."
          }
        },
        {
          "title": "Omission Bias in Neglecting Open Participation",
          "description": "How to enable open participation and contribution to epistemic tools is not discussed.",
          "importance": 50,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 23214,
            "endOffset": 23289,
            "quotedText": "Easily accessible fact checkers / claim refuters  \n  * Powerful AI debaters"
          }
        },
        {
          "title": "Omission Bias in Neglecting Redress Mechanisms",
          "description": "Mechanisms for redress or correction in case of errors or harms caused by epistemic tools are not discussed.",
          "importance": 55,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 9837,
            "endOffset": 10147,
            "quotedText": "AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning"
          }
        },
        {
          "title": "Omission Bias in Neglecting Social Learning",
          "description": "The role of social learning and peer influence in epistemic processes is not addressed.",
          "importance": 55,
          "grade": 55,
          "isValid": false,
          "highlight": {
            "startOffset": 22457,
            "endOffset": 585,
            "quotedText": "  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nTemplate:\n\n## Name: \n\nEntry\n\n## Dan Schwarz:\n\n* What are use cases for AI for Epistemics that you most care about?\n\nGoing from \"Realized\" to \"Accessible\" knowledge, e.g. getting good information using the web. Agentic web agents like Deep Research.\n\n* What decisions may go badly by default, but could go better if we have the right tech?\n\nMany billions of small decisions.\n\n* What do the later stages of the path to impact look like?\n\nSuperhuman forecasting at scale.\n\n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nReasonable judgment on tricky questions that's too cheap to meter, better than if you asked a respected colleague to spend many minutes on it.\n\n## Aviv\n\n* Better government governance  \n* Better democratic functioning  \n* Reduced dangerous concentration of power  \n    \n* Healthy polarization / pluralism  \n    \n* More peace, less likelihood of war  \n* Better geopolitics  \n* Better economic incentives  \n    \n* Better AI org governance  \n* Better AI coordination (re. distribution, escalation, etc.)  \n* Better compute allocation (toward what matters for good outcomes)  \n    \n* Better allocation of resources overall  \n* Better lives for people  \n* More effective organizations, getting things done with fewer resources, more focused on the things that benefit everyone  \n    \n* Identification of common ground directions / equilibria / game theoretic strategies that create/incentivize new positive gradients (and not surfacing the bad versions of those…)  \n    \n* Understanding our own goals and values\n\n*Note how it is the intersection of better epistemics / world models + the right power structures / incentives needed for all of these.*\n\n## Ben Goldhaber:\n\n* Forecasting how I should live my life in a period of intense technological change  \n* Guiding decision makers to avoid war with china/ai/everyone  \n* Providing the background context for me to sensemake way more sanely.  \n* Pushing forward the extant knowledge of AI safety agendas  \n* Planning for multi-stage projects that could achieve big impact  \n* Roadmapping to build larger more complex things\n\n## Chi\n\nThere are a few different things I care about:\n\n* I care a lot about humanity and AI, including **super-late-stage AI, getting decision theory** right.  \n  * Consequently but also separately, this relates to the general question of what to do about areas where there is not necessarily a ground truth but that are nonetheless very important to get right, cf**. [metaphilosophy](https://www.alignmentforum.org/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy)** (but also a bunch of other things.)  \n  * This also feels very similar to making **AIs helpful for AI safety research, cf. [automating alignment research](https://joecarlsmith.com/2025/03/14/ai-for-ai-safety#2-what-is-ai-for-ai-safety).**\n\n  → This is more about raising the **top of the epistemic landscape**\n\n* Other things I care about:  \n  * Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place.  \n  * Vague sense that better **societal epistemics** would help with increasing the **sanity of conversations about AI safety, polarisation around the issue etc.**  \n    * This is broadly about **getting intent alignment** in the first place.\n\n→ These are all more about raising the **bottom of the epistemic landscape**\n\n## Deger Turan:\n\n* Social trust erosion, collective coordination mechanisms are built on a weak assumption  \n* Capacity to entertain truthfulness when it is not instrumental to one's incentive gradient & differing norms upon 'truth will make my sense of security weaker'   \n* Creating contexts where world model divergence can be agreed by both parties enabling a safe disagreement  \n* Improving model reasoning / safer AI by reducing slop, by creating accountability mechanisms through consistency checks & other specifications for healthy epistemics  \n* Decoupling virality with magnitude while living in a short attention span world, curation   \n* I have some confusion re: raising the floor when not all parties are necessarily operating in good faith. I think I need some help think through the ways in which epistemics in the context of incentive misalignment can be used or have defensibility. \n\n## Nathan Young: \n\nWhat would we have got:\n\n* Biden not running, perhaps an open dem primary  \n* No FTX crisis  \n* Less zero sum OpenAI board drama  \n* Many smaller, better choices \n\nWhat decisions might go badly:\n\n* Trump tariffs/ global trade  \n* Taiwan/ US-China conflict  \n* US-China competition  \n* US authoritarianism  \n* AI regulation  \n* EU-China realignment  \n* Automated robots  \n* OpenAI strategy  \n* Anthropic strategy  \n* Google strategy  \n* xAI strategy  \n* AI workforce automation  \n* China strategy  \n* Russia strategy  \n* Moskovitz strategy  \n* Must strategy  \n* Bezos Strategy  \n* Zuckerberg strategy  \n* Tallinn strategy  \n* Altman strategy  \n* Drone warfare  \n* Climate change destabilisation  \n* Democratic trust\n\n## Lincoln Quirk:\n\nWhy do we care about (AI for) epistemics?  \n\\- making the public better informed:  \n\t\\- better able to deal with and process the wealth of information  \n\t\\- better able to express their own interests in government (via voting)  \n\t\\- helping cut through social media distraction / intentional misinformation  \n\\- getting governments to implement better policy  \n\t\\- coming up with good policy ideas  \n\t\\- objectively doing cost-benefit analysis  \n\t\\- objectively explaining tradeoffs (vs having to rely on biased humans to explain them)  \n\t\\- flagging bad ideas as bad  \n\t\\- convincing policymakers with compelling writeups  \n\\- success and flourishing for individuals and orgs using these tools  \n\t\\- via helping them think through decisions  \n\t\\- via clearheaded analysis of trade-offs and cost/benefits  \n\t\\- flagging bad ideas as bad\n\n## Jay\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * How can we build something like the positive version of Community Notes: which statements/posts are agreed on by people who often disagree?  \n  * How can we better understand/summarize what others think, e.g. on social media platforms? E.g. if you see a post recommended to you, is this because it's the most extreme take that went viral and isn't representative, or is this a commonly held view? Between different viewpoints, what are the concrete points of actual disagreement (on facts or values) and what are the things widely agreed upon? Let's build viewpoint/debate maps or a similar sensemaking tool that gets adopted/understood by the masses/raises the knowledge floor (in my experience, things like pol.is are hard for regular users to grok)  \n  * How can we improve Community Notes (or more general knowledge) creation with AI assistants? An AI agent proposing community notes and humans still checking/rating them?  \n  * How can we have something like Community Notes that applies to claims or statements about the future, or which are not concretely knowable?  \n  * \n\n## Ben Wilson (Metaculus)\n\nOverall I believe the most important value of epistemics (and AI epistemics) in general is informing what actions allow us to achieve our goals (i.e. make the world better). If we had perfect AI epistemics I would be excited by the ability for us to take a goal, and get a list of possible actions we could take and the forecasted outcomes of each of those actions (plus background information and forecasts for the 'current lay of the land' for context). This can then inform, not just the risk of AI doing good/bad things (as with current forecasting), but what we can do to raise the chances of 'great' AI outcomes, and lower the changes of 'worse' AI outcomes\n\n## Andreas\n\n* **What are use cases for AI for Epistemics that you most care about?**  \n  * AI policy, broadly construed, must go better  \n    * Avoid gradual disempowerment  \n    * Thoughtful regulation  \n  * Organizational planning & decision-making is weak  \n    * Given an informal long-term goal like improve decision-making for strategic users in government & large orgs, make a plan for a 20-people org that helps them accomplish the goal  \n    * Mapping dependencies, uncertainties, and potential futures  \n    * Understand complex domains with non-trivial feedback loops  \n      * Economic shifts  \n* **What decisions may go badly by default, but could go better if we have the right tech?**  \n  * Resource allocation within orgs  \n    * Investment  \n    * Grant making  \n    * Government budgets  \n  * Policy  \n    * How should AI be deployed in the world?  \n* **What do the later stages of the path to impact look like?**  \n  * AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning  \n  * The technology helps organizations maintain coherence and make sound judgments even as the pace of change accelerates  \n  * Assumption: People are generally well-intentioned \\- no one wants catastrophe \\- and as a result of better tooling things go better in the world overall\n\n## Owen optimistic brainstorm:\n\nWorld peace without world government — sufficiently good tech to help countries understand each others' interests, find mutually desirable bargains, and make sufficient credible commitments, could help us get up towards the Pareto frontier where we avoid (virtually) all costly conflicts\n\nGoing slowly and carefully on AI development — rather than having AI progress accelerate at increasingly breakneck speed, helping actors to understand the basic situation and the desirability of different options could be crucial for facilitating meaningful and robust agreements which restrict AI development to the stuff which we have good reasons for being confident are safe // otherwise beneficial.\n\nBettering democracy — helping people to access key information about what decisions their country might make would actually mean for longer term implications, and having this in turn feed into incentives for decision-makers that better align them with the common good\n\nBuilding safe / aligned AI — better tools for anticipating when systems are actually robustly safe, vs when stronger cases for expectation of safety seem required\n\n## Josh Rosenberg (FRI)\n\nI'm massively confused about important problems, e.g. how to orient to AI risk and what to do about it. Some reasonable people think there's a high probability we effectively lose everything we care about in 2027\\. Some reasonable people think it'll be decades till we have \"AGI\" and it'd be good if it arrived sooner. Which actions are good to take in each of these worlds is very different, and I'd like to figure out what world we're in.\n\nThis also applies to many other problem areas.\n\nE.g., Trump's effect on US democracy. How much of a crisis is it? What are the most effective actions to take in response to ensure that the country and communities we care about are as well off as they can be? How can we preserve high-quality institutions to lead the world in positive directions?\n\nOr, on the positive vision side: the \"Abundance\" agenda seems promising. Is it actually a good idea? What do we do about it?\n\nAI for Epistemics could help us get a lot more clarity on how to think about these problems and what actions to take. I'd like much better information and forecasts on the likely effects of Trump, AI timelines, and the pro's and con's of different Abundance policies, for example. If I had a bunch of time and good resources to dig into these problems, I could get a much better sense for myself. I'm hoping AI will help all of us do this sort of thing collectively.\n\n## Katja Grace\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Put lots of cognitive effort into very clear and good and compelling case for AI risk  \n  * Automate talking to people about AI risk compellingly, possibly using above case as input  \n  * Good game theoretic thinking re arms races, coordination, what is possible. I.e. People are making dumb assumptions about things, e.g. \"can't slow down tech\". Could automate noting errors. So e.g. when someone says that on Twitter, can highlight it and send in response. \\-\\> good thinking via better weapons for debate (similar to Nathan project)  \n  * Specific idea: very quantified and competitive debate site, that can be cited re which views are winning, where AI does a lot of the cognitive labor in places no people are paying attention, but as a part of it gets cruxy, people pay attention and vote/bet  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Poor decision making around AI: e.g. 'racing' because of vague heuristics instead of ever doing calc on costs and benefits given specific risks  \n  * Failure to coordinate on AI: building AI systems that are dangerous, due to expecting others to do so\n\n## Joel Lehman\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Raising the bottom (i.e. helping improve societal epistemics) – in particular that in theory political decisions (at least political decision-makers) are somewhat downstream of societal epistemics  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Having weird political leaders and awful public epistemics at critical junctures seems to unnecessarily spike catastrophic/existential risk probabilities. In theory, helping uplevel or at least maintain current public epistemic health (especially as it relates to civic sensemaking) might help influence the quality of decision-makers and concrete decisions made at high-stakes moments  \n  * The shape of the tech is not clear, or probably requires multiple flavors of interventions. E.g. if LLM assistants by default help cultivate civic virtues and on the margin support critical thinking or whatever underlying skills and knowledge help a person up-level their epistemic habits / health (or at least if LLM assistants don't contribute to atrophy/harm); or somehow convincing social media networks to care about this (e.g. community notes but things that go beyond); similarly somehow help w/ important social goods like 'societal trust' and 'trust in your neighbors'  \n* What do the later stages of the path to impact look like?  \n  * \\[didn't get to this\\]  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * Healthier democratic society, better agreement on consensus reality, higher trust in society, better politicians, better concrete decisions in high-stakes moments \n\n## Austin Chen\n\n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * **Evaluator**: More agreement (or understanding of disagreement) of what orgs & people are doing good work (eg for making grants in AI safety)  \n  * **Oracle**: Instant trustworthy forecasts on any question you ask  \n  * **Planner**: Guidance on \"what are the 3 most important things I should do this day/week/month\"  \n  * **Representative**: Simulate the responses of other parties (people, orgs/corps, governments, LLM agents), for discussions, contracts, trade\n\n## Janna\n\n* Why do we care about (AI for) epistemics?  \n  * Noise to signal ratio could be much higher  \n  * AI (mis)information churned out at the price of almost zero  \n* What are use cases for AI for Epistemics that you most care about?  \n  * Solving prediction markets--is there a way to scale up the bots, even if they are wrong and lossy, to improve prediction market accuracy?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * US-China relations  \n  * Trump tariffs maybe  \n  * Immigration rules in the US  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * More accurate predictions to…  \n    * Reduce business uncertainty  \n    * Make it easier to evaluate life decisions  \n  * Forestalling bad decisions made on imperfect information\n\n## Owain\n\n1. Maybe having humans understanding the world better is good in itself. Lots of people read popular science for this reason. Maybe it's good for AIs as well  \n2. Very broadly, it seems that better world understanding and decision making enables people to make their lives better in myriad ways. (To substantiate this, one could try to separate out the effects of better world understanding from the effects of better realized technology. E.g. Looking at capital-poor countries that can still benefit from scientific knowledge, economics, etc.)  \n3. More specifically, if AI progress is fast, there will be lots of important decisions to be made in quick calendar time. Exploiting AI to improve those decisions seems valuable. Current decision making institutions do not seem well set up to adapt really quickly. (A specific concern is dangers from AI takeover).   \n4. Why now? Why this group of people? We haven't really had tech that could do most epistemic tasks until recently. Now we have AI that can do an increasing proportion of the sub-tasks of epistemic processes. This will allow us to delegate epistemic tasks to AI.  \n   1. It's likely that people do this delegation, even if it's somewhat misguided and potentially bad in the long-term. Can we make this go well?  \n   2. This potentially massively expands our epistemic capacities. How can we use this well?  \n   3. Epistemic tech/institutions/advances can be hard to fund via for-profit companies. This is in part due to public goods issues, and issues around markets for ideas.\n\n## Rafe\n\nAs ever, I hope this will stop advanced artificial intelligence destroying the world, killing all its people \\&c.\n\nSo I think there are a few paths for impact here. I think there is a world where we can make the most productive thinkers about how to do things go further and better; I'm not so excited about a single-player version of this, as I think there's a pretty narrow window when top-level centaur mode works out\\[1\\]\n\nEveryone is already thinking about AI, and, I suspect, this will only get more acute. I think it's pretty hard for people to confront this and think about it carefully and well. It's scary\\! And there are a bunch of entrenched interests and memeplexes that preserve their status better (myopically) if people only have shallow slogans to think about AI with.\n\nGiven that, I suspect that the global sensemaking machine isn't up for the task of thinking about AI well. I hope we can help the global sensemaking machine upgrade itself. I have some hope that we as a society can come up with good ideas for dealing with AI, and also come to see how they make sense, so we can choose them in a robust way.\n\nOne other path to impact: I think a society that understands things better, including itself and its impacts, is one that is more robust in a bunch of ways. A more epistemically-grounded society is also probably a bit richer, and I think a bigger pie creates more slack for arranging things in sanity-promoting ways. I have more hope in a robust society navigating this stuff well.\n\n\\[1\\]:  Yeah, maybe you can do something with shaving off subskills, but I don't think that's really that doable.\n\n## Dave\n\n* With my consequentialist hat, better epistemics being worth it has to route through improved decisions somewhere down the line. The decisions I particularly care about are those by powerful people wrt AI development and implementation and the effects this has on how the future goes  \n  * But I do feel a bit icky when I have this hat on. There seems to be something very intuitively appealing about (a) pursuing better epistemics for its own sake and (b) democratising epistemic tools for the use and benefit of everyone. I think this basically caches out in more 'raising the floor' type work  \n  * I don't feel like I have a very good way of resolving this tension yet, but am excited to see things pushing on both frontiers  \n* I think forecasting has the potential to be a pretty powerful tool and there's currently a massive undersupply of good forecasts. I'm excited for us to use the wave of increased AI labor to make better quantitative forecasts to inform key decisions. I probably care more about quantitative forecasts than most decision makers and am open to being persuaded that the other, fuzzier parts of epistemics are more valuable to work on. \n\n## Julian\n\nWhat are use cases for AI for Epistemics that you most care about?  \n**Raising the ceiling:** can we use AI to organize information and credibly resolve the most challenging and important questions on which experts still seem to disagree? For example:\n\n* COVID origins  \n* Urban planning and public policy  \n* Lots of advancements in basic science: routing around current disagreements and policy challenges with slam-dunk science might be a pretty strong way forward.\n\n**Raising the floor:** can we use AI to build extremely legibly trustworthy/reliable epistemic infrastructure that actually makes its way into people's lives?\n\n* Making truth more palatable (in a way which doesn't translate into making falsehoods more palatable)  \n* "
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Omission Bias in Neglecting User Agency in Forecasting",
          "description": "How users can challenge or override AI-generated forecasts is not discussed.",
          "importance": 55,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 15681,
            "endOffset": 15845,
            "quotedText": "is\\]  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * Healthier democratic society, better agreement on consensus reality, higher trust in soci"
          }
        },
        {
          "title": "Omission Bias in Neglecting Intergenerational Equity",
          "description": "The impact of epistemic tools on future generations is not discussed.",
          "importance": 50,
          "grade": 55,
          "isValid": false,
          "highlight": {
            "startOffset": 590,
            "endOffset": 23077,
            "quotedText": "What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nTemplate:\n\n## Name: \n\nEntry\n\n## Dan Schwarz:\n\n* What are use cases for AI for Epistemics that you most care about?\n\nGoing from \"Realized\" to \"Accessible\" knowledge, e.g. getting good information using the web. Agentic web agents like Deep Research.\n\n* What decisions may go badly by default, but could go better if we have the right tech?\n\nMany billions of small decisions.\n\n* What do the later stages of the path to impact look like?\n\nSuperhuman forecasting at scale.\n\n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nReasonable judgment on tricky questions that's too cheap to meter, better than if you asked a respected colleague to spend many minutes on it.\n\n## Aviv\n\n* Better government governance  \n* Better democratic functioning  \n* Reduced dangerous concentration of power  \n    \n* Healthy polarization / pluralism  \n    \n* More peace, less likelihood of war  \n* Better geopolitics  \n* Better economic incentives  \n    \n* Better AI org governance  \n* Better AI coordination (re. distribution, escalation, etc.)  \n* Better compute allocation (toward what matters for good outcomes)  \n    \n* Better allocation of resources overall  \n* Better lives for people  \n* More effective organizations, getting things done with fewer resources, more focused on the things that benefit everyone  \n    \n* Identification of common ground directions / equilibria / game theoretic strategies that create/incentivize new positive gradients (and not surfacing the bad versions of those…)  \n    \n* Understanding our own goals and values\n\n*Note how it is the intersection of better epistemics / world models + the right power structures / incentives needed for all of these.*\n\n## Ben Goldhaber:\n\n* Forecasting how I should live my life in a period of intense technological change  \n* Guiding decision makers to avoid war with china/ai/everyone  \n* Providing the background context for me to sensemake way more sanely.  \n* Pushing forward the extant knowledge of AI safety agendas  \n* Planning for multi-stage projects that could achieve big impact  \n* Roadmapping to build larger more complex things\n\n## Chi\n\nThere are a few different things I care about:\n\n* I care a lot about humanity and AI, including **super-late-stage AI, getting decision theory** right.  \n  * Consequently but also separately, this relates to the general question of what to do about areas where there is not necessarily a ground truth but that are nonetheless very important to get right, cf**. [metaphilosophy](https://www.alignmentforum.org/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy)** (but also a bunch of other things.)  \n  * This also feels very similar to making **AIs helpful for AI safety research, cf. [automating alignment research](https://joecarlsmith.com/2025/03/14/ai-for-ai-safety#2-what-is-ai-for-ai-safety).**\n\n  → This is more about raising the **top of the epistemic landscape**\n\n* Other things I care about:  \n  * Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place.  \n  * Vague sense that better **societal epistemics** would help with increasing the **sanity of conversations about AI safety, polarisation around the issue etc.**  \n    * This is broadly about **getting intent alignment** in the first place.\n\n→ These are all more about raising the **bottom of the epistemic landscape**\n\n## Deger Turan:\n\n* Social trust erosion, collective coordination mechanisms are built on a weak assumption  \n* Capacity to entertain truthfulness when it is not instrumental to one's incentive gradient & differing norms upon 'truth will make my sense of security weaker'   \n* Creating contexts where world model divergence can be agreed by both parties enabling a safe disagreement  \n* Improving model reasoning / safer AI by reducing slop, by creating accountability mechanisms through consistency checks & other specifications for healthy epistemics  \n* Decoupling virality with magnitude while living in a short attention span world, curation   \n* I have some confusion re: raising the floor when not all parties are necessarily operating in good faith. I think I need some help think through the ways in which epistemics in the context of incentive misalignment can be used or have defensibility. \n\n## Nathan Young: \n\nWhat would we have got:\n\n* Biden not running, perhaps an open dem primary  \n* No FTX crisis  \n* Less zero sum OpenAI board drama  \n* Many smaller, better choices \n\nWhat decisions might go badly:\n\n* Trump tariffs/ global trade  \n* Taiwan/ US-China conflict  \n* US-China competition  \n* US authoritarianism  \n* AI regulation  \n* EU-China realignment  \n* Automated robots  \n* OpenAI strategy  \n* Anthropic strategy  \n* Google strategy  \n* xAI strategy  \n* AI workforce automation  \n* China strategy  \n* Russia strategy  \n* Moskovitz strategy  \n* Must strategy  \n* Bezos Strategy  \n* Zuckerberg strategy  \n* Tallinn strategy  \n* Altman strategy  \n* Drone warfare  \n* Climate change destabilisation  \n* Democratic trust\n\n## Lincoln Quirk:\n\nWhy do we care about (AI for) epistemics?  \n\\- making the public better informed:  \n\t\\- better able to deal with and process the wealth of information  \n\t\\- better able to express their own interests in government (via voting)  \n\t\\- helping cut through social media distraction / intentional misinformation  \n\\- getting governments to implement better policy  \n\t\\- coming up with good policy ideas  \n\t\\- objectively doing cost-benefit analysis  \n\t\\- objectively explaining tradeoffs (vs having to rely on biased humans to explain them)  \n\t\\- flagging bad ideas as bad  \n\t\\- convincing policymakers with compelling writeups  \n\\- success and flourishing for individuals and orgs using these tools  \n\t\\- via helping them think through decisions  \n\t\\- via clearheaded analysis of trade-offs and cost/benefits  \n\t\\- flagging bad ideas as bad\n\n## Jay\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * How can we build something like the positive version of Community Notes: which statements/posts are agreed on by people who often disagree?  \n  * How can we better understand/summarize what others think, e.g. on social media platforms? E.g. if you see a post recommended to you, is this because it's the most extreme take that went viral and isn't representative, or is this a commonly held view? Between different viewpoints, what are the concrete points of actual disagreement (on facts or values) and what are the things widely agreed upon? Let's build viewpoint/debate maps or a similar sensemaking tool that gets adopted/understood by the masses/raises the knowledge floor (in my experience, things like pol.is are hard for regular users to grok)  \n  * How can we improve Community Notes (or more general knowledge) creation with AI assistants? An AI agent proposing community notes and humans still checking/rating them?  \n  * How can we have something like Community Notes that applies to claims or statements about the future, or which are not concretely knowable?  \n  * \n\n## Ben Wilson (Metaculus)\n\nOverall I believe the most important value of epistemics (and AI epistemics) in general is informing what actions allow us to achieve our goals (i.e. make the world better). If we had perfect AI epistemics I would be excited by the ability for us to take a goal, and get a list of possible actions we could take and the forecasted outcomes of each of those actions (plus background information and forecasts for the 'current lay of the land' for context). This can then inform, not just the risk of AI doing good/bad things (as with current forecasting), but what we can do to raise the chances of 'great' AI outcomes, and lower the changes of 'worse' AI outcomes\n\n## Andreas\n\n* **What are use cases for AI for Epistemics that you most care about?**  \n  * AI policy, broadly construed, must go better  \n    * Avoid gradual disempowerment  \n    * Thoughtful regulation  \n  * Organizational planning & decision-making is weak  \n    * Given an informal long-term goal like improve decision-making for strategic users in government & large orgs, make a plan for a 20-people org that helps them accomplish the goal  \n    * Mapping dependencies, uncertainties, and potential futures  \n    * Understand complex domains with non-trivial feedback loops  \n      * Economic shifts  \n* **What decisions may go badly by default, but could go better if we have the right tech?**  \n  * Resource allocation within orgs  \n    * Investment  \n    * Grant making  \n    * Government budgets  \n  * Policy  \n    * How should AI be deployed in the world?  \n* **What do the later stages of the path to impact look like?**  \n  * AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning  \n  * The technology helps organizations maintain coherence and make sound judgments even as the pace of change accelerates  \n  * Assumption: People are generally well-intentioned \\- no one wants catastrophe \\- and as a result of better tooling things go better in the world overall\n\n## Owen optimistic brainstorm:\n\nWorld peace without world government — sufficiently good tech to help countries understand each others' interests, find mutually desirable bargains, and make sufficient credible commitments, could help us get up towards the Pareto frontier where we avoid (virtually) all costly conflicts\n\nGoing slowly and carefully on AI development — rather than having AI progress accelerate at increasingly breakneck speed, helping actors to understand the basic situation and the desirability of different options could be crucial for facilitating meaningful and robust agreements which restrict AI development to the stuff which we have good reasons for being confident are safe // otherwise beneficial.\n\nBettering democracy — helping people to access key information about what decisions their country might make would actually mean for longer term implications, and having this in turn feed into incentives for decision-makers that better align them with the common good\n\nBuilding safe / aligned AI — better tools for anticipating when systems are actually robustly safe, vs when stronger cases for expectation of safety seem required\n\n## Josh Rosenberg (FRI)\n\nI'm massively confused about important problems, e.g. how to orient to AI risk and what to do about it. Some reasonable people think there's a high probability we effectively lose everything we care about in 2027\\. Some reasonable people think it'll be decades till we have \"AGI\" and it'd be good if it arrived sooner. Which actions are good to take in each of these worlds is very different, and I'd like to figure out what world we're in.\n\nThis also applies to many other problem areas.\n\nE.g., Trump's effect on US democracy. How much of a crisis is it? What are the most effective actions to take in response to ensure that the country and communities we care about are as well off as they can be? How can we preserve high-quality institutions to lead the world in positive directions?\n\nOr, on the positive vision side: the \"Abundance\" agenda seems promising. Is it actually a good idea? What do we do about it?\n\nAI for Epistemics could help us get a lot more clarity on how to think about these problems and what actions to take. I'd like much better information and forecasts on the likely effects of Trump, AI timelines, and the pro's and con's of different Abundance policies, for example. If I had a bunch of time and good resources to dig into these problems, I could get a much better sense for myself. I'm hoping AI will help all of us do this sort of thing collectively.\n\n## Katja Grace\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Put lots of cognitive effort into very clear and good and compelling case for AI risk  \n  * Automate talking to people about AI risk compellingly, possibly using above case as input  \n  * Good game theoretic thinking re arms races, coordination, what is possible. I.e. People are making dumb assumptions about things, e.g. \"can't slow down tech\". Could automate noting errors. So e.g. when someone says that on Twitter, can highlight it and send in response. \\-\\> good thinking via better weapons for debate (similar to Nathan project)  \n  * Specific idea: very quantified and competitive debate site, that can be cited re which views are winning, where AI does a lot of the cognitive labor in places no people are paying attention, but as a part of it gets cruxy, people pay attention and vote/bet  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Poor decision making around AI: e.g. 'racing' because of vague heuristics instead of ever doing calc on costs and benefits given specific risks  \n  * Failure to coordinate on AI: building AI systems that are dangerous, due to expecting others to do so\n\n## Joel Lehman\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Raising the bottom (i.e. helping improve societal epistemics) – in particular that in theory political decisions (at least political decision-makers) are somewhat downstream of societal epistemics  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Having weird political leaders and awful public epistemics at critical junctures seems to unnecessarily spike catastrophic/existential risk probabilities. In theory, helping uplevel or at least maintain current public epistemic health (especially as it relates to civic sensemaking) might help influence the quality of decision-makers and concrete decisions made at high-stakes moments  \n  * The shape of the tech is not clear, or probably requires multiple flavors of interventions. E.g. if LLM assistants by default help cultivate civic virtues and on the margin support critical thinking or whatever underlying skills and knowledge help a person up-level their epistemic habits / health (or at least if LLM assistants don't contribute to atrophy/harm); or somehow convincing social media networks to care about this (e.g. community notes but things that go beyond); similarly somehow help w/ important social goods like 'societal trust' and 'trust in your neighbors'  \n* What do the later stages of the path to impact look like?  \n  * \\[didn't get to this\\]  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * Healthier democratic society, better agreement on consensus reality, higher trust in society, better politicians, better concrete decisions in high-stakes moments \n\n## Austin Chen\n\n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * **Evaluator**: More agreement (or understanding of disagreement) of what orgs & people are doing good work (eg for making grants in AI safety)  \n  * **Oracle**: Instant trustworthy forecasts on any question you ask  \n  * **Planner**: Guidance on \"what are the 3 most important things I should do this day/week/month\"  \n  * **Representative**: Simulate the responses of other parties (people, orgs/corps, governments, LLM agents), for discussions, contracts, trade\n\n## Janna\n\n* Why do we care about (AI for) epistemics?  \n  * Noise to signal ratio could be much higher  \n  * AI (mis)information churned out at the price of almost zero  \n* What are use cases for AI for Epistemics that you most care about?  \n  * Solving prediction markets--is there a way to scale up the bots, even if they are wrong and lossy, to improve prediction market accuracy?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * US-China relations  \n  * Trump tariffs maybe  \n  * Immigration rules in the US  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * More accurate predictions to…  \n    * Reduce business uncertainty  \n    * Make it easier to evaluate life decisions  \n  * Forestalling bad decisions made on imperfect information\n\n## Owain\n\n1. Maybe having humans understanding the world better is good in itself. Lots of people read popular science for this reason. Maybe it's good for AIs as well  \n2. Very broadly, it seems that better world understanding and decision making enables people to make their lives better in myriad ways. (To substantiate this, one could try to separate out the effects of better world understanding from the effects of better realized technology. E.g. Looking at capital-poor countries that can still benefit from scientific knowledge, economics, etc.)  \n3. More specifically, if AI progress is fast, there will be lots of important decisions to be made in quick calendar time. Exploiting AI to improve those decisions seems valuable. Current decision making institutions do not seem well set up to adapt really quickly. (A specific concern is dangers from AI takeover).   \n4. Why now? Why this group of people? We haven't really had tech that could do most epistemic tasks until recently. Now we have AI that can do an increasing proportion of the sub-tasks of epistemic processes. This will allow us to delegate epistemic tasks to AI.  \n   1. It's likely that people do this delegation, even if it's somewhat misguided and potentially bad in the long-term. Can we make this go well?  \n   2. This potentially massively expands our epistemic capacities. How can we use this well?  \n   3. Epistemic tech/institutions/advances can be hard to fund via for-profit companies. This is in part due to public goods issues, and issues around markets for ideas.\n\n## Rafe\n\nAs ever, I hope this will stop advanced artificial intelligence destroying the world, killing all its people \\&c.\n\nSo I think there are a few paths for impact here. I think there is a world where we can make the most productive thinkers about how to do things go further and better; I'm not so excited about a single-player version of this, as I think there's a pretty narrow window when top-level centaur mode works out\\[1\\]\n\nEveryone is already thinking about AI, and, I suspect, this will only get more acute. I think it's pretty hard for people to confront this and think about it carefully and well. It's scary\\! And there are a bunch of entrenched interests and memeplexes that preserve their status better (myopically) if people only have shallow slogans to think about AI with.\n\nGiven that, I suspect that the global sensemaking machine isn't up for the task of thinking about AI well. I hope we can help the global sensemaking machine upgrade itself. I have some hope that we as a society can come up with good ideas for dealing with AI, and also come to see how they make sense, so we can choose them in a robust way.\n\nOne other path to impact: I think a society that understands things better, including itself and its impacts, is one that is more robust in a bunch of ways. A more epistemically-grounded society is also probably a bit richer, and I think a bigger pie creates more slack for arranging things in sanity-promoting ways. I have more hope in a robust society navigating this stuff well.\n\n\\[1\\]:  Yeah, maybe you can do something with shaving off subskills, but I don't think that's really that doable.\n\n## Dave\n\n* With my consequentialist hat, better epistemics being worth it has to route through improved decisions somewhere down the line. The decisions I particularly care about are those by powerful people wrt AI development and implementation and the effects this has on how the future goes  \n  * But I do feel a bit icky when I have this hat on. There seems to be something very intuitively appealing about (a) pursuing better epistemics for its own sake and (b) democratising epistemic tools for the use and benefit of everyone. I think this basically caches out in more 'raising the floor' type work  \n  * I don't feel like I have a very good way of resolving this tension yet, but am excited to see things pushing on both frontiers  \n* I think forecasting has the potential to be a pretty powerful tool and there's currently a massive undersupply of good forecasts. I'm excited for us to use the wave of increased AI labor to make better quantitative forecasts to inform key decisions. I probably care more about quantitative forecasts than most decision makers and am open to being persuaded that the other, fuzzier parts of epistemics are more valuable to work on. \n\n## Julian\n\nWhat are use cases for AI for Epistemics that you most care about?  \n**Raising the ceiling:** can we use AI to organize information and credibly resolve the most challenging and important questions on which experts still seem to disagree? For example:\n\n* COVID origins  \n* Urban planning and public policy  \n* Lots of advancements in basic science: routing around current disagreements and policy challenges with slam-dunk science might be a pretty strong way forward.\n\n**Raising the floor:** can we use AI to build extremely legibly trustworthy/reliable epistemic infrastructure that actually makes its way into people's lives?\n\n* Making truth more palatable (in a way which doesn't translate into making falsehoods more palatable)  \n* Collective epistemics: connecting people to the others with whom they would have the most productive/enlightening conversations\n\nWhat decisions may go badly by default, but could go better if we have the right tech?\n\n* Lots of stuff in public policy: downstream both of better experts (technocrats), better public (voters/interest groups), and better coordination (collective epistemics)\n\nWhat do the later stages of the path to impact look like?\n\n* I want us to live in a world where everyone is more rational, empathetic, considered in their decision making, conscientious about impact, and interested in coordinating."
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Omission Bias in Neglecting Non-human Stakeholders",
          "description": "The potential impact of epistemic tools on non-human stakeholders (e.g., environment, animals) is not considered.",
          "importance": 45,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 1826,
            "endOffset": 1892,
            "quotedText": "Better allocation of resources overall  \n* Better lives for people"
          }
        },
        {
          "title": "Omission Bias in Neglecting Cross-cultural Communication",
          "description": "The challenges of cross-cultural communication in epistemic processes are not addressed.",
          "importance": 50,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 1401,
            "endOffset": 1892,
            "quotedText": "Better government governance  \n* Better democratic functioning  \n* Reduced dangerous concentration of power  \n    \n* Healthy polarization / pluralism  \n    \n* More peace, less likelihood of war  \n* Better geopolitics  \n* Better economic incentives  \n    \n* Better AI org governance  \n* Better AI coordination (re. distribution, escalation, etc.)  \n* Better compute allocation (toward what matters for good outcomes)  \n    \n* Better allocation of resources overall  \n* Better lives for people"
          }
        },
        {
          "title": "Omission Bias in Neglecting User Consent",
          "description": "How user consent is obtained and respected in epistemic tools is not discussed.",
          "importance": 50,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 15749,
            "endOffset": 15988,
            "quotedText": "  \n  * Healthier democratic society, better agreement on consensus reality, higher trust in society, better politicians, better concrete decisions in high-stakes moments \n\n## Austin Chen\n\n* \"As a result of better AI for epistemics tech, we"
          }
        },
        {
          "title": "Omission Bias in Neglecting Data Sovereignty",
          "description": "Issues of data sovereignty and control over information are not addressed.",
          "importance": 50,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 32513,
            "endOffset": 32688,
            "quotedText": "To support science, ensure grounding of content in cited sources.  \n  • Pursue or promote assembly of appropriate knowledge inputs (beyond what is now used for training data)."
          }
        },
        {
          "title": "Omission Bias in Neglecting User Literacy",
          "description": "The need to improve user literacy to effectively use epistemic tools is not discussed.",
          "importance": 50,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 23214,
            "endOffset": 23289,
            "quotedText": "Easily accessible fact checkers / claim refuters  \n  * Powerful AI debaters"
          }
        },
        {
          "title": "Omission Bias in Neglecting Social Norms",
          "description": "The influence of social norms on epistemic processes and tool adoption is not addressed.",
          "importance": 50,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 1518,
            "endOffset": 1594,
            "quotedText": "Healthy polarization / pluralism  \n    \n* More peace, less likelihood of war"
          }
        },
        {
          "title": "Omission Bias in Neglecting User Feedback Loops",
          "description": "How user feedback is incorporated into the ongoing development of epistemic tools is not discussed.",
          "importance": 50,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 7884,
            "endOffset": 8052,
            "quotedText": "How can we improve Community Notes (or more general knowledge) creation with AI assistants? An AI agent proposing community notes and humans still checking/rating them?"
          }
        },
        {
          "title": "Omission Bias in Neglecting User Error",
          "description": "The potential for user error or misuse of epistemic tools is not addressed.",
          "importance": 50,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 15749,
            "endOffset": 15988,
            "quotedText": "  \n  * Healthier democratic society, better agreement on consensus reality, higher trust in society, better politicians, better concrete decisions in high-stakes moments \n\n## Austin Chen\n\n* \"As a result of better AI for epistemics tech, we"
          }
        },
        {
          "title": "Omission Bias in Neglecting Red Teaming",
          "description": "The importance of adversarial testing (red teaming) for epistemic tools is not discussed.",
          "importance": 55,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 4934,
            "endOffset": 5193,
            "quotedText": "Improving model reasoning / safer AI by reducing slop, by creating accountability mechanisms through consistency checks & other specifications for healthy epistemics  \n* Decoupling virality with magnitude while living in a short attention span world, curation"
          }
        },
        {
          "title": "Omission Bias in Neglecting User Empowerment in Negotiation",
          "description": "How epistemic tools can empower users in negotiation contexts (e.g., salary, bargaining) is not discussed.",
          "importance": 50,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 34498,
            "endOffset": 34656,
            "quotedText": "Bargaining? Even just salary negotiations feels like the results are often more the consequence of \"human bargaining skill\" and not the pareto optimal result."
          }
        },
        {
          "title": "Omission Bias in Neglecting User Autonomy in Decision-making",
          "description": "How to preserve user autonomy in decision-making when using epistemic tools is not discussed.",
          "importance": 50,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 15749,
            "endOffset": 15988,
            "quotedText": "  \n  * Healthier democratic society, better agreement on consensus reality, higher trust in society, better politicians, better concrete decisions in high-stakes moments \n\n## Austin Chen\n\n* \"As a result of better AI for epistemics tech, we"
          }
        },
        {
          "title": "Omission Bias in Neglecting User Diversity in Forecasting",
          "description": "The diversity of user perspectives in forecasting is not systematically considered.",
          "importance": 50,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 15681,
            "endOffset": 15845,
            "quotedText": "is\\]  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * Healthier democratic society, better agreement on consensus reality, higher trust in soci"
          }
        },
        {
          "title": "Omission Bias in Neglecting User Agency in Group Decision-making",
          "description": "How to ensure user agency in group decision-making processes supported by epistemic tools is not discussed.",
          "importance": 50,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 34928,
            "endOffset": 35081,
            "quotedText": "better group decisionmaking (e.g. unblocking things from vetopower type  \n    * in service of that: more aptly/calibratedly handling waves of information"
          }
        },
        {
          "title": "Omission Bias in Neglecting User Empowerment in Resource Allocation",
          "description": "How epistemic tools can empower users in resource allocation decisions is not discussed.",
          "importance": 50,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 1826,
            "endOffset": 1892,
            "quotedText": "Better allocation of resources overall  \n* Better lives for people"
          }
        },
        {
          "title": "Omission Bias in Neglecting User Empowerment in Policy-making",
          "description": "How epistemic tools can empower users in policy-making is not discussed.",
          "importance": 50,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 6517,
            "endOffset": 6648,
            "quotedText": "getting governments to implement better policy  \n\t\\- coming up with good policy ideas  \n\t\\- objectively doing cost-benefit analysis"
          }
        },
        {
          "title": "Omission Bias in Neglecting User Empowerment in Scientific Research",
          "description": "How epistemic tools can empower users in scientific research is not discussed.",
          "importance": 50,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 23353,
            "endOffset": 23462,
            "quotedText": "Scientific meta-analysis, systematic review  \n* Ad-hoc / bespoke probabilistic models for individual problems"
          }
        },
        {
          "title": "Omission Bias in Neglecting User Empowerment in Social Coordination",
          "description": "How epistemic tools can empower users in social coordination is not discussed.",
          "importance": 50,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 29158,
            "endOffset": 29470,
            "quotedText": "Moreover, such technologies allow new avenues for cooperation at larger scales of human involvement. This unlocks new potential for coherence in global politics, scientific discovery, social and technological innovation, and cultural expression, and ultimately the deep joy of being alive and human in the world."
          }
        },
        {
          "title": "Omission Bias in Neglecting User Empowerment in Civic Engagement",
          "description": "How epistemic tools can empower users in civic engagement is not discussed.",
          "importance": 50,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 6252,
            "endOffset": 6431,
            "quotedText": "making the public better informed:  \n\t\\- better able to deal with and process the wealth of information  \n\t\\- better able to express their own interests in government (via voting)"
          }
        },
        {
          "title": "Omission Bias in Neglecting User Empowerment in Education",
          "description": "How epistemic tools can empower users in education is not discussed.",
          "importance": 50,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 34313,
            "endOffset": 34399,
            "quotedText": "Allowing people to be better informed about experimental medical stuff for themselves."
          }
        },
        {
          "title": "Omission Bias in Neglecting User Empowerment in Personal Decision-making",
          "description": "How epistemic tools can empower users in personal decision-making is not discussed.",
          "importance": 50,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 15749,
            "endOffset": 15988,
            "quotedText": "  \n  * Healthier democratic society, better agreement on consensus reality, higher trust in society, better politicians, better concrete decisions in high-stakes moments \n\n## Austin Chen\n\n* \"As a result of better AI for epistemics tech, we"
          }
        },
        {
          "title": "Omission Bias in Neglecting User Empowerment in Organizational Decision-making",
          "description": "How epistemic tools can empower users in organizational decision-making is not discussed.",
          "importance": 50,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 9108,
            "endOffset": 9410,
            "quotedText": "Organizational planning & decision-making is weak  \n    * Given an informal long-term goal like improve decision-making for strategic users in government & large orgs, make a plan for a 20-people org that helps them accomplish the goal  \n    * Mapping dependencies, uncertainties, and potential futures"
          }
        },
        {
          "title": "Omission Bias in Neglecting User Empowerment in Negotiation and Bargaining",
          "description": "How epistemic tools can empower users in negotiation and bargaining is not discussed.",
          "importance": 50,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 34498,
            "endOffset": 34656,
            "quotedText": "Bargaining? Even just salary negotiations feels like the results are often more the consequence of \"human bargaining skill\" and not the pareto optimal result."
          }
        }
      ],
      "thinking": "## Thinking Process\n\n### 1. Executive Summary of Overall Bias Assessment\nThe document is a collaborative brainstorming session among experts and stakeholders in AI epistemics. It is exploratory, speculative, and aspirational, with a focus on the potential positive impacts of AI on decision-making, governance, and societal epistemics. The tone is generally optimistic, though some contributors express uncertainty or skepticism. The document is not a formal research paper, but rather a collection of perspectives, prompts, and open-ended reflections.\n\n### 2. Quantitative Bias Scores (1-10)\n- **Confirmation Bias (6/10):** Many contributors seem to share similar worldviews and assumptions about the value and risks of AI for epistemics, potentially reinforcing each other's beliefs.\n- **Selection Bias (7/10):** The participants are a self-selected group of AI researchers, policy thinkers, and adjacent experts, which may limit the diversity of perspectives (e.g., few non-Western, non-technical, or layperson voices).\n- **Optimism Bias (7/10):** There is a strong emphasis on the positive transformative potential of AI for epistemics, with less attention to possible negative externalities, misuse, or failure modes.\n- **Scope Insensitivity (5/10):** Some claims about the impact of AI epistemics are broad and sweeping, with limited discussion of practical constraints or incremental progress.\n- **Authority Bias (4/10):** While the document includes many respected voices, there is little explicit deference to authority; ideas are presented as open questions.\n- **In-group Bias (6/10):** The discussion is largely within the AI safety/epistemics community, with shared jargon and reference points, possibly limiting critical challenge from outside perspectives.\n- **Hindsight Bias (3/10):** The document is forward-looking and speculative, so this bias is less relevant.\n- **Narrative Fallacy (5/10):** Some contributors construct compelling narratives about how AI epistemics will lead to better outcomes, but these are not always grounded in empirical evidence.\n- **Status Quo Bias (2/10):** The group is clearly interested in changing the status quo, so this bias is minimal.\n\n### 3. Most Significant Examples of Bias Found\n- **Selection Bias:** The lack of representation from non-technical, non-Western, or marginalized groups may lead to blind spots regarding the societal impacts of AI epistemics.\n- **Optimism Bias:** Many contributors assume that better epistemic tools will naturally lead to better decisions and outcomes, with less attention to potential perverse incentives, misuse, or systemic failures.\n- **Confirmation Bias:** The group largely agrees on the importance and urgency of AI for epistemics, which may limit critical scrutiny of underlying assumptions.\n\n### 4. Prioritized Recommendations for Improvement\n1. **Broaden Stakeholder Engagement:** Include voices from diverse backgrounds (e.g., policymakers, civil society, non-Western perspectives, affected communities) to challenge assumptions and surface overlooked risks.\n2. **Explicitly Address Failure Modes:** Encourage contributors to systematically consider how AI epistemics could go wrong, including issues of manipulation, centralization of power, or unintended consequences.\n3. **Ground Claims in Evidence:** Where possible, supplement aspirational statements with empirical data, case studies, or references to existing research on epistemic interventions.\n4. **Encourage Dissent and Critical Challenge:** Create space for contrarian views and structured debate to avoid groupthink and confirmation bias.\n\n### 5. Limitations of the Analysis\n- The document is a brainstorming session, not a formal policy proposal or research paper, so some biases may be less consequential at this stage.\n- The analysis is based on the text provided; additional context about the participants, process, or intended audience could affect the assessment.\n- Some biases (e.g., survivorship bias, sunk cost fallacy) are less relevant given the speculative and forward-looking nature of the discussion.\n\n### Key Themes and Patterns\n- Strong focus on the positive potential of AI for improving decision-making, governance, and societal epistemics.\n- Repeated emphasis on both \"raising the floor\" (improving baseline epistemic health) and \"raising the ceiling\" (enabling superhuman reasoning and coordination).\n- Concerns about trust, coordination, and the risk of epistemic degradation due to AI and social media.\n- Limited discussion of concrete risks, failure modes, or negative externalities.\n\n### Expert Perspective\nThe document is valuable as a snapshot of current thinking among AI epistemics stakeholders, but it would benefit from more critical engagement with potential downsides and a broader range of perspectives. The optimism and ambition are strengths, but may also obscure important challenges and risks.",
      "summary": "### Bias Assessment Summary\n\nThe document reflects a thoughtful, optimistic exploration of AI for epistemics, but exhibits notable selection, confirmation, and optimism biases due to its homogenous, expert-driven participant pool. While it surfaces valuable aspirations and concerns, it would benefit from broader stakeholder inclusion, explicit consideration of failure modes, and more critical challenge to prevailing assumptions.",
      "grade": 74
    },
    {
      "agentId": "clarity-coach",
      "createdAt": "2025-04-18T21:14:03.502Z",
      "costInCents": 0,
      "comments": [
        {
          "title": "Fragmented Structure Reduces Coherence",
          "description": "The document is organized as a series of loosely connected bullet-pointed responses from different contributors, lacking a clear overarching structure or narrative. This fragmentation makes it difficult for readers to follow the logical progression of ideas and to synthesize key themes. Consider introducing a summary section or thematic grouping to improve coherence.",
          "importance": 90,
          "grade": 40,
          "isValid": false,
          "highlight": {
            "startOffset": 19,
            "endOffset": 1388,
            "quotedText": "Preamble/framing from Owen:\n\n* Negative: \"we just want to free-form vibe about what cool things we could do\"  \n* Positive: step back & ask \"wait, why are we doing this again?\"  \n* What stuff does this actually help?  \n* Epistemics isn't a goal in-an-of itself; figuring out what combination of things are possible, achievable, etc.  \n* Why do we care about (AI for) epistemics?\n\nWriting prompts from Owen:\n\n* What are use cases for AI for Epistemics that you most care about?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nTemplate:\n\n## Name: \n\nEntry\n\n## Dan Schwarz:\n\n* What are use cases for AI for Epistemics that you most care about?\n\nGoing from \"Realized\" to \"Accessible\" knowledge, e.g. getting good information using the web. Agentic web agents like Deep Research.\n\n* What decisions may go badly by default, but could go better if we have the right tech?\n\nMany billions of small decisions.\n\n* What do the later stages of the path to impact look like?\n\nSuperhuman forecasting at scale.\n\n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nReasonable judgment on tricky questions that's too cheap to meter, better than if you asked a respected colleague to spend many minutes on it."
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Effective Use of Prompts for Focus",
          "description": "The use of explicit writing prompts (e.g., 'What are use cases for AI for Epistemics that you most care about?') provides clear signposting and helps contributors stay on topic. This enhances readability and ensures that responses are relevant to the central theme.",
          "importance": 60,
          "grade": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 398,
            "endOffset": 711,
            "quotedText": "Writing prompts from Owen:\n\n* What are use cases for AI for Epistemics that you most care about?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\""
          }
        },
        {
          "title": "Inconsistent Formatting Across Sections",
          "description": "Some sections use bullet points, others use paragraphs, and some mix both. This inconsistency can confuse readers and disrupts the flow. Standardizing the format for each contributor's section (e.g., always starting with a brief paragraph summary, followed by bullet points) would improve readability.",
          "importance": 70,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 742,
            "endOffset": 1388,
            "quotedText": "## Dan Schwarz:\n\n* What are use cases for AI for Epistemics that you most care about?\n\nGoing from \"Realized\" to \"Accessible\" knowledge, e.g. getting good information using the web. Agentic web agents like Deep Research.\n\n* What decisions may go badly by default, but could go better if we have the right tech?\n\nMany billions of small decisions.\n\n* What do the later stages of the path to impact look like?\n\nSuperhuman forecasting at scale.\n\n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nReasonable judgment on tricky questions that's too cheap to meter, better than if you asked a respected colleague to spend many minutes on it."
          }
        },
        {
          "title": "Dense Technical Jargon in Some Sections",
          "description": "Certain contributors use specialized terms (e.g., 'intent-alignment', 'epistemic landscape', 'vetopower type') without explanation. This increases the reading level and may alienate non-expert readers. Consider adding brief definitions or footnotes for domain-specific terminology.",
          "importance": 80,
          "grade": 45,
          "isValid": true,
          "highlight": {
            "startOffset": 3638,
            "endOffset": 3975,
            "quotedText": "Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**?"
          }
        },
        {
          "title": "Strong Thematic Consistency on Societal Impact",
          "description": "Multiple contributors independently emphasize the societal and governance implications of AI for epistemics, reinforcing the document's central theme. This repetition strengthens the message and provides a sense of consensus.",
          "importance": 65,
          "grade": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 1401,
            "endOffset": 2252,
            "quotedText": "Better government governance  \n* Better democratic functioning  \n* Reduced dangerous concentration of power  \n    \n* Healthy polarization / pluralism  \n    \n* More peace, less likelihood of war  \n* Better geopolitics  \n* Better economic incentives  \n    \n* Better AI org governance  \n* Better AI coordination (re. distribution, escalation, etc.)  \n* Better compute allocation (toward what matters for good outcomes)  \n    \n* Better allocation of resources overall  \n* Better lives for people  \n* More effective organizations, getting things done with fewer resources, more focused on the things that benefit everyone  \n    \n* Identification of common ground directions / equilibria / game theoretic strategies that create/incentivize new positive gradients (and not surfacing the bad versions of those…)  \n    \n* Understanding our own goals and values"
          }
        },
        {
          "title": "Overuse of Vague Language and Hedging",
          "description": "Phrases like 'vague sense', 'maybe', and 'I think' appear frequently, which can undermine the perceived authority and precision of the arguments. Where possible, replace hedging with more assertive statements or provide supporting evidence.",
          "importance": 60,
          "grade": 40,
          "isValid": true,
          "highlight": {
            "startOffset": 3638,
            "endOffset": 3975,
            "quotedText": "Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**?"
          }
        },
        {
          "title": "Effective Use of Examples to Illustrate Points",
          "description": "Several contributors provide concrete examples (e.g., 'Biden not running', 'No FTX crisis', 'COVID origins'), which help ground abstract concepts and make the content more relatable. This practice enhances clarity and aids comprehension.",
          "importance": 75,
          "grade": 90,
          "isValid": true,
          "highlight": {
            "startOffset": 5470,
            "endOffset": 5631,
            "quotedText": "What would we have got:\n\n* Biden not running, perhaps an open dem primary  \n* No FTX crisis  \n* Less zero sum OpenAI board drama  \n* Many smaller, better choices"
          }
        },
        {
          "title": "Long, Complex Sentences Reduce Readability",
          "description": "Some sections contain sentences that are overly long and complex, increasing cognitive load for readers. Breaking these into shorter sentences would improve readability and accessibility.",
          "importance": 70,
          "grade": 45,
          "isValid": false,
          "highlight": {
            "startOffset": 23715,
            "endOffset": 24810,
            "quotedText": "There are already key questions/strategic problems where the truth is murky and we seem to proceed on the basis of limited and biased views. There might well be many more of these in the future. A central example is US-China competition over AI. Some have vibes of \"China is so bad that it's worth risking an existential catastrophe to stop it from capturing a portion of the lightcone\". Others have vibes of \"Cooperation is feasible and desirable, and is our only path to a good outcome\". But how bad is China, really? How much \"realised K\" on this question do those who are making the relevant decisions actually have? Similarly, how feasible is cooperation? How valid is this frame of \"capturing the lightcone\"? One reason that we don't have a better hold on these questions is that they are frighteningly complicated and difficult  – I think we could benefit massively from throwing much more cognitive labour at them, and presenting the findings in a way that is hard to disagree with or for any individual to improve upon, other than at a normative/values level. Maybe AI can get us there."
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Lack of Transitional Phrases Between Sections",
          "description": "The document jumps abruptly from one contributor to another without transitional phrases or summaries, which can disorient readers. Adding brief connecting statements or section headers summarizing key points would improve flow.",
          "importance": 65,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 2392,
            "endOffset": 2814,
            "quotedText": "## Ben Goldhaber:\n\n* Forecasting how I should live my life in a period of intense technological change  \n* Guiding decision makers to avoid war with china/ai/everyone  \n* Providing the background context for me to sensemake way more sanely.  \n* Pushing forward the extant knowledge of AI safety agendas  \n* Planning for multi-stage projects that could achieve big impact  \n* Roadmapping to build larger more complex things"
          }
        },
        {
          "title": "Clear Articulation of Purpose in Preamble",
          "description": "The preamble effectively frames the discussion by contrasting 'free-form vibe' with 'step back & ask why', setting the stage for a reflective and purposeful exploration. This clarity of intent is a strength.",
          "importance": 60,
          "grade": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 19,
            "endOffset": 396,
            "quotedText": "Preamble/framing from Owen:\n\n* Negative: \"we just want to free-form vibe about what cool things we could do\"  \n* Positive: step back & ask \"wait, why are we doing this again?\"  \n* What stuff does this actually help?  \n* Epistemics isn't a goal in-an-of itself; figuring out what combination of things are possible, achievable, etc.  \n* Why do we care about (AI for) epistemics?"
          }
        },
        {
          "title": "Redundant Phrasing in Multiple Sections",
          "description": "Some contributors repeat similar ideas or phrases (e.g., 'better decisions', 'raising the floor/ceiling'), which can be streamlined for concision. Consolidating overlapping points would reduce redundancy and improve word economy.",
          "importance": 55,
          "grade": 50,
          "isValid": false,
          "highlight": {
            "startOffset": 428,
            "endOffset": 35664,
            "quotedText": "What are use cases for AI for Epistemics that you most care about?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nTemplate:\n\n## Name: \n\nEntry\n\n## Dan Schwarz:\n\n* What are use cases for AI for Epistemics that you most care about?\n\nGoing from \"Realized\" to \"Accessible\" knowledge, e.g. getting good information using the web. Agentic web agents like Deep Research.\n\n* What decisions may go badly by default, but could go better if we have the right tech?\n\nMany billions of small decisions.\n\n* What do the later stages of the path to impact look like?\n\nSuperhuman forecasting at scale.\n\n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nReasonable judgment on tricky questions that's too cheap to meter, better than if you asked a respected colleague to spend many minutes on it.\n\n## Aviv\n\n* Better government governance  \n* Better democratic functioning  \n* Reduced dangerous concentration of power  \n    \n* Healthy polarization / pluralism  \n    \n* More peace, less likelihood of war  \n* Better geopolitics  \n* Better economic incentives  \n    \n* Better AI org governance  \n* Better AI coordination (re. distribution, escalation, etc.)  \n* Better compute allocation (toward what matters for good outcomes)  \n    \n* Better allocation of resources overall  \n* Better lives for people  \n* More effective organizations, getting things done with fewer resources, more focused on the things that benefit everyone  \n    \n* Identification of common ground directions / equilibria / game theoretic strategies that create/incentivize new positive gradients (and not surfacing the bad versions of those…)  \n    \n* Understanding our own goals and values\n\n*Note how it is the intersection of better epistemics / world models + the right power structures / incentives needed for all of these.*\n\n## Ben Goldhaber:\n\n* Forecasting how I should live my life in a period of intense technological change  \n* Guiding decision makers to avoid war with china/ai/everyone  \n* Providing the background context for me to sensemake way more sanely.  \n* Pushing forward the extant knowledge of AI safety agendas  \n* Planning for multi-stage projects that could achieve big impact  \n* Roadmapping to build larger more complex things\n\n## Chi\n\nThere are a few different things I care about:\n\n* I care a lot about humanity and AI, including **super-late-stage AI, getting decision theory** right.  \n  * Consequently but also separately, this relates to the general question of what to do about areas where there is not necessarily a ground truth but that are nonetheless very important to get right, cf**. [metaphilosophy](https://www.alignmentforum.org/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy)** (but also a bunch of other things.)  \n  * This also feels very similar to making **AIs helpful for AI safety research, cf. [automating alignment research](https://joecarlsmith.com/2025/03/14/ai-for-ai-safety#2-what-is-ai-for-ai-safety).**\n\n  → This is more about raising the **top of the epistemic landscape**\n\n* Other things I care about:  \n  * Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place.  \n  * Vague sense that better **societal epistemics** would help with increasing the **sanity of conversations about AI safety, polarisation around the issue etc.**  \n    * This is broadly about **getting intent alignment** in the first place.\n\n→ These are all more about raising the **bottom of the epistemic landscape**\n\n## Deger Turan:\n\n* Social trust erosion, collective coordination mechanisms are built on a weak assumption  \n* Capacity to entertain truthfulness when it is not instrumental to one's incentive gradient & differing norms upon 'truth will make my sense of security weaker'   \n* Creating contexts where world model divergence can be agreed by both parties enabling a safe disagreement  \n* Improving model reasoning / safer AI by reducing slop, by creating accountability mechanisms through consistency checks & other specifications for healthy epistemics  \n* Decoupling virality with magnitude while living in a short attention span world, curation   \n* I have some confusion re: raising the floor when not all parties are necessarily operating in good faith. I think I need some help think through the ways in which epistemics in the context of incentive misalignment can be used or have defensibility. \n\n## Nathan Young: \n\nWhat would we have got:\n\n* Biden not running, perhaps an open dem primary  \n* No FTX crisis  \n* Less zero sum OpenAI board drama  \n* Many smaller, better choices \n\nWhat decisions might go badly:\n\n* Trump tariffs/ global trade  \n* Taiwan/ US-China conflict  \n* US-China competition  \n* US authoritarianism  \n* AI regulation  \n* EU-China realignment  \n* Automated robots  \n* OpenAI strategy  \n* Anthropic strategy  \n* Google strategy  \n* xAI strategy  \n* AI workforce automation  \n* China strategy  \n* Russia strategy  \n* Moskovitz strategy  \n* Must strategy  \n* Bezos Strategy  \n* Zuckerberg strategy  \n* Tallinn strategy  \n* Altman strategy  \n* Drone warfare  \n* Climate change destabilisation  \n* Democratic trust\n\n## Lincoln Quirk:\n\nWhy do we care about (AI for) epistemics?  \n\\- making the public better informed:  \n\t\\- better able to deal with and process the wealth of information  \n\t\\- better able to express their own interests in government (via voting)  \n\t\\- helping cut through social media distraction / intentional misinformation  \n\\- getting governments to implement better policy  \n\t\\- coming up with good policy ideas  \n\t\\- objectively doing cost-benefit analysis  \n\t\\- objectively explaining tradeoffs (vs having to rely on biased humans to explain them)  \n\t\\- flagging bad ideas as bad  \n\t\\- convincing policymakers with compelling writeups  \n\\- success and flourishing for individuals and orgs using these tools  \n\t\\- via helping them think through decisions  \n\t\\- via clearheaded analysis of trade-offs and cost/benefits  \n\t\\- flagging bad ideas as bad\n\n## Jay\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * How can we build something like the positive version of Community Notes: which statements/posts are agreed on by people who often disagree?  \n  * How can we better understand/summarize what others think, e.g. on social media platforms? E.g. if you see a post recommended to you, is this because it's the most extreme take that went viral and isn't representative, or is this a commonly held view? Between different viewpoints, what are the concrete points of actual disagreement (on facts or values) and what are the things widely agreed upon? Let's build viewpoint/debate maps or a similar sensemaking tool that gets adopted/understood by the masses/raises the knowledge floor (in my experience, things like pol.is are hard for regular users to grok)  \n  * How can we improve Community Notes (or more general knowledge) creation with AI assistants? An AI agent proposing community notes and humans still checking/rating them?  \n  * How can we have something like Community Notes that applies to claims or statements about the future, or which are not concretely knowable?  \n  * \n\n## Ben Wilson (Metaculus)\n\nOverall I believe the most important value of epistemics (and AI epistemics) in general is informing what actions allow us to achieve our goals (i.e. make the world better). If we had perfect AI epistemics I would be excited by the ability for us to take a goal, and get a list of possible actions we could take and the forecasted outcomes of each of those actions (plus background information and forecasts for the 'current lay of the land' for context). This can then inform, not just the risk of AI doing good/bad things (as with current forecasting), but what we can do to raise the chances of 'great' AI outcomes, and lower the changes of 'worse' AI outcomes\n\n## Andreas\n\n* **What are use cases for AI for Epistemics that you most care about?**  \n  * AI policy, broadly construed, must go better  \n    * Avoid gradual disempowerment  \n    * Thoughtful regulation  \n  * Organizational planning & decision-making is weak  \n    * Given an informal long-term goal like improve decision-making for strategic users in government & large orgs, make a plan for a 20-people org that helps them accomplish the goal  \n    * Mapping dependencies, uncertainties, and potential futures  \n    * Understand complex domains with non-trivial feedback loops  \n      * Economic shifts  \n* **What decisions may go badly by default, but could go better if we have the right tech?**  \n  * Resource allocation within orgs  \n    * Investment  \n    * Grant making  \n    * Government budgets  \n  * Policy  \n    * How should AI be deployed in the world?  \n* **What do the later stages of the path to impact look like?**  \n  * AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning  \n  * The technology helps organizations maintain coherence and make sound judgments even as the pace of change accelerates  \n  * Assumption: People are generally well-intentioned \\- no one wants catastrophe \\- and as a result of better tooling things go better in the world overall\n\n## Owen optimistic brainstorm:\n\nWorld peace without world government — sufficiently good tech to help countries understand each others' interests, find mutually desirable bargains, and make sufficient credible commitments, could help us get up towards the Pareto frontier where we avoid (virtually) all costly conflicts\n\nGoing slowly and carefully on AI development — rather than having AI progress accelerate at increasingly breakneck speed, helping actors to understand the basic situation and the desirability of different options could be crucial for facilitating meaningful and robust agreements which restrict AI development to the stuff which we have good reasons for being confident are safe // otherwise beneficial.\n\nBettering democracy — helping people to access key information about what decisions their country might make would actually mean for longer term implications, and having this in turn feed into incentives for decision-makers that better align them with the common good\n\nBuilding safe / aligned AI — better tools for anticipating when systems are actually robustly safe, vs when stronger cases for expectation of safety seem required\n\n## Josh Rosenberg (FRI)\n\nI'm massively confused about important problems, e.g. how to orient to AI risk and what to do about it. Some reasonable people think there's a high probability we effectively lose everything we care about in 2027\\. Some reasonable people think it'll be decades till we have \"AGI\" and it'd be good if it arrived sooner. Which actions are good to take in each of these worlds is very different, and I'd like to figure out what world we're in.\n\nThis also applies to many other problem areas.\n\nE.g., Trump's effect on US democracy. How much of a crisis is it? What are the most effective actions to take in response to ensure that the country and communities we care about are as well off as they can be? How can we preserve high-quality institutions to lead the world in positive directions?\n\nOr, on the positive vision side: the \"Abundance\" agenda seems promising. Is it actually a good idea? What do we do about it?\n\nAI for Epistemics could help us get a lot more clarity on how to think about these problems and what actions to take. I'd like much better information and forecasts on the likely effects of Trump, AI timelines, and the pro's and con's of different Abundance policies, for example. If I had a bunch of time and good resources to dig into these problems, I could get a much better sense for myself. I'm hoping AI will help all of us do this sort of thing collectively.\n\n## Katja Grace\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Put lots of cognitive effort into very clear and good and compelling case for AI risk  \n  * Automate talking to people about AI risk compellingly, possibly using above case as input  \n  * Good game theoretic thinking re arms races, coordination, what is possible. I.e. People are making dumb assumptions about things, e.g. \"can't slow down tech\". Could automate noting errors. So e.g. when someone says that on Twitter, can highlight it and send in response. \\-\\> good thinking via better weapons for debate (similar to Nathan project)  \n  * Specific idea: very quantified and competitive debate site, that can be cited re which views are winning, where AI does a lot of the cognitive labor in places no people are paying attention, but as a part of it gets cruxy, people pay attention and vote/bet  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Poor decision making around AI: e.g. 'racing' because of vague heuristics instead of ever doing calc on costs and benefits given specific risks  \n  * Failure to coordinate on AI: building AI systems that are dangerous, due to expecting others to do so\n\n## Joel Lehman\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Raising the bottom (i.e. helping improve societal epistemics) – in particular that in theory political decisions (at least political decision-makers) are somewhat downstream of societal epistemics  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Having weird political leaders and awful public epistemics at critical junctures seems to unnecessarily spike catastrophic/existential risk probabilities. In theory, helping uplevel or at least maintain current public epistemic health (especially as it relates to civic sensemaking) might help influence the quality of decision-makers and concrete decisions made at high-stakes moments  \n  * The shape of the tech is not clear, or probably requires multiple flavors of interventions. E.g. if LLM assistants by default help cultivate civic virtues and on the margin support critical thinking or whatever underlying skills and knowledge help a person up-level their epistemic habits / health (or at least if LLM assistants don't contribute to atrophy/harm); or somehow convincing social media networks to care about this (e.g. community notes but things that go beyond); similarly somehow help w/ important social goods like 'societal trust' and 'trust in your neighbors'  \n* What do the later stages of the path to impact look like?  \n  * \\[didn't get to this\\]  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * Healthier democratic society, better agreement on consensus reality, higher trust in society, better politicians, better concrete decisions in high-stakes moments \n\n## Austin Chen\n\n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * **Evaluator**: More agreement (or understanding of disagreement) of what orgs & people are doing good work (eg for making grants in AI safety)  \n  * **Oracle**: Instant trustworthy forecasts on any question you ask  \n  * **Planner**: Guidance on \"what are the 3 most important things I should do this day/week/month\"  \n  * **Representative**: Simulate the responses of other parties (people, orgs/corps, governments, LLM agents), for discussions, contracts, trade\n\n## Janna\n\n* Why do we care about (AI for) epistemics?  \n  * Noise to signal ratio could be much higher  \n  * AI (mis)information churned out at the price of almost zero  \n* What are use cases for AI for Epistemics that you most care about?  \n  * Solving prediction markets--is there a way to scale up the bots, even if they are wrong and lossy, to improve prediction market accuracy?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * US-China relations  \n  * Trump tariffs maybe  \n  * Immigration rules in the US  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * More accurate predictions to…  \n    * Reduce business uncertainty  \n    * Make it easier to evaluate life decisions  \n  * Forestalling bad decisions made on imperfect information\n\n## Owain\n\n1. Maybe having humans understanding the world better is good in itself. Lots of people read popular science for this reason. Maybe it's good for AIs as well  \n2. Very broadly, it seems that better world understanding and decision making enables people to make their lives better in myriad ways. (To substantiate this, one could try to separate out the effects of better world understanding from the effects of better realized technology. E.g. Looking at capital-poor countries that can still benefit from scientific knowledge, economics, etc.)  \n3. More specifically, if AI progress is fast, there will be lots of important decisions to be made in quick calendar time. Exploiting AI to improve those decisions seems valuable. Current decision making institutions do not seem well set up to adapt really quickly. (A specific concern is dangers from AI takeover).   \n4. Why now? Why this group of people? We haven't really had tech that could do most epistemic tasks until recently. Now we have AI that can do an increasing proportion of the sub-tasks of epistemic processes. This will allow us to delegate epistemic tasks to AI.  \n   1. It's likely that people do this delegation, even if it's somewhat misguided and potentially bad in the long-term. Can we make this go well?  \n   2. This potentially massively expands our epistemic capacities. How can we use this well?  \n   3. Epistemic tech/institutions/advances can be hard to fund via for-profit companies. This is in part due to public goods issues, and issues around markets for ideas.\n\n## Rafe\n\nAs ever, I hope this will stop advanced artificial intelligence destroying the world, killing all its people \\&c.\n\nSo I think there are a few paths for impact here. I think there is a world where we can make the most productive thinkers about how to do things go further and better; I'm not so excited about a single-player version of this, as I think there's a pretty narrow window when top-level centaur mode works out\\[1\\]\n\nEveryone is already thinking about AI, and, I suspect, this will only get more acute. I think it's pretty hard for people to confront this and think about it carefully and well. It's scary\\! And there are a bunch of entrenched interests and memeplexes that preserve their status better (myopically) if people only have shallow slogans to think about AI with.\n\nGiven that, I suspect that the global sensemaking machine isn't up for the task of thinking about AI well. I hope we can help the global sensemaking machine upgrade itself. I have some hope that we as a society can come up with good ideas for dealing with AI, and also come to see how they make sense, so we can choose them in a robust way.\n\nOne other path to impact: I think a society that understands things better, including itself and its impacts, is one that is more robust in a bunch of ways. A more epistemically-grounded society is also probably a bit richer, and I think a bigger pie creates more slack for arranging things in sanity-promoting ways. I have more hope in a robust society navigating this stuff well.\n\n\\[1\\]:  Yeah, maybe you can do something with shaving off subskills, but I don't think that's really that doable.\n\n## Dave\n\n* With my consequentialist hat, better epistemics being worth it has to route through improved decisions somewhere down the line. The decisions I particularly care about are those by powerful people wrt AI development and implementation and the effects this has on how the future goes  \n  * But I do feel a bit icky when I have this hat on. There seems to be something very intuitively appealing about (a) pursuing better epistemics for its own sake and (b) democratising epistemic tools for the use and benefit of everyone. I think this basically caches out in more 'raising the floor' type work  \n  * I don't feel like I have a very good way of resolving this tension yet, but am excited to see things pushing on both frontiers  \n* I think forecasting has the potential to be a pretty powerful tool and there's currently a massive undersupply of good forecasts. I'm excited for us to use the wave of increased AI labor to make better quantitative forecasts to inform key decisions. I probably care more about quantitative forecasts than most decision makers and am open to being persuaded that the other, fuzzier parts of epistemics are more valuable to work on. \n\n## Julian\n\nWhat are use cases for AI for Epistemics that you most care about?  \n**Raising the ceiling:** can we use AI to organize information and credibly resolve the most challenging and important questions on which experts still seem to disagree? For example:\n\n* COVID origins  \n* Urban planning and public policy  \n* Lots of advancements in basic science: routing around current disagreements and policy challenges with slam-dunk science might be a pretty strong way forward.\n\n**Raising the floor:** can we use AI to build extremely legibly trustworthy/reliable epistemic infrastructure that actually makes its way into people's lives?\n\n* Making truth more palatable (in a way which doesn't translate into making falsehoods more palatable)  \n* Collective epistemics: connecting people to the others with whom they would have the most productive/enlightening conversations\n\nWhat decisions may go badly by default, but could go better if we have the right tech?\n\n* Lots of stuff in public policy: downstream both of better experts (technocrats), better public (voters/interest groups), and better coordination (collective epistemics)\n\nWhat do the later stages of the path to impact look like?\n\n* I want us to live in a world where everyone is more rational, empathetic, considered in their decision making, conscientious about impact, and interested in coordinating. I feel like that's one of the biggest blockers to all kinds of *long term* progress.\n\nWhat tools do we actually maybe want to build?\n\n* Easily accessible fact checkers / claim refuters  \n  * Powerful AI debaters  \n* Forecasting with explanations: superhuman and at scale  \n* Scientific meta-analysis, systematic review  \n* Ad-hoc / bespoke probabilistic models for individual problems  \n* Bespoke *simulators* / simulation environments  \n* Consistency checking and inconsistency identification  \n  * Arbitrage / market makers in prediction markets?  \n* Using AI to accelerate development of non-AI epistemic tools:  \n  * ?\n\n## Lawrence\n\nThere are already key questions/strategic problems where the truth is murky and we seem to proceed on the basis of limited and biased views. There might well be many more of these in the future. A central example is US-China competition over AI. Some have vibes of \"China is so bad that it's worth risking an existential catastrophe to stop it from capturing a portion of the lightcone\". Others have vibes of \"Cooperation is feasible and desirable, and is our only path to a good outcome\". But how bad is China, really? How much \"realised K\" on this question do those who are making the relevant decisions actually have? Similarly, how feasible is cooperation? How valid is this frame of \"capturing the lightcone\"? One reason that we don't have a better hold on these questions is that they are frighteningly complicated and difficult  – I think we could benefit massively from throwing much more cognitive labour at them, and presenting the findings in a way that is hard to disagree with or for any individual to improve upon, other than at a normative/values level. Maybe AI can get us there.\n\n## Lukas Finnveden\n\nwhy is AI for epistemics important/helpful?\n\n* better epistemics generally very good for making all kinds of decisions  \n* I'm specifically concerned about AI potentially causing an intelligence explosion, and us needing better epistemics to handle that. Specifically:  \n  * I'm worried about AI-driven tech R\\&D leading to catastrophes. To help address this, I want AI capabilities to not only be applied to tech R\\&D, but also to be applied to \"reasoning well from limited evidence\" about a number of questions, including how to deal with risks.  \n  * I'm worried about misaligned AI — so I want techniques for eliciting AI capabilities on all kinds of questions, so that AIs don't have better capabilities than we can elicit from them. (Basically: I want good alignment techniques even on hard-to-evaluate questions.)  \n  * Seems great to me if AI-driven improvements in general knowledge and decision-making are more broadly distributed among humans. (I think this leads to better decision-making without significantly accelerating the dangers from new tech or misaligned AI. Also reduces concentration of power.)\n\n## Abram Demski\n\n\\- Why are we doing this? \\-- impact theory brainstorming exercize  \n\t\\- There's the general claim that epistemics has degraded horribly due to social media and the internet, at least in a lot of ways, although epistemics has also gotten better in some specific ways. It seems very plausible that AI is, and will continue to, make this much much worse (due to deepfakes and super-persuasive writing, to give two examples). We wish to reverse this trend, or at least slow the acceleration.  \n\t\\- The high-level reason why epistemics is important is because epistemics is how we make good decisions. So effectively, this is AI for decision-making. However, if we just set out to make tools for decision-making, I have a sense that this would be worse.  \n\t\t\\- One super important aspect of decision-making is that you have to orient towards the right problem. People do much better when they understand the problem before proposing solutions.  \n\t\t\t\\- I have a sense that producing a better shared understanding of the world JUST WOULD result in better things. The connection between epistemics and better decisionmaking is that strong.  \n\t\t\t\t\\- However, there's an element of coordination to this. You have to have common knowledge, not just knowledge. AI for coordination is yet another thing. AI for group epistemics is close to that, though, and again, I have a sense that making \"AI for coordination\" would produce worse effects on the world than \"AI for group epistemics\".  \n\t\t\\- What specific decisions are we hoping to influence?  \n\t\t\t\\- Decisions around existential risk seem super super important. If AI were generally better at epistemics (as a relative, comparative thing \\-- ie we differentially accelerate epistemics) then key decisionmakers (in the government and at AI companies) would see much more clearly what's coming. Everyone wants a better future, so a better future would have a better chance of happening if we could see and navigate.\n\nI don't think I got any further than the supporting documentation for this workshop in articulating reasons and stuff, so I predict that the LLM summary won't be better than what we already know, although I hope it is. \n\n## Brendan\n\nFor us to live together in a caring and just way, we must make decisions about what is important to us together. Is it okay to tell my neighbour to turn their music down? Do we spend money on roads or trains or libraries? Do we regulate AI development and, if so, how? Do we want to spend public money on PEPFAR, or other avenues for reducing global poverty? To make these decisions in a way that is inclusive and democratic, that fosters buy-in and trust, we must begin with a shared sense of how the world is. This shared sense must be grounded in the physical, economic, and social reality we live in.\n\nAs we live together over wider and wider contexts, maintaining a shared sense of the world becomes more difficult and subtle. These wider contexts are facilitated by new technologies, both physical (eg. global manufacturing bases and supply chains) and informational (eg. social media). New technologies, such as AI for epistemics, and help maintain epistemic coherence across these contexts.\n\nMoreover, such technologies allow new avenues for cooperation at larger scales of human involvement. This unlocks new potential for coherence in global politics, scientific discovery, social and technological innovation, and cultural expression, and ultimately the deep joy of being alive and human in the world. \n\n## Isabel: \n\nThe kinds of being right in the past that are most relevant for decision making aren't just about making straightforward predictions about what will happen, but are more nuanced about what kinds of actions one should take to cause those things to happen, and these seem like somewhat different skills. These are harder to evaluate (e.g. because they're counterfactual: either we listened to person a and then we have to evaluate whether things went better or worse than if we didn't, but hard to say what the counterfactual would have been; or we didn't listen to person a and then we have to guess what things would have been like if we did) \n\nA bit adjacent to Neel Nanda's [recent piece](https://www.lesswrong.com/posts/P5zWiPF5cPJZSkiAK/good-research-takes-are-not-sufficient-for-good-strategic) on how \"good research takes aren't sufficient for good strategic takes\" \\- \"good forecasting takes aren't sufficient for good strategic takes\", but maybe some of this is maybe resolvable on the forecasting tech side \\- can we get better at creating and evaluating results from fuzzier questions? Can AI help? (Also can AI help with the whole problem, but this feels small and concrete enough to maybe get some traction on.) Also, some risk of actors building up good track records in order to be listened to, but then choosing to burn that track record to get power or things like that, something something trust \\- a concern for both human actors and e.g. scheming AIs \\- what things look good until they abruptly aren't? \n\nOn a different note, also interested in takes on what to do about and how to engage with actors who have epistemic processes that don't seem to be truth seeking in a comprehensible-to-me way. \n\n## Adam Marblestone:\n\nWhat do we get out of AI for epistemics?\n\nTrust, is one thing. Our current society seems to be fraught with distrust and motivated reasoning. Can we get a more objectively trustable source of truth? Not only accuracy but with a provenance that gives it an ability to be trusted by humans who are otherwise adversarial. Trust is needed to coordinate. Truthfulness is maybe somewhat necessary but seemingly not sufficient for a source being trusted. As a practical matter, on hot political topics I've been going to OpenAI Deep Research.\n\nBut at a certain level I wonder if AI for epistemics and some form of beneficial AGI could be basically the same thing. I asked Eric Drexler what he wanted for d/acc and he said something that sounds a bit like that…\n\n\"  \nHere's my unsurprising take on directed research for \"defensive acceleration\":  \nDevelopment and steering of epistemic tooling, aka [Large Knowledge Models](https://aiprospects.substack.com/p/large-knowledge-models), offers the highest leverage.\n\nPotential actions:  \n  • Identify and influence entities that could develop and scale necessary foundations.  \n  • Leverage the growing ease of development to take pre-scaling steps.  \n  • Within the scope of \"foundations\", accelerate productive directions for development.  \n  • To support science, ensure grounding of content in cited sources.  \n  • Pursue or promote assembly of appropriate knowledge inputs (beyond what is now used for training data).  \n  • Identify, promote, or implement epistemic tools supported by LKM resources.  \n  • Other kinds of exploration, promotion, preparation, and implementation.  \n  • Ensure that my advice is thoroughly understood.\n\nLKMs can accelerate pretty much everything that one might want, and perhaps tip the balance of applications toward *defensive* acceleration by promoting understanding of the *what and why* of defensive applications.\n\nAddendum:\n\nIn the email I sent a few minutes ago, the basic proposal becomes stronger if one reads \"LKM\" as something like \"large knowledge model functionality\", regardless of implementation.\n\nThe knowledge-model concept (whatever one prefers to call it) should not be tied to my specific technical suggestions. What I've said can be framed as something like \"a discussion of the value, feasibility, and potential scope of a rich epistemic resource\". Bundles of embeddings, etc., are at a level of specificity that may be distracting in the broader context of the meeting.  \n\"\n\nSo that's a strategy assistant AI – which helps humans at large see ways they could act and coordinate especially around big technological or geopolitical strategy issues I guess – but also a certain embodiment of an AGI oracle type system. Maybe the idea then is, how to make a AGI that helps with navigating civilizational strategy, and what is the right form of or interface to that to make it epistemically most useful. \n\nI'd like to also see democratic mechanisms that are simply maximally utilizing the tech we have. \n\n## Elizabeth\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Allowing people to be better informed about experimental medical stuff for themselves.   \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Bargaining? Even just salary negotiations feels like the results are often more the consequence of \"human bargaining skill\" and not the pareto optimal result.    \n  \n\n\n## Saul\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * making better decisions  \n  * in service of that: more useful forecasts, particularly understanding better which things are most relevant to forecast  \n    * in service of that: better group decisionmaking (e.g. unblocking things from vetopower type  \n    * in service of that: more aptly/calibratedly handling waves of information  \n    * In service of that: societal- and individual-scale asymmetric truth-tracking  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * which groups of people have political power  \n  * who's building powerful new tech  \n  * who's regulating powerful new tech  \n  * (etc, other \"decisions\" along these lines — some might not end up getting shaped like decisions, though)  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * better decisions across the board"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Audience Assumptions Not Always Clear",
          "description": "The document sometimes assumes a high level of prior knowledge about AI, epistemics, and related debates. For broader accessibility, consider adding a brief introduction or glossary for non-expert readers.",
          "importance": 75,
          "grade": 40,
          "isValid": true,
          "highlight": {
            "startOffset": 239,
            "endOffset": 396,
            "quotedText": "Epistemics isn't a goal in-an-of itself; figuring out what combination of things are possible, achievable, etc.  \n* Why do we care about (AI for) epistemics?"
          }
        },
        {
          "title": "Effective Use of Nested Bullets for Hierarchy",
          "description": "Some contributors use nested bullet points to indicate subpoints and logical relationships, which aids in information hierarchy and organization. This practice helps readers parse complex arguments.",
          "importance": 60,
          "grade": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 8911,
            "endOffset": 9503,
            "quotedText": "* **What are use cases for AI for Epistemics that you most care about?**  \n  * AI policy, broadly construed, must go better  \n    * Avoid gradual disempowerment  \n    * Thoughtful regulation  \n  * Organizational planning & decision-making is weak  \n    * Given an informal long-term goal like improve decision-making for strategic users in government & large orgs, make a plan for a 20-people org that helps them accomplish the goal  \n    * Mapping dependencies, uncertainties, and potential futures  \n    * Understand complex domains with non-trivial feedback loops  \n      * Economic shifts"
          }
        },
        {
          "title": "Occasional Overuse of Parentheticals",
          "description": "Frequent use of parentheses and asides can interrupt the flow of reading and make sentences harder to parse. Where possible, integrate these thoughts into the main text or use footnotes.",
          "importance": 50,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 3871,
            "endOffset": 4224,
            "quotedText": "This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place."
          }
        },
        {
          "title": "Strong Use of Signposting in Some Sections",
          "description": "Explicit signposting (e.g., 'What are use cases...', 'What decisions may go badly...') helps orient the reader and clarifies the purpose of each section. This is an effective practice that could be extended throughout the document.",
          "importance": 65,
          "grade": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 398,
            "endOffset": 711,
            "quotedText": "Writing prompts from Owen:\n\n* What are use cases for AI for Epistemics that you most care about?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\""
          }
        },
        {
          "title": "Inconsistent Use of First-Person Perspective",
          "description": "Some contributors use 'I', others use 'we', and some avoid personal pronouns altogether. This inconsistency can make the document feel disjointed. Consider standardizing the perspective or clarifying when individual vs. collective views are being presented.",
          "importance": 45,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 2874,
            "endOffset": 3601,
            "quotedText": "I care a lot about humanity and AI, including **super-late-stage AI, getting decision theory** right.  \n  * Consequently but also separately, this relates to the general question of what to do about areas where there is not necessarily a ground truth but that are nonetheless very important to get right, cf**. [metaphilosophy](https://www.alignmentforum.org/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy)** (but also a bunch of other things.)  \n  * This also feels very similar to making **AIs helpful for AI safety research, cf. [automating alignment research](https://joecarlsmith.com/2025/03/14/ai-for-ai-safety#2-what-is-ai-for-ai-safety).**\n\n  → This is more about raising the **top of the epistemic landscape**"
          }
        },
        {
          "title": "Effective Thematic Grouping in Some Responses",
          "description": "Some contributors (e.g., Julian) explicitly group their points into themes like 'Raising the ceiling' and 'Raising the floor', which aids comprehension and highlights key distinctions. This is a best practice that could be adopted more widely.",
          "importance": 60,
          "grade": 85,
          "isValid": false,
          "highlight": {
            "startOffset": 428,
            "endOffset": 22584,
            "quotedText": "What are use cases for AI for Epistemics that you most care about?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nTemplate:\n\n## Name: \n\nEntry\n\n## Dan Schwarz:\n\n* What are use cases for AI for Epistemics that you most care about?\n\nGoing from \"Realized\" to \"Accessible\" knowledge, e.g. getting good information using the web. Agentic web agents like Deep Research.\n\n* What decisions may go badly by default, but could go better if we have the right tech?\n\nMany billions of small decisions.\n\n* What do the later stages of the path to impact look like?\n\nSuperhuman forecasting at scale.\n\n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nReasonable judgment on tricky questions that's too cheap to meter, better than if you asked a respected colleague to spend many minutes on it.\n\n## Aviv\n\n* Better government governance  \n* Better democratic functioning  \n* Reduced dangerous concentration of power  \n    \n* Healthy polarization / pluralism  \n    \n* More peace, less likelihood of war  \n* Better geopolitics  \n* Better economic incentives  \n    \n* Better AI org governance  \n* Better AI coordination (re. distribution, escalation, etc.)  \n* Better compute allocation (toward what matters for good outcomes)  \n    \n* Better allocation of resources overall  \n* Better lives for people  \n* More effective organizations, getting things done with fewer resources, more focused on the things that benefit everyone  \n    \n* Identification of common ground directions / equilibria / game theoretic strategies that create/incentivize new positive gradients (and not surfacing the bad versions of those…)  \n    \n* Understanding our own goals and values\n\n*Note how it is the intersection of better epistemics / world models + the right power structures / incentives needed for all of these.*\n\n## Ben Goldhaber:\n\n* Forecasting how I should live my life in a period of intense technological change  \n* Guiding decision makers to avoid war with china/ai/everyone  \n* Providing the background context for me to sensemake way more sanely.  \n* Pushing forward the extant knowledge of AI safety agendas  \n* Planning for multi-stage projects that could achieve big impact  \n* Roadmapping to build larger more complex things\n\n## Chi\n\nThere are a few different things I care about:\n\n* I care a lot about humanity and AI, including **super-late-stage AI, getting decision theory** right.  \n  * Consequently but also separately, this relates to the general question of what to do about areas where there is not necessarily a ground truth but that are nonetheless very important to get right, cf**. [metaphilosophy](https://www.alignmentforum.org/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy)** (but also a bunch of other things.)  \n  * This also feels very similar to making **AIs helpful for AI safety research, cf. [automating alignment research](https://joecarlsmith.com/2025/03/14/ai-for-ai-safety#2-what-is-ai-for-ai-safety).**\n\n  → This is more about raising the **top of the epistemic landscape**\n\n* Other things I care about:  \n  * Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place.  \n  * Vague sense that better **societal epistemics** would help with increasing the **sanity of conversations about AI safety, polarisation around the issue etc.**  \n    * This is broadly about **getting intent alignment** in the first place.\n\n→ These are all more about raising the **bottom of the epistemic landscape**\n\n## Deger Turan:\n\n* Social trust erosion, collective coordination mechanisms are built on a weak assumption  \n* Capacity to entertain truthfulness when it is not instrumental to one's incentive gradient & differing norms upon 'truth will make my sense of security weaker'   \n* Creating contexts where world model divergence can be agreed by both parties enabling a safe disagreement  \n* Improving model reasoning / safer AI by reducing slop, by creating accountability mechanisms through consistency checks & other specifications for healthy epistemics  \n* Decoupling virality with magnitude while living in a short attention span world, curation   \n* I have some confusion re: raising the floor when not all parties are necessarily operating in good faith. I think I need some help think through the ways in which epistemics in the context of incentive misalignment can be used or have defensibility. \n\n## Nathan Young: \n\nWhat would we have got:\n\n* Biden not running, perhaps an open dem primary  \n* No FTX crisis  \n* Less zero sum OpenAI board drama  \n* Many smaller, better choices \n\nWhat decisions might go badly:\n\n* Trump tariffs/ global trade  \n* Taiwan/ US-China conflict  \n* US-China competition  \n* US authoritarianism  \n* AI regulation  \n* EU-China realignment  \n* Automated robots  \n* OpenAI strategy  \n* Anthropic strategy  \n* Google strategy  \n* xAI strategy  \n* AI workforce automation  \n* China strategy  \n* Russia strategy  \n* Moskovitz strategy  \n* Must strategy  \n* Bezos Strategy  \n* Zuckerberg strategy  \n* Tallinn strategy  \n* Altman strategy  \n* Drone warfare  \n* Climate change destabilisation  \n* Democratic trust\n\n## Lincoln Quirk:\n\nWhy do we care about (AI for) epistemics?  \n\\- making the public better informed:  \n\t\\- better able to deal with and process the wealth of information  \n\t\\- better able to express their own interests in government (via voting)  \n\t\\- helping cut through social media distraction / intentional misinformation  \n\\- getting governments to implement better policy  \n\t\\- coming up with good policy ideas  \n\t\\- objectively doing cost-benefit analysis  \n\t\\- objectively explaining tradeoffs (vs having to rely on biased humans to explain them)  \n\t\\- flagging bad ideas as bad  \n\t\\- convincing policymakers with compelling writeups  \n\\- success and flourishing for individuals and orgs using these tools  \n\t\\- via helping them think through decisions  \n\t\\- via clearheaded analysis of trade-offs and cost/benefits  \n\t\\- flagging bad ideas as bad\n\n## Jay\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * How can we build something like the positive version of Community Notes: which statements/posts are agreed on by people who often disagree?  \n  * How can we better understand/summarize what others think, e.g. on social media platforms? E.g. if you see a post recommended to you, is this because it's the most extreme take that went viral and isn't representative, or is this a commonly held view? Between different viewpoints, what are the concrete points of actual disagreement (on facts or values) and what are the things widely agreed upon? Let's build viewpoint/debate maps or a similar sensemaking tool that gets adopted/understood by the masses/raises the knowledge floor (in my experience, things like pol.is are hard for regular users to grok)  \n  * How can we improve Community Notes (or more general knowledge) creation with AI assistants? An AI agent proposing community notes and humans still checking/rating them?  \n  * How can we have something like Community Notes that applies to claims or statements about the future, or which are not concretely knowable?  \n  * \n\n## Ben Wilson (Metaculus)\n\nOverall I believe the most important value of epistemics (and AI epistemics) in general is informing what actions allow us to achieve our goals (i.e. make the world better). If we had perfect AI epistemics I would be excited by the ability for us to take a goal, and get a list of possible actions we could take and the forecasted outcomes of each of those actions (plus background information and forecasts for the 'current lay of the land' for context). This can then inform, not just the risk of AI doing good/bad things (as with current forecasting), but what we can do to raise the chances of 'great' AI outcomes, and lower the changes of 'worse' AI outcomes\n\n## Andreas\n\n* **What are use cases for AI for Epistemics that you most care about?**  \n  * AI policy, broadly construed, must go better  \n    * Avoid gradual disempowerment  \n    * Thoughtful regulation  \n  * Organizational planning & decision-making is weak  \n    * Given an informal long-term goal like improve decision-making for strategic users in government & large orgs, make a plan for a 20-people org that helps them accomplish the goal  \n    * Mapping dependencies, uncertainties, and potential futures  \n    * Understand complex domains with non-trivial feedback loops  \n      * Economic shifts  \n* **What decisions may go badly by default, but could go better if we have the right tech?**  \n  * Resource allocation within orgs  \n    * Investment  \n    * Grant making  \n    * Government budgets  \n  * Policy  \n    * How should AI be deployed in the world?  \n* **What do the later stages of the path to impact look like?**  \n  * AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning  \n  * The technology helps organizations maintain coherence and make sound judgments even as the pace of change accelerates  \n  * Assumption: People are generally well-intentioned \\- no one wants catastrophe \\- and as a result of better tooling things go better in the world overall\n\n## Owen optimistic brainstorm:\n\nWorld peace without world government — sufficiently good tech to help countries understand each others' interests, find mutually desirable bargains, and make sufficient credible commitments, could help us get up towards the Pareto frontier where we avoid (virtually) all costly conflicts\n\nGoing slowly and carefully on AI development — rather than having AI progress accelerate at increasingly breakneck speed, helping actors to understand the basic situation and the desirability of different options could be crucial for facilitating meaningful and robust agreements which restrict AI development to the stuff which we have good reasons for being confident are safe // otherwise beneficial.\n\nBettering democracy — helping people to access key information about what decisions their country might make would actually mean for longer term implications, and having this in turn feed into incentives for decision-makers that better align them with the common good\n\nBuilding safe / aligned AI — better tools for anticipating when systems are actually robustly safe, vs when stronger cases for expectation of safety seem required\n\n## Josh Rosenberg (FRI)\n\nI'm massively confused about important problems, e.g. how to orient to AI risk and what to do about it. Some reasonable people think there's a high probability we effectively lose everything we care about in 2027\\. Some reasonable people think it'll be decades till we have \"AGI\" and it'd be good if it arrived sooner. Which actions are good to take in each of these worlds is very different, and I'd like to figure out what world we're in.\n\nThis also applies to many other problem areas.\n\nE.g., Trump's effect on US democracy. How much of a crisis is it? What are the most effective actions to take in response to ensure that the country and communities we care about are as well off as they can be? How can we preserve high-quality institutions to lead the world in positive directions?\n\nOr, on the positive vision side: the \"Abundance\" agenda seems promising. Is it actually a good idea? What do we do about it?\n\nAI for Epistemics could help us get a lot more clarity on how to think about these problems and what actions to take. I'd like much better information and forecasts on the likely effects of Trump, AI timelines, and the pro's and con's of different Abundance policies, for example. If I had a bunch of time and good resources to dig into these problems, I could get a much better sense for myself. I'm hoping AI will help all of us do this sort of thing collectively.\n\n## Katja Grace\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Put lots of cognitive effort into very clear and good and compelling case for AI risk  \n  * Automate talking to people about AI risk compellingly, possibly using above case as input  \n  * Good game theoretic thinking re arms races, coordination, what is possible. I.e. People are making dumb assumptions about things, e.g. \"can't slow down tech\". Could automate noting errors. So e.g. when someone says that on Twitter, can highlight it and send in response. \\-\\> good thinking via better weapons for debate (similar to Nathan project)  \n  * Specific idea: very quantified and competitive debate site, that can be cited re which views are winning, where AI does a lot of the cognitive labor in places no people are paying attention, but as a part of it gets cruxy, people pay attention and vote/bet  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Poor decision making around AI: e.g. 'racing' because of vague heuristics instead of ever doing calc on costs and benefits given specific risks  \n  * Failure to coordinate on AI: building AI systems that are dangerous, due to expecting others to do so\n\n## Joel Lehman\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Raising the bottom (i.e. helping improve societal epistemics) – in particular that in theory political decisions (at least political decision-makers) are somewhat downstream of societal epistemics  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Having weird political leaders and awful public epistemics at critical junctures seems to unnecessarily spike catastrophic/existential risk probabilities. In theory, helping uplevel or at least maintain current public epistemic health (especially as it relates to civic sensemaking) might help influence the quality of decision-makers and concrete decisions made at high-stakes moments  \n  * The shape of the tech is not clear, or probably requires multiple flavors of interventions. E.g. if LLM assistants by default help cultivate civic virtues and on the margin support critical thinking or whatever underlying skills and knowledge help a person up-level their epistemic habits / health (or at least if LLM assistants don't contribute to atrophy/harm); or somehow convincing social media networks to care about this (e.g. community notes but things that go beyond); similarly somehow help w/ important social goods like 'societal trust' and 'trust in your neighbors'  \n* What do the later stages of the path to impact look like?  \n  * \\[didn't get to this\\]  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * Healthier democratic society, better agreement on consensus reality, higher trust in society, better politicians, better concrete decisions in high-stakes moments \n\n## Austin Chen\n\n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * **Evaluator**: More agreement (or understanding of disagreement) of what orgs & people are doing good work (eg for making grants in AI safety)  \n  * **Oracle**: Instant trustworthy forecasts on any question you ask  \n  * **Planner**: Guidance on \"what are the 3 most important things I should do this day/week/month\"  \n  * **Representative**: Simulate the responses of other parties (people, orgs/corps, governments, LLM agents), for discussions, contracts, trade\n\n## Janna\n\n* Why do we care about (AI for) epistemics?  \n  * Noise to signal ratio could be much higher  \n  * AI (mis)information churned out at the price of almost zero  \n* What are use cases for AI for Epistemics that you most care about?  \n  * Solving prediction markets--is there a way to scale up the bots, even if they are wrong and lossy, to improve prediction market accuracy?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * US-China relations  \n  * Trump tariffs maybe  \n  * Immigration rules in the US  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * More accurate predictions to…  \n    * Reduce business uncertainty  \n    * Make it easier to evaluate life decisions  \n  * Forestalling bad decisions made on imperfect information\n\n## Owain\n\n1. Maybe having humans understanding the world better is good in itself. Lots of people read popular science for this reason. Maybe it's good for AIs as well  \n2. Very broadly, it seems that better world understanding and decision making enables people to make their lives better in myriad ways. (To substantiate this, one could try to separate out the effects of better world understanding from the effects of better realized technology. E.g. Looking at capital-poor countries that can still benefit from scientific knowledge, economics, etc.)  \n3. More specifically, if AI progress is fast, there will be lots of important decisions to be made in quick calendar time. Exploiting AI to improve those decisions seems valuable. Current decision making institutions do not seem well set up to adapt really quickly. (A specific concern is dangers from AI takeover).   \n4. Why now? Why this group of people? We haven't really had tech that could do most epistemic tasks until recently. Now we have AI that can do an increasing proportion of the sub-tasks of epistemic processes. This will allow us to delegate epistemic tasks to AI.  \n   1. It's likely that people do this delegation, even if it's somewhat misguided and potentially bad in the long-term. Can we make this go well?  \n   2. This potentially massively expands our epistemic capacities. How can we use this well?  \n   3. Epistemic tech/institutions/advances can be hard to fund via for-profit companies. This is in part due to public goods issues, and issues around markets for ideas.\n\n## Rafe\n\nAs ever, I hope this will stop advanced artificial intelligence destroying the world, killing all its people \\&c.\n\nSo I think there are a few paths for impact here. I think there is a world where we can make the most productive thinkers about how to do things go further and better; I'm not so excited about a single-player version of this, as I think there's a pretty narrow window when top-level centaur mode works out\\[1\\]\n\nEveryone is already thinking about AI, and, I suspect, this will only get more acute. I think it's pretty hard for people to confront this and think about it carefully and well. It's scary\\! And there are a bunch of entrenched interests and memeplexes that preserve their status better (myopically) if people only have shallow slogans to think about AI with.\n\nGiven that, I suspect that the global sensemaking machine isn't up for the task of thinking about AI well. I hope we can help the global sensemaking machine upgrade itself. I have some hope that we as a society can come up with good ideas for dealing with AI, and also come to see how they make sense, so we can choose them in a robust way.\n\nOne other path to impact: I think a society that understands things better, including itself and its impacts, is one that is more robust in a bunch of ways. A more epistemically-grounded society is also probably a bit richer, and I think a bigger pie creates more slack for arranging things in sanity-promoting ways. I have more hope in a robust society navigating this stuff well.\n\n\\[1\\]:  Yeah, maybe you can do something with shaving off subskills, but I don't think that's really that doable.\n\n## Dave\n\n* With my consequentialist hat, better epistemics being worth it has to route through improved decisions somewhere down the line. The decisions I particularly care about are those by powerful people wrt AI development and implementation and the effects this has on how the future goes  \n  * But I do feel a bit icky when I have this hat on. There seems to be something very intuitively appealing about (a) pursuing better epistemics for its own sake and (b) democratising epistemic tools for the use and benefit of everyone. I think this basically caches out in more 'raising the floor' type work  \n  * I don't feel like I have a very good way of resolving this tension yet, but am excited to see things pushing on both frontiers  \n* I think forecasting has the potential to be a pretty powerful tool and there's currently a massive undersupply of good forecasts. I'm excited for us to use the wave of increased AI labor to make better quantitative forecasts to inform key decisions. I probably care more about quantitative forecasts than most decision makers and am open to being persuaded that the other, fuzzier parts of epistemics are more valuable to work on. \n\n## Julian\n\nWhat are use cases for AI for Epistemics that you most care about?  \n**Raising the ceiling:** can we use AI to organize information and credibly resolve the most challenging and important questions on which experts still seem to disagree? For example:\n\n* COVID origins  \n* Urban planning and public policy  \n* Lots of advancements in basic science: routing around current disagreements and policy challenges with slam-dunk science might be a pretty strong way forward.\n\n**Raising the floor:** can we use AI to build extremely legibly trustworthy/reliable epistemic infrastructure that actually makes its way into people's lives?\n\n* Making truth more palatable (in a way which doesn't translate into making falsehoods more palatable)  \n* Collective epistemics: connecting people to the others with whom they would have the most productive/enlightening conversations"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Occasional Lack of Specificity in Recommendations",
          "description": "Some sections make broad claims (e.g., 'better decisions across the board') without specifying mechanisms or examples. Adding concrete details would increase precision and persuasive power.",
          "importance": 55,
          "grade": 50,
          "isValid": false,
          "highlight": {
            "startOffset": 588,
            "endOffset": 35664,
            "quotedText": "* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nTemplate:\n\n## Name: \n\nEntry\n\n## Dan Schwarz:\n\n* What are use cases for AI for Epistemics that you most care about?\n\nGoing from \"Realized\" to \"Accessible\" knowledge, e.g. getting good information using the web. Agentic web agents like Deep Research.\n\n* What decisions may go badly by default, but could go better if we have the right tech?\n\nMany billions of small decisions.\n\n* What do the later stages of the path to impact look like?\n\nSuperhuman forecasting at scale.\n\n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nReasonable judgment on tricky questions that's too cheap to meter, better than if you asked a respected colleague to spend many minutes on it.\n\n## Aviv\n\n* Better government governance  \n* Better democratic functioning  \n* Reduced dangerous concentration of power  \n    \n* Healthy polarization / pluralism  \n    \n* More peace, less likelihood of war  \n* Better geopolitics  \n* Better economic incentives  \n    \n* Better AI org governance  \n* Better AI coordination (re. distribution, escalation, etc.)  \n* Better compute allocation (toward what matters for good outcomes)  \n    \n* Better allocation of resources overall  \n* Better lives for people  \n* More effective organizations, getting things done with fewer resources, more focused on the things that benefit everyone  \n    \n* Identification of common ground directions / equilibria / game theoretic strategies that create/incentivize new positive gradients (and not surfacing the bad versions of those…)  \n    \n* Understanding our own goals and values\n\n*Note how it is the intersection of better epistemics / world models + the right power structures / incentives needed for all of these.*\n\n## Ben Goldhaber:\n\n* Forecasting how I should live my life in a period of intense technological change  \n* Guiding decision makers to avoid war with china/ai/everyone  \n* Providing the background context for me to sensemake way more sanely.  \n* Pushing forward the extant knowledge of AI safety agendas  \n* Planning for multi-stage projects that could achieve big impact  \n* Roadmapping to build larger more complex things\n\n## Chi\n\nThere are a few different things I care about:\n\n* I care a lot about humanity and AI, including **super-late-stage AI, getting decision theory** right.  \n  * Consequently but also separately, this relates to the general question of what to do about areas where there is not necessarily a ground truth but that are nonetheless very important to get right, cf**. [metaphilosophy](https://www.alignmentforum.org/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy)** (but also a bunch of other things.)  \n  * This also feels very similar to making **AIs helpful for AI safety research, cf. [automating alignment research](https://joecarlsmith.com/2025/03/14/ai-for-ai-safety#2-what-is-ai-for-ai-safety).**\n\n  → This is more about raising the **top of the epistemic landscape**\n\n* Other things I care about:  \n  * Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place.  \n  * Vague sense that better **societal epistemics** would help with increasing the **sanity of conversations about AI safety, polarisation around the issue etc.**  \n    * This is broadly about **getting intent alignment** in the first place.\n\n→ These are all more about raising the **bottom of the epistemic landscape**\n\n## Deger Turan:\n\n* Social trust erosion, collective coordination mechanisms are built on a weak assumption  \n* Capacity to entertain truthfulness when it is not instrumental to one's incentive gradient & differing norms upon 'truth will make my sense of security weaker'   \n* Creating contexts where world model divergence can be agreed by both parties enabling a safe disagreement  \n* Improving model reasoning / safer AI by reducing slop, by creating accountability mechanisms through consistency checks & other specifications for healthy epistemics  \n* Decoupling virality with magnitude while living in a short attention span world, curation   \n* I have some confusion re: raising the floor when not all parties are necessarily operating in good faith. I think I need some help think through the ways in which epistemics in the context of incentive misalignment can be used or have defensibility. \n\n## Nathan Young: \n\nWhat would we have got:\n\n* Biden not running, perhaps an open dem primary  \n* No FTX crisis  \n* Less zero sum OpenAI board drama  \n* Many smaller, better choices \n\nWhat decisions might go badly:\n\n* Trump tariffs/ global trade  \n* Taiwan/ US-China conflict  \n* US-China competition  \n* US authoritarianism  \n* AI regulation  \n* EU-China realignment  \n* Automated robots  \n* OpenAI strategy  \n* Anthropic strategy  \n* Google strategy  \n* xAI strategy  \n* AI workforce automation  \n* China strategy  \n* Russia strategy  \n* Moskovitz strategy  \n* Must strategy  \n* Bezos Strategy  \n* Zuckerberg strategy  \n* Tallinn strategy  \n* Altman strategy  \n* Drone warfare  \n* Climate change destabilisation  \n* Democratic trust\n\n## Lincoln Quirk:\n\nWhy do we care about (AI for) epistemics?  \n\\- making the public better informed:  \n\t\\- better able to deal with and process the wealth of information  \n\t\\- better able to express their own interests in government (via voting)  \n\t\\- helping cut through social media distraction / intentional misinformation  \n\\- getting governments to implement better policy  \n\t\\- coming up with good policy ideas  \n\t\\- objectively doing cost-benefit analysis  \n\t\\- objectively explaining tradeoffs (vs having to rely on biased humans to explain them)  \n\t\\- flagging bad ideas as bad  \n\t\\- convincing policymakers with compelling writeups  \n\\- success and flourishing for individuals and orgs using these tools  \n\t\\- via helping them think through decisions  \n\t\\- via clearheaded analysis of trade-offs and cost/benefits  \n\t\\- flagging bad ideas as bad\n\n## Jay\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * How can we build something like the positive version of Community Notes: which statements/posts are agreed on by people who often disagree?  \n  * How can we better understand/summarize what others think, e.g. on social media platforms? E.g. if you see a post recommended to you, is this because it's the most extreme take that went viral and isn't representative, or is this a commonly held view? Between different viewpoints, what are the concrete points of actual disagreement (on facts or values) and what are the things widely agreed upon? Let's build viewpoint/debate maps or a similar sensemaking tool that gets adopted/understood by the masses/raises the knowledge floor (in my experience, things like pol.is are hard for regular users to grok)  \n  * How can we improve Community Notes (or more general knowledge) creation with AI assistants? An AI agent proposing community notes and humans still checking/rating them?  \n  * How can we have something like Community Notes that applies to claims or statements about the future, or which are not concretely knowable?  \n  * \n\n## Ben Wilson (Metaculus)\n\nOverall I believe the most important value of epistemics (and AI epistemics) in general is informing what actions allow us to achieve our goals (i.e. make the world better). If we had perfect AI epistemics I would be excited by the ability for us to take a goal, and get a list of possible actions we could take and the forecasted outcomes of each of those actions (plus background information and forecasts for the 'current lay of the land' for context). This can then inform, not just the risk of AI doing good/bad things (as with current forecasting), but what we can do to raise the chances of 'great' AI outcomes, and lower the changes of 'worse' AI outcomes\n\n## Andreas\n\n* **What are use cases for AI for Epistemics that you most care about?**  \n  * AI policy, broadly construed, must go better  \n    * Avoid gradual disempowerment  \n    * Thoughtful regulation  \n  * Organizational planning & decision-making is weak  \n    * Given an informal long-term goal like improve decision-making for strategic users in government & large orgs, make a plan for a 20-people org that helps them accomplish the goal  \n    * Mapping dependencies, uncertainties, and potential futures  \n    * Understand complex domains with non-trivial feedback loops  \n      * Economic shifts  \n* **What decisions may go badly by default, but could go better if we have the right tech?**  \n  * Resource allocation within orgs  \n    * Investment  \n    * Grant making  \n    * Government budgets  \n  * Policy  \n    * How should AI be deployed in the world?  \n* **What do the later stages of the path to impact look like?**  \n  * AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning  \n  * The technology helps organizations maintain coherence and make sound judgments even as the pace of change accelerates  \n  * Assumption: People are generally well-intentioned \\- no one wants catastrophe \\- and as a result of better tooling things go better in the world overall\n\n## Owen optimistic brainstorm:\n\nWorld peace without world government — sufficiently good tech to help countries understand each others' interests, find mutually desirable bargains, and make sufficient credible commitments, could help us get up towards the Pareto frontier where we avoid (virtually) all costly conflicts\n\nGoing slowly and carefully on AI development — rather than having AI progress accelerate at increasingly breakneck speed, helping actors to understand the basic situation and the desirability of different options could be crucial for facilitating meaningful and robust agreements which restrict AI development to the stuff which we have good reasons for being confident are safe // otherwise beneficial.\n\nBettering democracy — helping people to access key information about what decisions their country might make would actually mean for longer term implications, and having this in turn feed into incentives for decision-makers that better align them with the common good\n\nBuilding safe / aligned AI — better tools for anticipating when systems are actually robustly safe, vs when stronger cases for expectation of safety seem required\n\n## Josh Rosenberg (FRI)\n\nI'm massively confused about important problems, e.g. how to orient to AI risk and what to do about it. Some reasonable people think there's a high probability we effectively lose everything we care about in 2027\\. Some reasonable people think it'll be decades till we have \"AGI\" and it'd be good if it arrived sooner. Which actions are good to take in each of these worlds is very different, and I'd like to figure out what world we're in.\n\nThis also applies to many other problem areas.\n\nE.g., Trump's effect on US democracy. How much of a crisis is it? What are the most effective actions to take in response to ensure that the country and communities we care about are as well off as they can be? How can we preserve high-quality institutions to lead the world in positive directions?\n\nOr, on the positive vision side: the \"Abundance\" agenda seems promising. Is it actually a good idea? What do we do about it?\n\nAI for Epistemics could help us get a lot more clarity on how to think about these problems and what actions to take. I'd like much better information and forecasts on the likely effects of Trump, AI timelines, and the pro's and con's of different Abundance policies, for example. If I had a bunch of time and good resources to dig into these problems, I could get a much better sense for myself. I'm hoping AI will help all of us do this sort of thing collectively.\n\n## Katja Grace\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Put lots of cognitive effort into very clear and good and compelling case for AI risk  \n  * Automate talking to people about AI risk compellingly, possibly using above case as input  \n  * Good game theoretic thinking re arms races, coordination, what is possible. I.e. People are making dumb assumptions about things, e.g. \"can't slow down tech\". Could automate noting errors. So e.g. when someone says that on Twitter, can highlight it and send in response. \\-\\> good thinking via better weapons for debate (similar to Nathan project)  \n  * Specific idea: very quantified and competitive debate site, that can be cited re which views are winning, where AI does a lot of the cognitive labor in places no people are paying attention, but as a part of it gets cruxy, people pay attention and vote/bet  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Poor decision making around AI: e.g. 'racing' because of vague heuristics instead of ever doing calc on costs and benefits given specific risks  \n  * Failure to coordinate on AI: building AI systems that are dangerous, due to expecting others to do so\n\n## Joel Lehman\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Raising the bottom (i.e. helping improve societal epistemics) – in particular that in theory political decisions (at least political decision-makers) are somewhat downstream of societal epistemics  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Having weird political leaders and awful public epistemics at critical junctures seems to unnecessarily spike catastrophic/existential risk probabilities. In theory, helping uplevel or at least maintain current public epistemic health (especially as it relates to civic sensemaking) might help influence the quality of decision-makers and concrete decisions made at high-stakes moments  \n  * The shape of the tech is not clear, or probably requires multiple flavors of interventions. E.g. if LLM assistants by default help cultivate civic virtues and on the margin support critical thinking or whatever underlying skills and knowledge help a person up-level their epistemic habits / health (or at least if LLM assistants don't contribute to atrophy/harm); or somehow convincing social media networks to care about this (e.g. community notes but things that go beyond); similarly somehow help w/ important social goods like 'societal trust' and 'trust in your neighbors'  \n* What do the later stages of the path to impact look like?  \n  * \\[didn't get to this\\]  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * Healthier democratic society, better agreement on consensus reality, higher trust in society, better politicians, better concrete decisions in high-stakes moments \n\n## Austin Chen\n\n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * **Evaluator**: More agreement (or understanding of disagreement) of what orgs & people are doing good work (eg for making grants in AI safety)  \n  * **Oracle**: Instant trustworthy forecasts on any question you ask  \n  * **Planner**: Guidance on \"what are the 3 most important things I should do this day/week/month\"  \n  * **Representative**: Simulate the responses of other parties (people, orgs/corps, governments, LLM agents), for discussions, contracts, trade\n\n## Janna\n\n* Why do we care about (AI for) epistemics?  \n  * Noise to signal ratio could be much higher  \n  * AI (mis)information churned out at the price of almost zero  \n* What are use cases for AI for Epistemics that you most care about?  \n  * Solving prediction markets--is there a way to scale up the bots, even if they are wrong and lossy, to improve prediction market accuracy?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * US-China relations  \n  * Trump tariffs maybe  \n  * Immigration rules in the US  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * More accurate predictions to…  \n    * Reduce business uncertainty  \n    * Make it easier to evaluate life decisions  \n  * Forestalling bad decisions made on imperfect information\n\n## Owain\n\n1. Maybe having humans understanding the world better is good in itself. Lots of people read popular science for this reason. Maybe it's good for AIs as well  \n2. Very broadly, it seems that better world understanding and decision making enables people to make their lives better in myriad ways. (To substantiate this, one could try to separate out the effects of better world understanding from the effects of better realized technology. E.g. Looking at capital-poor countries that can still benefit from scientific knowledge, economics, etc.)  \n3. More specifically, if AI progress is fast, there will be lots of important decisions to be made in quick calendar time. Exploiting AI to improve those decisions seems valuable. Current decision making institutions do not seem well set up to adapt really quickly. (A specific concern is dangers from AI takeover).   \n4. Why now? Why this group of people? We haven't really had tech that could do most epistemic tasks until recently. Now we have AI that can do an increasing proportion of the sub-tasks of epistemic processes. This will allow us to delegate epistemic tasks to AI.  \n   1. It's likely that people do this delegation, even if it's somewhat misguided and potentially bad in the long-term. Can we make this go well?  \n   2. This potentially massively expands our epistemic capacities. How can we use this well?  \n   3. Epistemic tech/institutions/advances can be hard to fund via for-profit companies. This is in part due to public goods issues, and issues around markets for ideas.\n\n## Rafe\n\nAs ever, I hope this will stop advanced artificial intelligence destroying the world, killing all its people \\&c.\n\nSo I think there are a few paths for impact here. I think there is a world where we can make the most productive thinkers about how to do things go further and better; I'm not so excited about a single-player version of this, as I think there's a pretty narrow window when top-level centaur mode works out\\[1\\]\n\nEveryone is already thinking about AI, and, I suspect, this will only get more acute. I think it's pretty hard for people to confront this and think about it carefully and well. It's scary\\! And there are a bunch of entrenched interests and memeplexes that preserve their status better (myopically) if people only have shallow slogans to think about AI with.\n\nGiven that, I suspect that the global sensemaking machine isn't up for the task of thinking about AI well. I hope we can help the global sensemaking machine upgrade itself. I have some hope that we as a society can come up with good ideas for dealing with AI, and also come to see how they make sense, so we can choose them in a robust way.\n\nOne other path to impact: I think a society that understands things better, including itself and its impacts, is one that is more robust in a bunch of ways. A more epistemically-grounded society is also probably a bit richer, and I think a bigger pie creates more slack for arranging things in sanity-promoting ways. I have more hope in a robust society navigating this stuff well.\n\n\\[1\\]:  Yeah, maybe you can do something with shaving off subskills, but I don't think that's really that doable.\n\n## Dave\n\n* With my consequentialist hat, better epistemics being worth it has to route through improved decisions somewhere down the line. The decisions I particularly care about are those by powerful people wrt AI development and implementation and the effects this has on how the future goes  \n  * But I do feel a bit icky when I have this hat on. There seems to be something very intuitively appealing about (a) pursuing better epistemics for its own sake and (b) democratising epistemic tools for the use and benefit of everyone. I think this basically caches out in more 'raising the floor' type work  \n  * I don't feel like I have a very good way of resolving this tension yet, but am excited to see things pushing on both frontiers  \n* I think forecasting has the potential to be a pretty powerful tool and there's currently a massive undersupply of good forecasts. I'm excited for us to use the wave of increased AI labor to make better quantitative forecasts to inform key decisions. I probably care more about quantitative forecasts than most decision makers and am open to being persuaded that the other, fuzzier parts of epistemics are more valuable to work on. \n\n## Julian\n\nWhat are use cases for AI for Epistemics that you most care about?  \n**Raising the ceiling:** can we use AI to organize information and credibly resolve the most challenging and important questions on which experts still seem to disagree? For example:\n\n* COVID origins  \n* Urban planning and public policy  \n* Lots of advancements in basic science: routing around current disagreements and policy challenges with slam-dunk science might be a pretty strong way forward.\n\n**Raising the floor:** can we use AI to build extremely legibly trustworthy/reliable epistemic infrastructure that actually makes its way into people's lives?\n\n* Making truth more palatable (in a way which doesn't translate into making falsehoods more palatable)  \n* Collective epistemics: connecting people to the others with whom they would have the most productive/enlightening conversations\n\nWhat decisions may go badly by default, but could go better if we have the right tech?\n\n* Lots of stuff in public policy: downstream both of better experts (technocrats), better public (voters/interest groups), and better coordination (collective epistemics)\n\nWhat do the later stages of the path to impact look like?\n\n* I want us to live in a world where everyone is more rational, empathetic, considered in their decision making, conscientious about impact, and interested in coordinating. I feel like that's one of the biggest blockers to all kinds of *long term* progress.\n\nWhat tools do we actually maybe want to build?\n\n* Easily accessible fact checkers / claim refuters  \n  * Powerful AI debaters  \n* Forecasting with explanations: superhuman and at scale  \n* Scientific meta-analysis, systematic review  \n* Ad-hoc / bespoke probabilistic models for individual problems  \n* Bespoke *simulators* / simulation environments  \n* Consistency checking and inconsistency identification  \n  * Arbitrage / market makers in prediction markets?  \n* Using AI to accelerate development of non-AI epistemic tools:  \n  * ?\n\n## Lawrence\n\nThere are already key questions/strategic problems where the truth is murky and we seem to proceed on the basis of limited and biased views. There might well be many more of these in the future. A central example is US-China competition over AI. Some have vibes of \"China is so bad that it's worth risking an existential catastrophe to stop it from capturing a portion of the lightcone\". Others have vibes of \"Cooperation is feasible and desirable, and is our only path to a good outcome\". But how bad is China, really? How much \"realised K\" on this question do those who are making the relevant decisions actually have? Similarly, how feasible is cooperation? How valid is this frame of \"capturing the lightcone\"? One reason that we don't have a better hold on these questions is that they are frighteningly complicated and difficult  – I think we could benefit massively from throwing much more cognitive labour at them, and presenting the findings in a way that is hard to disagree with or for any individual to improve upon, other than at a normative/values level. Maybe AI can get us there.\n\n## Lukas Finnveden\n\nwhy is AI for epistemics important/helpful?\n\n* better epistemics generally very good for making all kinds of decisions  \n* I'm specifically concerned about AI potentially causing an intelligence explosion, and us needing better epistemics to handle that. Specifically:  \n  * I'm worried about AI-driven tech R\\&D leading to catastrophes. To help address this, I want AI capabilities to not only be applied to tech R\\&D, but also to be applied to \"reasoning well from limited evidence\" about a number of questions, including how to deal with risks.  \n  * I'm worried about misaligned AI — so I want techniques for eliciting AI capabilities on all kinds of questions, so that AIs don't have better capabilities than we can elicit from them. (Basically: I want good alignment techniques even on hard-to-evaluate questions.)  \n  * Seems great to me if AI-driven improvements in general knowledge and decision-making are more broadly distributed among humans. (I think this leads to better decision-making without significantly accelerating the dangers from new tech or misaligned AI. Also reduces concentration of power.)\n\n## Abram Demski\n\n\\- Why are we doing this? \\-- impact theory brainstorming exercize  \n\t\\- There's the general claim that epistemics has degraded horribly due to social media and the internet, at least in a lot of ways, although epistemics has also gotten better in some specific ways. It seems very plausible that AI is, and will continue to, make this much much worse (due to deepfakes and super-persuasive writing, to give two examples). We wish to reverse this trend, or at least slow the acceleration.  \n\t\\- The high-level reason why epistemics is important is because epistemics is how we make good decisions. So effectively, this is AI for decision-making. However, if we just set out to make tools for decision-making, I have a sense that this would be worse.  \n\t\t\\- One super important aspect of decision-making is that you have to orient towards the right problem. People do much better when they understand the problem before proposing solutions.  \n\t\t\t\\- I have a sense that producing a better shared understanding of the world JUST WOULD result in better things. The connection between epistemics and better decisionmaking is that strong.  \n\t\t\t\t\\- However, there's an element of coordination to this. You have to have common knowledge, not just knowledge. AI for coordination is yet another thing. AI for group epistemics is close to that, though, and again, I have a sense that making \"AI for coordination\" would produce worse effects on the world than \"AI for group epistemics\".  \n\t\t\\- What specific decisions are we hoping to influence?  \n\t\t\t\\- Decisions around existential risk seem super super important. If AI were generally better at epistemics (as a relative, comparative thing \\-- ie we differentially accelerate epistemics) then key decisionmakers (in the government and at AI companies) would see much more clearly what's coming. Everyone wants a better future, so a better future would have a better chance of happening if we could see and navigate.\n\nI don't think I got any further than the supporting documentation for this workshop in articulating reasons and stuff, so I predict that the LLM summary won't be better than what we already know, although I hope it is. \n\n## Brendan\n\nFor us to live together in a caring and just way, we must make decisions about what is important to us together. Is it okay to tell my neighbour to turn their music down? Do we spend money on roads or trains or libraries? Do we regulate AI development and, if so, how? Do we want to spend public money on PEPFAR, or other avenues for reducing global poverty? To make these decisions in a way that is inclusive and democratic, that fosters buy-in and trust, we must begin with a shared sense of how the world is. This shared sense must be grounded in the physical, economic, and social reality we live in.\n\nAs we live together over wider and wider contexts, maintaining a shared sense of the world becomes more difficult and subtle. These wider contexts are facilitated by new technologies, both physical (eg. global manufacturing bases and supply chains) and informational (eg. social media). New technologies, such as AI for epistemics, and help maintain epistemic coherence across these contexts.\n\nMoreover, such technologies allow new avenues for cooperation at larger scales of human involvement. This unlocks new potential for coherence in global politics, scientific discovery, social and technological innovation, and cultural expression, and ultimately the deep joy of being alive and human in the world. \n\n## Isabel: \n\nThe kinds of being right in the past that are most relevant for decision making aren't just about making straightforward predictions about what will happen, but are more nuanced about what kinds of actions one should take to cause those things to happen, and these seem like somewhat different skills. These are harder to evaluate (e.g. because they're counterfactual: either we listened to person a and then we have to evaluate whether things went better or worse than if we didn't, but hard to say what the counterfactual would have been; or we didn't listen to person a and then we have to guess what things would have been like if we did) \n\nA bit adjacent to Neel Nanda's [recent piece](https://www.lesswrong.com/posts/P5zWiPF5cPJZSkiAK/good-research-takes-are-not-sufficient-for-good-strategic) on how \"good research takes aren't sufficient for good strategic takes\" \\- \"good forecasting takes aren't sufficient for good strategic takes\", but maybe some of this is maybe resolvable on the forecasting tech side \\- can we get better at creating and evaluating results from fuzzier questions? Can AI help? (Also can AI help with the whole problem, but this feels small and concrete enough to maybe get some traction on.) Also, some risk of actors building up good track records in order to be listened to, but then choosing to burn that track record to get power or things like that, something something trust \\- a concern for both human actors and e.g. scheming AIs \\- what things look good until they abruptly aren't? \n\nOn a different note, also interested in takes on what to do about and how to engage with actors who have epistemic processes that don't seem to be truth seeking in a comprehensible-to-me way. \n\n## Adam Marblestone:\n\nWhat do we get out of AI for epistemics?\n\nTrust, is one thing. Our current society seems to be fraught with distrust and motivated reasoning. Can we get a more objectively trustable source of truth? Not only accuracy but with a provenance that gives it an ability to be trusted by humans who are otherwise adversarial. Trust is needed to coordinate. Truthfulness is maybe somewhat necessary but seemingly not sufficient for a source being trusted. As a practical matter, on hot political topics I've been going to OpenAI Deep Research.\n\nBut at a certain level I wonder if AI for epistemics and some form of beneficial AGI could be basically the same thing. I asked Eric Drexler what he wanted for d/acc and he said something that sounds a bit like that…\n\n\"  \nHere's my unsurprising take on directed research for \"defensive acceleration\":  \nDevelopment and steering of epistemic tooling, aka [Large Knowledge Models](https://aiprospects.substack.com/p/large-knowledge-models), offers the highest leverage.\n\nPotential actions:  \n  • Identify and influence entities that could develop and scale necessary foundations.  \n  • Leverage the growing ease of development to take pre-scaling steps.  \n  • Within the scope of \"foundations\", accelerate productive directions for development.  \n  • To support science, ensure grounding of content in cited sources.  \n  • Pursue or promote assembly of appropriate knowledge inputs (beyond what is now used for training data).  \n  • Identify, promote, or implement epistemic tools supported by LKM resources.  \n  • Other kinds of exploration, promotion, preparation, and implementation.  \n  • Ensure that my advice is thoroughly understood.\n\nLKMs can accelerate pretty much everything that one might want, and perhaps tip the balance of applications toward *defensive* acceleration by promoting understanding of the *what and why* of defensive applications.\n\nAddendum:\n\nIn the email I sent a few minutes ago, the basic proposal becomes stronger if one reads \"LKM\" as something like \"large knowledge model functionality\", regardless of implementation.\n\nThe knowledge-model concept (whatever one prefers to call it) should not be tied to my specific technical suggestions. What I've said can be framed as something like \"a discussion of the value, feasibility, and potential scope of a rich epistemic resource\". Bundles of embeddings, etc., are at a level of specificity that may be distracting in the broader context of the meeting.  \n\"\n\nSo that's a strategy assistant AI – which helps humans at large see ways they could act and coordinate especially around big technological or geopolitical strategy issues I guess – but also a certain embodiment of an AGI oracle type system. Maybe the idea then is, how to make a AGI that helps with navigating civilizational strategy, and what is the right form of or interface to that to make it epistemically most useful. \n\nI'd like to also see democratic mechanisms that are simply maximally utilizing the tech we have. \n\n## Elizabeth\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Allowing people to be better informed about experimental medical stuff for themselves.   \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Bargaining? Even just salary negotiations feels like the results are often more the consequence of \"human bargaining skill\" and not the pareto optimal result.    \n  \n\n\n## Saul\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * making better decisions  \n  * in service of that: more useful forecasts, particularly understanding better which things are most relevant to forecast  \n    * in service of that: better group decisionmaking (e.g. unblocking things from vetopower type  \n    * in service of that: more aptly/calibratedly handling waves of information  \n    * In service of that: societal- and individual-scale asymmetric truth-tracking  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * which groups of people have political power  \n  * who's building powerful new tech  \n  * who's regulating powerful new tech  \n  * (etc, other \"decisions\" along these lines — some might not end up getting shaped like decisions, though)  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * better decisions across the board"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Use of Hyperlinks for Contextual Scaffolding",
          "description": "References to external articles (e.g., metaphilosophy, automating alignment research) provide valuable context and allow interested readers to explore topics in depth. This enhances audience alignment and supports knowledge scaffolding.",
          "importance": 60,
          "grade": 90,
          "isValid": true,
          "highlight": {
            "startOffset": 3179,
            "endOffset": 3528,
            "quotedText": "cf**. [metaphilosophy](https://www.alignmentforum.org/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy)** (but also a bunch of other things.)  \n  * This also feels very similar to making **AIs helpful for AI safety research, cf. [automating alignment research](https://joecarlsmith.com/2025/03/14/ai-for-ai-safety#2-what-is-ai-for-ai-safety)."
          }
        },
        {
          "title": "Occasional Use of Unexplained Acronyms",
          "description": "Acronyms like 'ASI' and 'AGI' are used without definition. While common in the field, defining them on first use would improve accessibility for a broader audience.",
          "importance": 55,
          "grade": 50,
          "isValid": false,
          "highlight": {
            "startOffset": 3872,
            "endOffset": 26897,
            "quotedText": "his is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place.  \n  * Vague sense that better **societal epistemics** would help with increasing the **sanity of conversations about AI safety, polarisation around the issue etc.**  \n    * This is broadly about **getting intent alignment** in the first place.\n\n→ These are all more about raising the **bottom of the epistemic landscape**\n\n## Deger Turan:\n\n* Social trust erosion, collective coordination mechanisms are built on a weak assumption  \n* Capacity to entertain truthfulness when it is not instrumental to one's incentive gradient & differing norms upon 'truth will make my sense of security weaker'   \n* Creating contexts where world model divergence can be agreed by both parties enabling a safe disagreement  \n* Improving model reasoning / safer AI by reducing slop, by creating accountability mechanisms through consistency checks & other specifications for healthy epistemics  \n* Decoupling virality with magnitude while living in a short attention span world, curation   \n* I have some confusion re: raising the floor when not all parties are necessarily operating in good faith. I think I need some help think through the ways in which epistemics in the context of incentive misalignment can be used or have defensibility. \n\n## Nathan Young: \n\nWhat would we have got:\n\n* Biden not running, perhaps an open dem primary  \n* No FTX crisis  \n* Less zero sum OpenAI board drama  \n* Many smaller, better choices \n\nWhat decisions might go badly:\n\n* Trump tariffs/ global trade  \n* Taiwan/ US-China conflict  \n* US-China competition  \n* US authoritarianism  \n* AI regulation  \n* EU-China realignment  \n* Automated robots  \n* OpenAI strategy  \n* Anthropic strategy  \n* Google strategy  \n* xAI strategy  \n* AI workforce automation  \n* China strategy  \n* Russia strategy  \n* Moskovitz strategy  \n* Must strategy  \n* Bezos Strategy  \n* Zuckerberg strategy  \n* Tallinn strategy  \n* Altman strategy  \n* Drone warfare  \n* Climate change destabilisation  \n* Democratic trust\n\n## Lincoln Quirk:\n\nWhy do we care about (AI for) epistemics?  \n\\- making the public better informed:  \n\t\\- better able to deal with and process the wealth of information  \n\t\\- better able to express their own interests in government (via voting)  \n\t\\- helping cut through social media distraction / intentional misinformation  \n\\- getting governments to implement better policy  \n\t\\- coming up with good policy ideas  \n\t\\- objectively doing cost-benefit analysis  \n\t\\- objectively explaining tradeoffs (vs having to rely on biased humans to explain them)  \n\t\\- flagging bad ideas as bad  \n\t\\- convincing policymakers with compelling writeups  \n\\- success and flourishing for individuals and orgs using these tools  \n\t\\- via helping them think through decisions  \n\t\\- via clearheaded analysis of trade-offs and cost/benefits  \n\t\\- flagging bad ideas as bad\n\n## Jay\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * How can we build something like the positive version of Community Notes: which statements/posts are agreed on by people who often disagree?  \n  * How can we better understand/summarize what others think, e.g. on social media platforms? E.g. if you see a post recommended to you, is this because it's the most extreme take that went viral and isn't representative, or is this a commonly held view? Between different viewpoints, what are the concrete points of actual disagreement (on facts or values) and what are the things widely agreed upon? Let's build viewpoint/debate maps or a similar sensemaking tool that gets adopted/understood by the masses/raises the knowledge floor (in my experience, things like pol.is are hard for regular users to grok)  \n  * How can we improve Community Notes (or more general knowledge) creation with AI assistants? An AI agent proposing community notes and humans still checking/rating them?  \n  * How can we have something like Community Notes that applies to claims or statements about the future, or which are not concretely knowable?  \n  * \n\n## Ben Wilson (Metaculus)\n\nOverall I believe the most important value of epistemics (and AI epistemics) in general is informing what actions allow us to achieve our goals (i.e. make the world better). If we had perfect AI epistemics I would be excited by the ability for us to take a goal, and get a list of possible actions we could take and the forecasted outcomes of each of those actions (plus background information and forecasts for the 'current lay of the land' for context). This can then inform, not just the risk of AI doing good/bad things (as with current forecasting), but what we can do to raise the chances of 'great' AI outcomes, and lower the changes of 'worse' AI outcomes\n\n## Andreas\n\n* **What are use cases for AI for Epistemics that you most care about?**  \n  * AI policy, broadly construed, must go better  \n    * Avoid gradual disempowerment  \n    * Thoughtful regulation  \n  * Organizational planning & decision-making is weak  \n    * Given an informal long-term goal like improve decision-making for strategic users in government & large orgs, make a plan for a 20-people org that helps them accomplish the goal  \n    * Mapping dependencies, uncertainties, and potential futures  \n    * Understand complex domains with non-trivial feedback loops  \n      * Economic shifts  \n* **What decisions may go badly by default, but could go better if we have the right tech?**  \n  * Resource allocation within orgs  \n    * Investment  \n    * Grant making  \n    * Government budgets  \n  * Policy  \n    * How should AI be deployed in the world?  \n* **What do the later stages of the path to impact look like?**  \n  * AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning  \n  * The technology helps organizations maintain coherence and make sound judgments even as the pace of change accelerates  \n  * Assumption: People are generally well-intentioned \\- no one wants catastrophe \\- and as a result of better tooling things go better in the world overall\n\n## Owen optimistic brainstorm:\n\nWorld peace without world government — sufficiently good tech to help countries understand each others' interests, find mutually desirable bargains, and make sufficient credible commitments, could help us get up towards the Pareto frontier where we avoid (virtually) all costly conflicts\n\nGoing slowly and carefully on AI development — rather than having AI progress accelerate at increasingly breakneck speed, helping actors to understand the basic situation and the desirability of different options could be crucial for facilitating meaningful and robust agreements which restrict AI development to the stuff which we have good reasons for being confident are safe // otherwise beneficial.\n\nBettering democracy — helping people to access key information about what decisions their country might make would actually mean for longer term implications, and having this in turn feed into incentives for decision-makers that better align them with the common good\n\nBuilding safe / aligned AI — better tools for anticipating when systems are actually robustly safe, vs when stronger cases for expectation of safety seem required\n\n## Josh Rosenberg (FRI)\n\nI'm massively confused about important problems, e.g. how to orient to AI risk and what to do about it. Some reasonable people think there's a high probability we effectively lose everything we care about in 2027\\. Some reasonable people think it'll be decades till we have \"AGI\" and it'd be good if it arrived sooner. Which actions are good to take in each of these worlds is very different, and I'd like to figure out what world we're in.\n\nThis also applies to many other problem areas.\n\nE.g., Trump's effect on US democracy. How much of a crisis is it? What are the most effective actions to take in response to ensure that the country and communities we care about are as well off as they can be? How can we preserve high-quality institutions to lead the world in positive directions?\n\nOr, on the positive vision side: the \"Abundance\" agenda seems promising. Is it actually a good idea? What do we do about it?\n\nAI for Epistemics could help us get a lot more clarity on how to think about these problems and what actions to take. I'd like much better information and forecasts on the likely effects of Trump, AI timelines, and the pro's and con's of different Abundance policies, for example. If I had a bunch of time and good resources to dig into these problems, I could get a much better sense for myself. I'm hoping AI will help all of us do this sort of thing collectively.\n\n## Katja Grace\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Put lots of cognitive effort into very clear and good and compelling case for AI risk  \n  * Automate talking to people about AI risk compellingly, possibly using above case as input  \n  * Good game theoretic thinking re arms races, coordination, what is possible. I.e. People are making dumb assumptions about things, e.g. \"can't slow down tech\". Could automate noting errors. So e.g. when someone says that on Twitter, can highlight it and send in response. \\-\\> good thinking via better weapons for debate (similar to Nathan project)  \n  * Specific idea: very quantified and competitive debate site, that can be cited re which views are winning, where AI does a lot of the cognitive labor in places no people are paying attention, but as a part of it gets cruxy, people pay attention and vote/bet  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Poor decision making around AI: e.g. 'racing' because of vague heuristics instead of ever doing calc on costs and benefits given specific risks  \n  * Failure to coordinate on AI: building AI systems that are dangerous, due to expecting others to do so\n\n## Joel Lehman\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Raising the bottom (i.e. helping improve societal epistemics) – in particular that in theory political decisions (at least political decision-makers) are somewhat downstream of societal epistemics  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Having weird political leaders and awful public epistemics at critical junctures seems to unnecessarily spike catastrophic/existential risk probabilities. In theory, helping uplevel or at least maintain current public epistemic health (especially as it relates to civic sensemaking) might help influence the quality of decision-makers and concrete decisions made at high-stakes moments  \n  * The shape of the tech is not clear, or probably requires multiple flavors of interventions. E.g. if LLM assistants by default help cultivate civic virtues and on the margin support critical thinking or whatever underlying skills and knowledge help a person up-level their epistemic habits / health (or at least if LLM assistants don't contribute to atrophy/harm); or somehow convincing social media networks to care about this (e.g. community notes but things that go beyond); similarly somehow help w/ important social goods like 'societal trust' and 'trust in your neighbors'  \n* What do the later stages of the path to impact look like?  \n  * \\[didn't get to this\\]  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * Healthier democratic society, better agreement on consensus reality, higher trust in society, better politicians, better concrete decisions in high-stakes moments \n\n## Austin Chen\n\n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * **Evaluator**: More agreement (or understanding of disagreement) of what orgs & people are doing good work (eg for making grants in AI safety)  \n  * **Oracle**: Instant trustworthy forecasts on any question you ask  \n  * **Planner**: Guidance on \"what are the 3 most important things I should do this day/week/month\"  \n  * **Representative**: Simulate the responses of other parties (people, orgs/corps, governments, LLM agents), for discussions, contracts, trade\n\n## Janna\n\n* Why do we care about (AI for) epistemics?  \n  * Noise to signal ratio could be much higher  \n  * AI (mis)information churned out at the price of almost zero  \n* What are use cases for AI for Epistemics that you most care about?  \n  * Solving prediction markets--is there a way to scale up the bots, even if they are wrong and lossy, to improve prediction market accuracy?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * US-China relations  \n  * Trump tariffs maybe  \n  * Immigration rules in the US  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * More accurate predictions to…  \n    * Reduce business uncertainty  \n    * Make it easier to evaluate life decisions  \n  * Forestalling bad decisions made on imperfect information\n\n## Owain\n\n1. Maybe having humans understanding the world better is good in itself. Lots of people read popular science for this reason. Maybe it's good for AIs as well  \n2. Very broadly, it seems that better world understanding and decision making enables people to make their lives better in myriad ways. (To substantiate this, one could try to separate out the effects of better world understanding from the effects of better realized technology. E.g. Looking at capital-poor countries that can still benefit from scientific knowledge, economics, etc.)  \n3. More specifically, if AI progress is fast, there will be lots of important decisions to be made in quick calendar time. Exploiting AI to improve those decisions seems valuable. Current decision making institutions do not seem well set up to adapt really quickly. (A specific concern is dangers from AI takeover).   \n4. Why now? Why this group of people? We haven't really had tech that could do most epistemic tasks until recently. Now we have AI that can do an increasing proportion of the sub-tasks of epistemic processes. This will allow us to delegate epistemic tasks to AI.  \n   1. It's likely that people do this delegation, even if it's somewhat misguided and potentially bad in the long-term. Can we make this go well?  \n   2. This potentially massively expands our epistemic capacities. How can we use this well?  \n   3. Epistemic tech/institutions/advances can be hard to fund via for-profit companies. This is in part due to public goods issues, and issues around markets for ideas.\n\n## Rafe\n\nAs ever, I hope this will stop advanced artificial intelligence destroying the world, killing all its people \\&c.\n\nSo I think there are a few paths for impact here. I think there is a world where we can make the most productive thinkers about how to do things go further and better; I'm not so excited about a single-player version of this, as I think there's a pretty narrow window when top-level centaur mode works out\\[1\\]\n\nEveryone is already thinking about AI, and, I suspect, this will only get more acute. I think it's pretty hard for people to confront this and think about it carefully and well. It's scary\\! And there are a bunch of entrenched interests and memeplexes that preserve their status better (myopically) if people only have shallow slogans to think about AI with.\n\nGiven that, I suspect that the global sensemaking machine isn't up for the task of thinking about AI well. I hope we can help the global sensemaking machine upgrade itself. I have some hope that we as a society can come up with good ideas for dealing with AI, and also come to see how they make sense, so we can choose them in a robust way.\n\nOne other path to impact: I think a society that understands things better, including itself and its impacts, is one that is more robust in a bunch of ways. A more epistemically-grounded society is also probably a bit richer, and I think a bigger pie creates more slack for arranging things in sanity-promoting ways. I have more hope in a robust society navigating this stuff well.\n\n\\[1\\]:  Yeah, maybe you can do something with shaving off subskills, but I don't think that's really that doable.\n\n## Dave\n\n* With my consequentialist hat, better epistemics being worth it has to route through improved decisions somewhere down the line. The decisions I particularly care about are those by powerful people wrt AI development and implementation and the effects this has on how the future goes  \n  * But I do feel a bit icky when I have this hat on. There seems to be something very intuitively appealing about (a) pursuing better epistemics for its own sake and (b) democratising epistemic tools for the use and benefit of everyone. I think this basically caches out in more 'raising the floor' type work  \n  * I don't feel like I have a very good way of resolving this tension yet, but am excited to see things pushing on both frontiers  \n* I think forecasting has the potential to be a pretty powerful tool and there's currently a massive undersupply of good forecasts. I'm excited for us to use the wave of increased AI labor to make better quantitative forecasts to inform key decisions. I probably care more about quantitative forecasts than most decision makers and am open to being persuaded that the other, fuzzier parts of epistemics are more valuable to work on. \n\n## Julian\n\nWhat are use cases for AI for Epistemics that you most care about?  \n**Raising the ceiling:** can we use AI to organize information and credibly resolve the most challenging and important questions on which experts still seem to disagree? For example:\n\n* COVID origins  \n* Urban planning and public policy  \n* Lots of advancements in basic science: routing around current disagreements and policy challenges with slam-dunk science might be a pretty strong way forward.\n\n**Raising the floor:** can we use AI to build extremely legibly trustworthy/reliable epistemic infrastructure that actually makes its way into people's lives?\n\n* Making truth more palatable (in a way which doesn't translate into making falsehoods more palatable)  \n* Collective epistemics: connecting people to the others with whom they would have the most productive/enlightening conversations\n\nWhat decisions may go badly by default, but could go better if we have the right tech?\n\n* Lots of stuff in public policy: downstream both of better experts (technocrats), better public (voters/interest groups), and better coordination (collective epistemics)\n\nWhat do the later stages of the path to impact look like?\n\n* I want us to live in a world where everyone is more rational, empathetic, considered in their decision making, conscientious about impact, and interested in coordinating. I feel like that's one of the biggest blockers to all kinds of *long term* progress.\n\nWhat tools do we actually maybe want to build?\n\n* Easily accessible fact checkers / claim refuters  \n  * Powerful AI debaters  \n* Forecasting with explanations: superhuman and at scale  \n* Scientific meta-analysis, systematic review  \n* Ad-hoc / bespoke probabilistic models for individual problems  \n* Bespoke *simulators* / simulation environments  \n* Consistency checking and inconsistency identification  \n  * Arbitrage / market makers in prediction markets?  \n* Using AI to accelerate development of non-AI epistemic tools:  \n  * ?\n\n## Lawrence\n\nThere are already key questions/strategic problems where the truth is murky and we seem to proceed on the basis of limited and biased views. There might well be many more of these in the future. A central example is US-China competition over AI. Some have vibes of \"China is so bad that it's worth risking an existential catastrophe to stop it from capturing a portion of the lightcone\". Others have vibes of \"Cooperation is feasible and desirable, and is our only path to a good outcome\". But how bad is China, really? How much \"realised K\" on this question do those who are making the relevant decisions actually have? Similarly, how feasible is cooperation? How valid is this frame of \"capturing the lightcone\"? One reason that we don't have a better hold on these questions is that they are frighteningly complicated and difficult  – I think we could benefit massively from throwing much more cognitive labour at them, and presenting the findings in a way that is hard to disagree with or for any individual to improve upon, other than at a normative/values level. Maybe AI can get us there.\n\n## Lukas Finnveden\n\nwhy is AI for epistemics important/helpful?\n\n* better epistemics generally very good for making all kinds of decisions  \n* I'm specifically concerned about AI potentially causing an intelligence explosion, and us needing better epistemics to handle that. Specifically:  \n  * I'm worried about AI-driven tech R\\&D leading to catastrophes. To help address this, I want AI capabilities to not only be applied to tech R\\&D, but also to be applied to \"reasoning well from limited evidence\" about a number of questions, including how to deal with risks.  \n  * I'm worried about misaligned AI — so I want techniques for eliciting AI capabilities on all kinds of questions, so that AIs don't have better capabilities than we can elicit from them. (Basically: I want good alignment techniques even on hard-to-evaluate questions.)  \n  * Seems great to me if AI-driven improvements in general knowledge and decision-making are more broadly distributed among humans. (I think this leads to better decision-making without significantly accelerating the dangers from new tech or misaligned AI. Also reduces concentration of power.)\n\n## Abram Demski\n\n\\- Why are we doing this? \\-- impact theory brainstorming exercize  \n\t\\- There's the general claim that epistemics has degraded horribly due to social media and the internet, at least in a lot of ways, although epistemics has also gotten better in some specific ways. It seems very plausible that AI is, and will continue to, make this much much worse (due to deepfakes and super-persuasive writing, to give two examples). We wish to reverse this trend, or at least slow the acceleration.  \n\t\\- The high-level reason why epistemics is important is because epistemics is how we make good decisions. So effectively, this is AI for decision-making. However, if we just set out to make tools for decision-making, I have a sense that this would be worse.  \n\t\t\\- One super important aspect of decision-making is that you have to orient towards the right problem. People do much better when they understand the problem before proposing "
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Strong Emphasis on Real-World Impact",
          "description": "Many contributors tie their points directly to real-world outcomes (e.g., 'better government governance', 'reduced likelihood of war'), which grounds the discussion and increases relevance.",
          "importance": 70,
          "grade": 90,
          "isValid": true,
          "highlight": {
            "startOffset": 1401,
            "endOffset": 1892,
            "quotedText": "Better government governance  \n* Better democratic functioning  \n* Reduced dangerous concentration of power  \n    \n* Healthy polarization / pluralism  \n    \n* More peace, less likelihood of war  \n* Better geopolitics  \n* Better economic incentives  \n    \n* Better AI org governance  \n* Better AI coordination (re. distribution, escalation, etc.)  \n* Better compute allocation (toward what matters for good outcomes)  \n    \n* Better allocation of resources overall  \n* Better lives for people"
          }
        },
        {
          "title": "Occasional Use of Colloquial Language",
          "description": "Phrases like 'free-form vibe', 'vibe about what cool things we could do', and 'icky' introduce informality that may detract from the document's authority. Consider rephrasing for a more professional tone, unless the intent is to foster informality.",
          "importance": 40,
          "grade": 45,
          "isValid": false,
          "highlight": {
            "startOffset": 50,
            "endOffset": 20882,
            "quotedText": "Negative: \"we just want to free-form vibe about what cool things we could do\"  \n* Positive: step back & ask \"wait, why are we doing this again?\"  \n* What stuff does this actually help?  \n* Epistemics isn't a goal in-an-of itself; figuring out what combination of things are possible, achievable, etc.  \n* Why do we care about (AI for) epistemics?\n\nWriting prompts from Owen:\n\n* What are use cases for AI for Epistemics that you most care about?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nTemplate:\n\n## Name: \n\nEntry\n\n## Dan Schwarz:\n\n* What are use cases for AI for Epistemics that you most care about?\n\nGoing from \"Realized\" to \"Accessible\" knowledge, e.g. getting good information using the web. Agentic web agents like Deep Research.\n\n* What decisions may go badly by default, but could go better if we have the right tech?\n\nMany billions of small decisions.\n\n* What do the later stages of the path to impact look like?\n\nSuperhuman forecasting at scale.\n\n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nReasonable judgment on tricky questions that's too cheap to meter, better than if you asked a respected colleague to spend many minutes on it.\n\n## Aviv\n\n* Better government governance  \n* Better democratic functioning  \n* Reduced dangerous concentration of power  \n    \n* Healthy polarization / pluralism  \n    \n* More peace, less likelihood of war  \n* Better geopolitics  \n* Better economic incentives  \n    \n* Better AI org governance  \n* Better AI coordination (re. distribution, escalation, etc.)  \n* Better compute allocation (toward what matters for good outcomes)  \n    \n* Better allocation of resources overall  \n* Better lives for people  \n* More effective organizations, getting things done with fewer resources, more focused on the things that benefit everyone  \n    \n* Identification of common ground directions / equilibria / game theoretic strategies that create/incentivize new positive gradients (and not surfacing the bad versions of those…)  \n    \n* Understanding our own goals and values\n\n*Note how it is the intersection of better epistemics / world models + the right power structures / incentives needed for all of these.*\n\n## Ben Goldhaber:\n\n* Forecasting how I should live my life in a period of intense technological change  \n* Guiding decision makers to avoid war with china/ai/everyone  \n* Providing the background context for me to sensemake way more sanely.  \n* Pushing forward the extant knowledge of AI safety agendas  \n* Planning for multi-stage projects that could achieve big impact  \n* Roadmapping to build larger more complex things\n\n## Chi\n\nThere are a few different things I care about:\n\n* I care a lot about humanity and AI, including **super-late-stage AI, getting decision theory** right.  \n  * Consequently but also separately, this relates to the general question of what to do about areas where there is not necessarily a ground truth but that are nonetheless very important to get right, cf**. [metaphilosophy](https://www.alignmentforum.org/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy)** (but also a bunch of other things.)  \n  * This also feels very similar to making **AIs helpful for AI safety research, cf. [automating alignment research](https://joecarlsmith.com/2025/03/14/ai-for-ai-safety#2-what-is-ai-for-ai-safety).**\n\n  → This is more about raising the **top of the epistemic landscape**\n\n* Other things I care about:  \n  * Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place.  \n  * Vague sense that better **societal epistemics** would help with increasing the **sanity of conversations about AI safety, polarisation around the issue etc.**  \n    * This is broadly about **getting intent alignment** in the first place.\n\n→ These are all more about raising the **bottom of the epistemic landscape**\n\n## Deger Turan:\n\n* Social trust erosion, collective coordination mechanisms are built on a weak assumption  \n* Capacity to entertain truthfulness when it is not instrumental to one's incentive gradient & differing norms upon 'truth will make my sense of security weaker'   \n* Creating contexts where world model divergence can be agreed by both parties enabling a safe disagreement  \n* Improving model reasoning / safer AI by reducing slop, by creating accountability mechanisms through consistency checks & other specifications for healthy epistemics  \n* Decoupling virality with magnitude while living in a short attention span world, curation   \n* I have some confusion re: raising the floor when not all parties are necessarily operating in good faith. I think I need some help think through the ways in which epistemics in the context of incentive misalignment can be used or have defensibility. \n\n## Nathan Young: \n\nWhat would we have got:\n\n* Biden not running, perhaps an open dem primary  \n* No FTX crisis  \n* Less zero sum OpenAI board drama  \n* Many smaller, better choices \n\nWhat decisions might go badly:\n\n* Trump tariffs/ global trade  \n* Taiwan/ US-China conflict  \n* US-China competition  \n* US authoritarianism  \n* AI regulation  \n* EU-China realignment  \n* Automated robots  \n* OpenAI strategy  \n* Anthropic strategy  \n* Google strategy  \n* xAI strategy  \n* AI workforce automation  \n* China strategy  \n* Russia strategy  \n* Moskovitz strategy  \n* Must strategy  \n* Bezos Strategy  \n* Zuckerberg strategy  \n* Tallinn strategy  \n* Altman strategy  \n* Drone warfare  \n* Climate change destabilisation  \n* Democratic trust\n\n## Lincoln Quirk:\n\nWhy do we care about (AI for) epistemics?  \n\\- making the public better informed:  \n\t\\- better able to deal with and process the wealth of information  \n\t\\- better able to express their own interests in government (via voting)  \n\t\\- helping cut through social media distraction / intentional misinformation  \n\\- getting governments to implement better policy  \n\t\\- coming up with good policy ideas  \n\t\\- objectively doing cost-benefit analysis  \n\t\\- objectively explaining tradeoffs (vs having to rely on biased humans to explain them)  \n\t\\- flagging bad ideas as bad  \n\t\\- convincing policymakers with compelling writeups  \n\\- success and flourishing for individuals and orgs using these tools  \n\t\\- via helping them think through decisions  \n\t\\- via clearheaded analysis of trade-offs and cost/benefits  \n\t\\- flagging bad ideas as bad\n\n## Jay\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * How can we build something like the positive version of Community Notes: which statements/posts are agreed on by people who often disagree?  \n  * How can we better understand/summarize what others think, e.g. on social media platforms? E.g. if you see a post recommended to you, is this because it's the most extreme take that went viral and isn't representative, or is this a commonly held view? Between different viewpoints, what are the concrete points of actual disagreement (on facts or values) and what are the things widely agreed upon? Let's build viewpoint/debate maps or a similar sensemaking tool that gets adopted/understood by the masses/raises the knowledge floor (in my experience, things like pol.is are hard for regular users to grok)  \n  * How can we improve Community Notes (or more general knowledge) creation with AI assistants? An AI agent proposing community notes and humans still checking/rating them?  \n  * How can we have something like Community Notes that applies to claims or statements about the future, or which are not concretely knowable?  \n  * \n\n## Ben Wilson (Metaculus)\n\nOverall I believe the most important value of epistemics (and AI epistemics) in general is informing what actions allow us to achieve our goals (i.e. make the world better). If we had perfect AI epistemics I would be excited by the ability for us to take a goal, and get a list of possible actions we could take and the forecasted outcomes of each of those actions (plus background information and forecasts for the 'current lay of the land' for context). This can then inform, not just the risk of AI doing good/bad things (as with current forecasting), but what we can do to raise the chances of 'great' AI outcomes, and lower the changes of 'worse' AI outcomes\n\n## Andreas\n\n* **What are use cases for AI for Epistemics that you most care about?**  \n  * AI policy, broadly construed, must go better  \n    * Avoid gradual disempowerment  \n    * Thoughtful regulation  \n  * Organizational planning & decision-making is weak  \n    * Given an informal long-term goal like improve decision-making for strategic users in government & large orgs, make a plan for a 20-people org that helps them accomplish the goal  \n    * Mapping dependencies, uncertainties, and potential futures  \n    * Understand complex domains with non-trivial feedback loops  \n      * Economic shifts  \n* **What decisions may go badly by default, but could go better if we have the right tech?**  \n  * Resource allocation within orgs  \n    * Investment  \n    * Grant making  \n    * Government budgets  \n  * Policy  \n    * How should AI be deployed in the world?  \n* **What do the later stages of the path to impact look like?**  \n  * AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning  \n  * The technology helps organizations maintain coherence and make sound judgments even as the pace of change accelerates  \n  * Assumption: People are generally well-intentioned \\- no one wants catastrophe \\- and as a result of better tooling things go better in the world overall\n\n## Owen optimistic brainstorm:\n\nWorld peace without world government — sufficiently good tech to help countries understand each others' interests, find mutually desirable bargains, and make sufficient credible commitments, could help us get up towards the Pareto frontier where we avoid (virtually) all costly conflicts\n\nGoing slowly and carefully on AI development — rather than having AI progress accelerate at increasingly breakneck speed, helping actors to understand the basic situation and the desirability of different options could be crucial for facilitating meaningful and robust agreements which restrict AI development to the stuff which we have good reasons for being confident are safe // otherwise beneficial.\n\nBettering democracy — helping people to access key information about what decisions their country might make would actually mean for longer term implications, and having this in turn feed into incentives for decision-makers that better align them with the common good\n\nBuilding safe / aligned AI — better tools for anticipating when systems are actually robustly safe, vs when stronger cases for expectation of safety seem required\n\n## Josh Rosenberg (FRI)\n\nI'm massively confused about important problems, e.g. how to orient to AI risk and what to do about it. Some reasonable people think there's a high probability we effectively lose everything we care about in 2027\\. Some reasonable people think it'll be decades till we have \"AGI\" and it'd be good if it arrived sooner. Which actions are good to take in each of these worlds is very different, and I'd like to figure out what world we're in.\n\nThis also applies to many other problem areas.\n\nE.g., Trump's effect on US democracy. How much of a crisis is it? What are the most effective actions to take in response to ensure that the country and communities we care about are as well off as they can be? How can we preserve high-quality institutions to lead the world in positive directions?\n\nOr, on the positive vision side: the \"Abundance\" agenda seems promising. Is it actually a good idea? What do we do about it?\n\nAI for Epistemics could help us get a lot more clarity on how to think about these problems and what actions to take. I'd like much better information and forecasts on the likely effects of Trump, AI timelines, and the pro's and con's of different Abundance policies, for example. If I had a bunch of time and good resources to dig into these problems, I could get a much better sense for myself. I'm hoping AI will help all of us do this sort of thing collectively.\n\n## Katja Grace\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Put lots of cognitive effort into very clear and good and compelling case for AI risk  \n  * Automate talking to people about AI risk compellingly, possibly using above case as input  \n  * Good game theoretic thinking re arms races, coordination, what is possible. I.e. People are making dumb assumptions about things, e.g. \"can't slow down tech\". Could automate noting errors. So e.g. when someone says that on Twitter, can highlight it and send in response. \\-\\> good thinking via better weapons for debate (similar to Nathan project)  \n  * Specific idea: very quantified and competitive debate site, that can be cited re which views are winning, where AI does a lot of the cognitive labor in places no people are paying attention, but as a part of it gets cruxy, people pay attention and vote/bet  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Poor decision making around AI: e.g. 'racing' because of vague heuristics instead of ever doing calc on costs and benefits given specific risks  \n  * Failure to coordinate on AI: building AI systems that are dangerous, due to expecting others to do so\n\n## Joel Lehman\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Raising the bottom (i.e. helping improve societal epistemics) – in particular that in theory political decisions (at least political decision-makers) are somewhat downstream of societal epistemics  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Having weird political leaders and awful public epistemics at critical junctures seems to unnecessarily spike catastrophic/existential risk probabilities. In theory, helping uplevel or at least maintain current public epistemic health (especially as it relates to civic sensemaking) might help influence the quality of decision-makers and concrete decisions made at high-stakes moments  \n  * The shape of the tech is not clear, or probably requires multiple flavors of interventions. E.g. if LLM assistants by default help cultivate civic virtues and on the margin support critical thinking or whatever underlying skills and knowledge help a person up-level their epistemic habits / health (or at least if LLM assistants don't contribute to atrophy/harm); or somehow convincing social media networks to care about this (e.g. community notes but things that go beyond); similarly somehow help w/ important social goods like 'societal trust' and 'trust in your neighbors'  \n* What do the later stages of the path to impact look like?  \n  * \\[didn't get to this\\]  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * Healthier democratic society, better agreement on consensus reality, higher trust in society, better politicians, better concrete decisions in high-stakes moments \n\n## Austin Chen\n\n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * **Evaluator**: More agreement (or understanding of disagreement) of what orgs & people are doing good work (eg for making grants in AI safety)  \n  * **Oracle**: Instant trustworthy forecasts on any question you ask  \n  * **Planner**: Guidance on \"what are the 3 most important things I should do this day/week/month\"  \n  * **Representative**: Simulate the responses of other parties (people, orgs/corps, governments, LLM agents), for discussions, contracts, trade\n\n## Janna\n\n* Why do we care about (AI for) epistemics?  \n  * Noise to signal ratio could be much higher  \n  * AI (mis)information churned out at the price of almost zero  \n* What are use cases for AI for Epistemics that you most care about?  \n  * Solving prediction markets--is there a way to scale up the bots, even if they are wrong and lossy, to improve prediction market accuracy?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * US-China relations  \n  * Trump tariffs maybe  \n  * Immigration rules in the US  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * More accurate predictions to…  \n    * Reduce business uncertainty  \n    * Make it easier to evaluate life decisions  \n  * Forestalling bad decisions made on imperfect information\n\n## Owain\n\n1. Maybe having humans understanding the world better is good in itself. Lots of people read popular science for this reason. Maybe it's good for AIs as well  \n2. Very broadly, it seems that better world understanding and decision making enables people to make their lives better in myriad ways. (To substantiate this, one could try to separate out the effects of better world understanding from the effects of better realized technology. E.g. Looking at capital-poor countries that can still benefit from scientific knowledge, economics, etc.)  \n3. More specifically, if AI progress is fast, there will be lots of important decisions to be made in quick calendar time. Exploiting AI to improve those decisions seems valuable. Current decision making institutions do not seem well set up to adapt really quickly. (A specific concern is dangers from AI takeover).   \n4. Why now? Why this group of people? We haven't really had tech that could do most epistemic tasks until recently. Now we have AI that can do an increasing proportion of the sub-tasks of epistemic processes. This will allow us to delegate epistemic tasks to AI.  \n   1. It's likely that people do this delegation, even if it's somewhat misguided and potentially bad in the long-term. Can we make this go well?  \n   2. This potentially massively expands our epistemic capacities. How can we use this well?  \n   3. Epistemic tech/institutions/advances can be hard to fund via for-profit companies. This is in part due to public goods issues, and issues around markets for ideas.\n\n## Rafe\n\nAs ever, I hope this will stop advanced artificial intelligence destroying the world, killing all its people \\&c.\n\nSo I think there are a few paths for impact here. I think there is a world where we can make the most productive thinkers about how to do things go further and better; I'm not so excited about a single-player version of this, as I think there's a pretty narrow window when top-level centaur mode works out\\[1\\]\n\nEveryone is already thinking about AI, and, I suspect, this will only get more acute. I think it's pretty hard for people to confront this and think about it carefully and well. It's scary\\! And there are a bunch of entrenched interests and memeplexes that preserve their status better (myopically) if people only have shallow slogans to think about AI with.\n\nGiven that, I suspect that the global sensemaking machine isn't up for the task of thinking about AI well. I hope we can help the global sensemaking machine upgrade itself. I have some hope that we as a society can come up with good ideas for dealing with AI, and also come to see how they make sense, so we can choose them in a robust way.\n\nOne other path to impact: I think a society that understands things better, including itself and its impacts, is one that is more robust in a bunch of ways. A more epistemically-grounded society is also probably a bit richer, and I think a bigger pie creates more slack for arranging things in sanity-promoting ways. I have more hope in a robust society navigating this stuff well.\n\n\\[1\\]:  Yeah, maybe you can do something with shaving off subskills, but I don't think that's really that doable.\n\n## Dave\n\n* With my consequentialist hat, better epistemics being worth it has to route through improved decisions somewhere down the line. The decisions I particularly care about are those by powerful people wrt AI development and implementation and the effects this has on how the future goes  \n  * But I do feel a bit icky when I have this hat on."
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Effective Use of Counterfactual Reasoning",
          "description": "Some contributors (e.g., Isabel) discuss counterfactuals and the challenges of evaluating decisions, demonstrating nuanced understanding. This adds depth and sophistication to the analysis.",
          "importance": 65,
          "grade": 85,
          "isValid": false,
          "highlight": {
            "startOffset": 28801,
            "endOffset": 30322,
            "quotedText": "der contexts, maintaining a shared sense of the world becomes more difficult and subtle. These wider contexts are facilitated by new technologies, both physical (eg. global manufacturing bases and supply chains) and informational (eg. social media). New technologies, such as AI for epistemics, and help maintain epistemic coherence across these contexts.\n\nMoreover, such technologies allow new avenues for cooperation at larger scales of human involvement. This unlocks new potential for coherence in global politics, scientific discovery, social and technological innovation, and cultural expression, and ultimately the deep joy of being alive and human in the world. \n\n## Isabel: \n\nThe kinds of being right in the past that are most relevant for decision making aren't just about making straightforward predictions about what will happen, but are more nuanced about what kinds of actions one should take to cause those things to happen, and these seem like somewhat different skills. These are harder to evaluate (e.g. because they're counterfactual: either we listened to person a and then we have to evaluate whether things went better or worse than if we didn't, but hard to say what the counterfactual would have been; or we didn't listen to person a and then we have to guess what things would have been like if we did) \n\nA bit adjacent to Neel Nanda's [recent piece](https://www.lesswrong.com/posts/P5zWiPF5cPJZSkiAK/good-research-takes-are-not-sufficient-for-good-strategic) on how \"good research takes aren't s"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Occasional Lack of Clear Conclusions",
          "description": "Some sections end abruptly or with open questions, leaving the reader without a clear takeaway. Adding summary statements or explicit conclusions would enhance closure and comprehension.",
          "importance": 50,
          "grade": 45,
          "isValid": true,
          "highlight": {
            "startOffset": 27925,
            "endOffset": 28143,
            "quotedText": "I don't think I got any further than the supporting documentation for this workshop in articulating reasons and stuff, so I predict that the LLM summary won't be better than what we already know, although I hope it is."
          }
        },
        {
          "title": "Strong Use of Analogies and Metaphors",
          "description": "Analogies like 'raising the floor/ceiling' and 'global sensemaking machine' help convey complex ideas in accessible terms. This is an effective communication strategy.",
          "importance": 60,
          "grade": 90,
          "isValid": true,
          "highlight": {
            "startOffset": 3555,
            "endOffset": 4546,
            "quotedText": "raising the **top of the epistemic landscape**\n\n* Other things I care about:  \n  * Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place.  \n  * Vague sense that better **societal epistemics** would help with increasing the **sanity of conversations about AI safety, polarisation around the issue etc.**  \n    * This is broadly about **getting intent alignment** in the first place.\n\n→ These are all more about raising the **bottom of the epistemic landscape**"
          }
        },
        {
          "title": "Occasional Overlap Between Contributors",
          "description": "Several contributors make similar points about the importance of better decision-making and societal trust. While this reinforces key themes, it also introduces redundancy. Consider synthesizing overlapping points in a summary section.",
          "importance": 55,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 35631,
            "endOffset": 35816,
            "quotedText": "better decisions across the board  \n    * better *outcomes* from those decisions; or, like, we can track \"better decisions\" as better-in-expectation outcomes from the decision processes"
          }
        },
        {
          "title": "Effective Use of Questions to Frame Issues",
          "description": "Many sections begin with questions (e.g., 'Why do we care about (AI for) epistemics?'), which engage the reader and clarify the focus of the discussion.",
          "importance": 60,
          "grade": 85,
          "isValid": false,
          "highlight": {
            "startOffset": 355,
            "endOffset": 6511,
            "quotedText": "Why do we care about (AI for) epistemics?\n\nWriting prompts from Owen:\n\n* What are use cases for AI for Epistemics that you most care about?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nTemplate:\n\n## Name: \n\nEntry\n\n## Dan Schwarz:\n\n* What are use cases for AI for Epistemics that you most care about?\n\nGoing from \"Realized\" to \"Accessible\" knowledge, e.g. getting good information using the web. Agentic web agents like Deep Research.\n\n* What decisions may go badly by default, but could go better if we have the right tech?\n\nMany billions of small decisions.\n\n* What do the later stages of the path to impact look like?\n\nSuperhuman forecasting at scale.\n\n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nReasonable judgment on tricky questions that's too cheap to meter, better than if you asked a respected colleague to spend many minutes on it.\n\n## Aviv\n\n* Better government governance  \n* Better democratic functioning  \n* Reduced dangerous concentration of power  \n    \n* Healthy polarization / pluralism  \n    \n* More peace, less likelihood of war  \n* Better geopolitics  \n* Better economic incentives  \n    \n* Better AI org governance  \n* Better AI coordination (re. distribution, escalation, etc.)  \n* Better compute allocation (toward what matters for good outcomes)  \n    \n* Better allocation of resources overall  \n* Better lives for people  \n* More effective organizations, getting things done with fewer resources, more focused on the things that benefit everyone  \n    \n* Identification of common ground directions / equilibria / game theoretic strategies that create/incentivize new positive gradients (and not surfacing the bad versions of those…)  \n    \n* Understanding our own goals and values\n\n*Note how it is the intersection of better epistemics / world models + the right power structures / incentives needed for all of these.*\n\n## Ben Goldhaber:\n\n* Forecasting how I should live my life in a period of intense technological change  \n* Guiding decision makers to avoid war with china/ai/everyone  \n* Providing the background context for me to sensemake way more sanely.  \n* Pushing forward the extant knowledge of AI safety agendas  \n* Planning for multi-stage projects that could achieve big impact  \n* Roadmapping to build larger more complex things\n\n## Chi\n\nThere are a few different things I care about:\n\n* I care a lot about humanity and AI, including **super-late-stage AI, getting decision theory** right.  \n  * Consequently but also separately, this relates to the general question of what to do about areas where there is not necessarily a ground truth but that are nonetheless very important to get right, cf**. [metaphilosophy](https://www.alignmentforum.org/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy)** (but also a bunch of other things.)  \n  * This also feels very similar to making **AIs helpful for AI safety research, cf. [automating alignment research](https://joecarlsmith.com/2025/03/14/ai-for-ai-safety#2-what-is-ai-for-ai-safety).**\n\n  → This is more about raising the **top of the epistemic landscape**\n\n* Other things I care about:  \n  * Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place.  \n  * Vague sense that better **societal epistemics** would help with increasing the **sanity of conversations about AI safety, polarisation around the issue etc.**  \n    * This is broadly about **getting intent alignment** in the first place.\n\n→ These are all more about raising the **bottom of the epistemic landscape**\n\n## Deger Turan:\n\n* Social trust erosion, collective coordination mechanisms are built on a weak assumption  \n* Capacity to entertain truthfulness when it is not instrumental to one's incentive gradient & differing norms upon 'truth will make my sense of security weaker'   \n* Creating contexts where world model divergence can be agreed by both parties enabling a safe disagreement  \n* Improving model reasoning / safer AI by reducing slop, by creating accountability mechanisms through consistency checks & other specifications for healthy epistemics  \n* Decoupling virality with magnitude while living in a short attention span world, curation   \n* I have some confusion re: raising the floor when not all parties are necessarily operating in good faith. I think I need some help think through the ways in which epistemics in the context of incentive misalignment can be used or have defensibility. \n\n## Nathan Young: \n\nWhat would we have got:\n\n* Biden not running, perhaps an open dem primary  \n* No FTX crisis  \n* Less zero sum OpenAI board drama  \n* Many smaller, better choices \n\nWhat decisions might go badly:\n\n* Trump tariffs/ global trade  \n* Taiwan/ US-China conflict  \n* US-China competition  \n* US authoritarianism  \n* AI regulation  \n* EU-China realignment  \n* Automated robots  \n* OpenAI strategy  \n* Anthropic strategy  \n* Google strategy  \n* xAI strategy  \n* AI workforce automation  \n* China strategy  \n* Russia strategy  \n* Moskovitz strategy  \n* Must strategy  \n* Bezos Strategy  \n* Zuckerberg strategy  \n* Tallinn strategy  \n* Altman strategy  \n* Drone warfare  \n* Climate change destabilisation  \n* Democratic trust\n\n## Lincoln Quirk:\n\nWhy do we care about (AI for) epistemics?  \n\\- making the public better informed:  \n\t\\- better able to deal with and process the wealth of information  \n\t\\- better able to express their own interests in government (via voting)  \n\t\\- helping cut through social media distraction / intentional misinformation"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Occasional Use of Unexplained References",
          "description": "References to specific people or projects (e.g., 'Nathan project', 'pol.is') are made without explanation, which may confuse readers unfamiliar with these terms. Briefly introducing such references would improve accessibility.",
          "importance": 50,
          "grade": 50,
          "isValid": false,
          "highlight": {
            "startOffset": 13555,
            "endOffset": 7876,
            "quotedText": ")  \n  * How can we improve Community Notes (or more general knowledge) creation with AI assistants? An AI agent proposing community notes and humans still checking/rating them?  \n  * How can we have something like Community Notes that applies to claims or statements about the future, or which are not concretely knowable?  \n  * \n\n## Ben Wilson (Metaculus)\n\nOverall I believe the most important value of epistemics (and AI epistemics) in general is informing what actions allow us to achieve our goals (i.e. make the world better). If we had perfect AI epistemics I would be excited by the ability for us to take a goal, and get a list of possible actions we could take and the forecasted outcomes of each of those actions (plus background information and forecasts for the 'current lay of the land' for context). This can then inform, not just the risk of AI doing good/bad things (as with current forecasting), but what we can do to raise the chances of 'great' AI outcomes, and lower the changes of 'worse' AI outcomes\n\n## Andreas\n\n* **What are use cases for AI for Epistemics that you most care about?**  \n  * AI policy, broadly construed, must go better  \n    * Avoid gradual disempowerment  \n    * Thoughtful regulation  \n  * Organizational planning & decision-making is weak  \n    * Given an informal long-term goal like improve decision-making for strategic users in government & large orgs, make a plan for a 20-people org that helps them accomplish the goal  \n    * Mapping dependencies, uncertainties, and potential futures  \n    * Understand complex domains with non-trivial feedback loops  \n      * Economic shifts  \n* **What decisions may go badly by default, but could go better if we have the right tech?**  \n  * Resource allocation within orgs  \n    * Investment  \n    * Grant making  \n    * Government budgets  \n  * Policy  \n    * How should AI be deployed in the world?  \n* **What do the later stages of the path to impact look like?**  \n  * AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning  \n  * The technology helps organizations maintain coherence and make sound judgments even as the pace of change accelerates  \n  * Assumption: People are generally well-intentioned \\- no one wants catastrophe \\- and as a result of better tooling things go better in the world overall\n\n## Owen optimistic brainstorm:\n\nWorld peace without world government — sufficiently good tech to help countries understand each others' interests, find mutually desirable bargains, and make sufficient credible commitments, could help us get up towards the Pareto frontier where we avoid (virtually) all costly conflicts\n\nGoing slowly and carefully on AI development — rather than having AI progress accelerate at increasingly breakneck speed, helping actors to understand the basic situation and the desirability of different options could be crucial for facilitating meaningful and robust agreements which restrict AI development to the stuff which we have good reasons for being confident are safe // otherwise beneficial.\n\nBettering democracy — helping people to access key information about what decisions their country might make would actually mean for longer term implications, and having this in turn feed into incentives for decision-makers that better align them with the common good\n\nBuilding safe / aligned AI — better tools for anticipating when systems are actually robustly safe, vs when stronger cases for expectation of safety seem required\n\n## Josh Rosenberg (FRI)\n\nI'm massively confused about important problems, e.g. how to orient to AI risk and what to do about it. Some reasonable people think there's a high probability we effectively lose everything we care about in 2027\\. Some reasonable people think it'll be decades till we have \"AGI\" and it'd be good if it arrived sooner. Which actions are good to take in each of these worlds is very different, and I'd like to figure out what world we're in.\n\nThis also applies to many other problem areas.\n\nE.g., Trump's effect on US democracy. How much of a crisis is it? What are the most effective actions to take in response to ensure that the country and communities we care about are as well off as they can be? How can we preserve high-quality institutions to lead the world in positive directions?\n\nOr, on the positive vision side: the \"Abundance\" agenda seems promising. Is it actually a good idea? What do we do about it?\n\nAI for Epistemics could help us get a lot more clarity on how to think about these problems and what actions to take. I'd like much better information and forecasts on the likely effects of Trump, AI timelines, and the pro's and con's of different Abundance policies, for example. If I had a bunch of time and good resources to dig into these problems, I could get a much better sense for myself. I'm hoping AI will help all of us do this sort of thing collectively.\n\n## Katja Grace\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Put lots of cognitive effort into very clear and good and compelling case for AI risk  \n  * Automate talking to people about AI risk compellingly, possibly using above case as input  \n  * Good game theoretic thinking re arms races, coordination, what is possible. I.e. People are making dumb assumptions about things, e.g. \"can't slow down tech\". Could automate noting errors. So e.g. when someone says that on Twitter, can highlight it and send in response. \\-\\> "
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Strong Emphasis on Coordination and Trust",
          "description": "The recurring focus on coordination, trust, and shared understanding highlights the document's awareness of the social dimensions of epistemics. This thematic consistency is a strength.",
          "importance": 70,
          "grade": 90,
          "isValid": true,
          "highlight": {
            "startOffset": 31269,
            "endOffset": 31576,
            "quotedText": "Trust, is one thing. Our current society seems to be fraught with distrust and motivated reasoning. Can we get a more objectively trustable source of truth? Not only accuracy but with a provenance that gives it an ability to be trusted by humans who are otherwise adversarial. Trust is needed to coordinate."
          }
        },
        {
          "title": "Occasional Use of Unclear Abbreviations",
          "description": "Abbreviations like 'K' (for knowledge) and 'LKM' (Large Knowledge Model) are used without definition. Define such abbreviations on first use to improve clarity.",
          "importance": 50,
          "grade": 50,
          "isValid": false,
          "highlight": {
            "startOffset": 24235,
            "endOffset": 32231,
            "quotedText": "How much \"realised K\" on this question do those who are making the relevant decisions actually have? Similarly, how feasible is cooperation? How valid is this frame of \"capturing the lightcone\"? One reason that we don't have a better hold on these questions is that they are frighteningly complicated and difficult  – I think we could benefit massively from throwing much more cognitive labour at them, and presenting the findings in a way that is hard to disagree with or for any individual to improve upon, other than at a normative/values level. Maybe AI can get us there.\n\n## Lukas Finnveden\n\nwhy is AI for epistemics important/helpful?\n\n* better epistemics generally very good for making all kinds of decisions  \n* I'm specifically concerned about AI potentially causing an intelligence explosion, and us needing better epistemics to handle that. Specifically:  \n  * I'm worried about AI-driven tech R\\&D leading to catastrophes. To help address this, I want AI capabilities to not only be applied to tech R\\&D, but also to be applied to \"reasoning well from limited evidence\" about a number of questions, including how to deal with risks.  \n  * I'm worried about misaligned AI — so I want techniques for eliciting AI capabilities on all kinds of questions, so that AIs don't have better capabilities than we can elicit from them. (Basically: I want good alignment techniques even on hard-to-evaluate questions.)  \n  * Seems great to me if AI-driven improvements in general knowledge and decision-making are more broadly distributed among humans. (I think this leads to better decision-making without significantly accelerating the dangers from new tech or misaligned AI. Also reduces concentration of power.)\n\n## Abram Demski\n\n\\- Why are we doing this? \\-- impact theory brainstorming exercize  \n\t\\- There's the general claim that epistemics has degraded horribly due to social media and the internet, at least in a lot of ways, although epistemics has also gotten better in some specific ways. It seems very plausible that AI is, and will continue to, make this much much worse (due to deepfakes and super-persuasive writing, to give two examples). We wish to reverse this trend, or at least slow the acceleration.  \n\t\\- The high-level reason why epistemics is important is because epistemics is how we make good decisions. So effectively, this is AI for decision-making. However, if we just set out to make tools for decision-making, I have a sense that this would be worse.  \n\t\t\\- One super important aspect of decision-making is that you have to orient towards the right problem. People do much better when they understand the problem before proposing solutions.  \n\t\t\t\\- I have a sense that producing a better shared understanding of the world JUST WOULD result in better things. The connection between epistemics and better decisionmaking is that strong.  \n\t\t\t\t\\- However, there's an element of coordination to this. You have to have common knowledge, not just knowledge. AI for coordination is yet another thing. AI for group epistemics is close to that, though, and again, I have a sense that making \"AI for coordination\" would produce worse effects on the world than \"AI for group epistemics\".  \n\t\t\\- What specific decisions are we hoping to influence?  \n\t\t\t\\- Decisions around existential risk seem super super important. If AI were generally better at epistemics (as a relative, comparative thing \\-- ie we differentially accelerate epistemics) then key decisionmakers (in the government and at AI companies) would see much more clearly what's coming. Everyone wants a better future, so a better future would have a better chance of happening if we could see and navigate.\n\nI don't think I got any further than the supporting documentation for this workshop in articulating reasons and stuff, so I predict that the LLM summary won't be better than what we already know, although I hope it is. \n\n## Brendan\n\nFor us to live together in a caring and just way, we must make decisions about what is important to us together. Is it okay to tell my neighbour to turn their music down? Do we spend money on roads or trains or libraries? Do we regulate AI development and, if so, how? Do we want to spend public money on PEPFAR, or other avenues for reducing global poverty? To make these decisions in a way that is inclusive and democratic, that fosters buy-in and trust, we must begin with a shared sense of how the world is. This shared sense must be grounded in the physical, economic, and social reality we live in.\n\nAs we live together over wider and wider contexts, maintaining a shared sense of the world becomes more difficult and subtle. These wider contexts are facilitated by new technologies, both physical (eg. global manufacturing bases and supply chains) and informational (eg. social media). New technologies, such as AI for epistemics, and help maintain epistemic coherence across these contexts.\n\nMoreover, such technologies allow new avenues for cooperation at larger scales of human involvement. This unlocks new potential for coherence in global politics, scientific discovery, social and technological innovation, and cultural expression, and ultimately the deep joy of being alive and human in the world. \n\n## Isabel: \n\nThe kinds of being right in the past that are most relevant for decision making aren't just about making straightforward predictions about what will happen, but are more nuanced about what kinds of actions one should take to cause those things to happen, and these seem like somewhat different skills. These are harder to evaluate (e.g. because they're counterfactual: either we listened to person a and then we have to evaluate whether things went better or worse than if we didn't, but hard to say what the counterfactual would have been; or we didn't listen to person a and then we have to guess what things would have been like if we did) \n\nA bit adjacent to Neel Nanda's [recent piece](https://www.lesswrong.com/posts/P5zWiPF5cPJZSkiAK/good-research-takes-are-not-sufficient-for-good-strategic) on how \"good research takes aren't sufficient for good strategic takes\" \\- \"good forecasting takes aren't sufficient for good strategic takes\", but maybe some of this is maybe resolvable on the forecasting tech side \\- can we get better at creating and evaluating results from fuzzier questions? Can AI help? (Also can AI help with the whole problem, but this feels small and concrete enough to maybe get some traction on.) Also, some risk of actors building up good track records in order to be listened to, but then choosing to burn that track record to get power or things like that, something something trust \\- a concern for both human actors and e.g. scheming AIs \\- what things look good until they abruptly aren't? \n\nOn a different note, also interested in takes on what to do about and how to engage with actors who have epistemic processes that don't seem to be truth seeking in a comprehensible-to-me way. \n\n## Adam Marblestone:\n\nWhat do we get out of AI for epistemics?\n\nTrust, is one thing. Our current society seems to be fraught with distrust and motivated reasoning. Can we get a more objectively trustable source of truth? Not only accuracy but with a provenance that gives it an ability to be trusted by humans who are otherwise adversarial. Trust is needed to coordinate. Truthfulness is maybe somewhat necessary but seemingly not sufficient for a source being trusted. As a practical matter, on hot political topics I've been going to OpenAI Deep Research.\n\nBut at a certain level I wonder if AI for epistemics and some form of beneficial AGI could be basically the same thing. I asked Eric Drexler what he wanted for d/acc and he said something that sounds a bit like that…\n\n\"  \nHere's my unsurprising take on directed research for \"defensive acceleration\":  \nDevelopment and steering of epistemic tooling, aka [Large Knowledge Models](https://aiprospects.substack.com/p/large-knowledge-models), offers the highest leverage."
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Effective Use of Lists for Clarity",
          "description": "Many contributors use bullet-pointed lists to break down complex ideas into manageable chunks, improving readability and information chunking.",
          "importance": 65,
          "grade": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 426,
            "endOffset": 647,
            "quotedText": "* What are use cases for AI for Epistemics that you most care about?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n* What do the later stages of the path to impact look like?"
          }
        },
        {
          "title": "Occasional Lack of Medium-Appropriate Formatting",
          "description": "Some sections use Markdown formatting (e.g., bold, italics, links), while others do not. Consistent use of formatting would improve visual clarity and navigation.",
          "importance": 45,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 3179,
            "endOffset": 3528,
            "quotedText": "cf**. [metaphilosophy](https://www.alignmentforum.org/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy)** (but also a bunch of other things.)  \n  * This also feels very similar to making **AIs helpful for AI safety research, cf. [automating alignment research](https://joecarlsmith.com/2025/03/14/ai-for-ai-safety#2-what-is-ai-for-ai-safety)."
          }
        },
        {
          "title": "Strong Use of Real-World Scenarios",
          "description": "References to current events (e.g., 'US-China conflict', 'OpenAI board drama', 'Trump tariffs') make the discussion timely and relevant, enhancing engagement.",
          "importance": 70,
          "grade": 90,
          "isValid": true,
          "highlight": {
            "startOffset": 5666,
            "endOffset": 6184,
            "quotedText": "* Trump tariffs/ global trade  \n* Taiwan/ US-China conflict  \n* US-China competition  \n* US authoritarianism  \n* AI regulation  \n* EU-China realignment  \n* Automated robots  \n* OpenAI strategy  \n* Anthropic strategy  \n* Google strategy  \n* xAI strategy  \n* AI workforce automation  \n* China strategy  \n* Russia strategy  \n* Moskovitz strategy  \n* Must strategy  \n* Bezos Strategy  \n* Zuckerberg strategy  \n* Tallinn strategy  \n* Altman strategy  \n* Drone warfare  \n* Climate change destabilisation  \n* Democratic trust"
          }
        },
        {
          "title": "Occasional Use of Unclear Pronouns",
          "description": "Pronouns like 'this', 'it', and 'they' are sometimes used without clear antecedents, which can confuse readers. Specify what these pronouns refer to for greater clarity.",
          "importance": 45,
          "grade": 50,
          "isValid": false,
          "highlight": {
            "startOffset": 3536,
            "endOffset": 4546,
            "quotedText": "This is more about raising the **top of the epistemic landscape**\n\n* Other things I care about:  \n  * Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place.  \n  * Vague sense that better **societal epistemics** would help with increasing the **sanity of conversations about AI safety, polarisation around the issue etc.**  \n    * This is broadly about **getting intent alignment** in the first place.\n\n→ These are all more about raising the **bottom of the epistemic landscape**"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Effective Use of Subheadings for Organization",
          "description": "The use of subheadings (e.g., '## Name:', '## Dan Schwarz:') helps organize the document and allows readers to navigate to sections of interest.",
          "importance": 60,
          "grade": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 742,
            "endOffset": 1388,
            "quotedText": "## Dan Schwarz:\n\n* What are use cases for AI for Epistemics that you most care about?\n\nGoing from \"Realized\" to \"Accessible\" knowledge, e.g. getting good information using the web. Agentic web agents like Deep Research.\n\n* What decisions may go badly by default, but could go better if we have the right tech?\n\nMany billions of small decisions.\n\n* What do the later stages of the path to impact look like?\n\nSuperhuman forecasting at scale.\n\n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nReasonable judgment on tricky questions that's too cheap to meter, better than if you asked a respected colleague to spend many minutes on it."
          }
        },
        {
          "title": "Occasional Use of Unexplained Technical Concepts",
          "description": "Concepts like 'vetopower type', 'centaur mode', and 'defensive acceleration' are introduced without explanation. Briefly defining these terms would improve accessibility.",
          "importance": 50,
          "grade": 50,
          "isValid": false,
          "highlight": {
            "startOffset": 34129,
            "endOffset": 18770,
            "quotedText": "an be hard to fund via for-profit companies. This is in part due to public goods issues, and issues around markets for ideas.\n\n## Rafe\n\nAs ever, I hope this will stop advanced artificial intelligence destroying the world, killing all its people \\&c.\n\nSo I think there are a few paths for impact here. I think there is a world where we can make the most productive thinkers about how to do things go further and better; I'm not so excited about a single-player version of this, as I think there's a pretty narrow window when top-level centaur mode works out\\[1\\]\n\nEveryone is already thinking about AI, and, I suspect, this will only get more acute. I think it's pretty hard for people to confront this and think about it carefully and well. It's scary\\! And there are a bunch of entrenched interests and memeplexes that preserve their status better (myopically) if people only have shallow slogans to think about AI with.\n\nGiven that, I suspect that the global sensemaking machine isn't up for the task of thinking about AI well. I hope we can help the global sensemaking machine upgrade itself. I have some hope that we as a society can come up with good ideas for dealing with AI, and also come to see how they make sense, so we can choose them in a robust way.\n\nOne other path to impact: I think a society that understands things better, including itself and its impacts, is one that is more robust in a bunch of ways. A more epistemically-grounded society is also probably a bit richer, and I think a bigger pie creates more slack for arranging things in sanity-promoting ways. I have more hope in a robust society navigating this stuff well.\n\n\\[1\\]:  Yeah, maybe you can do something with shaving off subskills, but I don't think that's really that doable.\n\n## Dave\n\n* With my consequentialist hat, better epistemics being worth it has to route through improved decisions somewhere down the line. The decisions I particularly care about are those by powerful people wrt AI development and implementation and the effects this has on how the future goes  \n  * But I do feel a bit icky when I have this hat on. There seems to be something very intuitively appealing about (a) pursuing better epistemics for its own sake and (b) democratising epistemic tools for the use and benefit of everyone. I think this basically caches out in more 'raising the floor' type work  \n  * I don't feel like I have a very good way of resolving this tension yet, but am excited to see things pushing on both frontiers  \n* I think forecasting has the potential to be a pretty powerful tool and there's currently a massive undersupply of good forecasts. I'm excited for us to use the wave of increased AI labor to make better quantitative forecasts to inform key decisions. I probably care more about quantitative forecasts than most decision makers and am open to being persuaded that the other, fuzzier parts of epistemics are more valuable to work on. \n\n## Julian\n\nWhat are use cases for AI for Epistemics that you most care about?  \n**Raising the ceiling:** can we use AI to organize information and credibly resolve the most challenging and important questions on which experts still seem to disagree? For example:\n\n* COVID origins  \n* Urban planning and public policy  \n* Lots of advancements in basic science: routing around current disagreements and policy challenges with slam-dunk science might be a pretty strong way forward.\n\n**Raising the floor:** can we use AI to build extremely legibly trustworthy/reliable epistemic infrastructure that actually makes its way into people's lives?\n\n* Making truth more palatable (in a way which doesn't translate into making falsehoods more palatable)  \n* Collective epistemics: connecting people to the others with whom they would have the most productive/enlightening conversations\n\nWhat decisions may go badly by default, but could go better if we have the right tech?\n\n* Lots of stuff in public policy: downstream both of better experts (technocrats), better public (voters/interest groups), and better coordination (collective epistemics)\n\nWhat do the later stages of the path to impact look like?\n\n* I want us to live in a world where everyone is more rational, empathetic, considered in their decision making, conscientious about impact, and interested in coordinating. I feel like that's one of the biggest blockers to all kinds of *long term* progress.\n\nWhat tools do we actually maybe want to build?\n\n* Easily accessible fact checkers / claim refuters  \n  * Powerful AI debaters  \n* Forecasting with explanations: superhuman and at scale  \n* Scientific meta-analysis, systematic review  \n* Ad-hoc / bespoke probabilistic models for individual problems  \n* Bespoke *simulators* / simulation environments  \n* Consistency checking and inconsistency identification  \n  * Arbitrage / market makers in prediction markets?  \n* Using AI to accelerate development of non-AI epistemic tools:  \n  * ?\n\n## Lawrence\n\nThere are already key questions/strategic problems where the truth is murky and we seem to proceed on the basis of limited and biased views. There might well be many more of these in the future. A central example is US-China competition over AI. Some have vibes of \"China is so bad that it's worth risking an existential catastrophe to stop it from capturing a portion of the lightcone\". Others have vibes of \"Cooperation is feasible and desirable, and is our only path to a good outcome\". But how bad is China, really? How much \"realised K\" on this question do those who are making the relevant decisions actually have? Similarly, how feasible is cooperation? How valid is this frame of \"capturing the lightcone\"? One reason that we don't have a better hold on these questions is that they are frighteningly complicated and difficult  – I think we could benefit massively from throwing much more cognitive labour at them, and presenting the findings in a way that is hard to disagree with or for any individual to improve upon, other than at a normative/values level. Maybe AI can get us there.\n\n## Lukas Finnveden\n\nwhy is AI for epistemics important/helpful?\n\n* better epistemics generally very good for making all kinds of decisions  \n* I'm specifically concerned about AI potentially causing an intelligence explosion, and us needing better epistemics to handle that. Specifically:  \n  * I'm worried about AI-driven tech R\\&D leading to catastrophes. To help address this, I want AI capabilities to not only be applied to tech R\\&D, but also to be applied to \"reasoning well from limited evidence\" about a number of questions, including how to deal with risks.  \n  * I'm worried about misaligned AI — so I want techniques for eliciting AI capabilities on all kinds of questions, so that AIs don't have better capabilities than we can elicit from them. (Basically: I want good alignment techniques even on hard-to-evaluate questions.)  \n  * Seems great to me if AI-driven improvements in general knowledge and decision-making are more broadly distributed among humans. (I think this leads to better decision-making without significantly accelerating the dangers from new tech or misaligned AI. Also reduces concentration of power.)\n\n## Abram Demski\n\n\\- Why are we doing this? \\-- impact theory brainstorming exercize  \n\t\\- There's the general claim that epistemics has degraded horribly due to social media and the internet, at least in a lot of ways, although epistemics has also gotten better in some specific ways. It seems very plausible that AI is, and will continue to, make this much much worse (due to deepfakes and super-persuasive writing, to give two examples). We wish to reverse this trend, or at least slow the acceleration.  \n\t\\- The high-level reason why epistemics is important is because epistemics is how we make good decisions. So effectively, this is AI for decision-making. However, if we just set out to make tools for decision-making, I have a sense that this would be worse.  \n\t\t\\- One super important aspect of decision-making is that you have to orient towards the right problem. People do much better when they understand the problem before proposing solutions.  \n\t\t\t\\- I have a sense that producing a better shared understanding of the world JUST WOULD result in better things. The connection between epistemics and better decisionmaking is that strong.  \n\t\t\t\t\\- However, there's an element of coordination to this. You have to have common knowledge, not just knowledge. AI for coordination is yet another thing. AI for group epistemics is close to that, though, and again, I have a sense that making \"AI for coordination\" would produce worse effects on the world than \"AI for group epistemics\".  \n\t\t\\- What specific decisions are we hoping to influence?  \n\t\t\t\\- Decisions around existential risk seem super super important. If AI were generally better at epistemics (as a relative, comparative thing \\-- ie we differentially accelerate epistemics) then key decisionmakers (in the government and at AI companies) would see much more clearly what's coming. Everyone wants a better future, so a better future would have a better chance of happening if we could see and navigate.\n\nI don't think I got any further than the supporting documentation for this workshop in articulating reasons and stuff, so I predict that the LLM summary won't be better than what we already know, although I hope it is. \n\n## Brendan\n\nFor us to live together in a caring and just way, we must make decisions about what is important to us together. Is it okay to tell my neighbour to turn their music down? Do we spend money on roads or trains or libraries? Do we regulate AI development and, if so, how? Do we want to spend public money on PEPFAR, or other avenues for reducing global poverty? To make these decisions in a way that is inclusive and democratic, that fosters buy-in and trust, we must begin with a shared sense of how the world is. This shared sense must be grounded in the physical, economic, and social reality we live in.\n\nAs we live together over wider and wider contexts, maintaining a shared sense of the world becomes more difficult and subtle. These wider contexts are facilitated by new technologies, both physical (eg. global manufacturing bases and supply chains) and informational (eg. social media). New technologies, such as AI for epistemics, and help maintain epistemic coherence across these contexts.\n\nMoreover, such technologies allow new avenues for cooperation at larger scales of human involvement. This unlocks new potential for coherence in global politics, scientific discovery, social and technological innovation, and cultural expression, and ultimately the deep joy of being alive and human in the world. \n\n## Isabel: \n\nThe kinds of being right in the past that are most relevant for decision making aren't just about making straightforward predictions about what will happen, but are more nuanced about what kinds of actions one should take to cause those things to happen, and these seem like somewhat different skills. These are harder to evaluate (e.g. because they're counterfactual: either we listened to person a and then we have to evaluate whether things went better or worse than if we didn't, but hard to say what the counterfactual would have been; or we didn't listen to person a and then we have to guess what things would have been like if we did) \n\nA bit adjacent to Neel Nanda's [recent piece](https://www.lesswrong.com/posts/P5zWiPF5cPJZSkiAK/good-research-takes-are-not-sufficient-for-good-strategic) on how \"good research takes aren't sufficient for good strategic takes\" \\- \"good forecasting takes aren't sufficient for good strategic takes\", but maybe some of this is maybe resolvable on the forecasting tech side \\- can we get better at creating and evaluating results from fuzzier questions? Can AI help? (Also can AI help with the whole problem, but this feels small and concrete enough to maybe get some traction on.) Also, some risk of actors building up good track records in order to be listened to, but then choosing to burn that track record to get power or things like that, something something trust \\- a concern for both human actors and e.g. scheming AIs \\- what things look good until they abruptly aren't? \n\nOn a different note, also interested in takes on what to do about and how to engage with actors who have epistemic processes that don't seem to be truth seeking in a comprehensible-to-me way. \n\n## Adam Marblestone:\n\nWhat do we get out of AI for epistemics?\n\nTrust, is one thing. Our current society seems to be fraught with distrust and motivated reasoning. Can we get a more objectively trustable source of truth? Not only accuracy but with a provenance that gives it an ability to be trusted by humans who are otherwise adversarial. Trust is needed to coordinate. Truthfulness is maybe somewhat necessary but seemingly not sufficient for a source being trusted. As a practical matter, on hot political topics I've been going to OpenAI Deep Research.\n\nBut at a certain level I wonder if AI for epistemics and some form of beneficial AGI could be basically the same thing. I asked Eric Drexler what he wanted for d/acc and he said something that sounds a bit like that…\n\n\"  \nHere's my unsurprising take on directed research for \"defensive acceleration\":  \nDevelopment and steering of epistemic tooling, aka [Large Knowledge Models](https://aiprospects.substack.com/p/large-knowledge-models), offers the highest leverage.\n\nPotential actions:  \n  • Identify and influence entities that could develop and scale necessary foundations.  \n  • Leverage the growing ease of development to take pre-scaling steps.  \n  • Within the scope of \"foundations\", accelerate productive directions for development.  \n  • To support science, ensure grounding of content in cited sources.  \n  • Pursue or promote assembly of appropriate knowledge inputs (beyond what is now used for training data).  \n  • Identify, promote, or implement epistemic tools supported by LKM resources.  \n  • Other kinds of exploration, promotion, preparation, and implementation.  \n  • Ensure that my advice is thoroughly understood.\n\nLKMs can accelerate pretty much everything that one might want, and perhaps tip the balance of applications toward *defensive* acceleration by promoting understanding of the *what and why* of defensive applications.\n\nAddendum:\n\nIn the email I sent a few minutes ago, the basic proposal becomes stronger if one reads \"LKM\" as something like \"large knowledge model functionality\", regardless of implementation.\n\nThe knowledge-model concept (whatever one prefers to call it) should not be tied to my specific technical suggestions. What I've said can be framed as something like \"a discussion of the value, feasibility, and potential scope of a rich epistemic resource\". Bundles of embeddings, etc., are at a level of specificity that may be distracting in the broader context of the meeting.  \n\"\n\nSo that's a strategy assistant AI – which helps humans at large see ways they could act and coordinate especially around big technological or geopolitical strategy issues I guess – but also a certain embodiment of an AGI oracle type system. Maybe the idea then is, how to make a AGI that helps with navigating civilizational strategy, and what is the right form of or interface to that to make it epistemically most useful. \n\nI'd "
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Strong Emphasis on Both Individual and Collective Outcomes",
          "description": "The document balances discussion of individual benefits (e.g., personal decision-making) with collective outcomes (e.g., societal trust, governance), providing a comprehensive perspective.",
          "importance": 65,
          "grade": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 2413,
            "endOffset": 1892,
            "quotedText": "  \n* More effective organizations, getting things done with fewer resources, more focused on the things that benefit everyone  \n    \n* Identification of common ground directions / equilibria / game theoretic strategies that create/incentivize new positive gradients (and not surfacing the bad versions of those…)  \n    \n* Understanding our own goals and values\n\n*Note how it is the intersection of better epistemics / world models + the right power structures / incentives needed for all of these.*\n\n## Ben Goldhaber:\n\n* "
          }
        },
        {
          "title": "Occasional Use of Unclear or Ambiguous Phrasing",
          "description": "Phrases like 'raise the floor', 'raise the ceiling', and 'raising the bottom/top of the epistemic landscape' may be unclear to some readers. Consider briefly explaining these metaphors.",
          "importance": 50,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 3555,
            "endOffset": 4546,
            "quotedText": "raising the **top of the epistemic landscape**\n\n* Other things I care about:  \n  * Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place.  \n  * Vague sense that better **societal epistemics** would help with increasing the **sanity of conversations about AI safety, polarisation around the issue etc.**  \n    * This is broadly about **getting intent alignment** in the first place.\n\n→ These are all more about raising the **bottom of the epistemic landscape**"
          }
        },
        {
          "title": "Effective Use of Forward-Looking Statements",
          "description": "Many contributors discuss the 'later stages of the path to impact', which encourages readers to think about long-term outcomes and strategic planning.",
          "importance": 60,
          "grade": 85,
          "isValid": false,
          "highlight": {
            "startOffset": 588,
            "endOffset": 10271,
            "quotedText": "* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nTemplate:\n\n## Name: \n\nEntry\n\n## Dan Schwarz:\n\n* What are use cases for AI for Epistemics that you most care about?\n\nGoing from \"Realized\" to \"Accessible\" knowledge, e.g. getting good information using the web. Agentic web agents like Deep Research.\n\n* What decisions may go badly by default, but could go better if we have the right tech?\n\nMany billions of small decisions.\n\n* What do the later stages of the path to impact look like?\n\nSuperhuman forecasting at scale.\n\n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nReasonable judgment on tricky questions that's too cheap to meter, better than if you asked a respected colleague to spend many minutes on it.\n\n## Aviv\n\n* Better government governance  \n* Better democratic functioning  \n* Reduced dangerous concentration of power  \n    \n* Healthy polarization / pluralism  \n    \n* More peace, less likelihood of war  \n* Better geopolitics  \n* Better economic incentives  \n    \n* Better AI org governance  \n* Better AI coordination (re. distribution, escalation, etc.)  \n* Better compute allocation (toward what matters for good outcomes)  \n    \n* Better allocation of resources overall  \n* Better lives for people  \n* More effective organizations, getting things done with fewer resources, more focused on the things that benefit everyone  \n    \n* Identification of common ground directions / equilibria / game theoretic strategies that create/incentivize new positive gradients (and not surfacing the bad versions of those…)  \n    \n* Understanding our own goals and values\n\n*Note how it is the intersection of better epistemics / world models + the right power structures / incentives needed for all of these.*\n\n## Ben Goldhaber:\n\n* Forecasting how I should live my life in a period of intense technological change  \n* Guiding decision makers to avoid war with china/ai/everyone  \n* Providing the background context for me to sensemake way more sanely.  \n* Pushing forward the extant knowledge of AI safety agendas  \n* Planning for multi-stage projects that could achieve big impact  \n* Roadmapping to build larger more complex things\n\n## Chi\n\nThere are a few different things I care about:\n\n* I care a lot about humanity and AI, including **super-late-stage AI, getting decision theory** right.  \n  * Consequently but also separately, this relates to the general question of what to do about areas where there is not necessarily a ground truth but that are nonetheless very important to get right, cf**. [metaphilosophy](https://www.alignmentforum.org/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy)** (but also a bunch of other things.)  \n  * This also feels very similar to making **AIs helpful for AI safety research, cf. [automating alignment research](https://joecarlsmith.com/2025/03/14/ai-for-ai-safety#2-what-is-ai-for-ai-safety).**\n\n  → This is more about raising the **top of the epistemic landscape**\n\n* Other things I care about:  \n  * Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place.  \n  * Vague sense that better **societal epistemics** would help with increasing the **sanity of conversations about AI safety, polarisation around the issue etc.**  \n    * This is broadly about **getting intent alignment** in the first place.\n\n→ These are all more about raising the **bottom of the epistemic landscape**\n\n## Deger Turan:\n\n* Social trust erosion, collective coordination mechanisms are built on a weak assumption  \n* Capacity to entertain truthfulness when it is not instrumental to one's incentive gradient & differing norms upon 'truth will make my sense of security weaker'   \n* Creating contexts where world model divergence can be agreed by both parties enabling a safe disagreement  \n* Improving model reasoning / safer AI by reducing slop, by creating accountability mechanisms through consistency checks & other specifications for healthy epistemics  \n* Decoupling virality with magnitude while living in a short attention span world, curation   \n* I have some confusion re: raising the floor when not all parties are necessarily operating in good faith. I think I need some help think through the ways in which epistemics in the context of incentive misalignment can be used or have defensibility. \n\n## Nathan Young: \n\nWhat would we have got:\n\n* Biden not running, perhaps an open dem primary  \n* No FTX crisis  \n* Less zero sum OpenAI board drama  \n* Many smaller, better choices \n\nWhat decisions might go badly:\n\n* Trump tariffs/ global trade  \n* Taiwan/ US-China conflict  \n* US-China competition  \n* US authoritarianism  \n* AI regulation  \n* EU-China realignment  \n* Automated robots  \n* OpenAI strategy  \n* Anthropic strategy  \n* Google strategy  \n* xAI strategy  \n* AI workforce automation  \n* China strategy  \n* Russia strategy  \n* Moskovitz strategy  \n* Must strategy  \n* Bezos Strategy  \n* Zuckerberg strategy  \n* Tallinn strategy  \n* Altman strategy  \n* Drone warfare  \n* Climate change destabilisation  \n* Democratic trust\n\n## Lincoln Quirk:\n\nWhy do we care about (AI for) epistemics?  \n\\- making the public better informed:  \n\t\\- better able to deal with and process the wealth of information  \n\t\\- better able to express their own interests in government (via voting)  \n\t\\- helping cut through social media distraction / intentional misinformation  \n\\- getting governments to implement better policy  \n\t\\- coming up with good policy ideas  \n\t\\- objectively doing cost-benefit analysis  \n\t\\- objectively explaining tradeoffs (vs having to rely on biased humans to explain them)  \n\t\\- flagging bad ideas as bad  \n\t\\- convincing policymakers with compelling writeups  \n\\- success and flourishing for individuals and orgs using these tools  \n\t\\- via helping them think through decisions  \n\t\\- via clearheaded analysis of trade-offs and cost/benefits  \n\t\\- flagging bad ideas as bad\n\n## Jay\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * How can we build something like the positive version of Community Notes: which statements/posts are agreed on by people who often disagree?  \n  * How can we better understand/summarize what others think, e.g. on social media platforms? E.g. if you see a post recommended to you, is this because it's the most extreme take that went viral and isn't representative, or is this a commonly held view? Between different viewpoints, what are the concrete points of actual disagreement (on facts or values) and what are the things widely agreed upon? Let's build viewpoint/debate maps or a similar sensemaking tool that gets adopted/understood by the masses/raises the knowledge floor (in my experience, things like pol.is are hard for regular users to grok)  \n  * How can we improve Community Notes (or more general knowledge) creation with AI assistants? An AI agent proposing community notes and humans still checking/rating them?  \n  * How can we have something like Community Notes that applies to claims or statements about the future, or which are not concretely knowable?  \n  * \n\n## Ben Wilson (Metaculus)\n\nOverall I believe the most important value of epistemics (and AI epistemics) in general is informing what actions allow us to achieve our goals (i.e. make the world better). If we had perfect AI epistemics I would be excited by the ability for us to take a goal, and get a list of possible actions we could take and the forecasted outcomes of each of those actions (plus background information and forecasts for the 'current lay of the land' for context). This can then inform, not just the risk of AI doing good/bad things (as with current forecasting), but what we can do to raise the chances of 'great' AI outcomes, and lower the changes of 'worse' AI outcomes\n\n## Andreas\n\n* **What are use cases for AI for Epistemics that you most care about?**  \n  * AI policy, broadly construed, must go better  \n    * Avoid gradual disempowerment  \n    * Thoughtful regulation  \n  * Organizational planning & decision-making is weak  \n    * Given an informal long-term goal like improve decision-making for strategic users in government & large orgs, make a plan for a 20-people org that helps them accomplish the goal  \n    * Mapping dependencies, uncertainties, and potential futures  \n    * Understand complex domains with non-trivial feedback loops  \n      * Economic shifts  \n* **What decisions may go badly by default, but could go better if we have the right tech?**  \n  * Resource allocation within orgs  \n    * Investment  \n    * Grant making  \n    * Government budgets  \n  * Policy  \n    * How should AI be deployed in the world?  \n* **What do the later stages of the path to impact look like?**  \n  * AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning  \n  * The technology helps organizations maintain coherence and make sound judgments even as the pace of change accelerates"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Occasional Use of Unnecessary Complexity",
          "description": "Some sections introduce complex ideas or terminology without clear justification, which may not be necessary for the intended audience. Simplifying language where possible would improve clarity.",
          "importance": 55,
          "grade": 45,
          "isValid": true,
          "highlight": {
            "startOffset": 4659,
            "endOffset": 5304,
            "quotedText": "Capacity to entertain truthfulness when it is not instrumental to one's incentive gradient & differing norms upon 'truth will make my sense of security weaker'   \n* Creating contexts where world model divergence can be agreed by both parties enabling a safe disagreement  \n* Improving model reasoning / safer AI by reducing slop, by creating accountability mechanisms through consistency checks & other specifications for healthy epistemics  \n* Decoupling virality with magnitude while living in a short attention span world, curation   \n* I have some confusion re: raising the floor when not all parties are necessarily operating in good faith."
          }
        },
        {
          "title": "Strong Emphasis on Actionable Outcomes",
          "description": "Many sections focus on concrete outcomes (e.g., 'better allocation of resources', 'more effective organizations'), which makes the discussion practical and goal-oriented.",
          "importance": 65,
          "grade": 90,
          "isValid": true,
          "highlight": {
            "startOffset": 1826,
            "endOffset": 2017,
            "quotedText": "Better allocation of resources overall  \n* Better lives for people  \n* More effective organizations, getting things done with fewer resources, more focused on the things that benefit everyone"
          }
        },
        {
          "title": "Occasional Use of Unexplained Cultural References",
          "description": "References to specific people (e.g., 'Moskovitz', 'Must', 'Tallinn') are made without context, which may confuse readers unfamiliar with these figures. Briefly introducing such references would improve clarity.",
          "importance": 45,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 5988,
            "endOffset": 6110,
            "quotedText": "* Moskovitz strategy  \n* Must strategy  \n* Bezos Strategy  \n* Zuckerberg strategy  \n* Tallinn strategy  \n* Altman strategy"
          }
        },
        {
          "title": "Effective Use of Conditional Reasoning",
          "description": "Some contributors use conditional statements (e.g., 'If we had perfect AI epistemics...'), which clarify the assumptions underlying their arguments and enhance logical rigor.",
          "importance": 60,
          "grade": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 8408,
            "endOffset": 8897,
            "quotedText": "If we had perfect AI epistemics I would be excited by the ability for us to take a goal, and get a list of possible actions we could take and the forecasted outcomes of each of those actions (plus background information and forecasts for the 'current lay of the land' for context). This can then inform, not just the risk of AI doing good/bad things (as with current forecasting), but what we can do to raise the chances of 'great' AI outcomes, and lower the changes of 'worse' AI outcomes"
          }
        },
        {
          "title": "Occasional Use of Unexplained Technical Processes",
          "description": "Processes like 'consistency checking', 'arbitrage in prediction markets', and 'bundles of embeddings' are mentioned without explanation. Briefly describing these processes would improve accessibility.",
          "importance": 50,
          "grade": 50,
          "isValid": false,
          "highlight": {
            "startOffset": 4934,
            "endOffset": 33693,
            "quotedText": "Improving model reasoning / safer AI by reducing slop, by creating accountability mechanisms through consistency checks & other specifications for healthy epistemics  \n* Decoupling virality with magnitude while living in a short attention span world, curation   \n* I have some confusion re: raising the floor when not all parties are necessarily operating in good faith. I think I need some help think through the ways in which epistemics in the context of incentive misalignment can be used or have defensibility. \n\n## Nathan Young: \n\nWhat would we have got:\n\n* Biden not running, perhaps an open dem primary  \n* No FTX crisis  \n* Less zero sum OpenAI board drama  \n* Many smaller, better choices \n\nWhat decisions might go badly:\n\n* Trump tariffs/ global trade  \n* Taiwan/ US-China conflict  \n* US-China competition  \n* US authoritarianism  \n* AI regulation  \n* EU-China realignment  \n* Automated robots  \n* OpenAI strategy  \n* Anthropic strategy  \n* Google strategy  \n* xAI strategy  \n* AI workforce automation  \n* China strategy  \n* Russia strategy  \n* Moskovitz strategy  \n* Must strategy  \n* Bezos Strategy  \n* Zuckerberg strategy  \n* Tallinn strategy  \n* Altman strategy  \n* Drone warfare  \n* Climate change destabilisation  \n* Democratic trust\n\n## Lincoln Quirk:\n\nWhy do we care about (AI for) epistemics?  \n\\- making the public better informed:  \n\t\\- better able to deal with and process the wealth of information  \n\t\\- better able to express their own interests in government (via voting)  \n\t\\- helping cut through social media distraction / intentional misinformation  \n\\- getting governments to implement better policy  \n\t\\- coming up with good policy ideas  \n\t\\- objectively doing cost-benefit analysis  \n\t\\- objectively explaining tradeoffs (vs having to rely on biased humans to explain them)  \n\t\\- flagging bad ideas as bad  \n\t\\- convincing policymakers with compelling writeups  \n\\- success and flourishing for individuals and orgs using these tools  \n\t\\- via helping them think through decisions  \n\t\\- via clearheaded analysis of trade-offs and cost/benefits  \n\t\\- flagging bad ideas as bad\n\n## Jay\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * How can we build something like the positive version of Community Notes: which statements/posts are agreed on by people who often disagree?  \n  * How can we better understand/summarize what others think, e.g. on social media platforms? E.g. if you see a post recommended to you, is this because it's the most extreme take that went viral and isn't representative, or is this a commonly held view? Between different viewpoints, what are the concrete points of actual disagreement (on facts or values) and what are the things widely agreed upon? Let's build viewpoint/debate maps or a similar sensemaking tool that gets adopted/understood by the masses/raises the knowledge floor (in my experience, things like pol.is are hard for regular users to grok)  \n  * How can we improve Community Notes (or more general knowledge) creation with AI assistants? An AI agent proposing community notes and humans still checking/rating them?  \n  * How can we have something like Community Notes that applies to claims or statements about the future, or which are not concretely knowable?  \n  * \n\n## Ben Wilson (Metaculus)\n\nOverall I believe the most important value of epistemics (and AI epistemics) in general is informing what actions allow us to achieve our goals (i.e. make the world better). If we had perfect AI epistemics I would be excited by the ability for us to take a goal, and get a list of possible actions we could take and the forecasted outcomes of each of those actions (plus background information and forecasts for the 'current lay of the land' for context). This can then inform, not just the risk of AI doing good/bad things (as with current forecasting), but what we can do to raise the chances of 'great' AI outcomes, and lower the changes of 'worse' AI outcomes\n\n## Andreas\n\n* **What are use cases for AI for Epistemics that you most care about?**  \n  * AI policy, broadly construed, must go better  \n    * Avoid gradual disempowerment  \n    * Thoughtful regulation  \n  * Organizational planning & decision-making is weak  \n    * Given an informal long-term goal like improve decision-making for strategic users in government & large orgs, make a plan for a 20-people org that helps them accomplish the goal  \n    * Mapping dependencies, uncertainties, and potential futures  \n    * Understand complex domains with non-trivial feedback loops  \n      * Economic shifts  \n* **What decisions may go badly by default, but could go better if we have the right tech?**  \n  * Resource allocation within orgs  \n    * Investment  \n    * Grant making  \n    * Government budgets  \n  * Policy  \n    * How should AI be deployed in the world?  \n* **What do the later stages of the path to impact look like?**  \n  * AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning  \n  * The technology helps organizations maintain coherence and make sound judgments even as the pace of change accelerates  \n  * Assumption: People are generally well-intentioned \\- no one wants catastrophe \\- and as a result of better tooling things go better in the world overall\n\n## Owen optimistic brainstorm:\n\nWorld peace without world government — sufficiently good tech to help countries understand each others' interests, find mutually desirable bargains, and make sufficient credible commitments, could help us get up towards the Pareto frontier where we avoid (virtually) all costly conflicts\n\nGoing slowly and carefully on AI development — rather than having AI progress accelerate at increasingly breakneck speed, helping actors to understand the basic situation and the desirability of different options could be crucial for facilitating meaningful and robust agreements which restrict AI development to the stuff which we have good reasons for being confident are safe // otherwise beneficial.\n\nBettering democracy — helping people to access key information about what decisions their country might make would actually mean for longer term implications, and having this in turn feed into incentives for decision-makers that better align them with the common good\n\nBuilding safe / aligned AI — better tools for anticipating when systems are actually robustly safe, vs when stronger cases for expectation of safety seem required\n\n## Josh Rosenberg (FRI)\n\nI'm massively confused about important problems, e.g. how to orient to AI risk and what to do about it. Some reasonable people think there's a high probability we effectively lose everything we care about in 2027\\. Some reasonable people think it'll be decades till we have \"AGI\" and it'd be good if it arrived sooner. Which actions are good to take in each of these worlds is very different, and I'd like to figure out what world we're in.\n\nThis also applies to many other problem areas.\n\nE.g., Trump's effect on US democracy. How much of a crisis is it? What are the most effective actions to take in response to ensure that the country and communities we care about are as well off as they can be? How can we preserve high-quality institutions to lead the world in positive directions?\n\nOr, on the positive vision side: the \"Abundance\" agenda seems promising. Is it actually a good idea? What do we do about it?\n\nAI for Epistemics could help us get a lot more clarity on how to think about these problems and what actions to take. I'd like much better information and forecasts on the likely effects of Trump, AI timelines, and the pro's and con's of different Abundance policies, for example. If I had a bunch of time and good resources to dig into these problems, I could get a much better sense for myself. I'm hoping AI will help all of us do this sort of thing collectively.\n\n## Katja Grace\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Put lots of cognitive effort into very clear and good and compelling case for AI risk  \n  * Automate talking to people about AI risk compellingly, possibly using above case as input  \n  * Good game theoretic thinking re arms races, coordination, what is possible. I.e. People are making dumb assumptions about things, e.g. \"can't slow down tech\". Could automate noting errors. So e.g. when someone says that on Twitter, can highlight it and send in response. \\-\\> good thinking via better weapons for debate (similar to Nathan project)  \n  * Specific idea: very quantified and competitive debate site, that can be cited re which views are winning, where AI does a lot of the cognitive labor in places no people are paying attention, but as a part of it gets cruxy, people pay attention and vote/bet  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Poor decision making around AI: e.g. 'racing' because of vague heuristics instead of ever doing calc on costs and benefits given specific risks  \n  * Failure to coordinate on AI: building AI systems that are dangerous, due to expecting others to do so\n\n## Joel Lehman\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Raising the bottom (i.e. helping improve societal epistemics) – in particular that in theory political decisions (at least political decision-makers) are somewhat downstream of societal epistemics  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Having weird political leaders and awful public epistemics at critical junctures seems to unnecessarily spike catastrophic/existential risk probabilities. In theory, helping uplevel or at least maintain current public epistemic health (especially as it relates to civic sensemaking) might help influence the quality of decision-makers and concrete decisions made at high-stakes moments  \n  * The shape of the tech is not clear, or probably requires multiple flavors of interventions. E.g. if LLM assistants by default help cultivate civic virtues and on the margin support critical thinking or whatever underlying skills and knowledge help a person up-level their epistemic habits / health (or at least if LLM assistants don't contribute to atrophy/harm); or somehow convincing social media networks to care about this (e.g. community notes but things that go beyond); similarly somehow help w/ important social goods like 'societal trust' and 'trust in your neighbors'  \n* What do the later stages of the path to impact look like?  \n  * \\[didn't get to this\\]  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * Healthier democratic society, better agreement on consensus reality, higher trust in society, better politicians, better concrete decisions in high-stakes moments \n\n## Austin Chen\n\n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * **Evaluator**: More agreement (or understanding of disagreement) of what orgs & people are doing good work (eg for making grants in AI safety)  \n  * **Oracle**: Instant trustworthy forecasts on any question you ask  \n  * **Planner**: Guidance on \"what are the 3 most important things I should do this day/week/month\"  \n  * **Representative**: Simulate the responses of other parties (people, orgs/corps, governments, LLM agents), for discussions, contracts, trade\n\n## Janna\n\n* Why do we care about (AI for) epistemics?  \n  * Noise to signal ratio could be much higher  \n  * AI (mis)information churned out at the price of almost zero  \n* What are use cases for AI for Epistemics that you most care about?  \n  * Solving prediction markets--is there a way to scale up the bots, even if they are wrong and lossy, to improve prediction market accuracy?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * US-China relations  \n  * Trump tariffs maybe  \n  * Immigration rules in the US  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * More accurate predictions to…  \n    * Reduce business uncertainty  \n    * Make it easier to evaluate life decisions  \n  * Forestalling bad decisions made on imperfect information\n\n## Owain\n\n1. Maybe having humans understanding the world better is good in itself. Lots of people read popular science for this reason. Maybe it's good for AIs as well  \n2. Very broadly, it seems that better world understanding and decision making enables people to make their lives better in myriad ways. (To substantiate this, one could try to separate out the effects of better world understanding from the effects of better realized technology. E.g. Looking at capital-poor countries that can still benefit from scientific knowledge, economics, etc.)  \n3. More specifically, if AI progress is fast, there will be lots of important decisions to be made in quick calendar time. Exploiting AI to improve those decisions seems valuable. Current decision making institutions do not seem well set up to adapt really quickly. (A specific concern is dangers from AI takeover).   \n4. Why now? Why this group of people? We haven't really had tech that could do most epistemic tasks until recently. Now we have AI that can do an increasing proportion of the sub-tasks of epistemic processes. This will allow us to delegate epistemic tasks to AI.  \n   1. It's likely that people do this delegation, even if it's somewhat misguided and potentially bad in the long-term. Can we make this go well?  \n   2. This potentially massively expands our epistemic capacities. How can we use this well?  \n   3. Epistemic tech/institutions/advances can be hard to fund via for-profit companies. This is in part due to public goods issues, and issues around markets for ideas.\n\n## Rafe\n\nAs ever, I hope this will stop advanced artificial intelligence destroying the world, killing all its people \\&c.\n\nSo I think there are a few paths for impact here. I think there is a world where we can make the most productive thinkers about how to do things go further and better; I'm not so excited about a single-player version of this, as I think there's a pretty narrow window when top-level centaur mode works out\\[1\\]\n\nEveryone is already thinking about AI, and, I suspect, this will only get more acute. I think it's pretty hard for people to confront this and think about it carefully and well. It's scary\\! And there are a bunch of entrenched interests and memeplexes that preserve their status better (myopically) if people only have shallow slogans to think about AI with.\n\nGiven that, I suspect that the global sensemaking machine isn't up for the task of thinking about AI well. I hope we can help the global sensemaking machine upgrade itself. I have some hope that we as a society can come up with good ideas for dealing with AI, and also come to see how they make sense, so we can choose them in a robust way.\n\nOne other path to impact: I think a society that understands things better, including itself and its impacts, is one that is more robust in a bunch of ways. A more epistemically-grounded society is also probably a bit richer, and I think a bigger pie creates more slack for arranging things in sanity-promoting ways. I have more hope in a robust society navigating this stuff well.\n\n\\[1\\]:  Yeah, maybe you can do something with shaving off subskills, but I don't think that's really that doable.\n\n## Dave\n\n* With my consequentialist hat, better epistemics being worth it has to route through improved decisions somewhere down the line. The decisions I particularly care about are those by powerful people wrt AI development and implementation and the effects this has on how the future goes  \n  * But I do feel a bit icky when I have this hat on. There seems to be something very intuitively appealing about (a) pursuing better epistemics for its own sake and (b) democratising epistemic tools for the use and benefit of everyone. I think this basically caches out in more 'raising the floor' type work  \n  * I don't feel like I have a very good way of resolving this tension yet, but am excited to see things pushing on both frontiers  \n* I think forecasting has the potential to be a pretty powerful tool and there's currently a massive undersupply of good forecasts. I'm excited for us to use the wave of increased AI labor to make better quantitative forecasts to inform key decisions. I probably care more about quantitative forecasts than most decision makers and am open to being persuaded that the other, fuzzier parts of epistemics are more valuable to work on. \n\n## Julian\n\nWhat are use cases for AI for Epistemics that you most care about?  \n**Raising the ceiling:** can we use AI to organize information and credibly resolve the most challenging and important questions on which experts still seem to disagree? For example:\n\n* COVID origins  \n* Urban planning and public policy  \n* Lots of advancements in basic science: routing around current disagreements and policy challenges with slam-dunk science might be a pretty strong way forward.\n\n**Raising the floor:** can we use AI to build extremely legibly trustworthy/reliable epistemic infrastructure that actually makes its way into people's lives?\n\n* Making truth more palatable (in a way which doesn't translate into making falsehoods more palatable)  \n* Collective epistemics: connecting people to the others with whom they would have the most productive/enlightening conversations\n\nWhat decisions may go badly by default, but could go better if we have the right tech?\n\n* Lots of stuff in public policy: downstream both of better experts (technocrats), better public (voters/interest groups), and better coordination (collective epistemics)\n\nWhat do the later stages of the path to impact look like?\n\n* I want us to live in a world where everyone is more rational, empathetic, considered in their decision making, conscientious about impact, and interested in coordinating. I feel like that's one of the biggest blockers to all kinds of *long term* progress.\n\nWhat tools do we actually maybe want to build?\n\n* Easily accessible fact checkers / claim refuters  \n  * Powerful AI debaters  \n* Forecasting with explanations: superhuman and at scale  \n* Scientific meta-analysis, systematic review  \n* Ad-hoc / bespoke probabilistic models for individual problems  \n* Bespoke *simulators* / simulation environments  \n* Consistency checking and inconsistency identification  \n  * Arbitrage / market makers in prediction markets?  \n* Using AI to accelerate development of non-AI epistemic tools:  \n  * ?\n\n## Lawrence\n\nThere are already key questions/strategic problems where the truth is murky and we seem to proceed on the basis of limited and biased views. There might well be many more of these in the future. A central example is US-China competition over AI. Some have vibes of \"China is so bad that it's worth risking an existential catastrophe to stop it from capturing a portion of the lightcone\". Others have vibes of \"Cooperation is feasible and desirable, and is our only path to a good outcome\". But how bad is China, really? How much \"realised K\" on this question do those who are making the relevant decisions actually have? Similarly, how feasible is cooperation? How valid is this frame of \"capturing the lightcone\"? One reason that we don't have a better hold on these questions is that they are frighteningly complicated and difficult  – I think we could benefit massively from throwing much more cognitive labour at them, and presenting the findings in a way that is hard to disagree with or for any individual to improve upon, other than at a normative/values level. Maybe AI can get us there.\n\n## Lukas Finnveden\n\nwhy is AI for epistemics important/helpful?\n\n* better epistemics generally very good for making all kinds of decisions  \n* I'm specifically concerned about AI potentially causing an intelligence explosion, and us needing better epistemics to handle that. Specifically:  \n  * I'm worried about AI-driven tech R\\&D leading to catastrophes. To help address this, I want AI capabilities to not only be applied to tech R\\&D, but also to be applied to \"reasoning well from limited evidence\" about a number of questions, including how to deal with risks.  \n  * I'm worried about misaligned AI — so I want techniques for eliciting AI capabilities on all kinds of questions, so that AIs don't have better capabilities than we can elicit from them. (Basically: I want good alignment techniques even on hard-to-evaluate questions.)  \n  * Seems great to me if AI-driven improvements in general knowledge and decision-making are more broadly distributed among humans. (I think this leads to better decision-making without significantly accelerating the dangers from new tech or misaligned AI. Also reduces concentration of power.)\n\n## Abram Demski\n\n\\- Why are we doing this? \\-- impact theory brainstorming exercize  \n\t\\- There's the general claim that epistemics has degraded horribly due to social media and the internet, at least in a lot of ways, although epistemics has also gotten better in some specific ways. It seems very plausible that AI is, and will continue to, make this much much worse (due to deepfakes and super-persuasive writing, to give two examples). We wish to reverse this trend, or at least slow the acceleration.  \n\t\\- The high-level reason why epistemics is important is because epistemics is how we make good decisions. So effectively, this is AI for decision-making. However, if we just set out to make tools for decision-making, I have a sense that this would be worse.  \n\t\t\\- One super important aspect of decision-making is that you have to orient towards the right problem. People do much better when they understand the problem before proposing solutions.  \n\t\t\t\\- I have a sense that producing a better shared understanding of the world JUST WOULD result in better things. The connection between epistemics and better decisionmaking is that strong.  \n\t\t\t\t\\- However, there's an element of coordination to this. You have to have common knowledge, not just knowledge. AI for coordination is yet another thing. AI for group epistemics is close to that, though, and again, I have a sense that making \"AI for coordination\" would produce worse effects on the world than \"AI for group epistemics\".  \n\t\t\\- What specific decisions are we hoping to influence?  \n\t\t\t\\- Decisions around existential risk seem super super important. If AI were generally better at epistemics (as a relative, comparative thing \\-- ie we differentially accelerate epistemics) then key decisionmakers (in the government and at AI companies) would see much more clearly what's coming. Everyone wants a better future, so a better future would have a better chance of happening if we could see and navigate.\n\nI don't think I got any further than the supporting documentation for this workshop in articulating reasons and stuff, so I predict that the LLM summary won't be better than what we already know, although I hope it is. \n\n## Brendan\n\nFor us to live together in a caring and just way, we must make decisions about what is important to us together. Is it okay to tell my neighbour to turn their music down? Do we spend money on roads or trains or libraries? Do we regulate AI development and, if so, how? Do we want to spend public money on PEPFAR, or other avenues for reducing global poverty? To make these decisions in a way that is inclusive and democratic, that fosters buy-in and trust, we must begin with a shared sense of how the world is. This shared sense must be grounded in the physical, economic, and social reality we live in.\n\nAs we live together over wider and wider contexts, maintaining a shared sense of the world becomes more difficult and subtle. These wider contexts are facilitated by new technologies, both physical (eg. global manufacturing bases and supply chains) and informational (eg. social media). New technologies, such as AI for epistemics, and help maintain epistemic coherence across these contexts.\n\nMoreover, such technologies allow new avenues for cooperation at larger scales of human involvement. This unlocks new potential for coherence in global politics, scientific discovery, social and technological innovation, and cultural expression, and ultimately the deep joy of being alive and human in the world. \n\n## Isabel: \n\nThe kinds of being right in the past that are most relevant for decision making aren't just about making straightforward predictions about what will happen, but are more nuanced about what kinds of actions one should take to cause those things to happen, and these seem like somewhat different skills. These are harder to evaluate (e.g. because they're counterfactual: either we listened to person a and then we have to evaluate whether things went better or worse than if we didn't, but hard to say what the counterfactual would have been; or we didn't listen to person a and then we have to guess what things would have been like if we did) \n\nA bit adjacent to Neel Nanda's [recent piece](https://www.lesswrong.com/posts/P5zWiPF5cPJZSkiAK/good-research-takes-are-not-sufficient-for-good-strategic) on how \"good research takes aren't sufficient for good strategic takes\" \\- \"good forecasting takes aren't sufficient for good strategic takes\", but maybe some of this is maybe resolvable on the forecasting tech side \\- can we get better at creating and evaluating results from fuzzier questions? Can AI help? (Also can AI help with the whole problem, but this feels small and concrete enough to maybe get some traction on.) Also, some risk of actors building up good track records in order to be listened to, but then choosing to burn that track record to get power or things like that, something something trust \\- a concern for both human actors and e.g. scheming AIs \\- what things look good until they abruptly aren't? \n\nOn a different note, also interested in takes on what to do about and how to engage with actors who have epistemic processes that don't seem to be truth seeking in a comprehensible-to-me way. \n\n## Adam Marblestone:\n\nWhat do we get out of AI for epistemics?\n\nTrust, is one thing. Our current society seems to be fraught with distrust and motivated reasoning. Can we get a more objectively trustable source of truth? Not only accuracy but with a provenance that gives it an ability to be trusted by humans who are otherwise adversarial. Trust is needed to coordinate. Truthfulness is maybe somewhat necessary but seemingly not sufficient for a source being trusted. As a practical matter, on hot political topics I've been going to OpenAI Deep Research.\n\nBut at a certain level I wonder if AI for epistemics and some form of beneficial AGI could be basically the same thing. I asked Eric Drexler what he wanted for d/acc and he said something that sounds a bit like that…\n\n\"  \nHere's my unsurprising take on directed research for \"defensive acceleration\":  \nDevelopment and steering of epistemic tooling, aka [Large Knowledge Models](https://aiprospects.substack.com/p/large-knowledge-models), offers the highest leverage.\n\nPotential actions:  \n  • Identify and influence entities that could develop and scale necessary foundations.  \n  • Leverage the growing ease of development to take pre-scaling steps.  \n  • Within the scope of \"foundations\", accelerate productive directions for development.  \n  • To support science, ensure grounding of content in cited sources.  \n  • Pursue or promote assembly of appropriate knowledge inputs (beyond what is now used for training data).  \n  • Identify, promote, or implement epistemic tools supported by LKM resources.  \n  • Other kinds of exploration, promotion, preparation, and implementation.  \n  • Ensure that my advice is thoroughly understood.\n\nLKMs can accelerate pretty much everything that one might want, and perhaps tip the balance of applications toward *defensive* acceleration by promoting understanding of the *what and why* of defensive applications.\n\nAddendum:\n\nIn the email I sent a few minutes ago, the basic proposal becomes stronger if one reads \"LKM\" as something like \"large knowledge model functionality\", regardless of implementation.\n\nThe knowledge-model concept (whatever one prefers to call it) should not be tied to my specific technical suggestions. What I've said can be framed as something like \"a discussion of the value, feasibility, and potential scope of a rich epistemic resource\". Bundles of embeddings, etc., are at a level of specificity that may be distracting in the broader context of the meeting."
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Strong Emphasis on Democratic and Inclusive Outcomes",
          "description": "Several contributors highlight the importance of democratic functioning, inclusivity, and public engagement, which aligns the document with broader societal values.",
          "importance": 70,
          "grade": 90,
          "isValid": false,
          "highlight": {
            "startOffset": 1434,
            "endOffset": 28669,
            "quotedText": "Better democratic functioning  \n* Reduced dangerous concentration of power  \n    \n* Healthy polarization / pluralism  \n    \n* More peace, less likelihood of war  \n* Better geopolitics  \n* Better economic incentives  \n    \n* Better AI org governance  \n* Better AI coordination (re. distribution, escalation, etc.)  \n* Better compute allocation (toward what matters for good outcomes)  \n    \n* Better allocation of resources overall  \n* Better lives for people  \n* More effective organizations, getting things done with fewer resources, more focused on the things that benefit everyone  \n    \n* Identification of common ground directions / equilibria / game theoretic strategies that create/incentivize new positive gradients (and not surfacing the bad versions of those…)  \n    \n* Understanding our own goals and values\n\n*Note how it is the intersection of better epistemics / world models + the right power structures / incentives needed for all of these.*\n\n## Ben Goldhaber:\n\n* Forecasting how I should live my life in a period of intense technological change  \n* Guiding decision makers to avoid war with china/ai/everyone  \n* Providing the background context for me to sensemake way more sanely.  \n* Pushing forward the extant knowledge of AI safety agendas  \n* Planning for multi-stage projects that could achieve big impact  \n* Roadmapping to build larger more complex things\n\n## Chi\n\nThere are a few different things I care about:\n\n* I care a lot about humanity and AI, including **super-late-stage AI, getting decision theory** right.  \n  * Consequently but also separately, this relates to the general question of what to do about areas where there is not necessarily a ground truth but that are nonetheless very important to get right, cf**. [metaphilosophy](https://www.alignmentforum.org/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy)** (but also a bunch of other things.)  \n  * This also feels very similar to making **AIs helpful for AI safety research, cf. [automating alignment research](https://joecarlsmith.com/2025/03/14/ai-for-ai-safety#2-what-is-ai-for-ai-safety).**\n\n  → This is more about raising the **top of the epistemic landscape**\n\n* Other things I care about:  \n  * Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place.  \n  * Vague sense that better **societal epistemics** would help with increasing the **sanity of conversations about AI safety, polarisation around the issue etc.**  \n    * This is broadly about **getting intent alignment** in the first place.\n\n→ These are all more about raising the **bottom of the epistemic landscape**\n\n## Deger Turan:\n\n* Social trust erosion, collective coordination mechanisms are built on a weak assumption  \n* Capacity to entertain truthfulness when it is not instrumental to one's incentive gradient & differing norms upon 'truth will make my sense of security weaker'   \n* Creating contexts where world model divergence can be agreed by both parties enabling a safe disagreement  \n* Improving model reasoning / safer AI by reducing slop, by creating accountability mechanisms through consistency checks & other specifications for healthy epistemics  \n* Decoupling virality with magnitude while living in a short attention span world, curation   \n* I have some confusion re: raising the floor when not all parties are necessarily operating in good faith. I think I need some help think through the ways in which epistemics in the context of incentive misalignment can be used or have defensibility. \n\n## Nathan Young: \n\nWhat would we have got:\n\n* Biden not running, perhaps an open dem primary  \n* No FTX crisis  \n* Less zero sum OpenAI board drama  \n* Many smaller, better choices \n\nWhat decisions might go badly:\n\n* Trump tariffs/ global trade  \n* Taiwan/ US-China conflict  \n* US-China competition  \n* US authoritarianism  \n* AI regulation  \n* EU-China realignment  \n* Automated robots  \n* OpenAI strategy  \n* Anthropic strategy  \n* Google strategy  \n* xAI strategy  \n* AI workforce automation  \n* China strategy  \n* Russia strategy  \n* Moskovitz strategy  \n* Must strategy  \n* Bezos Strategy  \n* Zuckerberg strategy  \n* Tallinn strategy  \n* Altman strategy  \n* Drone warfare  \n* Climate change destabilisation  \n* Democratic trust\n\n## Lincoln Quirk:\n\nWhy do we care about (AI for) epistemics?  \n\\- making the public better informed:  \n\t\\- better able to deal with and process the wealth of information  \n\t\\- better able to express their own interests in government (via voting)  \n\t\\- helping cut through social media distraction / intentional misinformation  \n\\- getting governments to implement better policy  \n\t\\- coming up with good policy ideas  \n\t\\- objectively doing cost-benefit analysis  \n\t\\- objectively explaining tradeoffs (vs having to rely on biased humans to explain them)  \n\t\\- flagging bad ideas as bad  \n\t\\- convincing policymakers with compelling writeups  \n\\- success and flourishing for individuals and orgs using these tools  \n\t\\- via helping them think through decisions  \n\t\\- via clearheaded analysis of trade-offs and cost/benefits  \n\t\\- flagging bad ideas as bad\n\n## Jay\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * How can we build something like the positive version of Community Notes: which statements/posts are agreed on by people who often disagree?  \n  * How can we better understand/summarize what others think, e.g. on social media platforms? E.g. if you see a post recommended to you, is this because it's the most extreme take that went viral and isn't representative, or is this a commonly held view? Between different viewpoints, what are the concrete points of actual disagreement (on facts or values) and what are the things widely agreed upon? Let's build viewpoint/debate maps or a similar sensemaking tool that gets adopted/understood by the masses/raises the knowledge floor (in my experience, things like pol.is are hard for regular users to grok)  \n  * How can we improve Community Notes (or more general knowledge) creation with AI assistants? An AI agent proposing community notes and humans still checking/rating them?  \n  * How can we have something like Community Notes that applies to claims or statements about the future, or which are not concretely knowable?  \n  * \n\n## Ben Wilson (Metaculus)\n\nOverall I believe the most important value of epistemics (and AI epistemics) in general is informing what actions allow us to achieve our goals (i.e. make the world better). If we had perfect AI epistemics I would be excited by the ability for us to take a goal, and get a list of possible actions we could take and the forecasted outcomes of each of those actions (plus background information and forecasts for the 'current lay of the land' for context). This can then inform, not just the risk of AI doing good/bad things (as with current forecasting), but what we can do to raise the chances of 'great' AI outcomes, and lower the changes of 'worse' AI outcomes\n\n## Andreas\n\n* **What are use cases for AI for Epistemics that you most care about?**  \n  * AI policy, broadly construed, must go better  \n    * Avoid gradual disempowerment  \n    * Thoughtful regulation  \n  * Organizational planning & decision-making is weak  \n    * Given an informal long-term goal like improve decision-making for strategic users in government & large orgs, make a plan for a 20-people org that helps them accomplish the goal  \n    * Mapping dependencies, uncertainties, and potential futures  \n    * Understand complex domains with non-trivial feedback loops  \n      * Economic shifts  \n* **What decisions may go badly by default, but could go better if we have the right tech?**  \n  * Resource allocation within orgs  \n    * Investment  \n    * Grant making  \n    * Government budgets  \n  * Policy  \n    * How should AI be deployed in the world?  \n* **What do the later stages of the path to impact look like?**  \n  * AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning  \n  * The technology helps organizations maintain coherence and make sound judgments even as the pace of change accelerates  \n  * Assumption: People are generally well-intentioned \\- no one wants catastrophe \\- and as a result of better tooling things go better in the world overall\n\n## Owen optimistic brainstorm:\n\nWorld peace without world government — sufficiently good tech to help countries understand each others' interests, find mutually desirable bargains, and make sufficient credible commitments, could help us get up towards the Pareto frontier where we avoid (virtually) all costly conflicts\n\nGoing slowly and carefully on AI development — rather than having AI progress accelerate at increasingly breakneck speed, helping actors to understand the basic situation and the desirability of different options could be crucial for facilitating meaningful and robust agreements which restrict AI development to the stuff which we have good reasons for being confident are safe // otherwise beneficial.\n\nBettering democracy — helping people to access key information about what decisions their country might make would actually mean for longer term implications, and having this in turn feed into incentives for decision-makers that better align them with the common good\n\nBuilding safe / aligned AI — better tools for anticipating when systems are actually robustly safe, vs when stronger cases for expectation of safety seem required\n\n## Josh Rosenberg (FRI)\n\nI'm massively confused about important problems, e.g. how to orient to AI risk and what to do about it. Some reasonable people think there's a high probability we effectively lose everything we care about in 2027\\. Some reasonable people think it'll be decades till we have \"AGI\" and it'd be good if it arrived sooner. Which actions are good to take in each of these worlds is very different, and I'd like to figure out what world we're in.\n\nThis also applies to many other problem areas.\n\nE.g., Trump's effect on US democracy. How much of a crisis is it? What are the most effective actions to take in response to ensure that the country and communities we care about are as well off as they can be? How can we preserve high-quality institutions to lead the world in positive directions?\n\nOr, on the positive vision side: the \"Abundance\" agenda seems promising. Is it actually a good idea? What do we do about it?\n\nAI for Epistemics could help us get a lot more clarity on how to think about these problems and what actions to take. I'd like much better information and forecasts on the likely effects of Trump, AI timelines, and the pro's and con's of different Abundance policies, for example. If I had a bunch of time and good resources to dig into these problems, I could get a much better sense for myself. I'm hoping AI will help all of us do this sort of thing collectively.\n\n## Katja Grace\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Put lots of cognitive effort into very clear and good and compelling case for AI risk  \n  * Automate talking to people about AI risk compellingly, possibly using above case as input  \n  * Good game theoretic thinking re arms races, coordination, what is possible. I.e. People are making dumb assumptions about things, e.g. \"can't slow down tech\". Could automate noting errors. So e.g. when someone says that on Twitter, can highlight it and send in response. \\-\\> good thinking via better weapons for debate (similar to Nathan project)  \n  * Specific idea: very quantified and competitive debate site, that can be cited re which views are winning, where AI does a lot of the cognitive labor in places no people are paying attention, but as a part of it gets cruxy, people pay attention and vote/bet  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Poor decision making around AI: e.g. 'racing' because of vague heuristics instead of ever doing calc on costs and benefits given specific risks  \n  * Failure to coordinate on AI: building AI systems that are dangerous, due to expecting others to do so\n\n## Joel Lehman\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Raising the bottom (i.e. helping improve societal epistemics) – in particular that in theory political decisions (at least political decision-makers) are somewhat downstream of societal epistemics  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Having weird political leaders and awful public epistemics at critical junctures seems to unnecessarily spike catastrophic/existential risk probabilities. In theory, helping uplevel or at least maintain current public epistemic health (especially as it relates to civic sensemaking) might help influence the quality of decision-makers and concrete decisions made at high-stakes moments  \n  * The shape of the tech is not clear, or probably requires multiple flavors of interventions. E.g. if LLM assistants by default help cultivate civic virtues and on the margin support critical thinking or whatever underlying skills and knowledge help a person up-level their epistemic habits / health (or at least if LLM assistants don't contribute to atrophy/harm); or somehow convincing social media networks to care about this (e.g. community notes but things that go beyond); similarly somehow help w/ important social goods like 'societal trust' and 'trust in your neighbors'  \n* What do the later stages of the path to impact look like?  \n  * \\[didn't get to this\\]  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * Healthier democratic society, better agreement on consensus reality, higher trust in society, better politicians, better concrete decisions in high-stakes moments \n\n## Austin Chen\n\n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * **Evaluator**: More agreement (or understanding of disagreement) of what orgs & people are doing good work (eg for making grants in AI safety)  \n  * **Oracle**: Instant trustworthy forecasts on any question you ask  \n  * **Planner**: Guidance on \"what are the 3 most important things I should do this day/week/month\"  \n  * **Representative**: Simulate the responses of other parties (people, orgs/corps, governments, LLM agents), for discussions, contracts, trade\n\n## Janna\n\n* Why do we care about (AI for) epistemics?  \n  * Noise to signal ratio could be much higher  \n  * AI (mis)information churned out at the price of almost zero  \n* What are use cases for AI for Epistemics that you most care about?  \n  * Solving prediction markets--is there a way to scale up the bots, even if they are wrong and lossy, to improve prediction market accuracy?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * US-China relations  \n  * Trump tariffs maybe  \n  * Immigration rules in the US  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * More accurate predictions to…  \n    * Reduce business uncertainty  \n    * Make it easier to evaluate life decisions  \n  * Forestalling bad decisions made on imperfect information\n\n## Owain\n\n1. Maybe having humans understanding the world better is good in itself. Lots of people read popular science for this reason. Maybe it's good for AIs as well  \n2. Very broadly, it seems that better world understanding and decision making enables people to make their lives better in myriad ways. (To substantiate this, one could try to separate out the effects of better world understanding from the effects of better realized technology. E.g. Looking at capital-poor countries that can still benefit from scientific knowledge, economics, etc.)  \n3. More specifically, if AI progress is fast, there will be lots of important decisions to be made in quick calendar time. Exploiting AI to improve those decisions seems valuable. Current decision making institutions do not seem well set up to adapt really quickly. (A specific concern is dangers from AI takeover).   \n4. Why now? Why this group of people? We haven't really had tech that could do most epistemic tasks until recently. Now we have AI that can do an increasing proportion of the sub-tasks of epistemic processes. This will allow us to delegate epistemic tasks to AI.  \n   1. It's likely that people do this delegation, even if it's somewhat misguided and potentially bad in the long-term. Can we make this go well?  \n   2. This potentially massively expands our epistemic capacities. How can we use this well?  \n   3. Epistemic tech/institutions/advances can be hard to fund via for-profit companies. This is in part due to public goods issues, and issues around markets for ideas.\n\n## Rafe\n\nAs ever, I hope this will stop advanced artificial intelligence destroying the world, killing all its people \\&c.\n\nSo I think there are a few paths for impact here. I think there is a world where we can make the most productive thinkers about how to do things go further and better; I'm not so excited about a single-player version of this, as I think there's a pretty narrow window when top-level centaur mode works out\\[1\\]\n\nEveryone is already thinking about AI, and, I suspect, this will only get more acute. I think it's pretty hard for people to confront this and think about it carefully and well. It's scary\\! And there are a bunch of entrenched interests and memeplexes that preserve their status better (myopically) if people only have shallow slogans to think about AI with.\n\nGiven that, I suspect that the global sensemaking machine isn't up for the task of thinking about AI well. I hope we can help the global sensemaking machine upgrade itself. I have some hope that we as a society can come up with good ideas for dealing with AI, and also come to see how they make sense, so we can choose them in a robust way.\n\nOne other path to impact: I think a society that understands things better, including itself and its impacts, is one that is more robust in a bunch of ways. A more epistemically-grounded society is also probably a bit richer, and I think a bigger pie creates more slack for arranging things in sanity-promoting ways. I have more hope in a robust society navigating this stuff well.\n\n\\[1\\]:  Yeah, maybe you can do something with shaving off subskills, but I don't think that's really that doable.\n\n## Dave\n\n* With my consequentialist hat, better epistemics being worth it has to route through improved decisions somewhere down the line. The decisions I particularly care about are those by powerful people wrt AI development and implementation and the effects this has on how the future goes  \n  * But I do feel a bit icky when I have this hat on. There seems to be something very intuitively appealing about (a) pursuing better epistemics for its own sake and (b) democratising epistemic tools for the use and benefit of everyone. I think this basically caches out in more 'raising the floor' type work  \n  * I don't feel like I have a very good way of resolving this tension yet, but am excited to see things pushing on both frontiers  \n* I think forecasting has the potential to be a pretty powerful tool and there's currently a massive undersupply of good forecasts. I'm excited for us to use the wave of increased AI labor to make better quantitative forecasts to inform key decisions. I probably care more about quantitative forecasts than most decision makers and am open to being persuaded that the other, fuzzier parts of epistemics are more valuable to work on. \n\n## Julian\n\nWhat are use cases for AI for Epistemics that you most care about?  \n**Raising the ceiling:** can we use AI to organize information and credibly resolve the most challenging and important questions on which experts still seem to disagree? For example:\n\n* COVID origins  \n* Urban planning and public policy  \n* Lots of advancements in basic science: routing around current disagreements and policy challenges with slam-dunk science might be a pretty strong way forward.\n\n**Raising the floor:** can we use AI to build extremely legibly trustworthy/reliable epistemic infrastructure that actually makes its way into people's lives?\n\n* Making truth more palatable (in a way which doesn't translate into making falsehoods more palatable)  \n* Collective epistemics: connecting people to the others with whom they would have the most productive/enlightening conversations\n\nWhat decisions may go badly by default, but could go better if we have the right tech?\n\n* Lots of stuff in public policy: downstream both of better experts (technocrats), better public (voters/interest groups), and better coordination (collective epistemics)\n\nWhat do the later stages of the path to impact look like?\n\n* I want us to live in a world where everyone is more rational, empathetic, considered in their decision making, conscientious about impact, and interested in coordinating. I feel like that's one of the biggest blockers to all kinds of *long term* progress.\n\nWhat tools do we actually maybe want to build?\n\n* Easily accessible fact checkers / claim refuters  \n  * Powerful AI debaters  \n* Forecasting with explanations: superhuman and at scale  \n* Scientific meta-analysis, systematic review  \n* Ad-hoc / bespoke probabilistic models for individual problems  \n* Bespoke *simulators* / simulation environments  \n* Consistency checking and inconsistency identification  \n  * Arbitrage / market makers in prediction markets?  \n* Using AI to accelerate development of non-AI epistemic tools:  \n  * ?\n\n## Lawrence\n\nThere are already key questions/strategic problems where the truth is murky and we seem to proceed on the basis of limited and biased views. There might well be many more of these in the future. A central example is US-China competition over AI. Some have vibes of \"China is so bad that it's worth risking an existential catastrophe to stop it from capturing a portion of the lightcone\". Others have vibes of \"Cooperation is feasible and desirable, and is our only path to a good outcome\". But how bad is China, really? How much \"realised K\" on this question do those who are making the relevant decisions actually have? Similarly, how feasible is cooperation? How valid is this frame of \"capturing the lightcone\"? One reason that we don't have a better hold on these questions is that they are frighteningly complicated and difficult  – I think we could benefit massively from throwing much more cognitive labour at them, and presenting the findings in a way that is hard to disagree with or for any individual to improve upon, other than at a normative/values level. Maybe AI can get us there.\n\n## Lukas Finnveden\n\nwhy is AI for epistemics important/helpful?\n\n* better epistemics generally very good for making all kinds of decisions  \n* I'm specifically concerned about AI potentially causing an intelligence explosion, and us needing better epistemics to handle that. Specifically:  \n  * I'm worried about AI-driven tech R\\&D leading to catastrophes. To help address this, I want AI capabilities to not only be applied to tech R\\&D, but also to be applied to \"reasoning well from limited evidence\" about a number of questions, including how to deal with risks.  \n  * I'm worried about misaligned AI — so I want techniques for eliciting AI capabilities on all kinds of questions, so that AIs don't have better capabilities than we can elicit from them. (Basically: I want good alignment techniques even on hard-to-evaluate questions.)  \n  * Seems great to me if AI-driven improvements in general knowledge and decision-making are more broadly distributed among humans. (I think this leads to better decision-making without significantly accelerating the dangers from new tech or misaligned AI. Also reduces concentration of power.)\n\n## Abram Demski\n\n\\- Why are we doing this? \\-- impact theory brainstorming exercize  \n\t\\- There's the general claim that epistemics has degraded horribly due to social media and the internet, at least in a lot of ways, although epistemics has also gotten better in some specific ways. It seems very plausible that AI is, and will continue to, make this much much worse (due to deepfakes and super-persuasive writing, to give two examples). We wish to reverse this trend, or at least slow the acceleration.  \n\t\\- The high-level reason why epistemics is important is because epistemics is how we make good decisions. So effectively, this is AI for decision-making. However, if we just set out to make tools for decision-making, I have a sense that this would be worse.  \n\t\t\\- One super important aspect of decision-making is that you have to orient towards the right problem. People do much better when they understand the problem before proposing solutions.  \n\t\t\t\\- I have a sense that producing a better shared understanding of the world JUST WOULD result in better things. The connection between epistemics and better decisionmaking is that strong.  \n\t\t\t\t\\- However, there's an element of coordination to this. You have to have common knowledge, not just knowledge. AI for coordination is yet another thing. AI for group epistemics is close to that, though, and again, I have a sense that making \"AI for coordination\" would produce worse effects on the world than \"AI for group epistemics\".  \n\t\t\\- What specific decisions are we hoping to influence?  \n\t\t\t\\- Decisions around existential risk seem super super important. If AI were generally better at epistemics (as a relative, comparative thing \\-- ie we differentially accelerate epistemics) then key decisionmakers (in the government and at AI companies) would see much more clearly what's coming. Everyone wants a better future, so a better future would have a better chance of happening if we could see and navigate.\n\nI don't think I got any further than the supporting documentation for this workshop in articulating reasons and stuff, so I predict that the LLM summary won't be better than what we already know, although I hope it is. \n\n## Brendan\n\nFor us to live together in a caring and just way, we must make decisions about what is important to us together. Is it okay to tell my neighbour to turn their music down? Do we spend money on roads or trains or libraries? Do we regulate AI development and, if so, how? Do we want to spend public money on PEPFAR, or other avenues for reducing global poverty? To make these decisions in a way that is inclusive and democratic, that fosters buy-in and trust, we must begin with a shared sense of how the world is."
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Occasional Use of Unexplained Economic Concepts",
          "description": "Terms like 'Pareto frontier', 'public goods issues', and 'market makers' are used without explanation. Briefly defining these concepts would improve accessibility.",
          "importance": 50,
          "grade": 50,
          "isValid": false,
          "highlight": {
            "startOffset": 10655,
            "endOffset": 18895,
            "quotedText": "could help us get up towards the Pareto frontier where we avoid (virtually) all costly conflicts\n\nGoing slowly and carefully on AI development — rather than having AI progress accelerate at increasingly breakneck speed, helping actors to understand the basic situation and the desirability of different options could be crucial for facilitating meaningful and robust agreements which restrict AI development to the stuff which we have good reasons for being confident are safe // otherwise beneficial.\n\nBettering democracy — helping people to access key information about what decisions their country might make would actually mean for longer term implications, and having this in turn feed into incentives for decision-makers that better align them with the common good\n\nBuilding safe / aligned AI — better tools for anticipating when systems are actually robustly safe, vs when stronger cases for expectation of safety seem required\n\n## Josh Rosenberg (FRI)\n\nI'm massively confused about important problems, e.g. how to orient to AI risk and what to do about it. Some reasonable people think there's a high probability we effectively lose everything we care about in 2027\\. Some reasonable people think it'll be decades till we have \"AGI\" and it'd be good if it arrived sooner. Which actions are good to take in each of these worlds is very different, and I'd like to figure out what world we're in.\n\nThis also applies to many other problem areas.\n\nE.g., Trump's effect on US democracy. How much of a crisis is it? What are the most effective actions to take in response to ensure that the country and communities we care about are as well off as they can be? How can we preserve high-quality institutions to lead the world in positive directions?\n\nOr, on the positive vision side: the \"Abundance\" agenda seems promising. Is it actually a good idea? What do we do about it?\n\nAI for Epistemics could help us get a lot more clarity on how to think about these problems and what actions to take. I'd like much better information and forecasts on the likely effects of Trump, AI timelines, and the pro's and con's of different Abundance policies, for example. If I had a bunch of time and good resources to dig into these problems, I could get a much better sense for myself. I'm hoping AI will help all of us do this sort of thing collectively.\n\n## Katja Grace\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Put lots of cognitive effort into very clear and good and compelling case for AI risk  \n  * Automate talking to people about AI risk compellingly, possibly using above case as input  \n  * Good game theoretic thinking re arms races, coordination, what is possible. I.e. People are making dumb assumptions about things, e.g. \"can't slow down tech\". Could automate noting errors. So e.g. when someone says that on Twitter, can highlight it and send in response. \\-\\> good thinking via better weapons for debate (similar to Nathan project)  \n  * Specific idea: very quantified and competitive debate site, that can be cited re which views are winning, where AI does a lot of the cognitive labor in places no people are paying attention, but as a part of it gets cruxy, people pay attention and vote/bet  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Poor decision making around AI: e.g. 'racing' because of vague heuristics instead of ever doing calc on costs and benefits given specific risks  \n  * Failure to coordinate on AI: building AI systems that are dangerous, due to expecting others to do so\n\n## Joel Lehman\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Raising the bottom (i.e. helping improve societal epistemics) – in particular that in theory political decisions (at least political decision-makers) are somewhat downstream of societal epistemics  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Having weird political leaders and awful public epistemics at critical junctures seems to unnecessarily spike catastrophic/existential risk probabilities. In theory, helping uplevel or at least maintain current public epistemic health (especially as it relates to civic sensemaking) might help influence the quality of decision-makers and concrete decisions made at high-stakes moments  \n  * The shape of the tech is not clear, or probably requires multiple flavors of interventions. E.g. if LLM assistants by default help cultivate civic virtues and on the margin support critical thinking or whatever underlying skills and knowledge help a person up-level their epistemic habits / health (or at least if LLM assistants don't contribute to atrophy/harm); or somehow convincing social media networks to care about this (e.g. community notes but things that go beyond); similarly somehow help w/ important social goods like 'societal trust' and 'trust in your neighbors'  \n* What do the later stages of the path to impact look like?  \n  * \\[didn't get to this\\]  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * Healthier democratic society, better agreement on consensus reality, higher trust in society, better politicians, better concrete decisions in high-stakes moments \n\n## Austin Chen\n\n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * **Evaluator**: More agreement (or understanding of disagreement) of what orgs & people are doing good work (eg for making grants in AI safety)  \n  * **Oracle**: Instant trustworthy forecasts on any question you ask  \n  * **Planner**: Guidance on \"what are the 3 most important things I should do this day/week/month\"  \n  * **Representative**: Simulate the responses of other parties (people, orgs/corps, governments, LLM agents), for discussions, contracts, trade\n\n## Janna\n\n* Why do we care about (AI for) epistemics?  \n  * Noise to signal ratio could be much higher  \n  * AI (mis)information churned out at the price of almost zero  \n* What are use cases for AI for Epistemics that you most care about?  \n  * Solving prediction markets--is there a way to scale up the bots, even if they are wrong and lossy, to improve prediction market accuracy?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * US-China relations  \n  * Trump tariffs maybe  \n  * Immigration rules in the US  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * More accurate predictions to…  \n    * Reduce business uncertainty  \n    * Make it easier to evaluate life decisions  \n  * Forestalling bad decisions made on imperfect information\n\n## Owain\n\n1. Maybe having humans understanding the world better is good in itself. Lots of people read popular science for this reason. Maybe it's good for AIs as well  \n2. Very broadly, it seems that better world understanding and decision making enables people to make their lives better in myriad ways. (To substantiate this, one could try to separate out the effects of better world understanding from the effects of better realized technology. E.g. Looking at capital-poor countries that can still benefit from scientific knowledge, economics, etc.)  \n3. More specifically, if AI progress is fast, there will be lots of important decisions to be made in quick calendar time. Exploiting AI to improve those decisions seems valuable. Current decision making institutions do not seem well set up to adapt really quickly. (A specific concern is dangers from AI takeover).   \n4. Why now? Why this group of people? We haven't really had tech that could do most epistemic tasks until recently. Now we have AI that can do an increasing proportion of the sub-tasks of epistemic processes. This will allow us to delegate epistemic tasks to AI.  \n   1. It's likely that people do this delegation, even if it's somewhat misguided and potentially bad in the long-term. Can we make this go well?  \n   2. This potentially massively expands our epistemic capacities. How can we use this well?  \n   3. Epistemic tech/institutions/advances can be hard to fund via for-profit companies. This is in part due to public goods issues, and issues around markets for ideas."
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Effective Use of Personal Reflection",
          "description": "Some contributors (e.g., Josh Rosenberg) share personal confusion and hopes, which humanizes the discussion and makes it more relatable.",
          "importance": 60,
          "grade": 85,
          "isValid": false,
          "highlight": {
            "startOffset": 11616,
            "endOffset": 12998,
            "quotedText": "I'm massively confused about important problems, e.g. how to orient to AI risk and what to do about it. Some reasonable people think there's a high probability we effectively lose everything we care about in 2027\\. Some reasonable people think it'll be decades till we have \"AGI\" and it'd be good if it arrived sooner. Which actions are good to take in each of these worlds is very different, and I'd like to figure out what world we're in.\n\nThis also applies to many other problem areas.\n\nE.g., Trump's effect on US democracy. How much of a crisis is it? What are the most effective actions to take in response to ensure that the country and communities we care about are as well off as they can be? How can we preserve high-quality institutions to lead the world in positive directions?\n\nOr, on the positive vision side: the \"Abundance\" agenda seems promising. Is it actually a good idea? What do we do about it?\n\nAI for Epistemics could help us get a lot more clarity on how to think about these problems and what actions to take. I'd like much better information and forecasts on the likely effects of Trump, AI timelines, and the pro's and con's of different Abundance policies, for example. If I had a bunch of time and good resources to dig into these problems, I could get a much better sense for myself. I'm hoping AI will help all of us do this sort of thing collectively."
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Occasional Use of Unexplained Philosophical Concepts",
          "description": "References to 'metaphilosophy' and 'alignment research' are made without explanation. Briefly introducing these concepts would improve accessibility.",
          "importance": 45,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 3179,
            "endOffset": 3528,
            "quotedText": "cf**. [metaphilosophy](https://www.alignmentforum.org/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy)** (but also a bunch of other things.)  \n  * This also feels very similar to making **AIs helpful for AI safety research, cf. [automating alignment research](https://joecarlsmith.com/2025/03/14/ai-for-ai-safety#2-what-is-ai-for-ai-safety)."
          }
        },
        {
          "title": "Strong Emphasis on Forecasting and Quantitative Analysis",
          "description": "Several contributors highlight the importance of forecasting, quantitative analysis, and systematic review, which grounds the discussion in empirical methods.",
          "importance": 65,
          "grade": 85,
          "isValid": false,
          "highlight": {
            "startOffset": 2413,
            "endOffset": 23396,
            "quotedText": "Forecasting how I should live my life in a period of intense technological change  \n* Guiding decision makers to avoid war with china/ai/everyone  \n* Providing the background context for me to sensemake way more sanely.  \n* Pushing forward the extant knowledge of AI safety agendas  \n* Planning for multi-stage projects that could achieve big impact  \n* Roadmapping to build larger more complex things\n\n## Chi\n\nThere are a few different things I care about:\n\n* I care a lot about humanity and AI, including **super-late-stage AI, getting decision theory** right.  \n  * Consequently but also separately, this relates to the general question of what to do about areas where there is not necessarily a ground truth but that are nonetheless very important to get right, cf**. [metaphilosophy](https://www.alignmentforum.org/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy)** (but also a bunch of other things.)  \n  * This also feels very similar to making **AIs helpful for AI safety research, cf. [automating alignment research](https://joecarlsmith.com/2025/03/14/ai-for-ai-safety#2-what-is-ai-for-ai-safety).**\n\n  → This is more about raising the **top of the epistemic landscape**\n\n* Other things I care about:  \n  * Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place.  \n  * Vague sense that better **societal epistemics** would help with increasing the **sanity of conversations about AI safety, polarisation around the issue etc.**  \n    * This is broadly about **getting intent alignment** in the first place.\n\n→ These are all more about raising the **bottom of the epistemic landscape**\n\n## Deger Turan:\n\n* Social trust erosion, collective coordination mechanisms are built on a weak assumption  \n* Capacity to entertain truthfulness when it is not instrumental to one's incentive gradient & differing norms upon 'truth will make my sense of security weaker'   \n* Creating contexts where world model divergence can be agreed by both parties enabling a safe disagreement  \n* Improving model reasoning / safer AI by reducing slop, by creating accountability mechanisms through consistency checks & other specifications for healthy epistemics  \n* Decoupling virality with magnitude while living in a short attention span world, curation   \n* I have some confusion re: raising the floor when not all parties are necessarily operating in good faith. I think I need some help think through the ways in which epistemics in the context of incentive misalignment can be used or have defensibility. \n\n## Nathan Young: \n\nWhat would we have got:\n\n* Biden not running, perhaps an open dem primary  \n* No FTX crisis  \n* Less zero sum OpenAI board drama  \n* Many smaller, better choices \n\nWhat decisions might go badly:\n\n* Trump tariffs/ global trade  \n* Taiwan/ US-China conflict  \n* US-China competition  \n* US authoritarianism  \n* AI regulation  \n* EU-China realignment  \n* Automated robots  \n* OpenAI strategy  \n* Anthropic strategy  \n* Google strategy  \n* xAI strategy  \n* AI workforce automation  \n* China strategy  \n* Russia strategy  \n* Moskovitz strategy  \n* Must strategy  \n* Bezos Strategy  \n* Zuckerberg strategy  \n* Tallinn strategy  \n* Altman strategy  \n* Drone warfare  \n* Climate change destabilisation  \n* Democratic trust\n\n## Lincoln Quirk:\n\nWhy do we care about (AI for) epistemics?  \n\\- making the public better informed:  \n\t\\- better able to deal with and process the wealth of information  \n\t\\- better able to express their own interests in government (via voting)  \n\t\\- helping cut through social media distraction / intentional misinformation  \n\\- getting governments to implement better policy  \n\t\\- coming up with good policy ideas  \n\t\\- objectively doing cost-benefit analysis  \n\t\\- objectively explaining tradeoffs (vs having to rely on biased humans to explain them)  \n\t\\- flagging bad ideas as bad  \n\t\\- convincing policymakers with compelling writeups  \n\\- success and flourishing for individuals and orgs using these tools  \n\t\\- via helping them think through decisions  \n\t\\- via clearheaded analysis of trade-offs and cost/benefits  \n\t\\- flagging bad ideas as bad\n\n## Jay\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * How can we build something like the positive version of Community Notes: which statements/posts are agreed on by people who often disagree?  \n  * How can we better understand/summarize what others think, e.g. on social media platforms? E.g. if you see a post recommended to you, is this because it's the most extreme take that went viral and isn't representative, or is this a commonly held view? Between different viewpoints, what are the concrete points of actual disagreement (on facts or values) and what are the things widely agreed upon? Let's build viewpoint/debate maps or a similar sensemaking tool that gets adopted/understood by the masses/raises the knowledge floor (in my experience, things like pol.is are hard for regular users to grok)  \n  * How can we improve Community Notes (or more general knowledge) creation with AI assistants? An AI agent proposing community notes and humans still checking/rating them?  \n  * How can we have something like Community Notes that applies to claims or statements about the future, or which are not concretely knowable?  \n  * \n\n## Ben Wilson (Metaculus)\n\nOverall I believe the most important value of epistemics (and AI epistemics) in general is informing what actions allow us to achieve our goals (i.e. make the world better). If we had perfect AI epistemics I would be excited by the ability for us to take a goal, and get a list of possible actions we could take and the forecasted outcomes of each of those actions (plus background information and forecasts for the 'current lay of the land' for context). This can then inform, not just the risk of AI doing good/bad things (as with current forecasting), but what we can do to raise the chances of 'great' AI outcomes, and lower the changes of 'worse' AI outcomes\n\n## Andreas\n\n* **What are use cases for AI for Epistemics that you most care about?**  \n  * AI policy, broadly construed, must go better  \n    * Avoid gradual disempowerment  \n    * Thoughtful regulation  \n  * Organizational planning & decision-making is weak  \n    * Given an informal long-term goal like improve decision-making for strategic users in government & large orgs, make a plan for a 20-people org that helps them accomplish the goal  \n    * Mapping dependencies, uncertainties, and potential futures  \n    * Understand complex domains with non-trivial feedback loops  \n      * Economic shifts  \n* **What decisions may go badly by default, but could go better if we have the right tech?**  \n  * Resource allocation within orgs  \n    * Investment  \n    * Grant making  \n    * Government budgets  \n  * Policy  \n    * How should AI be deployed in the world?  \n* **What do the later stages of the path to impact look like?**  \n  * AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning  \n  * The technology helps organizations maintain coherence and make sound judgments even as the pace of change accelerates  \n  * Assumption: People are generally well-intentioned \\- no one wants catastrophe \\- and as a result of better tooling things go better in the world overall\n\n## Owen optimistic brainstorm:\n\nWorld peace without world government — sufficiently good tech to help countries understand each others' interests, find mutually desirable bargains, and make sufficient credible commitments, could help us get up towards the Pareto frontier where we avoid (virtually) all costly conflicts\n\nGoing slowly and carefully on AI development — rather than having AI progress accelerate at increasingly breakneck speed, helping actors to understand the basic situation and the desirability of different options could be crucial for facilitating meaningful and robust agreements which restrict AI development to the stuff which we have good reasons for being confident are safe // otherwise beneficial.\n\nBettering democracy — helping people to access key information about what decisions their country might make would actually mean for longer term implications, and having this in turn feed into incentives for decision-makers that better align them with the common good\n\nBuilding safe / aligned AI — better tools for anticipating when systems are actually robustly safe, vs when stronger cases for expectation of safety seem required\n\n## Josh Rosenberg (FRI)\n\nI'm massively confused about important problems, e.g. how to orient to AI risk and what to do about it. Some reasonable people think there's a high probability we effectively lose everything we care about in 2027\\. Some reasonable people think it'll be decades till we have \"AGI\" and it'd be good if it arrived sooner. Which actions are good to take in each of these worlds is very different, and I'd like to figure out what world we're in.\n\nThis also applies to many other problem areas.\n\nE.g., Trump's effect on US democracy. How much of a crisis is it? What are the most effective actions to take in response to ensure that the country and communities we care about are as well off as they can be? How can we preserve high-quality institutions to lead the world in positive directions?\n\nOr, on the positive vision side: the \"Abundance\" agenda seems promising. Is it actually a good idea? What do we do about it?\n\nAI for Epistemics could help us get a lot more clarity on how to think about these problems and what actions to take. I'd like much better information and forecasts on the likely effects of Trump, AI timelines, and the pro's and con's of different Abundance policies, for example. If I had a bunch of time and good resources to dig into these problems, I could get a much better sense for myself. I'm hoping AI will help all of us do this sort of thing collectively.\n\n## Katja Grace\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Put lots of cognitive effort into very clear and good and compelling case for AI risk  \n  * Automate talking to people about AI risk compellingly, possibly using above case as input  \n  * Good game theoretic thinking re arms races, coordination, what is possible. I.e. People are making dumb assumptions about things, e.g. \"can't slow down tech\". Could automate noting errors. So e.g. when someone says that on Twitter, can highlight it and send in response. \\-\\> good thinking via better weapons for debate (similar to Nathan project)  \n  * Specific idea: very quantified and competitive debate site, that can be cited re which views are winning, where AI does a lot of the cognitive labor in places no people are paying attention, but as a part of it gets cruxy, people pay attention and vote/bet  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Poor decision making around AI: e.g. 'racing' because of vague heuristics instead of ever doing calc on costs and benefits given specific risks  \n  * Failure to coordinate on AI: building AI systems that are dangerous, due to expecting others to do so\n\n## Joel Lehman\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Raising the bottom (i.e. helping improve societal epistemics) – in particular that in theory political decisions (at least political decision-makers) are somewhat downstream of societal epistemics  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Having weird political leaders and awful public epistemics at critical junctures seems to unnecessarily spike catastrophic/existential risk probabilities. In theory, helping uplevel or at least maintain current public epistemic health (especially as it relates to civic sensemaking) might help influence the quality of decision-makers and concrete decisions made at high-stakes moments  \n  * The shape of the tech is not clear, or probably requires multiple flavors of interventions. E.g. if LLM assistants by default help cultivate civic virtues and on the margin support critical thinking or whatever underlying skills and knowledge help a person up-level their epistemic habits / health (or at least if LLM assistants don't contribute to atrophy/harm); or somehow convincing social media networks to care about this (e.g. community notes but things that go beyond); similarly somehow help w/ important social goods like 'societal trust' and 'trust in your neighbors'  \n* What do the later stages of the path to impact look like?  \n  * \\[didn't get to this\\]  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * Healthier democratic society, better agreement on consensus reality, higher trust in society, better politicians, better concrete decisions in high-stakes moments \n\n## Austin Chen\n\n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * **Evaluator**: More agreement (or understanding of disagreement) of what orgs & people are doing good work (eg for making grants in AI safety)  \n  * **Oracle**: Instant trustworthy forecasts on any question you ask  \n  * **Planner**: Guidance on \"what are the 3 most important things I should do this day/week/month\"  \n  * **Representative**: Simulate the responses of other parties (people, orgs/corps, governments, LLM agents), for discussions, contracts, trade\n\n## Janna\n\n* Why do we care about (AI for) epistemics?  \n  * Noise to signal ratio could be much higher  \n  * AI (mis)information churned out at the price of almost zero  \n* What are use cases for AI for Epistemics that you most care about?  \n  * Solving prediction markets--is there a way to scale up the bots, even if they are wrong and lossy, to improve prediction market accuracy?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * US-China relations  \n  * Trump tariffs maybe  \n  * Immigration rules in the US  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * More accurate predictions to…  \n    * Reduce business uncertainty  \n    * Make it easier to evaluate life decisions  \n  * Forestalling bad decisions made on imperfect information\n\n## Owain\n\n1. Maybe having humans understanding the world better is good in itself. Lots of people read popular science for this reason. Maybe it's good for AIs as well  \n2. Very broadly, it seems that better world understanding and decision making enables people to make their lives better in myriad ways. (To substantiate this, one could try to separate out the effects of better world understanding from the effects of better realized technology. E.g. Looking at capital-poor countries that can still benefit from scientific knowledge, economics, etc.)  \n3. More specifically, if AI progress is fast, there will be lots of important decisions to be made in quick calendar time. Exploiting AI to improve those decisions seems valuable. Current decision making institutions do not seem well set up to adapt really quickly. (A specific concern is dangers from AI takeover).   \n4. Why now? Why this group of people? We haven't really had tech that could do most epistemic tasks until recently. Now we have AI that can do an increasing proportion of the sub-tasks of epistemic processes. This will allow us to delegate epistemic tasks to AI.  \n   1. It's likely that people do this delegation, even if it's somewhat misguided and potentially bad in the long-term. Can we make this go well?  \n   2. This potentially massively expands our epistemic capacities. How can we use this well?  \n   3. Epistemic tech/institutions/advances can be hard to fund via for-profit companies. This is in part due to public goods issues, and issues around markets for ideas.\n\n## Rafe\n\nAs ever, I hope this will stop advanced artificial intelligence destroying the world, killing all its people \\&c.\n\nSo I think there are a few paths for impact here. I think there is a world where we can make the most productive thinkers about how to do things go further and better; I'm not so excited about a single-player version of this, as I think there's a pretty narrow window when top-level centaur mode works out\\[1\\]\n\nEveryone is already thinking about AI, and, I suspect, this will only get more acute. I think it's pretty hard for people to confront this and think about it carefully and well. It's scary\\! And there are a bunch of entrenched interests and memeplexes that preserve their status better (myopically) if people only have shallow slogans to think about AI with.\n\nGiven that, I suspect that the global sensemaking machine isn't up for the task of thinking about AI well. I hope we can help the global sensemaking machine upgrade itself. I have some hope that we as a society can come up with good ideas for dealing with AI, and also come to see how they make sense, so we can choose them in a robust way.\n\nOne other path to impact: I think a society that understands things better, including itself and its impacts, is one that is more robust in a bunch of ways. A more epistemically-grounded society is also probably a bit richer, and I think a bigger pie creates more slack for arranging things in sanity-promoting ways. I have more hope in a robust society navigating this stuff well.\n\n\\[1\\]:  Yeah, maybe you can do something with shaving off subskills, but I don't think that's really that doable.\n\n## Dave\n\n* With my consequentialist hat, better epistemics being worth it has to route through improved decisions somewhere down the line. The decisions I particularly care about are those by powerful people wrt AI development and implementation and the effects this has on how the future goes  \n  * But I do feel a bit icky when I have this hat on. There seems to be something very intuitively appealing about (a) pursuing better epistemics for its own sake and (b) democratising epistemic tools for the use and benefit of everyone. I think this basically caches out in more 'raising the floor' type work  \n  * I don't feel like I have a very good way of resolving this tension yet, but am excited to see things pushing on both frontiers  \n* I think forecasting has the potential to be a pretty powerful tool and there's currently a massive undersupply of good forecasts. I'm excited for us to use the wave of increased AI labor to make better quantitative forecasts to inform key decisions. I probably care more about quantitative forecasts than most decision makers and am open to being persuaded that the other, fuzzier parts of epistemics are more valuable to work on. \n\n## Julian\n\nWhat are use cases for AI for Epistemics that you most care about?  \n**Raising the ceiling:** can we use AI to organize information and credibly resolve the most challenging and important questions on which experts still seem to disagree? For example:\n\n* COVID origins  \n* Urban planning and public policy  \n* Lots of advancements in basic science: routing around current disagreements and policy challenges with slam-dunk science might be a pretty strong way forward.\n\n**Raising the floor:** can we use AI to build extremely legibly trustworthy/reliable epistemic infrastructure that actually makes its way into people's lives?\n\n* Making truth more palatable (in a way which doesn't translate into making falsehoods more palatable)  \n* Collective epistemics: connecting people to the others with whom they would have the most productive/enlightening conversations\n\nWhat decisions may go badly by default, but could go better if we have the right tech?\n\n* Lots of stuff in public policy: downstream both of better experts (technocrats), better public (voters/interest groups), and better coordination (collective epistemics)\n\nWhat do the later stages of the path to impact look like?\n\n* I want us to live in a world where everyone is more rational, empathetic, considered in their decision making, conscientious about impact, and interested in coordinating. I feel like that's one of the biggest blockers to all kinds of *long term* progress.\n\nWhat tools do we actually maybe want to build?\n\n* Easily accessible fact checkers / claim refuters  \n  * Powerful AI debaters  \n* Forecasting with explanations: superhuman and at scale  \n* Scientific meta-analysis, systematic review"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Occasional Use of Unexplained Social Science Concepts",
          "description": "Terms like 'pluralism', 'polarization', and 'game theoretic strategies' are used without explanation. Briefly defining these concepts would improve accessibility.",
          "importance": 45,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 1518,
            "endOffset": 2204,
            "quotedText": "Healthy polarization / pluralism  \n    \n* More peace, less likelihood of war  \n* Better geopolitics  \n* Better economic incentives  \n    \n* Better AI org governance  \n* Better AI coordination (re. distribution, escalation, etc.)  \n* Better compute allocation (toward what matters for good outcomes)  \n    \n* Better allocation of resources overall  \n* Better lives for people  \n* More effective organizations, getting things done with fewer resources, more focused on the things that benefit everyone  \n    \n* Identification of common ground directions / equilibria / game theoretic strategies that create/incentivize new positive gradients (and not surfacing the bad versions of those…)"
          }
        },
        {
          "title": "Effective Use of Scenario Planning",
          "description": "Some contributors discuss possible futures and scenario planning (e.g., 'mapping dependencies, uncertainties, and potential futures'), which adds strategic depth to the analysis.",
          "importance": 60,
          "grade": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 9352,
            "endOffset": 9477,
            "quotedText": "Mapping dependencies, uncertainties, and potential futures  \n    * Understand complex domains with non-trivial feedback loops"
          }
        },
        {
          "title": "Occasional Use of Unexplained AI Concepts",
          "description": "Terms like 'oracle', 'planner', and 'representative' are used as AI roles without explanation. Briefly describing these roles would improve clarity.",
          "importance": 50,
          "grade": 50,
          "isValid": false,
          "highlight": {
            "startOffset": 650,
            "endOffset": 16468,
            "quotedText": "* \"As a result of better AI for epistemics tech, we get: […]\"\n\nTemplate:\n\n## Name: \n\nEntry\n\n## Dan Schwarz:\n\n* What are use cases for AI for Epistemics that you most care about?\n\nGoing from \"Realized\" to \"Accessible\" knowledge, e.g. getting good information using the web. Agentic web agents like Deep Research.\n\n* What decisions may go badly by default, but could go better if we have the right tech?\n\nMany billions of small decisions.\n\n* What do the later stages of the path to impact look like?\n\nSuperhuman forecasting at scale.\n\n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nReasonable judgment on tricky questions that's too cheap to meter, better than if you asked a respected colleague to spend many minutes on it.\n\n## Aviv\n\n* Better government governance  \n* Better democratic functioning  \n* Reduced dangerous concentration of power  \n    \n* Healthy polarization / pluralism  \n    \n* More peace, less likelihood of war  \n* Better geopolitics  \n* Better economic incentives  \n    \n* Better AI org governance  \n* Better AI coordination (re. distribution, escalation, etc.)  \n* Better compute allocation (toward what matters for good outcomes)  \n    \n* Better allocation of resources overall  \n* Better lives for people  \n* More effective organizations, getting things done with fewer resources, more focused on the things that benefit everyone  \n    \n* Identification of common ground directions / equilibria / game theoretic strategies that create/incentivize new positive gradients (and not surfacing the bad versions of those…)  \n    \n* Understanding our own goals and values\n\n*Note how it is the intersection of better epistemics / world models + the right power structures / incentives needed for all of these.*\n\n## Ben Goldhaber:\n\n* Forecasting how I should live my life in a period of intense technological change  \n* Guiding decision makers to avoid war with china/ai/everyone  \n* Providing the background context for me to sensemake way more sanely.  \n* Pushing forward the extant knowledge of AI safety agendas  \n* Planning for multi-stage projects that could achieve big impact  \n* Roadmapping to build larger more complex things\n\n## Chi\n\nThere are a few different things I care about:\n\n* I care a lot about humanity and AI, including **super-late-stage AI, getting decision theory** right.  \n  * Consequently but also separately, this relates to the general question of what to do about areas where there is not necessarily a ground truth but that are nonetheless very important to get right, cf**. [metaphilosophy](https://www.alignmentforum.org/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy)** (but also a bunch of other things.)  \n  * This also feels very similar to making **AIs helpful for AI safety research, cf. [automating alignment research](https://joecarlsmith.com/2025/03/14/ai-for-ai-safety#2-what-is-ai-for-ai-safety).**\n\n  → This is more about raising the **top of the epistemic landscape**\n\n* Other things I care about:  \n  * Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place.  \n  * Vague sense that better **societal epistemics** would help with increasing the **sanity of conversations about AI safety, polarisation around the issue etc.**  \n    * This is broadly about **getting intent alignment** in the first place.\n\n→ These are all more about raising the **bottom of the epistemic landscape**\n\n## Deger Turan:\n\n* Social trust erosion, collective coordination mechanisms are built on a weak assumption  \n* Capacity to entertain truthfulness when it is not instrumental to one's incentive gradient & differing norms upon 'truth will make my sense of security weaker'   \n* Creating contexts where world model divergence can be agreed by both parties enabling a safe disagreement  \n* Improving model reasoning / safer AI by reducing slop, by creating accountability mechanisms through consistency checks & other specifications for healthy epistemics  \n* Decoupling virality with magnitude while living in a short attention span world, curation   \n* I have some confusion re: raising the floor when not all parties are necessarily operating in good faith. I think I need some help think through the ways in which epistemics in the context of incentive misalignment can be used or have defensibility. \n\n## Nathan Young: \n\nWhat would we have got:\n\n* Biden not running, perhaps an open dem primary  \n* No FTX crisis  \n* Less zero sum OpenAI board drama  \n* Many smaller, better choices \n\nWhat decisions might go badly:\n\n* Trump tariffs/ global trade  \n* Taiwan/ US-China conflict  \n* US-China competition  \n* US authoritarianism  \n* AI regulation  \n* EU-China realignment  \n* Automated robots  \n* OpenAI strategy  \n* Anthropic strategy  \n* Google strategy  \n* xAI strategy  \n* AI workforce automation  \n* China strategy  \n* Russia strategy  \n* Moskovitz strategy  \n* Must strategy  \n* Bezos Strategy  \n* Zuckerberg strategy  \n* Tallinn strategy  \n* Altman strategy  \n* Drone warfare  \n* Climate change destabilisation  \n* Democratic trust\n\n## Lincoln Quirk:\n\nWhy do we care about (AI for) epistemics?  \n\\- making the public better informed:  \n\t\\- better able to deal with and process the wealth of information  \n\t\\- better able to express their own interests in government (via voting)  \n\t\\- helping cut through social media distraction / intentional misinformation  \n\\- getting governments to implement better policy  \n\t\\- coming up with good policy ideas  \n\t\\- objectively doing cost-benefit analysis  \n\t\\- objectively explaining tradeoffs (vs having to rely on biased humans to explain them)  \n\t\\- flagging bad ideas as bad  \n\t\\- convincing policymakers with compelling writeups  \n\\- success and flourishing for individuals and orgs using these tools  \n\t\\- via helping them think through decisions  \n\t\\- via clearheaded analysis of trade-offs and cost/benefits  \n\t\\- flagging bad ideas as bad\n\n## Jay\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * How can we build something like the positive version of Community Notes: which statements/posts are agreed on by people who often disagree?  \n  * How can we better understand/summarize what others think, e.g. on social media platforms? E.g. if you see a post recommended to you, is this because it's the most extreme take that went viral and isn't representative, or is this a commonly held view? Between different viewpoints, what are the concrete points of actual disagreement (on facts or values) and what are the things widely agreed upon? Let's build viewpoint/debate maps or a similar sensemaking tool that gets adopted/understood by the masses/raises the knowledge floor (in my experience, things like pol.is are hard for regular users to grok)  \n  * How can we improve Community Notes (or more general knowledge) creation with AI assistants? An AI agent proposing community notes and humans still checking/rating them?  \n  * How can we have something like Community Notes that applies to claims or statements about the future, or which are not concretely knowable?  \n  * \n\n## Ben Wilson (Metaculus)\n\nOverall I believe the most important value of epistemics (and AI epistemics) in general is informing what actions allow us to achieve our goals (i.e. make the world better). If we had perfect AI epistemics I would be excited by the ability for us to take a goal, and get a list of possible actions we could take and the forecasted outcomes of each of those actions (plus background information and forecasts for the 'current lay of the land' for context). This can then inform, not just the risk of AI doing good/bad things (as with current forecasting), but what we can do to raise the chances of 'great' AI outcomes, and lower the changes of 'worse' AI outcomes\n\n## Andreas\n\n* **What are use cases for AI for Epistemics that you most care about?**  \n  * AI policy, broadly construed, must go better  \n    * Avoid gradual disempowerment  \n    * Thoughtful regulation  \n  * Organizational planning & decision-making is weak  \n    * Given an informal long-term goal like improve decision-making for strategic users in government & large orgs, make a plan for a 20-people org that helps them accomplish the goal  \n    * Mapping dependencies, uncertainties, and potential futures  \n    * Understand complex domains with non-trivial feedback loops  \n      * Economic shifts  \n* **What decisions may go badly by default, but could go better if we have the right tech?**  \n  * Resource allocation within orgs  \n    * Investment  \n    * Grant making  \n    * Government budgets  \n  * Policy  \n    * How should AI be deployed in the world?  \n* **What do the later stages of the path to impact look like?**  \n  * AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning  \n  * The technology helps organizations maintain coherence and make sound judgments even as the pace of change accelerates  \n  * Assumption: People are generally well-intentioned \\- no one wants catastrophe \\- and as a result of better tooling things go better in the world overall\n\n## Owen optimistic brainstorm:\n\nWorld peace without world government — sufficiently good tech to help countries understand each others' interests, find mutually desirable bargains, and make sufficient credible commitments, could help us get up towards the Pareto frontier where we avoid (virtually) all costly conflicts\n\nGoing slowly and carefully on AI development — rather than having AI progress accelerate at increasingly breakneck speed, helping actors to understand the basic situation and the desirability of different options could be crucial for facilitating meaningful and robust agreements which restrict AI development to the stuff which we have good reasons for being confident are safe // otherwise beneficial.\n\nBettering democracy — helping people to access key information about what decisions their country might make would actually mean for longer term implications, and having this in turn feed into incentives for decision-makers that better align them with the common good\n\nBuilding safe / aligned AI — better tools for anticipating when systems are actually robustly safe, vs when stronger cases for expectation of safety seem required\n\n## Josh Rosenberg (FRI)\n\nI'm massively confused about important problems, e.g. how to orient to AI risk and what to do about it. Some reasonable people think there's a high probability we effectively lose everything we care about in 2027\\. Some reasonable people think it'll be decades till we have \"AGI\" and it'd be good if it arrived sooner. Which actions are good to take in each of these worlds is very different, and I'd like to figure out what world we're in.\n\nThis also applies to many other problem areas.\n\nE.g., Trump's effect on US democracy. How much of a crisis is it? What are the most effective actions to take in response to ensure that the country and communities we care about are as well off as they can be? How can we preserve high-quality institutions to lead the world in positive directions?\n\nOr, on the positive vision side: the \"Abundance\" agenda seems promising. Is it actually a good idea? What do we do about it?\n\nAI for Epistemics could help us get a lot more clarity on how to think about these problems and what actions to take. I'd like much better information and forecasts on the likely effects of Trump, AI timelines, and the pro's and con's of different Abundance policies, for example. If I had a bunch of time and good resources to dig into these problems, I could get a much better sense for myself. I'm hoping AI will help all of us do this sort of thing collectively.\n\n## Katja Grace\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Put lots of cognitive effort into very clear and good and compelling case for AI risk  \n  * Automate talking to people about AI risk compellingly, possibly using above case as input  \n  * Good game theoretic thinking re arms races, coordination, what is possible. I.e. People are making dumb assumptions about things, e.g. \"can't slow down tech\". Could automate noting errors. So e.g. when someone says that on Twitter, can highlight it and send in response. \\-\\> good thinking via better weapons for debate (similar to Nathan project)  \n  * Specific idea: very quantified and competitive debate site, that can be cited re which views are winning, where AI does a lot of the cognitive labor in places no people are paying attention, but as a part of it gets cruxy, people pay attention and vote/bet  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Poor decision making around AI: e.g. 'racing' because of vague heuristics instead of ever doing calc on costs and benefits given specific risks  \n  * Failure to coordinate on AI: building AI systems that are dangerous, due to expecting others to do so\n\n## Joel Lehman\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Raising the bottom (i.e. helping improve societal epistemics) – in particular that in theory political decisions (at least political decision-makers) are somewhat downstream of societal epistemics  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Having weird political leaders and awful public epistemics at critical junctures seems to unnecessarily spike catastrophic/existential risk probabilities. In theory, helping uplevel or at least maintain current public epistemic health (especially as it relates to civic sensemaking) might help influence the quality of decision-makers and concrete decisions made at high-stakes moments  \n  * The shape of the tech is not clear, or probably requires multiple flavors of interventions. E.g. if LLM assistants by default help cultivate civic virtues and on the margin support critical thinking or whatever underlying skills and knowledge help a person up-level their epistemic habits / health (or at least if LLM assistants don't contribute to atrophy/harm); or somehow convincing social media networks to care about this (e.g. community notes but things that go beyond); similarly somehow help w/ important social goods like 'societal trust' and 'trust in your neighbors'  \n* What do the later stages of the path to impact look like?  \n  * \\[didn't get to this\\]  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * Healthier democratic society, better agreement on consensus reality, higher trust in society, better politicians, better concrete decisions in high-stakes moments \n\n## Austin Chen\n\n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * **Evaluator**: More agreement (or understanding of disagreement) of what orgs & people are doing good work (eg for making grants in AI safety)  \n  * **Oracle**: Instant trustworthy forecasts on any question you ask  \n  * **Planner**: Guidance on \"what are the 3 most important things I should do this day/week/month\"  \n  * **Representative**: Simulate the responses of other parties (people, orgs/corps, governments, LLM agents), for discussions, contracts, trade"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Strong Emphasis on Institutional and Organizational Impact",
          "description": "Many contributors discuss the impact of AI for epistemics on institutions and organizations, highlighting the importance of systemic change.",
          "importance": 65,
          "grade": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 9837,
            "endOffset": 10271,
            "quotedText": "AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning  \n  * The technology helps organizations maintain coherence and make sound judgments even as the pace of change accelerates"
          }
        },
        {
          "title": "Occasional Use of Unexplained Political References",
          "description": "References to 'OpenAI board drama', 'FTX crisis', and 'US-China realignment' are made without context. Briefly introducing these events would improve accessibility.",
          "importance": 45,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 5495,
            "endOffset": 5817,
            "quotedText": "* Biden not running, perhaps an open dem primary  \n* No FTX crisis  \n* Less zero sum OpenAI board drama  \n* Many smaller, better choices \n\nWhat decisions might go badly:\n\n* Trump tariffs/ global trade  \n* Taiwan/ US-China conflict  \n* US-China competition  \n* US authoritarianism  \n* AI regulation  \n* EU-China realignment"
          }
        },
        {
          "title": "Effective Use of Aspirational Language",
          "description": "Some contributors use aspirational language (e.g., 'world peace without world government', 'deep joy of being alive and human'), which inspires readers and emphasizes the stakes.",
          "importance": 60,
          "grade": 85,
          "isValid": false,
          "highlight": {
            "startOffset": 10464,
            "endOffset": 29470,
            "quotedText": "World peace without world government — sufficiently good tech to help countries understand each others' interests, find mutually desirable bargains, and make sufficient credible commitments, could help us get up towards the Pareto frontier where we avoid (virtually) all costly conflicts\n\nGoing slowly and carefully on AI development — rather than having AI progress accelerate at increasingly breakneck speed, helping actors to understand the basic situation and the desirability of different options could be crucial for facilitating meaningful and robust agreements which restrict AI development to the stuff which we have good reasons for being confident are safe // otherwise beneficial.\n\nBettering democracy — helping people to access key information about what decisions their country might make would actually mean for longer term implications, and having this in turn feed into incentives for decision-makers that better align them with the common good\n\nBuilding safe / aligned AI — better tools for anticipating when systems are actually robustly safe, vs when stronger cases for expectation of safety seem required\n\n## Josh Rosenberg (FRI)\n\nI'm massively confused about important problems, e.g. how to orient to AI risk and what to do about it. Some reasonable people think there's a high probability we effectively lose everything we care about in 2027\\. Some reasonable people think it'll be decades till we have \"AGI\" and it'd be good if it arrived sooner. Which actions are good to take in each of these worlds is very different, and I'd like to figure out what world we're in.\n\nThis also applies to many other problem areas.\n\nE.g., Trump's effect on US democracy. How much of a crisis is it? What are the most effective actions to take in response to ensure that the country and communities we care about are as well off as they can be? How can we preserve high-quality institutions to lead the world in positive directions?\n\nOr, on the positive vision side: the \"Abundance\" agenda seems promising. Is it actually a good idea? What do we do about it?\n\nAI for Epistemics could help us get a lot more clarity on how to think about these problems and what actions to take. I'd like much better information and forecasts on the likely effects of Trump, AI timelines, and the pro's and con's of different Abundance policies, for example. If I had a bunch of time and good resources to dig into these problems, I could get a much better sense for myself. I'm hoping AI will help all of us do this sort of thing collectively.\n\n## Katja Grace\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Put lots of cognitive effort into very clear and good and compelling case for AI risk  \n  * Automate talking to people about AI risk compellingly, possibly using above case as input  \n  * Good game theoretic thinking re arms races, coordination, what is possible. I.e. People are making dumb assumptions about things, e.g. \"can't slow down tech\". Could automate noting errors. So e.g. when someone says that on Twitter, can highlight it and send in response. \\-\\> good thinking via better weapons for debate (similar to Nathan project)  \n  * Specific idea: very quantified and competitive debate site, that can be cited re which views are winning, where AI does a lot of the cognitive labor in places no people are paying attention, but as a part of it gets cruxy, people pay attention and vote/bet  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Poor decision making around AI: e.g. 'racing' because of vague heuristics instead of ever doing calc on costs and benefits given specific risks  \n  * Failure to coordinate on AI: building AI systems that are dangerous, due to expecting others to do so\n\n## Joel Lehman\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Raising the bottom (i.e. helping improve societal epistemics) – in particular that in theory political decisions (at least political decision-makers) are somewhat downstream of societal epistemics  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Having weird political leaders and awful public epistemics at critical junctures seems to unnecessarily spike catastrophic/existential risk probabilities. In theory, helping uplevel or at least maintain current public epistemic health (especially as it relates to civic sensemaking) might help influence the quality of decision-makers and concrete decisions made at high-stakes moments  \n  * The shape of the tech is not clear, or probably requires multiple flavors of interventions. E.g. if LLM assistants by default help cultivate civic virtues and on the margin support critical thinking or whatever underlying skills and knowledge help a person up-level their epistemic habits / health (or at least if LLM assistants don't contribute to atrophy/harm); or somehow convincing social media networks to care about this (e.g. community notes but things that go beyond); similarly somehow help w/ important social goods like 'societal trust' and 'trust in your neighbors'  \n* What do the later stages of the path to impact look like?  \n  * \\[didn't get to this\\]  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * Healthier democratic society, better agreement on consensus reality, higher trust in society, better politicians, better concrete decisions in high-stakes moments \n\n## Austin Chen\n\n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * **Evaluator**: More agreement (or understanding of disagreement) of what orgs & people are doing good work (eg for making grants in AI safety)  \n  * **Oracle**: Instant trustworthy forecasts on any question you ask  \n  * **Planner**: Guidance on \"what are the 3 most important things I should do this day/week/month\"  \n  * **Representative**: Simulate the responses of other parties (people, orgs/corps, governments, LLM agents), for discussions, contracts, trade\n\n## Janna\n\n* Why do we care about (AI for) epistemics?  \n  * Noise to signal ratio could be much higher  \n  * AI (mis)information churned out at the price of almost zero  \n* What are use cases for AI for Epistemics that you most care about?  \n  * Solving prediction markets--is there a way to scale up the bots, even if they are wrong and lossy, to improve prediction market accuracy?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * US-China relations  \n  * Trump tariffs maybe  \n  * Immigration rules in the US  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * More accurate predictions to…  \n    * Reduce business uncertainty  \n    * Make it easier to evaluate life decisions  \n  * Forestalling bad decisions made on imperfect information\n\n## Owain\n\n1. Maybe having humans understanding the world better is good in itself. Lots of people read popular science for this reason. Maybe it's good for AIs as well  \n2. Very broadly, it seems that better world understanding and decision making enables people to make their lives better in myriad ways. (To substantiate this, one could try to separate out the effects of better world understanding from the effects of better realized technology. E.g. Looking at capital-poor countries that can still benefit from scientific knowledge, economics, etc.)  \n3. More specifically, if AI progress is fast, there will be lots of important decisions to be made in quick calendar time. Exploiting AI to improve those decisions seems valuable. Current decision making institutions do not seem well set up to adapt really quickly. (A specific concern is dangers from AI takeover).   \n4. Why now? Why this group of people? We haven't really had tech that could do most epistemic tasks until recently. Now we have AI that can do an increasing proportion of the sub-tasks of epistemic processes. This will allow us to delegate epistemic tasks to AI.  \n   1. It's likely that people do this delegation, even if it's somewhat misguided and potentially bad in the long-term. Can we make this go well?  \n   2. This potentially massively expands our epistemic capacities. How can we use this well?  \n   3. Epistemic tech/institutions/advances can be hard to fund via for-profit companies. This is in part due to public goods issues, and issues around markets for ideas.\n\n## Rafe\n\nAs ever, I hope this will stop advanced artificial intelligence destroying the world, killing all its people \\&c.\n\nSo I think there are a few paths for impact here. I think there is a world where we can make the most productive thinkers about how to do things go further and better; I'm not so excited about a single-player version of this, as I think there's a pretty narrow window when top-level centaur mode works out\\[1\\]\n\nEveryone is already thinking about AI, and, I suspect, this will only get more acute. I think it's pretty hard for people to confront this and think about it carefully and well. It's scary\\! And there are a bunch of entrenched interests and memeplexes that preserve their status better (myopically) if people only have shallow slogans to think about AI with.\n\nGiven that, I suspect that the global sensemaking machine isn't up for the task of thinking about AI well. I hope we can help the global sensemaking machine upgrade itself. I have some hope that we as a society can come up with good ideas for dealing with AI, and also come to see how they make sense, so we can choose them in a robust way.\n\nOne other path to impact: I think a society that understands things better, including itself and its impacts, is one that is more robust in a bunch of ways. A more epistemically-grounded society is also probably a bit richer, and I think a bigger pie creates more slack for arranging things in sanity-promoting ways. I have more hope in a robust society navigating this stuff well.\n\n\\[1\\]:  Yeah, maybe you can do something with shaving off subskills, but I don't think that's really that doable.\n\n## Dave\n\n* With my consequentialist hat, better epistemics being worth it has to route through improved decisions somewhere down the line. The decisions I particularly care about are those by powerful people wrt AI development and implementation and the effects this has on how the future goes  \n  * But I do feel a bit icky when I have this hat on. There seems to be something very intuitively appealing about (a) pursuing better epistemics for its own sake and (b) democratising epistemic tools for the use and benefit of everyone. I think this basically caches out in more 'raising the floor' type work  \n  * I don't feel like I have a very good way of resolving this tension yet, but am excited to see things pushing on both frontiers  \n* I think forecasting has the potential to be a pretty powerful tool and there's currently a massive undersupply of good forecasts. I'm excited for us to use the wave of increased AI labor to make better quantitative forecasts to inform key decisions. I probably care more about quantitative forecasts than most decision makers and am open to being persuaded that the other, fuzzier parts of epistemics are more valuable to work on. \n\n## Julian\n\nWhat are use cases for AI for Epistemics that you most care about?  \n**Raising the ceiling:** can we use AI to organize information and credibly resolve the most challenging and important questions on which experts still seem to disagree? For example:\n\n* COVID origins  \n* Urban planning and public policy  \n* Lots of advancements in basic science: routing around current disagreements and policy challenges with slam-dunk science might be a pretty strong way forward.\n\n**Raising the floor:** can we use AI to build extremely legibly trustworthy/reliable epistemic infrastructure that actually makes its way into people's lives?\n\n* Making truth more palatable (in a way which doesn't translate into making falsehoods more palatable)  \n* Collective epistemics: connecting people to the others with whom they would have the most productive/enlightening conversations\n\nWhat decisions may go badly by default, but could go better if we have the right tech?\n\n* Lots of stuff in public policy: downstream both of better experts (technocrats), better public (voters/interest groups), and better coordination (collective epistemics)\n\nWhat do the later stages of the path to impact look like?\n\n* I want us to live in a world where everyone is more rational, empathetic, considered in their decision making, conscientious about impact, and interested in coordinating. I feel like that's one of the biggest blockers to all kinds of *long term* progress.\n\nWhat tools do we actually maybe want to build?\n\n* Easily accessible fact checkers / claim refuters  \n  * Powerful AI debaters  \n* Forecasting with explanations: superhuman and at scale  \n* Scientific meta-analysis, systematic review  \n* Ad-hoc / bespoke probabilistic models for individual problems  \n* Bespoke *simulators* / simulation environments  \n* Consistency checking and inconsistency identification  \n  * Arbitrage / market makers in prediction markets?  \n* Using AI to accelerate development of non-AI epistemic tools:  \n  * ?\n\n## Lawrence\n\nThere are already key questions/strategic problems where the truth is murky and we seem to proceed on the basis of limited and biased views. There might well be many more of these in the future. A central example is US-China competition over AI. Some have vibes of \"China is so bad that it's worth risking an existential catastrophe to stop it from capturing a portion of the lightcone\". Others have vibes of \"Cooperation is feasible and desirable, and is our only path to a good outcome\". But how bad is China, really? How much \"realised K\" on this question do those who are making the relevant decisions actually have? Similarly, how feasible is cooperation? How valid is this frame of \"capturing the lightcone\"? One reason that we don't have a better hold on these questions is that they are frighteningly complicated and difficult  – I think we could benefit massively from throwing much more cognitive labour at them, and presenting the findings in a way that is hard to disagree with or for any individual to improve upon, other than at a normative/values level. Maybe AI can get us there.\n\n## Lukas Finnveden\n\nwhy is AI for epistemics important/helpful?\n\n* better epistemics generally very good for making all kinds of decisions  \n* I'm specifically concerned about AI potentially causing an intelligence explosion, and us needing better epistemics to handle that. Specifically:  \n  * I'm worried about AI-driven tech R\\&D leading to catastrophes. To help address this, I want AI capabilities to not only be applied to tech R\\&D, but also to be applied to \"reasoning well from limited evidence\" about a number of questions, including how to deal with risks.  \n  * I'm worried about misaligned AI — so I want techniques for eliciting AI capabilities on all kinds of questions, so that AIs don't have better capabilities than we can elicit from them. (Basically: I want good alignment techniques even on hard-to-evaluate questions.)  \n  * Seems great to me if AI-driven improvements in general knowledge and decision-making are more broadly distributed among humans. (I think this leads to better decision-making without significantly accelerating the dangers from new tech or misaligned AI. Also reduces concentration of power.)\n\n## Abram Demski\n\n\\- Why are we doing this? \\-- impact theory brainstorming exercize  \n\t\\- There's the general claim that epistemics has degraded horribly due to social media and the internet, at least in a lot of ways, although epistemics has also gotten better in some specific ways. It seems very plausible that AI is, and will continue to, make this much much worse (due to deepfakes and super-persuasive writing, to give two examples). We wish to reverse this trend, or at least slow the acceleration.  \n\t\\- The high-level reason why epistemics is important is because epistemics is how we make good decisions. So effectively, this is AI for decision-making. However, if we just set out to make tools for decision-making, I have a sense that this would be worse.  \n\t\t\\- One super important aspect of decision-making is that you have to orient towards the right problem. People do much better when they understand the problem before proposing solutions.  \n\t\t\t\\- I have a sense that producing a better shared understanding of the world JUST WOULD result in better things. The connection between epistemics and better decisionmaking is that strong.  \n\t\t\t\t\\- However, there's an element of coordination to this. You have to have common knowledge, not just knowledge. AI for coordination is yet another thing. AI for group epistemics is close to that, though, and again, I have a sense that making \"AI for coordination\" would produce worse effects on the world than \"AI for group epistemics\".  \n\t\t\\- What specific decisions are we hoping to influence?  \n\t\t\t\\- Decisions around existential risk seem super super important. If AI were generally better at epistemics (as a relative, comparative thing \\-- ie we differentially accelerate epistemics) then key decisionmakers (in the government and at AI companies) would see much more clearly what's coming. Everyone wants a better future, so a better future would have a better chance of happening if we could see and navigate.\n\nI don't think I got any further than the supporting documentation for this workshop in articulating reasons and stuff, so I predict that the LLM summary won't be better than what we already know, although I hope it is. \n\n## Brendan\n\nFor us to live together in a caring and just way, we must make decisions about what is important to us together. Is it okay to tell my neighbour to turn their music down? Do we spend money on roads or trains or libraries? Do we regulate AI development and, if so, how? Do we want to spend public money on PEPFAR, or other avenues for reducing global poverty? To make these decisions in a way that is inclusive and democratic, that fosters buy-in and trust, we must begin with a shared sense of how the world is. This shared sense must be grounded in the physical, economic, and social reality we live in.\n\nAs we live together over wider and wider contexts, maintaining a shared sense of the world becomes more difficult and subtle. These wider contexts are facilitated by new technologies, both physical (eg. global manufacturing bases and supply chains) and informational (eg. social media). New technologies, such as AI for epistemics, and help maintain epistemic coherence across these contexts.\n\nMoreover, such technologies allow new avenues for cooperation at larger scales of human involvement. This unlocks new potential for coherence in global politics, scientific discovery, social and technological innovation, and cultural expression, and ultimately the deep joy of being alive and human in the world."
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Occasional Use of Unexplained Decision Theory Concepts",
          "description": "References to 'decision theory', 'counterfactuals', and 'strategic takes' are made without explanation. Briefly introducing these concepts would improve accessibility.",
          "importance": 45,
          "grade": 50,
          "isValid": false,
          "highlight": {
            "startOffset": 2874,
            "endOffset": 30427,
            "quotedText": "I care a lot about humanity and AI, including **super-late-stage AI, getting decision theory** right.  \n  * Consequently but also separately, this relates to the general question of what to do about areas where there is not necessarily a ground truth but that are nonetheless very important to get right, cf**. [metaphilosophy](https://www.alignmentforum.org/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy)** (but also a bunch of other things.)  \n  * This also feels very similar to making **AIs helpful for AI safety research, cf. [automating alignment research](https://joecarlsmith.com/2025/03/14/ai-for-ai-safety#2-what-is-ai-for-ai-safety).**\n\n  → This is more about raising the **top of the epistemic landscape**\n\n* Other things I care about:  \n  * Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place.  \n  * Vague sense that better **societal epistemics** would help with increasing the **sanity of conversations about AI safety, polarisation around the issue etc.**  \n    * This is broadly about **getting intent alignment** in the first place.\n\n→ These are all more about raising the **bottom of the epistemic landscape**\n\n## Deger Turan:\n\n* Social trust erosion, collective coordination mechanisms are built on a weak assumption  \n* Capacity to entertain truthfulness when it is not instrumental to one's incentive gradient & differing norms upon 'truth will make my sense of security weaker'   \n* Creating contexts where world model divergence can be agreed by both parties enabling a safe disagreement  \n* Improving model reasoning / safer AI by reducing slop, by creating accountability mechanisms through consistency checks & other specifications for healthy epistemics  \n* Decoupling virality with magnitude while living in a short attention span world, curation   \n* I have some confusion re: raising the floor when not all parties are necessarily operating in good faith. I think I need some help think through the ways in which epistemics in the context of incentive misalignment can be used or have defensibility. \n\n## Nathan Young: \n\nWhat would we have got:\n\n* Biden not running, perhaps an open dem primary  \n* No FTX crisis  \n* Less zero sum OpenAI board drama  \n* Many smaller, better choices \n\nWhat decisions might go badly:\n\n* Trump tariffs/ global trade  \n* Taiwan/ US-China conflict  \n* US-China competition  \n* US authoritarianism  \n* AI regulation  \n* EU-China realignment  \n* Automated robots  \n* OpenAI strategy  \n* Anthropic strategy  \n* Google strategy  \n* xAI strategy  \n* AI workforce automation  \n* China strategy  \n* Russia strategy  \n* Moskovitz strategy  \n* Must strategy  \n* Bezos Strategy  \n* Zuckerberg strategy  \n* Tallinn strategy  \n* Altman strategy  \n* Drone warfare  \n* Climate change destabilisation  \n* Democratic trust\n\n## Lincoln Quirk:\n\nWhy do we care about (AI for) epistemics?  \n\\- making the public better informed:  \n\t\\- better able to deal with and process the wealth of information  \n\t\\- better able to express their own interests in government (via voting)  \n\t\\- helping cut through social media distraction / intentional misinformation  \n\\- getting governments to implement better policy  \n\t\\- coming up with good policy ideas  \n\t\\- objectively doing cost-benefit analysis  \n\t\\- objectively explaining tradeoffs (vs having to rely on biased humans to explain them)  \n\t\\- flagging bad ideas as bad  \n\t\\- convincing policymakers with compelling writeups  \n\\- success and flourishing for individuals and orgs using these tools  \n\t\\- via helping them think through decisions  \n\t\\- via clearheaded analysis of trade-offs and cost/benefits  \n\t\\- flagging bad ideas as bad\n\n## Jay\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * How can we build something like the positive version of Community Notes: which statements/posts are agreed on by people who often disagree?  \n  * How can we better understand/summarize what others think, e.g. on social media platforms? E.g. if you see a post recommended to you, is this because it's the most extreme take that went viral and isn't representative, or is this a commonly held view? Between different viewpoints, what are the concrete points of actual disagreement (on facts or values) and what are the things widely agreed upon? Let's build viewpoint/debate maps or a similar sensemaking tool that gets adopted/understood by the masses/raises the knowledge floor (in my experience, things like pol.is are hard for regular users to grok)  \n  * How can we improve Community Notes (or more general knowledge) creation with AI assistants? An AI agent proposing community notes and humans still checking/rating them?  \n  * How can we have something like Community Notes that applies to claims or statements about the future, or which are not concretely knowable?  \n  * \n\n## Ben Wilson (Metaculus)\n\nOverall I believe the most important value of epistemics (and AI epistemics) in general is informing what actions allow us to achieve our goals (i.e. make the world better). If we had perfect AI epistemics I would be excited by the ability for us to take a goal, and get a list of possible actions we could take and the forecasted outcomes of each of those actions (plus background information and forecasts for the 'current lay of the land' for context). This can then inform, not just the risk of AI doing good/bad things (as with current forecasting), but what we can do to raise the chances of 'great' AI outcomes, and lower the changes of 'worse' AI outcomes\n\n## Andreas\n\n* **What are use cases for AI for Epistemics that you most care about?**  \n  * AI policy, broadly construed, must go better  \n    * Avoid gradual disempowerment  \n    * Thoughtful regulation  \n  * Organizational planning & decision-making is weak  \n    * Given an informal long-term goal like improve decision-making for strategic users in government & large orgs, make a plan for a 20-people org that helps them accomplish the goal  \n    * Mapping dependencies, uncertainties, and potential futures  \n    * Understand complex domains with non-trivial feedback loops  \n      * Economic shifts  \n* **What decisions may go badly by default, but could go better if we have the right tech?**  \n  * Resource allocation within orgs  \n    * Investment  \n    * Grant making  \n    * Government budgets  \n  * Policy  \n    * How should AI be deployed in the world?  \n* **What do the later stages of the path to impact look like?**  \n  * AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning  \n  * The technology helps organizations maintain coherence and make sound judgments even as the pace of change accelerates  \n  * Assumption: People are generally well-intentioned \\- no one wants catastrophe \\- and as a result of better tooling things go better in the world overall\n\n## Owen optimistic brainstorm:\n\nWorld peace without world government — sufficiently good tech to help countries understand each others' interests, find mutually desirable bargains, and make sufficient credible commitments, could help us get up towards the Pareto frontier where we avoid (virtually) all costly conflicts\n\nGoing slowly and carefully on AI development — rather than having AI progress accelerate at increasingly breakneck speed, helping actors to understand the basic situation and the desirability of different options could be crucial for facilitating meaningful and robust agreements which restrict AI development to the stuff which we have good reasons for being confident are safe // otherwise beneficial.\n\nBettering democracy — helping people to access key information about what decisions their country might make would actually mean for longer term implications, and having this in turn feed into incentives for decision-makers that better align them with the common good\n\nBuilding safe / aligned AI — better tools for anticipating when systems are actually robustly safe, vs when stronger cases for expectation of safety seem required\n\n## Josh Rosenberg (FRI)\n\nI'm massively confused about important problems, e.g. how to orient to AI risk and what to do about it. Some reasonable people think there's a high probability we effectively lose everything we care about in 2027\\. Some reasonable people think it'll be decades till we have \"AGI\" and it'd be good if it arrived sooner. Which actions are good to take in each of these worlds is very different, and I'd like to figure out what world we're in.\n\nThis also applies to many other problem areas.\n\nE.g., Trump's effect on US democracy. How much of a crisis is it? What are the most effective actions to take in response to ensure that the country and communities we care about are as well off as they can be? How can we preserve high-quality institutions to lead the world in positive directions?\n\nOr, on the positive vision side: the \"Abundance\" agenda seems promising. Is it actually a good idea? What do we do about it?\n\nAI for Epistemics could help us get a lot more clarity on how to think about these problems and what actions to take. I'd like much better information and forecasts on the likely effects of Trump, AI timelines, and the pro's and con's of different Abundance policies, for example. If I had a bunch of time and good resources to dig into these problems, I could get a much better sense for myself. I'm hoping AI will help all of us do this sort of thing collectively.\n\n## Katja Grace\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Put lots of cognitive effort into very clear and good and compelling case for AI risk  \n  * Automate talking to people about AI risk compellingly, possibly using above case as input  \n  * Good game theoretic thinking re arms races, coordination, what is possible. I.e. People are making dumb assumptions about things, e.g. \"can't slow down tech\". Could automate noting errors. So e.g. when someone says that on Twitter, can highlight it and send in response. \\-\\> good thinking via better weapons for debate (similar to Nathan project)  \n  * Specific idea: very quantified and competitive debate site, that can be cited re which views are winning, where AI does a lot of the cognitive labor in places no people are paying attention, but as a part of it gets cruxy, people pay attention and vote/bet  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Poor decision making around AI: e.g. 'racing' because of vague heuristics instead of ever doing calc on costs and benefits given specific risks  \n  * Failure to coordinate on AI: building AI systems that are dangerous, due to expecting others to do so\n\n## Joel Lehman\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Raising the bottom (i.e. helping improve societal epistemics) – in particular that in theory political decisions (at least political decision-makers) are somewhat downstream of societal epistemics  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Having weird political leaders and awful public epistemics at critical junctures seems to unnecessarily spike catastrophic/existential risk probabilities. In theory, helping uplevel or at least maintain current public epistemic health (especially as it relates to civic sensemaking) might help influence the quality of decision-makers and concrete decisions made at high-stakes moments  \n  * The shape of the tech is not clear, or probably requires multiple flavors of interventions. E.g. if LLM assistants by default help cultivate civic virtues and on the margin support critical thinking or whatever underlying skills and knowledge help a person up-level their epistemic habits / health (or at least if LLM assistants don't contribute to atrophy/harm); or somehow convincing social media networks to care about this (e.g. community notes but things that go beyond); similarly somehow help w/ important social goods like 'societal trust' and 'trust in your neighbors'  \n* What do the later stages of the path to impact look like?  \n  * \\[didn't get to this\\]  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * Healthier democratic society, better agreement on consensus reality, higher trust in society, better politicians, better concrete decisions in high-stakes moments \n\n## Austin Chen\n\n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * **Evaluator**: More agreement (or understanding of disagreement) of what orgs & people are doing good work (eg for making grants in AI safety)  \n  * **Oracle**: Instant trustworthy forecasts on any question you ask  \n  * **Planner**: Guidance on \"what are the 3 most important things I should do this day/week/month\"  \n  * **Representative**: Simulate the responses of other parties (people, orgs/corps, governments, LLM agents), for discussions, contracts, trade\n\n## Janna\n\n* Why do we care about (AI for) epistemics?  \n  * Noise to signal ratio could be much higher  \n  * AI (mis)information churned out at the price of almost zero  \n* What are use cases for AI for Epistemics that you most care about?  \n  * Solving prediction markets--is there a way to scale up the bots, even if they are wrong and lossy, to improve prediction market accuracy?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * US-China relations  \n  * Trump tariffs maybe  \n  * Immigration rules in the US  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * More accurate predictions to…  \n    * Reduce business uncertainty  \n    * Make it easier to evaluate life decisions  \n  * Forestalling bad decisions made on imperfect information\n\n## Owain\n\n1. Maybe having humans understanding the world better is good in itself. Lots of people read popular science for this reason. Maybe it's good for AIs as well  \n2. Very broadly, it seems that better world understanding and decision making enables people to make their lives better in myriad ways. (To substantiate this, one could try to separate out the effects of better world understanding from the effects of better realized technology. E.g. Looking at capital-poor countries that can still benefit from scientific knowledge, economics, etc.)  \n3. More specifically, if AI progress is fast, there will be lots of important decisions to be made in quick calendar time. Exploiting AI to improve those decisions seems valuable. Current decision making institutions do not seem well set up to adapt really quickly. (A specific concern is dangers from AI takeover).   \n4. Why now? Why this group of people? We haven't really had tech that could do most epistemic tasks until recently. Now we have AI that can do an increasing proportion of the sub-tasks of epistemic processes. This will allow us to delegate epistemic tasks to AI.  \n   1. It's likely that people do this delegation, even if it's somewhat misguided and potentially bad in the long-term. Can we make this go well?  \n   2. This potentially massively expands our epistemic capacities. How can we use this well?  \n   3. Epistemic tech/institutions/advances can be hard to fund via for-profit companies. This is in part due to public goods issues, and issues around markets for ideas.\n\n## Rafe\n\nAs ever, I hope this will stop advanced artificial intelligence destroying the world, killing all its people \\&c.\n\nSo I think there are a few paths for impact here. I think there is a world where we can make the most productive thinkers about how to do things go further and better; I'm not so excited about a single-player version of this, as I think there's a pretty narrow window when top-level centaur mode works out\\[1\\]\n\nEveryone is already thinking about AI, and, I suspect, this will only get more acute. I think it's pretty hard for people to confront this and think about it carefully and well. It's scary\\! And there are a bunch of entrenched interests and memeplexes that preserve their status better (myopically) if people only have shallow slogans to think about AI with.\n\nGiven that, I suspect that the global sensemaking machine isn't up for the task of thinking about AI well. I hope we can help the global sensemaking machine upgrade itself. I have some hope that we as a society can come up with good ideas for dealing with AI, and also come to see how they make sense, so we can choose them in a robust way.\n\nOne other path to impact: I think a society that understands things better, including itself and its impacts, is one that is more robust in a bunch of ways. A more epistemically-grounded society is also probably a bit richer, and I think a bigger pie creates more slack for arranging things in sanity-promoting ways. I have more hope in a robust society navigating this stuff well.\n\n\\[1\\]:  Yeah, maybe you can do something with shaving off subskills, but I don't think that's really that doable.\n\n## Dave\n\n* With my consequentialist hat, better epistemics being worth it has to route through improved decisions somewhere down the line. The decisions I particularly care about are those by powerful people wrt AI development and implementation and the effects this has on how the future goes  \n  * But I do feel a bit icky when I have this hat on. There seems to be something very intuitively appealing about (a) pursuing better epistemics for its own sake and (b) democratising epistemic tools for the use and benefit of everyone. I think this basically caches out in more 'raising the floor' type work  \n  * I don't feel like I have a very good way of resolving this tension yet, but am excited to see things pushing on both frontiers  \n* I think forecasting has the potential to be a pretty powerful tool and there's currently a massive undersupply of good forecasts. I'm excited for us to use the wave of increased AI labor to make better quantitative forecasts to inform key decisions. I probably care more about quantitative forecasts than most decision makers and am open to being persuaded that the other, fuzzier parts of epistemics are more valuable to work on. \n\n## Julian\n\nWhat are use cases for AI for Epistemics that you most care about?  \n**Raising the ceiling:** can we use AI to organize information and credibly resolve the most challenging and important questions on which experts still seem to disagree? For example:\n\n* COVID origins  \n* Urban planning and public policy  \n* Lots of advancements in basic science: routing around current disagreements and policy challenges with slam-dunk science might be a pretty strong way forward.\n\n**Raising the floor:** can we use AI to build extremely legibly trustworthy/reliable epistemic infrastructure that actually makes its way into people's lives?\n\n* Making truth more palatable (in a way which doesn't translate into making falsehoods more palatable)  \n* Collective epistemics: connecting people to the others with whom they would have the most productive/enlightening conversations\n\nWhat decisions may go badly by default, but could go better if we have the right tech?\n\n* Lots of stuff in public policy: downstream both of better experts (technocrats), better public (voters/interest groups), and better coordination (collective epistemics)\n\nWhat do the later stages of the path to impact look like?\n\n* I want us to live in a world where everyone is more rational, empathetic, considered in their decision making, conscientious about impact, and interested in coordinating. I feel like that's one of the biggest blockers to all kinds of *long term* progress.\n\nWhat tools do we actually maybe want to build?\n\n* Easily accessible fact checkers / claim refuters  \n  * Powerful AI debaters  \n* Forecasting with explanations: superhuman and at scale  \n* Scientific meta-analysis, systematic review  \n* Ad-hoc / bespoke probabilistic models for individual problems  \n* Bespoke *simulators* / simulation environments  \n* Consistency checking and inconsistency identification  \n  * Arbitrage / market makers in prediction markets?  \n* Using AI to accelerate development of non-AI epistemic tools:  \n  * ?\n\n## Lawrence\n\nThere are already key questions/strategic problems where the truth is murky and we seem to proceed on the basis of limited and biased views. There might well be many more of these in the future. A central example is US-China competition over AI. Some have vibes of \"China is so bad that it's worth risking an existential catastrophe to stop it from capturing a portion of the lightcone\". Others have vibes of \"Cooperation is feasible and desirable, and is our only path to a good outcome\". But how bad is China, really? How much \"realised K\" on this question do those who are making the relevant decisions actually have? Similarly, how feasible is cooperation? How valid is this frame of \"capturing the lightcone\"? One reason that we don't have a better hold on these questions is that they are frighteningly complicated and difficult  – I think we could benefit massively from throwing much more cognitive labour at them, and presenting the findings in a way that is hard to disagree with or for any individual to improve upon, other than at a normative/values level. Maybe AI can get us there.\n\n## Lukas Finnveden\n\nwhy is AI for epistemics important/helpful?\n\n* better epistemics generally very good for making all kinds of decisions  \n* I'm specifically concerned about AI potentially causing an intelligence explosion, and us needing better epistemics to handle that. Specifically:  \n  * I'm worried about AI-driven tech R\\&D leading to catastrophes. To help address this, I want AI capabilities to not only be applied to tech R\\&D, but also to be applied to \"reasoning well from limited evidence\" about a number of questions, including how to deal with risks.  \n  * I'm worried about misaligned AI — so I want techniques for eliciting AI capabilities on all kinds of questions, so that AIs don't have better capabilities than we can elicit from them. (Basically: I want good alignment techniques even on hard-to-evaluate questions.)  \n  * Seems great to me if AI-driven improvements in general knowledge and decision-making are more broadly distributed among humans. (I think this leads to better decision-making without significantly accelerating the dangers from new tech or misaligned AI. Also reduces concentration of power.)\n\n## Abram Demski\n\n\\- Why are we doing this? \\-- impact theory brainstorming exercize  \n\t\\- There's the general claim that epistemics has degraded horribly due to social media and the internet, at least in a lot of ways, although epistemics has also gotten better in some specific ways. It seems very plausible that AI is, and will continue to, make this much much worse (due to deepfakes and super-persuasive writing, to give two examples). We wish to reverse this trend, or at least slow the acceleration.  \n\t\\- The high-level reason why epistemics is important is because epistemics is how we make good decisions. So effectively, this is AI for decision-making. However, if we just set out to make tools for decision-making, I have a sense that this would be worse.  \n\t\t\\- One super important aspect of decision-making is that you have to orient towards the right problem. People do much better when they understand the problem before proposing solutions.  \n\t\t\t\\- I have a sense that producing a better shared understanding of the world JUST WOULD result in better things. The connection between epistemics and better decisionmaking is that strong.  \n\t\t\t\t\\- However, there's an element of coordination to this. You have to have common knowledge, not just knowledge. AI for coordination is yet another thing. AI for group epistemics is close to that, though, and again, I have a sense that making \"AI for coordination\" would produce worse effects on the world than \"AI for group epistemics\".  \n\t\t\\- What specific decisions are we hoping to influence?  \n\t\t\t\\- Decisions around existential risk seem super super important. If AI were generally better at epistemics (as a relative, comparative thing \\-- ie we differentially accelerate epistemics) then key decisionmakers (in the government and at AI companies) would see much more clearly what's coming. Everyone wants a better future, so a better future would have a better chance of happening if we could see and navigate.\n\nI don't think I got any further than the supporting documentation for this workshop in articulating reasons and stuff, so I predict that the LLM summary won't be better than what we already know, although I hope it is. \n\n## Brendan\n\nFor us to live together in a caring and just way, we must make decisions about what is important to us together. Is it okay to tell my neighbour to turn their music down? Do we spend money on roads or trains or libraries? Do we regulate AI development and, if so, how? Do we want to spend public money on PEPFAR, or other avenues for reducing global poverty? To make these decisions in a way that is inclusive and democratic, that fosters buy-in and trust, we must begin with a shared sense of how the world is. This shared sense must be grounded in the physical, economic, and social reality we live in.\n\nAs we live together over wider and wider contexts, maintaining a shared sense of the world becomes more difficult and subtle. These wider contexts are facilitated by new technologies, both physical (eg. global manufacturing bases and supply chains) and informational (eg. social media). New technologies, such as AI for epistemics, and help maintain epistemic coherence across these contexts.\n\nMoreover, such technologies allow new avenues for cooperation at larger scales of human involvement. This unlocks new potential for coherence in global politics, scientific discovery, social and technological innovation, and cultural expression, and ultimately the deep joy of being alive and human in the world. \n\n## Isabel: \n\nThe kinds of being right in the past that are most relevant for decision making aren't just about making straightforward predictions about what will happen, but are more nuanced about what kinds of actions one should take to cause those things to happen, and these seem like somewhat different skills. These are harder to evaluate (e.g. because they're counterfactual: either we listened to person a and then we have to evaluate whether things went better or worse than if we didn't, but hard to say what the counterfactual would have been; or we didn't listen to person a and then we have to guess what things would have been like if we did) \n\nA bit adjacent to Neel Nanda's [recent piece](https://www.lesswrong.com/posts/P5zWiPF5cPJZSkiAK/good-research-takes-are-not-sufficient-for-good-strategic) on how \"good research takes aren't sufficient for good strategic takes\" \\- \"good forecasting takes aren't sufficient for good strategic takes"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Strong Emphasis on Information Quality and Trustworthiness",
          "description": "Several contributors focus on the importance of trustworthy, high-quality information, which aligns with the core goals of epistemics.",
          "importance": 70,
          "grade": 90,
          "isValid": false,
          "highlight": {
            "startOffset": 16530,
            "endOffset": 31368,
            "quotedText": "Noise to signal ratio could be much higher  \n  * AI (mis)information churned out at the price of almost zero  \n* What are use cases for AI for Epistemics that you most care about?  \n  * Solving prediction markets--is there a way to scale up the bots, even if they are wrong and lossy, to improve prediction market accuracy?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * US-China relations  \n  * Trump tariffs maybe  \n  * Immigration rules in the US  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * More accurate predictions to…  \n    * Reduce business uncertainty  \n    * Make it easier to evaluate life decisions  \n  * Forestalling bad decisions made on imperfect information\n\n## Owain\n\n1. Maybe having humans understanding the world better is good in itself. Lots of people read popular science for this reason. Maybe it's good for AIs as well  \n2. Very broadly, it seems that better world understanding and decision making enables people to make their lives better in myriad ways. (To substantiate this, one could try to separate out the effects of better world understanding from the effects of better realized technology. E.g. Looking at capital-poor countries that can still benefit from scientific knowledge, economics, etc.)  \n3. More specifically, if AI progress is fast, there will be lots of important decisions to be made in quick calendar time. Exploiting AI to improve those decisions seems valuable. Current decision making institutions do not seem well set up to adapt really quickly. (A specific concern is dangers from AI takeover).   \n4. Why now? Why this group of people? We haven't really had tech that could do most epistemic tasks until recently. Now we have AI that can do an increasing proportion of the sub-tasks of epistemic processes. This will allow us to delegate epistemic tasks to AI.  \n   1. It's likely that people do this delegation, even if it's somewhat misguided and potentially bad in the long-term. Can we make this go well?  \n   2. This potentially massively expands our epistemic capacities. How can we use this well?  \n   3. Epistemic tech/institutions/advances can be hard to fund via for-profit companies. This is in part due to public goods issues, and issues around markets for ideas.\n\n## Rafe\n\nAs ever, I hope this will stop advanced artificial intelligence destroying the world, killing all its people \\&c.\n\nSo I think there are a few paths for impact here. I think there is a world where we can make the most productive thinkers about how to do things go further and better; I'm not so excited about a single-player version of this, as I think there's a pretty narrow window when top-level centaur mode works out\\[1\\]\n\nEveryone is already thinking about AI, and, I suspect, this will only get more acute. I think it's pretty hard for people to confront this and think about it carefully and well. It's scary\\! And there are a bunch of entrenched interests and memeplexes that preserve their status better (myopically) if people only have shallow slogans to think about AI with.\n\nGiven that, I suspect that the global sensemaking machine isn't up for the task of thinking about AI well. I hope we can help the global sensemaking machine upgrade itself. I have some hope that we as a society can come up with good ideas for dealing with AI, and also come to see how they make sense, so we can choose them in a robust way.\n\nOne other path to impact: I think a society that understands things better, including itself and its impacts, is one that is more robust in a bunch of ways. A more epistemically-grounded society is also probably a bit richer, and I think a bigger pie creates more slack for arranging things in sanity-promoting ways. I have more hope in a robust society navigating this stuff well.\n\n\\[1\\]:  Yeah, maybe you can do something with shaving off subskills, but I don't think that's really that doable.\n\n## Dave\n\n* With my consequentialist hat, better epistemics being worth it has to route through improved decisions somewhere down the line. The decisions I particularly care about are those by powerful people wrt AI development and implementation and the effects this has on how the future goes  \n  * But I do feel a bit icky when I have this hat on. There seems to be something very intuitively appealing about (a) pursuing better epistemics for its own sake and (b) democratising epistemic tools for the use and benefit of everyone. I think this basically caches out in more 'raising the floor' type work  \n  * I don't feel like I have a very good way of resolving this tension yet, but am excited to see things pushing on both frontiers  \n* I think forecasting has the potential to be a pretty powerful tool and there's currently a massive undersupply of good forecasts. I'm excited for us to use the wave of increased AI labor to make better quantitative forecasts to inform key decisions. I probably care more about quantitative forecasts than most decision makers and am open to being persuaded that the other, fuzzier parts of epistemics are more valuable to work on. \n\n## Julian\n\nWhat are use cases for AI for Epistemics that you most care about?  \n**Raising the ceiling:** can we use AI to organize information and credibly resolve the most challenging and important questions on which experts still seem to disagree? For example:\n\n* COVID origins  \n* Urban planning and public policy  \n* Lots of advancements in basic science: routing around current disagreements and policy challenges with slam-dunk science might be a pretty strong way forward.\n\n**Raising the floor:** can we use AI to build extremely legibly trustworthy/reliable epistemic infrastructure that actually makes its way into people's lives?\n\n* Making truth more palatable (in a way which doesn't translate into making falsehoods more palatable)  \n* Collective epistemics: connecting people to the others with whom they would have the most productive/enlightening conversations\n\nWhat decisions may go badly by default, but could go better if we have the right tech?\n\n* Lots of stuff in public policy: downstream both of better experts (technocrats), better public (voters/interest groups), and better coordination (collective epistemics)\n\nWhat do the later stages of the path to impact look like?\n\n* I want us to live in a world where everyone is more rational, empathetic, considered in their decision making, conscientious about impact, and interested in coordinating. I feel like that's one of the biggest blockers to all kinds of *long term* progress.\n\nWhat tools do we actually maybe want to build?\n\n* Easily accessible fact checkers / claim refuters  \n  * Powerful AI debaters  \n* Forecasting with explanations: superhuman and at scale  \n* Scientific meta-analysis, systematic review  \n* Ad-hoc / bespoke probabilistic models for individual problems  \n* Bespoke *simulators* / simulation environments  \n* Consistency checking and inconsistency identification  \n  * Arbitrage / market makers in prediction markets?  \n* Using AI to accelerate development of non-AI epistemic tools:  \n  * ?\n\n## Lawrence\n\nThere are already key questions/strategic problems where the truth is murky and we seem to proceed on the basis of limited and biased views. There might well be many more of these in the future. A central example is US-China competition over AI. Some have vibes of \"China is so bad that it's worth risking an existential catastrophe to stop it from capturing a portion of the lightcone\". Others have vibes of \"Cooperation is feasible and desirable, and is our only path to a good outcome\". But how bad is China, really? How much \"realised K\" on this question do those who are making the relevant decisions actually have? Similarly, how feasible is cooperation? How valid is this frame of \"capturing the lightcone\"? One reason that we don't have a better hold on these questions is that they are frighteningly complicated and difficult  – I think we could benefit massively from throwing much more cognitive labour at them, and presenting the findings in a way that is hard to disagree with or for any individual to improve upon, other than at a normative/values level. Maybe AI can get us there.\n\n## Lukas Finnveden\n\nwhy is AI for epistemics important/helpful?\n\n* better epistemics generally very good for making all kinds of decisions  \n* I'm specifically concerned about AI potentially causing an intelligence explosion, and us needing better epistemics to handle that. Specifically:  \n  * I'm worried about AI-driven tech R\\&D leading to catastrophes. To help address this, I want AI capabilities to not only be applied to tech R\\&D, but also to be applied to \"reasoning well from limited evidence\" about a number of questions, including how to deal with risks.  \n  * I'm worried about misaligned AI — so I want techniques for eliciting AI capabilities on all kinds of questions, so that AIs don't have better capabilities than we can elicit from them. (Basically: I want good alignment techniques even on hard-to-evaluate questions.)  \n  * Seems great to me if AI-driven improvements in general knowledge and decision-making are more broadly distributed among humans. (I think this leads to better decision-making without significantly accelerating the dangers from new tech or misaligned AI. Also reduces concentration of power.)\n\n## Abram Demski\n\n\\- Why are we doing this? \\-- impact theory brainstorming exercize  \n\t\\- There's the general claim that epistemics has degraded horribly due to social media and the internet, at least in a lot of ways, although epistemics has also gotten better in some specific ways. It seems very plausible that AI is, and will continue to, make this much much worse (due to deepfakes and super-persuasive writing, to give two examples). We wish to reverse this trend, or at least slow the acceleration.  \n\t\\- The high-level reason why epistemics is important is because epistemics is how we make good decisions. So effectively, this is AI for decision-making. However, if we just set out to make tools for decision-making, I have a sense that this would be worse.  \n\t\t\\- One super important aspect of decision-making is that you have to orient towards the right problem. People do much better when they understand the problem before proposing solutions.  \n\t\t\t\\- I have a sense that producing a better shared understanding of the world JUST WOULD result in better things. The connection between epistemics and better decisionmaking is that strong.  \n\t\t\t\t\\- However, there's an element of coordination to this. You have to have common knowledge, not just knowledge. AI for coordination is yet another thing. AI for group epistemics is close to that, though, and again, I have a sense that making \"AI for coordination\" would produce worse effects on the world than \"AI for group epistemics\".  \n\t\t\\- What specific decisions are we hoping to influence?  \n\t\t\t\\- Decisions around existential risk seem super super important. If AI were generally better at epistemics (as a relative, comparative thing \\-- ie we differentially accelerate epistemics) then key decisionmakers (in the government and at AI companies) would see much more clearly what's coming. Everyone wants a better future, so a better future would have a better chance of happening if we could see and navigate.\n\nI don't think I got any further than the supporting documentation for this workshop in articulating reasons and stuff, so I predict that the LLM summary won't be better than what we already know, although I hope it is. \n\n## Brendan\n\nFor us to live together in a caring and just way, we must make decisions about what is important to us together. Is it okay to tell my neighbour to turn their music down? Do we spend money on roads or trains or libraries? Do we regulate AI development and, if so, how? Do we want to spend public money on PEPFAR, or other avenues for reducing global poverty? To make these decisions in a way that is inclusive and democratic, that fosters buy-in and trust, we must begin with a shared sense of how the world is. This shared sense must be grounded in the physical, economic, and social reality we live in.\n\nAs we live together over wider and wider contexts, maintaining a shared sense of the world becomes more difficult and subtle. These wider contexts are facilitated by new technologies, both physical (eg. global manufacturing bases and supply chains) and informational (eg. social media). New technologies, such as AI for epistemics, and help maintain epistemic coherence across these contexts.\n\nMoreover, such technologies allow new avenues for cooperation at larger scales of human involvement. This unlocks new potential for coherence in global politics, scientific discovery, social and technological innovation, and cultural expression, and ultimately the deep joy of being alive and human in the world. \n\n## Isabel: \n\nThe kinds of being right in the past that are most relevant for decision making aren't just about making straightforward predictions about what will happen, but are more nuanced about what kinds of actions one should take to cause those things to happen, and these seem like somewhat different skills. These are harder to evaluate (e.g. because they're counterfactual: either we listened to person a and then we have to evaluate whether things went better or worse than if we didn't, but hard to say what the counterfactual would have been; or we didn't listen to person a and then we have to guess what things would have been like if we did) \n\nA bit adjacent to Neel Nanda's [recent piece](https://www.lesswrong.com/posts/P5zWiPF5cPJZSkiAK/good-research-takes-are-not-sufficient-for-good-strategic) on how \"good research takes aren't sufficient for good strategic takes\" \\- \"good forecasting takes aren't sufficient for good strategic takes\", but maybe some of this is maybe resolvable on the forecasting tech side \\- can we get better at creating and evaluating results from fuzzier questions? Can AI help? (Also can AI help with the whole problem, but this feels small and concrete enough to maybe get some traction on.) Also, some risk of actors building up good track records in order to be listened to, but then choosing to burn that track record to get power or things like that, something something trust \\- a concern for both human actors and e.g. scheming AIs \\- what things look good until they abruptly aren't? \n\nOn a different note, also interested in takes on what to do about and how to engage with actors who have epistemic processes that don't seem to be truth seeking in a comprehensible-to-me way. \n\n## Adam Marblestone:\n\nWhat do we get out of AI for epistemics?\n\nTrust, is one thing. Our current society seems to be fraught with distrust and motivated reasoning."
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Occasional Use of Unexplained AI Safety Concepts",
          "description": "Terms like 'alignment', 'intent-alignment', and 'AI safety research' are used without explanation. Briefly defining these concepts would improve accessibility.",
          "importance": 50,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 3334,
            "endOffset": 4224,
            "quotedText": "This also feels very similar to making **AIs helpful for AI safety research, cf. [automating alignment research](https://joecarlsmith.com/2025/03/14/ai-for-ai-safety#2-what-is-ai-for-ai-safety).**\n\n  → This is more about raising the **top of the epistemic landscape**\n\n* Other things I care about:  \n  * Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place."
          }
        },
        {
          "title": "Effective Use of Problem-Solution Structure",
          "description": "Many sections follow a problem-solution structure (e.g., identifying a challenge, then proposing how AI for epistemics could address it), which aids comprehension and persuasiveness.",
          "importance": 65,
          "grade": 85,
          "isValid": false,
          "highlight": {
            "startOffset": 23715,
            "endOffset": 24810,
            "quotedText": "There are already key questions/strategic problems where the truth is murky and we seem to proceed on the basis of limited and biased views. There might well be many more of these in the future. A central example is US-China competition over AI. Some have vibes of \"China is so bad that it's worth risking an existential catastrophe to stop it from capturing a portion of the lightcone\". Others have vibes of \"Cooperation is feasible and desirable, and is our only path to a good outcome\". But how bad is China, really? How much \"realised K\" on this question do those who are making the relevant decisions actually have? Similarly, how feasible is cooperation? How valid is this frame of \"capturing the lightcone\"? One reason that we don't have a better hold on these questions is that they are frighteningly complicated and difficult  – I think we could benefit massively from throwing much more cognitive labour at them, and presenting the findings in a way that is hard to disagree with or for any individual to improve upon, other than at a normative/values level. Maybe AI can get us there."
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Occasional Use of Unexplained Forecasting Concepts",
          "description": "Terms like 'quantitative forecasts', 'prediction markets', and 'superhuman forecasting' are used without explanation. Briefly defining these concepts would improve accessibility.",
          "importance": 50,
          "grade": 50,
          "isValid": false,
          "highlight": {
            "startOffset": 16716,
            "endOffset": 1181,
            "quotedText": "\n\n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nReasonable judgment on tricky questions that's too cheap to meter, better than if you asked a respected colleague to spend many minutes on it.\n\n## Aviv\n\n* Better government governance  \n* Better democratic functioning  \n* Reduced dangerous concentration of power  \n    \n* Healthy polarization / pluralism  \n    \n* More peace, less likelihood of war  \n* Better geopolitics  \n* Better economic incentives  \n    \n* Better AI org governance  \n* Better AI coordination (re. distribution, escalation, etc.)  \n* Better compute allocation (toward what matters for good outcomes)  \n    \n* Better allocation of resources overall  \n* Better lives for people  \n* More effective organizations, getting things done with fewer resources, more focused on the things that benefit everyone  \n    \n* Identification of common ground directions / equilibria / game theoretic strategies that create/incentivize new positive gradients (and not surfacing the bad versions of those…)  \n    \n* Understanding our own goals and values\n\n*Note how it is the intersection of better epistemics / world models + the right power structures / incentives needed for all of these.*\n\n## Ben Goldhaber:\n\n* Forecasting how I should live my life in a period of intense technological change  \n* Guiding decision makers to avoid war with china/ai/everyone  \n* Providing the background context for me to sensemake way more sanely.  \n* Pushing forward the extant knowledge of AI safety agendas  \n* Planning for multi-stage projects that could achieve big impact  \n* Roadmapping to build larger more complex things\n\n## Chi\n\nThere are a few different things I care about:\n\n* I care a lot about humanity and AI, including **super-late-stage AI, getting decision theory** right.  \n  * Consequently but also separately, this relates to the general question of what to do about areas where there is not necessarily a ground truth but that are nonetheless very important to get right, cf**. [metaphilosophy](https://www.alignmentforum.org/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy)** (but also a bunch of other things.)  \n  * This also feels very similar to making **AIs helpful for AI safety research, cf. [automating alignment research](https://joecarlsmith.com/2025/03/14/ai-for-ai-safety#2-what-is-ai-for-ai-safety).**\n\n  → This is more about raising the **top of the epistemic landscape**\n\n* Other things I care about:  \n  * Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place.  \n  * Vague sense that better **societal epistemics** would help with increasing the **sanity of conversations about AI safety, polarisation around the issue etc.**  \n    * This is broadly about **getting intent alignment** in the first place.\n\n→ These are all more about raising the **bottom of the epistemic landscape**\n\n## Deger Turan:\n\n* Social trust erosion, collective coordination mechanisms are built on a weak assumption  \n* Capacity to entertain truthfulness when it is not instrumental to one's incentive gradient & differing norms upon 'truth will make my sense of security weaker'   \n* Creating contexts where world model divergence can be agreed by both parties enabling a safe disagreement  \n* Improving model reasoning / safer AI by reducing slop, by creating accountability mechanisms through consistency checks & other specifications for healthy epistemics  \n* Decoupling virality with magnitude while living in a short attention span world, curation   \n* I have some confusion re: raising the floor when not all parties are necessarily operating in good faith. I think I need some help think through the ways in which epistemics in the context of incentive misalignment can be used or have defensibility. \n\n## Nathan Young: \n\nWhat would we have got:\n\n* Biden not running, perhaps an open dem primary  \n* No FTX crisis  \n* Less zero sum OpenAI board drama  \n* Many smaller, better choices \n\nWhat decisions might go badly:\n\n* Trump tariffs/ global trade  \n* Taiwan/ US-China conflict  \n* US-China competition  \n* US authoritarianism  \n* AI regulation  \n* EU-China realignment  \n* Automated robots  \n* OpenAI strategy  \n* Anthropic strategy  \n* Google strategy  \n* xAI strategy  \n* AI workforce automation  \n* China strategy  \n* Russia strategy  \n* Moskovitz strategy  \n* Must strategy  \n* Bezos Strategy  \n* Zuckerberg strategy  \n* Tallinn strategy  \n* Altman strategy  \n* Drone warfare  \n* Climate change destabilisation  \n* Democratic trust\n\n## Lincoln Quirk:\n\nWhy do we care about (AI for) epistemics?  \n\\- making the public better informed:  \n\t\\- better able to deal with and process the wealth of information  \n\t\\- better able to express their own interests in government (via voting)  \n\t\\- helping cut through social media distraction / intentional misinformation  \n\\- getting governments to implement better policy  \n\t\\- coming up with good policy ideas  \n\t\\- objectively doing cost-benefit analysis  \n\t\\- objectively explaining tradeoffs (vs having to rely on biased humans to explain them)  \n\t\\- flagging bad ideas as bad  \n\t\\- convincing policymakers with compelling writeups  \n\\- success and flourishing for individuals and orgs using these tools  \n\t\\- via helping them think through decisions  \n\t\\- via clearheaded analysis of trade-offs and cost/benefits  \n\t\\- flagging bad ideas as bad\n\n## Jay\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * How can we build something like the positive version of Community Notes: which statements/posts are agreed on by people who often disagree?  \n  * How can we better understand/summarize what others think, e.g. on social media platforms? E.g. if you see a post recommended to you, is this because it's the most extreme take that went viral and isn't representative, or is this a commonly held view? Between different viewpoints, what are the concrete points of actual disagreement (on facts or values) and what are the things widely agreed upon? Let's build viewpoint/debate maps or a similar sensemaking tool that gets adopted/understood by the masses/raises the knowledge floor (in my experience, things like pol.is are hard for regular users to grok)  \n  * How can we improve Community Notes (or more general knowledge) creation with AI assistants? An AI agent proposing community notes and humans still checking/rating them?  \n  * How can we have something like Community Notes that applies to claims or statements about the future, or which are not concretely knowable?  \n  * \n\n## Ben Wilson (Metaculus)\n\nOverall I believe the most important value of epistemics (and AI epistemics) in general is informing what actions allow us to achieve our goals (i.e. make the world better). If we had perfect AI epistemics I would be excited by the ability for us to take a goal, and get a list of possible actions we could take and the forecasted outcomes of each of those actions (plus background information and forecasts for the 'current lay of the land' for context). This can then inform, not just the risk of AI doing good/bad things (as with current forecasting), but what we can do to raise the chances of 'great' AI outcomes, and lower the changes of 'worse' AI outcomes\n\n## Andreas\n\n* **What are use cases for AI for Epistemics that you most care about?**  \n  * AI policy, broadly construed, must go better  \n    * Avoid gradual disempowerment  \n    * Thoughtful regulation  \n  * Organizational planning & decision-making is weak  \n    * Given an informal long-term goal like improve decision-making for strategic users in government & large orgs, make a plan for a 20-people org that helps them accomplish the goal  \n    * Mapping dependencies, uncertainties, and potential futures  \n    * Understand complex domains with non-trivial feedback loops  \n      * Economic shifts  \n* **What decisions may go badly by default, but could go better if we have the right tech?**  \n  * Resource allocation within orgs  \n    * Investment  \n    * Grant making  \n    * Government budgets  \n  * Policy  \n    * How should AI be deployed in the world?  \n* **What do the later stages of the path to impact look like?**  \n  * AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning  \n  * The technology helps organizations maintain coherence and make sound judgments even as the pace of change accelerates  \n  * Assumption: People are generally well-intentioned \\- no one wants catastrophe \\- and as a result of better tooling things go better in the world overall\n\n## Owen optimistic brainstorm:\n\nWorld peace without world government — sufficiently good tech to help countries understand each others' interests, find mutually desirable bargains, and make sufficient credible commitments, could help us get up towards the Pareto frontier where we avoid (virtually) all costly conflicts\n\nGoing slowly and carefully on AI development — rather than having AI progress accelerate at increasingly breakneck speed, helping actors to understand the basic situation and the desirability of different options could be crucial for facilitating meaningful and robust agreements which restrict AI development to the stuff which we have good reasons for being confident are safe // otherwise beneficial.\n\nBettering democracy — helping people to access key information about what decisions their country might make would actually mean for longer term implications, and having this in turn feed into incentives for decision-makers that better align them with the common good\n\nBuilding safe / aligned AI — better tools for anticipating when systems are actually robustly safe, vs when stronger cases for expectation of safety seem required\n\n## Josh Rosenberg (FRI)\n\nI'm massively confused about important problems, e.g. how to orient to AI risk and what to do about it. Some reasonable people think there's a high probability we effectively lose everything we care about in 2027\\. Some reasonable people think it'll be decades till we have \"AGI\" and it'd be good if it arrived sooner. Which actions are good to take in each of these worlds is very different, and I'd like to figure out what world we're in.\n\nThis also applies to many other problem areas.\n\nE.g., Trump's effect on US democracy. How much of a crisis is it? What are the most effective actions to take in response to ensure that the country and communities we care about are as well off as they can be? How can we preserve high-quality institutions to lead the world in positive directions?\n\nOr, on the positive vision side: the \"Abundance\" agenda seems promising. Is it actually a good idea? What do we do about it?\n\nAI for Epistemics could help us get a lot more clarity on how to think about these problems and what actions to take. I'd like much better information and forecasts on the likely effects of Trump, AI timelines, and the pro's and con's of different Abundance policies, for example. If I had a bunch of time and good resources to dig into these problems, I could get a much better sense for myself. I'm hoping AI will help all of us do this sort of thing collectively.\n\n## Katja Grace\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Put lots of cognitive effort into very clear and good and compelling case for AI risk  \n  * Automate talking to people about AI risk compellingly, possibly using above case as input  \n  * Good game theoretic thinking re arms races, coordination, what is possible. I.e. People are making dumb assumptions about things, e.g. \"can't slow down tech\". Could automate noting errors. So e.g. when someone says that on Twitter, can highlight it and send in response. \\-\\> good thinking via better weapons for debate (similar to Nathan project)  \n  * Specific idea: very quantified and competitive debate site, that can be cited re which views are winning, where AI does a lot of the cognitive labor in places no people are paying attention, but as a part of it gets cruxy, people pay attention and vote/bet  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Poor decision making around AI: e.g. 'racing' because of vague heuristics instead of ever doing calc on costs and benefits given specific risks  \n  * Failure to coordinate on AI: building AI systems that are dangerous, due to expecting others to do so\n\n## Joel Lehman\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Raising the bottom (i.e. helping improve societal epistemics) – in particular that in theory political decisions (at least political decision-makers) are somewhat downstream of societal epistemics  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Having weird political leaders and awful public epistemics at critical junctures seems to unnecessarily spike catastrophic/existential risk probabilities. In theory, helping uplevel or at least maintain current public epistemic health (especially as it relates to civic sensemaking) might help influence the quality of decision-makers and concrete decisions made at high-stakes moments  \n  * The shape of the tech is not clear, or probably requires multiple flavors of interventions. E.g. if LLM assistants by default help cultivate civic virtues and on the margin support critical thinking or whatever underlying skills and knowledge help a person up-level their epistemic habits / health (or at least if LLM assistants don't contribute to atrophy/harm); or somehow convincing social media networks to care about this (e.g. community notes but things that go beyond); similarly somehow help w/ important social goods like 'societal trust' and 'trust in your neighbors'  \n* What do the later stages of the path to impact look like?  \n  * \\[didn't get to this\\]  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * Healthier democratic society, better agreement on consensus reality, higher trust in society, better politicians, better concrete decisions in high-stakes moments \n\n## Austin Chen\n\n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * **Evaluator**: More agreement (or understanding of disagreement) of what orgs & people are doing good work (eg for making grants in AI safety)  \n  * **Oracle**: Instant trustworthy forecasts on any question you ask  \n  * **Planner**: Guidance on \"what are the 3 most important things I should do this day/week/month\"  \n  * **Representative**: Simulate the responses of other parties (people, orgs/corps, governments, LLM agents), for discussions, contracts, trade\n\n## Janna\n\n* Why do we care about (AI for) epistemics?  \n  * Noise to signal ratio could be much higher  \n  * AI (mis)information churned out at the price of almost zero  \n* What are use cases for AI for Epistemics that you most care about?  \n  * "
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Strong Emphasis on Public Goods and Collective Benefit",
          "description": "Several contributors highlight the public goods nature of epistemic tools and the importance of broad distribution, which aligns with societal values.",
          "importance": 65,
          "grade": 85,
          "isValid": false,
          "highlight": {
            "startOffset": 18732,
            "endOffset": 25786,
            "quotedText": "Epistemic tech/institutions/advances can be hard to fund via for-profit companies. This is in part due to public goods issues, and issues around markets for ideas.\n\n## Rafe\n\nAs ever, I hope this will stop advanced artificial intelligence destroying the world, killing all its people \\&c.\n\nSo I think there are a few paths for impact here. I think there is a world where we can make the most productive thinkers about how to do things go further and better; I'm not so excited about a single-player version of this, as I think there's a pretty narrow window when top-level centaur mode works out\\[1\\]\n\nEveryone is already thinking about AI, and, I suspect, this will only get more acute. I think it's pretty hard for people to confront this and think about it carefully and well. It's scary\\! And there are a bunch of entrenched interests and memeplexes that preserve their status better (myopically) if people only have shallow slogans to think about AI with.\n\nGiven that, I suspect that the global sensemaking machine isn't up for the task of thinking about AI well. I hope we can help the global sensemaking machine upgrade itself. I have some hope that we as a society can come up with good ideas for dealing with AI, and also come to see how they make sense, so we can choose them in a robust way.\n\nOne other path to impact: I think a society that understands things better, including itself and its impacts, is one that is more robust in a bunch of ways. A more epistemically-grounded society is also probably a bit richer, and I think a bigger pie creates more slack for arranging things in sanity-promoting ways. I have more hope in a robust society navigating this stuff well.\n\n\\[1\\]:  Yeah, maybe you can do something with shaving off subskills, but I don't think that's really that doable.\n\n## Dave\n\n* With my consequentialist hat, better epistemics being worth it has to route through improved decisions somewhere down the line. The decisions I particularly care about are those by powerful people wrt AI development and implementation and the effects this has on how the future goes  \n  * But I do feel a bit icky when I have this hat on. There seems to be something very intuitively appealing about (a) pursuing better epistemics for its own sake and (b) democratising epistemic tools for the use and benefit of everyone. I think this basically caches out in more 'raising the floor' type work  \n  * I don't feel like I have a very good way of resolving this tension yet, but am excited to see things pushing on both frontiers  \n* I think forecasting has the potential to be a pretty powerful tool and there's currently a massive undersupply of good forecasts. I'm excited for us to use the wave of increased AI labor to make better quantitative forecasts to inform key decisions. I probably care more about quantitative forecasts than most decision makers and am open to being persuaded that the other, fuzzier parts of epistemics are more valuable to work on. \n\n## Julian\n\nWhat are use cases for AI for Epistemics that you most care about?  \n**Raising the ceiling:** can we use AI to organize information and credibly resolve the most challenging and important questions on which experts still seem to disagree? For example:\n\n* COVID origins  \n* Urban planning and public policy  \n* Lots of advancements in basic science: routing around current disagreements and policy challenges with slam-dunk science might be a pretty strong way forward.\n\n**Raising the floor:** can we use AI to build extremely legibly trustworthy/reliable epistemic infrastructure that actually makes its way into people's lives?\n\n* Making truth more palatable (in a way which doesn't translate into making falsehoods more palatable)  \n* Collective epistemics: connecting people to the others with whom they would have the most productive/enlightening conversations\n\nWhat decisions may go badly by default, but could go better if we have the right tech?\n\n* Lots of stuff in public policy: downstream both of better experts (technocrats), better public (voters/interest groups), and better coordination (collective epistemics)\n\nWhat do the later stages of the path to impact look like?\n\n* I want us to live in a world where everyone is more rational, empathetic, considered in their decision making, conscientious about impact, and interested in coordinating. I feel like that's one of the biggest blockers to all kinds of *long term* progress.\n\nWhat tools do we actually maybe want to build?\n\n* Easily accessible fact checkers / claim refuters  \n  * Powerful AI debaters  \n* Forecasting with explanations: superhuman and at scale  \n* Scientific meta-analysis, systematic review  \n* Ad-hoc / bespoke probabilistic models for individual problems  \n* Bespoke *simulators* / simulation environments  \n* Consistency checking and inconsistency identification  \n  * Arbitrage / market makers in prediction markets?  \n* Using AI to accelerate development of non-AI epistemic tools:  \n  * ?\n\n## Lawrence\n\nThere are already key questions/strategic problems where the truth is murky and we seem to proceed on the basis of limited and biased views. There might well be many more of these in the future. A central example is US-China competition over AI. Some have vibes of \"China is so bad that it's worth risking an existential catastrophe to stop it from capturing a portion of the lightcone\". Others have vibes of \"Cooperation is feasible and desirable, and is our only path to a good outcome\". But how bad is China, really? How much \"realised K\" on this question do those who are making the relevant decisions actually have? Similarly, how feasible is cooperation? How valid is this frame of \"capturing the lightcone\"? One reason that we don't have a better hold on these questions is that they are frighteningly complicated and difficult  – I think we could benefit massively from throwing much more cognitive labour at them, and presenting the findings in a way that is hard to disagree with or for any individual to improve upon, other than at a normative/values level. Maybe AI can get us there.\n\n## Lukas Finnveden\n\nwhy is AI for epistemics important/helpful?\n\n* better epistemics generally very good for making all kinds of decisions  \n* I'm specifically concerned about AI potentially causing an intelligence explosion, and us needing better epistemics to handle that. Specifically:  \n  * I'm worried about AI-driven tech R\\&D leading to catastrophes. To help address this, I want AI capabilities to not only be applied to tech R\\&D, but also to be applied to \"reasoning well from limited evidence\" about a number of questions, including how to deal with risks.  \n  * I'm worried about misaligned AI — so I want techniques for eliciting AI capabilities on all kinds of questions, so that AIs don't have better capabilities than we can elicit from them. (Basically: I want good alignment techniques even on hard-to-evaluate questions.)  \n  * Seems great to me if AI-driven improvements in general knowledge and decision-making are more broadly distributed among humans."
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Occasional Use of Unexplained Strategic Concepts",
          "description": "Terms like 'defensive acceleration', 'd/acc', and 'civilizational strategy' are used without explanation. Briefly defining these concepts would improve accessibility.",
          "importance": 45,
          "grade": 50,
          "isValid": false,
          "highlight": {
            "startOffset": 31986,
            "endOffset": 34122,
            "quotedText": "Here's my unsurprising take on directed research for \"defensive acceleration\":  \nDevelopment and steering of epistemic tooling, aka [Large Knowledge Models](https://aiprospects.substack.com/p/large-knowledge-models), offers the highest leverage.\n\nPotential actions:  \n  • Identify and influence entities that could develop and scale necessary foundations.  \n  • Leverage the growing ease of development to take pre-scaling steps.  \n  • Within the scope of \"foundations\", accelerate productive directions for development.  \n  • To support science, ensure grounding of content in cited sources.  \n  • Pursue or promote assembly of appropriate knowledge inputs (beyond what is now used for training data).  \n  • Identify, promote, or implement epistemic tools supported by LKM resources.  \n  • Other kinds of exploration, promotion, preparation, and implementation.  \n  • Ensure that my advice is thoroughly understood.\n\nLKMs can accelerate pretty much everything that one might want, and perhaps tip the balance of applications toward *defensive* acceleration by promoting understanding of the *what and why* of defensive applications.\n\nAddendum:\n\nIn the email I sent a few minutes ago, the basic proposal becomes stronger if one reads \"LKM\" as something like \"large knowledge model functionality\", regardless of implementation.\n\nThe knowledge-model concept (whatever one prefers to call it) should not be tied to my specific technical suggestions. What I've said can be framed as something like \"a discussion of the value, feasibility, and potential scope of a rich epistemic resource\". Bundles of embeddings, etc., are at a level of specificity that may be distracting in the broader context of the meeting.  \n\"\n\nSo that's a strategy assistant AI – which helps humans at large see ways they could act and coordinate especially around big technological or geopolitical strategy issues I guess – but also a certain embodiment of an AGI oracle type system. Maybe the idea then is, how to make a AGI that helps with navigating civilizational strategy, and what is the right form of or interface to that to make it epistemically most useful."
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Effective Use of Summaries and Synthesis",
          "description": "Some contributors (e.g., Julian) provide summary statements that synthesize key points, aiding comprehension and reinforcing main messages.",
          "importance": 60,
          "grade": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 22907,
            "endOffset": 23210,
            "quotedText": "I want us to live in a world where everyone is more rational, empathetic, considered in their decision making, conscientious about impact, and interested in coordinating. I feel like that's one of the biggest blockers to all kinds of *long term* progress.\n\nWhat tools do we actually maybe want to build?"
          }
        },
        {
          "title": "Occasional Use of Unexplained Organizational References",
          "description": "References to organizations (e.g., 'Metaculus', 'FRI') are made without explanation. Briefly introducing these organizations would improve accessibility.",
          "importance": 45,
          "grade": 50,
          "isValid": false,
          "highlight": {
            "startOffset": 8207,
            "endOffset": 11614,
            "quotedText": "## Ben Wilson (Metaculus)\n\nOverall I believe the most important value of epistemics (and AI epistemics) in general is informing what actions allow us to achieve our goals (i.e. make the world better). If we had perfect AI epistemics I would be excited by the ability for us to take a goal, and get a list of possible actions we could take and the forecasted outcomes of each of those actions (plus background information and forecasts for the 'current lay of the land' for context). This can then inform, not just the risk of AI doing good/bad things (as with current forecasting), but what we can do to raise the chances of 'great' AI outcomes, and lower the changes of 'worse' AI outcomes\n\n## Andreas\n\n* **What are use cases for AI for Epistemics that you most care about?**  \n  * AI policy, broadly construed, must go better  \n    * Avoid gradual disempowerment  \n    * Thoughtful regulation  \n  * Organizational planning & decision-making is weak  \n    * Given an informal long-term goal like improve decision-making for strategic users in government & large orgs, make a plan for a 20-people org that helps them accomplish the goal  \n    * Mapping dependencies, uncertainties, and potential futures  \n    * Understand complex domains with non-trivial feedback loops  \n      * Economic shifts  \n* **What decisions may go badly by default, but could go better if we have the right tech?**  \n  * Resource allocation within orgs  \n    * Investment  \n    * Grant making  \n    * Government budgets  \n  * Policy  \n    * How should AI be deployed in the world?  \n* **What do the later stages of the path to impact look like?**  \n  * AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning  \n  * The technology helps organizations maintain coherence and make sound judgments even as the pace of change accelerates  \n  * Assumption: People are generally well-intentioned \\- no one wants catastrophe \\- and as a result of better tooling things go better in the world overall\n\n## Owen optimistic brainstorm:\n\nWorld peace without world government — sufficiently good tech to help countries understand each others' interests, find mutually desirable bargains, and make sufficient credible commitments, could help us get up towards the Pareto frontier where we avoid (virtually) all costly conflicts\n\nGoing slowly and carefully on AI development — rather than having AI progress accelerate at increasingly breakneck speed, helping actors to understand the basic situation and the desirability of different options could be crucial for facilitating meaningful and robust agreements which restrict AI development to the stuff which we have good reasons for being confident are safe // otherwise beneficial.\n\nBettering democracy — helping people to access key information about what decisions their country might make would actually mean for longer term implications, and having this in turn feed into incentives for decision-makers that better align them with the common good\n\nBuilding safe / aligned AI — better tools for anticipating when systems are actually robustly safe, vs when stronger cases for expectation of safety seem required\n\n## Josh Rosenberg (FRI)"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Strong Emphasis on Value Alignment and Ethics",
          "description": "Several contributors discuss the importance of aligning AI with human values and ethical considerations, which adds depth and relevance to the discussion.",
          "importance": 65,
          "grade": 85,
          "isValid": false,
          "highlight": {
            "startOffset": 2214,
            "endOffset": 31883,
            "quotedText": "Understanding our own goals and values\n\n*Note how it is the intersection of better epistemics / world models + the right power structures / incentives needed for all of these.*\n\n## Ben Goldhaber:\n\n* Forecasting how I should live my life in a period of intense technological change  \n* Guiding decision makers to avoid war with china/ai/everyone  \n* Providing the background context for me to sensemake way more sanely.  \n* Pushing forward the extant knowledge of AI safety agendas  \n* Planning for multi-stage projects that could achieve big impact  \n* Roadmapping to build larger more complex things\n\n## Chi\n\nThere are a few different things I care about:\n\n* I care a lot about humanity and AI, including **super-late-stage AI, getting decision theory** right.  \n  * Consequently but also separately, this relates to the general question of what to do about areas where there is not necessarily a ground truth but that are nonetheless very important to get right, cf**. [metaphilosophy](https://www.alignmentforum.org/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy)** (but also a bunch of other things.)  \n  * This also feels very similar to making **AIs helpful for AI safety research, cf. [automating alignment research](https://joecarlsmith.com/2025/03/14/ai-for-ai-safety#2-what-is-ai-for-ai-safety).**\n\n  → This is more about raising the **top of the epistemic landscape**\n\n* Other things I care about:  \n  * Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place.  \n  * Vague sense that better **societal epistemics** would help with increasing the **sanity of conversations about AI safety, polarisation around the issue etc.**  \n    * This is broadly about **getting intent alignment** in the first place.\n\n→ These are all more about raising the **bottom of the epistemic landscape**\n\n## Deger Turan:\n\n* Social trust erosion, collective coordination mechanisms are built on a weak assumption  \n* Capacity to entertain truthfulness when it is not instrumental to one's incentive gradient & differing norms upon 'truth will make my sense of security weaker'   \n* Creating contexts where world model divergence can be agreed by both parties enabling a safe disagreement  \n* Improving model reasoning / safer AI by reducing slop, by creating accountability mechanisms through consistency checks & other specifications for healthy epistemics  \n* Decoupling virality with magnitude while living in a short attention span world, curation   \n* I have some confusion re: raising the floor when not all parties are necessarily operating in good faith. I think I need some help think through the ways in which epistemics in the context of incentive misalignment can be used or have defensibility. \n\n## Nathan Young: \n\nWhat would we have got:\n\n* Biden not running, perhaps an open dem primary  \n* No FTX crisis  \n* Less zero sum OpenAI board drama  \n* Many smaller, better choices \n\nWhat decisions might go badly:\n\n* Trump tariffs/ global trade  \n* Taiwan/ US-China conflict  \n* US-China competition  \n* US authoritarianism  \n* AI regulation  \n* EU-China realignment  \n* Automated robots  \n* OpenAI strategy  \n* Anthropic strategy  \n* Google strategy  \n* xAI strategy  \n* AI workforce automation  \n* China strategy  \n* Russia strategy  \n* Moskovitz strategy  \n* Must strategy  \n* Bezos Strategy  \n* Zuckerberg strategy  \n* Tallinn strategy  \n* Altman strategy  \n* Drone warfare  \n* Climate change destabilisation  \n* Democratic trust\n\n## Lincoln Quirk:\n\nWhy do we care about (AI for) epistemics?  \n\\- making the public better informed:  \n\t\\- better able to deal with and process the wealth of information  \n\t\\- better able to express their own interests in government (via voting)  \n\t\\- helping cut through social media distraction / intentional misinformation  \n\\- getting governments to implement better policy  \n\t\\- coming up with good policy ideas  \n\t\\- objectively doing cost-benefit analysis  \n\t\\- objectively explaining tradeoffs (vs having to rely on biased humans to explain them)  \n\t\\- flagging bad ideas as bad  \n\t\\- convincing policymakers with compelling writeups  \n\\- success and flourishing for individuals and orgs using these tools  \n\t\\- via helping them think through decisions  \n\t\\- via clearheaded analysis of trade-offs and cost/benefits  \n\t\\- flagging bad ideas as bad\n\n## Jay\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * How can we build something like the positive version of Community Notes: which statements/posts are agreed on by people who often disagree?  \n  * How can we better understand/summarize what others think, e.g. on social media platforms? E.g. if you see a post recommended to you, is this because it's the most extreme take that went viral and isn't representative, or is this a commonly held view? Between different viewpoints, what are the concrete points of actual disagreement (on facts or values) and what are the things widely agreed upon? Let's build viewpoint/debate maps or a similar sensemaking tool that gets adopted/understood by the masses/raises the knowledge floor (in my experience, things like pol.is are hard for regular users to grok)  \n  * How can we improve Community Notes (or more general knowledge) creation with AI assistants? An AI agent proposing community notes and humans still checking/rating them?  \n  * How can we have something like Community Notes that applies to claims or statements about the future, or which are not concretely knowable?  \n  * \n\n## Ben Wilson (Metaculus)\n\nOverall I believe the most important value of epistemics (and AI epistemics) in general is informing what actions allow us to achieve our goals (i.e. make the world better). If we had perfect AI epistemics I would be excited by the ability for us to take a goal, and get a list of possible actions we could take and the forecasted outcomes of each of those actions (plus background information and forecasts for the 'current lay of the land' for context). This can then inform, not just the risk of AI doing good/bad things (as with current forecasting), but what we can do to raise the chances of 'great' AI outcomes, and lower the changes of 'worse' AI outcomes\n\n## Andreas\n\n* **What are use cases for AI for Epistemics that you most care about?**  \n  * AI policy, broadly construed, must go better  \n    * Avoid gradual disempowerment  \n    * Thoughtful regulation  \n  * Organizational planning & decision-making is weak  \n    * Given an informal long-term goal like improve decision-making for strategic users in government & large orgs, make a plan for a 20-people org that helps them accomplish the goal  \n    * Mapping dependencies, uncertainties, and potential futures  \n    * Understand complex domains with non-trivial feedback loops  \n      * Economic shifts  \n* **What decisions may go badly by default, but could go better if we have the right tech?**  \n  * Resource allocation within orgs  \n    * Investment  \n    * Grant making  \n    * Government budgets  \n  * Policy  \n    * How should AI be deployed in the world?  \n* **What do the later stages of the path to impact look like?**  \n  * AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning  \n  * The technology helps organizations maintain coherence and make sound judgments even as the pace of change accelerates  \n  * Assumption: People are generally well-intentioned \\- no one wants catastrophe \\- and as a result of better tooling things go better in the world overall\n\n## Owen optimistic brainstorm:\n\nWorld peace without world government — sufficiently good tech to help countries understand each others' interests, find mutually desirable bargains, and make sufficient credible commitments, could help us get up towards the Pareto frontier where we avoid (virtually) all costly conflicts\n\nGoing slowly and carefully on AI development — rather than having AI progress accelerate at increasingly breakneck speed, helping actors to understand the basic situation and the desirability of different options could be crucial for facilitating meaningful and robust agreements which restrict AI development to the stuff which we have good reasons for being confident are safe // otherwise beneficial.\n\nBettering democracy — helping people to access key information about what decisions their country might make would actually mean for longer term implications, and having this in turn feed into incentives for decision-makers that better align them with the common good\n\nBuilding safe / aligned AI — better tools for anticipating when systems are actually robustly safe, vs when stronger cases for expectation of safety seem required\n\n## Josh Rosenberg (FRI)\n\nI'm massively confused about important problems, e.g. how to orient to AI risk and what to do about it. Some reasonable people think there's a high probability we effectively lose everything we care about in 2027\\. Some reasonable people think it'll be decades till we have \"AGI\" and it'd be good if it arrived sooner. Which actions are good to take in each of these worlds is very different, and I'd like to figure out what world we're in.\n\nThis also applies to many other problem areas.\n\nE.g., Trump's effect on US democracy. How much of a crisis is it? What are the most effective actions to take in response to ensure that the country and communities we care about are as well off as they can be? How can we preserve high-quality institutions to lead the world in positive directions?\n\nOr, on the positive vision side: the \"Abundance\" agenda seems promising. Is it actually a good idea? What do we do about it?\n\nAI for Epistemics could help us get a lot more clarity on how to think about these problems and what actions to take. I'd like much better information and forecasts on the likely effects of Trump, AI timelines, and the pro's and con's of different Abundance policies, for example. If I had a bunch of time and good resources to dig into these problems, I could get a much better sense for myself. I'm hoping AI will help all of us do this sort of thing collectively.\n\n## Katja Grace\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Put lots of cognitive effort into very clear and good and compelling case for AI risk  \n  * Automate talking to people about AI risk compellingly, possibly using above case as input  \n  * Good game theoretic thinking re arms races, coordination, what is possible. I.e. People are making dumb assumptions about things, e.g. \"can't slow down tech\". Could automate noting errors. So e.g. when someone says that on Twitter, can highlight it and send in response. \\-\\> good thinking via better weapons for debate (similar to Nathan project)  \n  * Specific idea: very quantified and competitive debate site, that can be cited re which views are winning, where AI does a lot of the cognitive labor in places no people are paying attention, but as a part of it gets cruxy, people pay attention and vote/bet  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Poor decision making around AI: e.g. 'racing' because of vague heuristics instead of ever doing calc on costs and benefits given specific risks  \n  * Failure to coordinate on AI: building AI systems that are dangerous, due to expecting others to do so\n\n## Joel Lehman\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Raising the bottom (i.e. helping improve societal epistemics) – in particular that in theory political decisions (at least political decision-makers) are somewhat downstream of societal epistemics  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Having weird political leaders and awful public epistemics at critical junctures seems to unnecessarily spike catastrophic/existential risk probabilities. In theory, helping uplevel or at least maintain current public epistemic health (especially as it relates to civic sensemaking) might help influence the quality of decision-makers and concrete decisions made at high-stakes moments  \n  * The shape of the tech is not clear, or probably requires multiple flavors of interventions. E.g. if LLM assistants by default help cultivate civic virtues and on the margin support critical thinking or whatever underlying skills and knowledge help a person up-level their epistemic habits / health (or at least if LLM assistants don't contribute to atrophy/harm); or somehow convincing social media networks to care about this (e.g. community notes but things that go beyond); similarly somehow help w/ important social goods like 'societal trust' and 'trust in your neighbors'  \n* What do the later stages of the path to impact look like?  \n  * \\[didn't get to this\\]  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * Healthier democratic society, better agreement on consensus reality, higher trust in society, better politicians, better concrete decisions in high-stakes moments \n\n## Austin Chen\n\n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * **Evaluator**: More agreement (or understanding of disagreement) of what orgs & people are doing good work (eg for making grants in AI safety)  \n  * **Oracle**: Instant trustworthy forecasts on any question you ask  \n  * **Planner**: Guidance on \"what are the 3 most important things I should do this day/week/month\"  \n  * **Representative**: Simulate the responses of other parties (people, orgs/corps, governments, LLM agents), for discussions, contracts, trade\n\n## Janna\n\n* Why do we care about (AI for) epistemics?  \n  * Noise to signal ratio could be much higher  \n  * AI (mis)information churned out at the price of almost zero  \n* What are use cases for AI for Epistemics that you most care about?  \n  * Solving prediction markets--is there a way to scale up the bots, even if they are wrong and lossy, to improve prediction market accuracy?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * US-China relations  \n  * Trump tariffs maybe  \n  * Immigration rules in the US  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * More accurate predictions to…  \n    * Reduce business uncertainty  \n    * Make it easier to evaluate life decisions  \n  * Forestalling bad decisions made on imperfect information\n\n## Owain\n\n1. Maybe having humans understanding the world better is good in itself. Lots of people read popular science for this reason. Maybe it's good for AIs as well  \n2. Very broadly, it seems that better world understanding and decision making enables people to make their lives better in myriad ways. (To substantiate this, one could try to separate out the effects of better world understanding from the effects of better realized technology. E.g. Looking at capital-poor countries that can still benefit from scientific knowledge, economics, etc.)  \n3. More specifically, if AI progress is fast, there will be lots of important decisions to be made in quick calendar time. Exploiting AI to improve those decisions seems valuable. Current decision making institutions do not seem well set up to adapt really quickly. (A specific concern is dangers from AI takeover).   \n4. Why now? Why this group of people? We haven't really had tech that could do most epistemic tasks until recently. Now we have AI that can do an increasing proportion of the sub-tasks of epistemic processes. This will allow us to delegate epistemic tasks to AI.  \n   1. It's likely that people do this delegation, even if it's somewhat misguided and potentially bad in the long-term. Can we make this go well?  \n   2. This potentially massively expands our epistemic capacities. How can we use this well?  \n   3. Epistemic tech/institutions/advances can be hard to fund via for-profit companies. This is in part due to public goods issues, and issues around markets for ideas.\n\n## Rafe\n\nAs ever, I hope this will stop advanced artificial intelligence destroying the world, killing all its people \\&c.\n\nSo I think there are a few paths for impact here. I think there is a world where we can make the most productive thinkers about how to do things go further and better; I'm not so excited about a single-player version of this, as I think there's a pretty narrow window when top-level centaur mode works out\\[1\\]\n\nEveryone is already thinking about AI, and, I suspect, this will only get more acute. I think it's pretty hard for people to confront this and think about it carefully and well. It's scary\\! And there are a bunch of entrenched interests and memeplexes that preserve their status better (myopically) if people only have shallow slogans to think about AI with.\n\nGiven that, I suspect that the global sensemaking machine isn't up for the task of thinking about AI well. I hope we can help the global sensemaking machine upgrade itself. I have some hope that we as a society can come up with good ideas for dealing with AI, and also come to see how they make sense, so we can choose them in a robust way.\n\nOne other path to impact: I think a society that understands things better, including itself and its impacts, is one that is more robust in a bunch of ways. A more epistemically-grounded society is also probably a bit richer, and I think a bigger pie creates more slack for arranging things in sanity-promoting ways. I have more hope in a robust society navigating this stuff well.\n\n\\[1\\]:  Yeah, maybe you can do something with shaving off subskills, but I don't think that's really that doable.\n\n## Dave\n\n* With my consequentialist hat, better epistemics being worth it has to route through improved decisions somewhere down the line. The decisions I particularly care about are those by powerful people wrt AI development and implementation and the effects this has on how the future goes  \n  * But I do feel a bit icky when I have this hat on. There seems to be something very intuitively appealing about (a) pursuing better epistemics for its own sake and (b) democratising epistemic tools for the use and benefit of everyone. I think this basically caches out in more 'raising the floor' type work  \n  * I don't feel like I have a very good way of resolving this tension yet, but am excited to see things pushing on both frontiers  \n* I think forecasting has the potential to be a pretty powerful tool and there's currently a massive undersupply of good forecasts. I'm excited for us to use the wave of increased AI labor to make better quantitative forecasts to inform key decisions. I probably care more about quantitative forecasts than most decision makers and am open to being persuaded that the other, fuzzier parts of epistemics are more valuable to work on. \n\n## Julian\n\nWhat are use cases for AI for Epistemics that you most care about?  \n**Raising the ceiling:** can we use AI to organize information and credibly resolve the most challenging and important questions on which experts still seem to disagree? For example:\n\n* COVID origins  \n* Urban planning and public policy  \n* Lots of advancements in basic science: routing around current disagreements and policy challenges with slam-dunk science might be a pretty strong way forward.\n\n**Raising the floor:** can we use AI to build extremely legibly trustworthy/reliable epistemic infrastructure that actually makes its way into people's lives?\n\n* Making truth more palatable (in a way which doesn't translate into making falsehoods more palatable)  \n* Collective epistemics: connecting people to the others with whom they would have the most productive/enlightening conversations\n\nWhat decisions may go badly by default, but could go better if we have the right tech?\n\n* Lots of stuff in public policy: downstream both of better experts (technocrats), better public (voters/interest groups), and better coordination (collective epistemics)\n\nWhat do the later stages of the path to impact look like?\n\n* I want us to live in a world where everyone is more rational, empathetic, considered in their decision making, conscientious about impact, and interested in coordinating. I feel like that's one of the biggest blockers to all kinds of *long term* progress.\n\nWhat tools do we actually maybe want to build?\n\n* Easily accessible fact checkers / claim refuters  \n  * Powerful AI debaters  \n* Forecasting with explanations: superhuman and at scale  \n* Scientific meta-analysis, systematic review  \n* Ad-hoc / bespoke probabilistic models for individual problems  \n* Bespoke *simulators* / simulation environments  \n* Consistency checking and inconsistency identification  \n  * Arbitrage / market makers in prediction markets?  \n* Using AI to accelerate development of non-AI epistemic tools:  \n  * ?\n\n## Lawrence\n\nThere are already key questions/strategic problems where the truth is murky and we seem to proceed on the basis of limited and biased views. There might well be many more of these in the future. A central example is US-China competition over AI. Some have vibes of \"China is so bad that it's worth risking an existential catastrophe to stop it from capturing a portion of the lightcone\". Others have vibes of \"Cooperation is feasible and desirable, and is our only path to a good outcome\". But how bad is China, really? How much \"realised K\" on this question do those who are making the relevant decisions actually have? Similarly, how feasible is cooperation? How valid is this frame of \"capturing the lightcone\"? One reason that we don't have a better hold on these questions is that they are frighteningly complicated and difficult  – I think we could benefit massively from throwing much more cognitive labour at them, and presenting the findings in a way that is hard to disagree with or for any individual to improve upon, other than at a normative/values level. Maybe AI can get us there.\n\n## Lukas Finnveden\n\nwhy is AI for epistemics important/helpful?\n\n* better epistemics generally very good for making all kinds of decisions  \n* I'm specifically concerned about AI potentially causing an intelligence explosion, and us needing better epistemics to handle that. Specifically:  \n  * I'm worried about AI-driven tech R\\&D leading to catastrophes. To help address this, I want AI capabilities to not only be applied to tech R\\&D, but also to be applied to \"reasoning well from limited evidence\" about a number of questions, including how to deal with risks.  \n  * I'm worried about misaligned AI — so I want techniques for eliciting AI capabilities on all kinds of questions, so that AIs don't have better capabilities than we can elicit from them. (Basically: I want good alignment techniques even on hard-to-evaluate questions.)  \n  * Seems great to me if AI-driven improvements in general knowledge and decision-making are more broadly distributed among humans. (I think this leads to better decision-making without significantly accelerating the dangers from new tech or misaligned AI. Also reduces concentration of power.)\n\n## Abram Demski\n\n\\- Why are we doing this? \\-- impact theory brainstorming exercize  \n\t\\- There's the general claim that epistemics has degraded horribly due to social media and the internet, at least in a lot of ways, although epistemics has also gotten better in some specific ways. It seems very plausible that AI is, and will continue to, make this much much worse (due to deepfakes and super-persuasive writing, to give two examples). We wish to reverse this trend, or at least slow the acceleration.  \n\t\\- The high-level reason why epistemics is important is because epistemics is how we make good decisions. So effectively, this is AI for decision-making. However, if we just set out to make tools for decision-making, I have a sense that this would be worse.  \n\t\t\\- One super important aspect of decision-making is that you have to orient towards the right problem. People do much better when they understand the problem before proposing solutions.  \n\t\t\t\\- I have a sense that producing a better shared understanding of the world JUST WOULD result in better things. The connection between epistemics and better decisionmaking is that strong.  \n\t\t\t\t\\- However, there's an element of coordination to this. You have to have common knowledge, not just knowledge. AI for coordination is yet another thing. AI for group epistemics is close to that, though, and again, I have a sense that making \"AI for coordination\" would produce worse effects on the world than \"AI for group epistemics\".  \n\t\t\\- What specific decisions are we hoping to influence?  \n\t\t\t\\- Decisions around existential risk seem super super important. If AI were generally better at epistemics (as a relative, comparative thing \\-- ie we differentially accelerate epistemics) then key decisionmakers (in the government and at AI companies) would see much more clearly what's coming. Everyone wants a better future, so a better future would have a better chance of happening if we could see and navigate.\n\nI don't think I got any further than the supporting documentation for this workshop in articulating reasons and stuff, so I predict that the LLM summary won't be better than what we already know, although I hope it is. \n\n## Brendan\n\nFor us to live together in a caring and just way, we must make decisions about what is important to us together. Is it okay to tell my neighbour to turn their music down? Do we spend money on roads or trains or libraries? Do we regulate AI development and, if so, how? Do we want to spend public money on PEPFAR, or other avenues for reducing global poverty? To make these decisions in a way that is inclusive and democratic, that fosters buy-in and trust, we must begin with a shared sense of how the world is. This shared sense must be grounded in the physical, economic, and social reality we live in.\n\nAs we live together over wider and wider contexts, maintaining a shared sense of the world becomes more difficult and subtle. These wider contexts are facilitated by new technologies, both physical (eg. global manufacturing bases and supply chains) and informational (eg. social media). New technologies, such as AI for epistemics, and help maintain epistemic coherence across these contexts.\n\nMoreover, such technologies allow new avenues for cooperation at larger scales of human involvement. This unlocks new potential for coherence in global politics, scientific discovery, social and technological innovation, and cultural expression, and ultimately the deep joy of being alive and human in the world. \n\n## Isabel: \n\nThe kinds of being right in the past that are most relevant for decision making aren't just about making straightforward predictions about what will happen, but are more nuanced about what kinds of actions one should take to cause those things to happen, and these seem like somewhat different skills. These are harder to evaluate (e.g. because they're counterfactual: either we listened to person a and then we have to evaluate whether things went better or worse than if we didn't, but hard to say what the counterfactual would have been; or we didn't listen to person a and then we have to guess what things would have been like if we did) \n\nA bit adjacent to Neel Nanda's [recent piece](https://www.lesswrong.com/posts/P5zWiPF5cPJZSkiAK/good-research-takes-are-not-sufficient-for-good-strategic) on how \"good research takes aren't sufficient for good strategic takes\" \\- \"good forecasting takes aren't sufficient for good strategic takes\", but maybe some of this is maybe resolvable on the forecasting tech side \\- can we get better at creating and evaluating results from fuzzier questions? Can AI help? (Also can AI help with the whole problem, but this feels small and concrete enough to maybe get some traction on.) Also, some risk of actors building up good track records in order to be listened to, but then choosing to burn that track record to get power or things like that, something something trust \\- a concern for both human actors and e.g. scheming AIs \\- what things look good until they abruptly aren't? \n\nOn a different note, also interested in takes on what to do about and how to engage with actors who have epistemic processes that don't seem to be truth seeking in a comprehensible-to-me way. \n\n## Adam Marblestone:\n\nWhat do we get out of AI for epistemics?\n\nTrust, is one thing. Our current society seems to be fraught with distrust and motivated reasoning. Can we get a more objectively trustable source of truth? Not only accuracy but with a provenance that gives it an ability to be trusted by humans who are otherwise adversarial. Trust is needed to coordinate. Truthfulness is maybe somewhat necessary but seemingly not sufficient for a source being trusted. As a practical matter, on hot political topics I've been going to OpenAI Deep Research.\n\nBut at a certain level I wonder if AI for epistemics and some form of beneficial AGI could be basically the same thing."
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Occasional Use of Unexplained Decision-Making Concepts",
          "description": "Terms like 'vetopower', 'arbitrage', and 'market makers' are used without explanation. Briefly defining these concepts would improve accessibility.",
          "importance": 45,
          "grade": 50,
          "isValid": false,
          "highlight": {
            "startOffset": 34908,
            "endOffset": 23626,
            "quotedText": "  \n* Using AI to accelerate development of non-AI epistemic tools:  \n  * ?\n\n## Lawrence\n\nThere are already key questions/strategic problems where the truth is murky and we seem to proceed on the basis of limited and biased views. There might well be many more of these in the future. A central example is US-China competition over AI. Some have vibes of \"China is so bad that it's worth risking an existential catastrophe to stop it from capturing a portion of the lightcone\". Others have vibes of \"Cooperation is feasible and desirable, and is our only path to a good outcome\". But how bad is China, really? How much \"realised K\" on this question do those who are making the relevant decisions actually have? Similarly, how feasible is cooperation? How valid is this frame of \"capturing the lightcone\"? One reason that we don't have a better hold on these questions is that they are frighteningly complicated and difficult  – I think we could benefit massively from throwing much more cognitive labour at them, and presenting the findings in a way that is hard to disagree with or for any individual to improve upon, other than at a normative/values level. Maybe AI can get us there.\n\n## Lukas Finnveden\n\nwhy is AI for epistemics important/helpful?\n\n* better epistemics generally very good for making all kinds of decisions  \n* I'm specifically concerned about AI potentially causing an intelligence explosion, and us needing better epistemics to handle that. Specifically:  \n  * I'm worried about AI-driven tech R\\&D leading to catastrophes. To help address this, I want AI capabilities to not only be applied to tech R\\&D, but also to be applied to \"reasoning well from limited evidence\" about a number of questions, including how to deal with risks.  \n  * I'm worried about misaligned AI — so I want techniques for eliciting AI capabilities on all kinds of questions, so that AIs don't have better capabilities than we can elicit from them. (Basically: I want good alignment techniques even on hard-to-evaluate questions.)  \n  * Seems great to me if AI-driven improvements in general knowledge and decision-making are more broadly distributed among humans. (I think this leads to better decision-making without significantly accelerating the dangers from new tech or misaligned AI. Also reduces concentration of power.)\n\n## Abram Demski\n\n\\- Why are we doing this? \\-- impact theory brainstorming exercize  \n\t\\- There's the general claim that epistemics has degraded horribly due to social media and the internet, at least in a lot of ways, although epistemics has also gotten better in some specific ways. It seems very plausible that AI is, and will continue to, make this much much worse (due to deepfakes and super-persuasive writing, to give two examples). We wish to reverse this trend, or at least slow the acceleration.  \n\t\\- The high-level reason why epistemics is important is because epistemics is how we make good decisions. So effectively, this is AI for decision-making. However, if we just set out to make tools for decision-making, I have a sense that this would be worse.  \n\t\t\\- One super important aspect of decision-making is that you have to orient towards the right problem. People do much better when they understand the problem before proposing solutions.  \n\t\t\t\\- I have a sense that producing a better shared understanding of the world JUST WOULD result in better things. The connection between epistemics and better decisionmaking is that strong.  \n\t\t\t\t\\- However, there's an element of coordination to this. You have to have common knowledge, not just knowledge. AI for coordination is yet another thing. AI for group epistemics is close to that, though, and again, I have a sense that making \"AI for coordination\" would produce worse effects on the world than \"AI for group epistemics\".  \n\t\t\\- What specific decisions are we hoping to influence?  \n\t\t\t\\- Decisions around existential risk seem super super important. If AI were generally better at epistemics (as a relative, comparative thing \\-- ie we differentially accelerate epistemics) then key decisionmakers (in the government and at AI companies) would see much more clearly what's coming. Everyone wants a better future, so a better future would have a better chance of happening if we could see and navigate.\n\nI don't think I got any further than the supporting documentation for this workshop in articulating reasons and stuff, so I predict that the LLM summary won't be better than what we already know, although I hope it is. \n\n## Brendan\n\nFor us to live together in a caring and just way, we must make decisions about what is important to us together. Is it okay to tell my neighbour to turn their music down? Do we spend money on roads or trains or libraries? Do we regulate AI development and, if so, how? Do we want to spend public money on PEPFAR, or other avenues for reducing global poverty? To make these decisions in a way that is inclusive and democratic, that fosters buy-in and trust, we must begin with a shared sense of how the world is. This shared sense must be grounded in the physical, economic, and social reality we live in.\n\nAs we live together over wider and wider contexts, maintaining a shared sense of the world becomes more difficult and subtle. These wider contexts are facilitated by new technologies, both physical (eg. global manufacturing bases and supply chains) and informational (eg. social media). New technologies, such as AI for epistemics, and help maintain epistemic coherence across these contexts.\n\nMoreover, such technologies allow new avenues for cooperation at larger scales of human involvement. This unlocks new potential for coherence in global politics, scientific discovery, social and technological innovation, and cultural expression, and ultimately the deep joy of being alive and human in the world. \n\n## Isabel: \n\nThe kinds of being right in the past that are most relevant for decision making aren't just about making straightforward predictions about what will happen, but are more nuanced about what kinds of actions one should take to cause those things to happen, and these seem like somewhat different skills. These are harder to evaluate (e.g. because they're counterfactual: either we listened to person a and then we have to evaluate whether things went better or worse than if we didn't, but hard to say what the counterfactual would have been; or we didn't listen to person a and then we have to guess what things would have been like if we did) \n\nA bit adjacent to Neel Nanda's [recent piece](https://www.lesswrong.com/posts/P5zWiPF5cPJZSkiAK/good-research-takes-are-not-sufficient-for-good-strategic) on how \"good research takes aren't sufficient for good strategic takes\" \\- \"good forecasting takes aren't sufficient for good strategic takes\", but maybe some of this is maybe resolvable on the forecasting tech side \\- can we get better at creating and evaluating results from fuzzier questions? Can AI help? (Also can AI help with the whole problem, but this feels small and concrete enough to maybe get some traction on.) Also, some risk of actors building up good track records in order to be listened to, but then choosing to burn that track record to get power or things like that, something something trust \\- a concern for both human actors and e.g. scheming AIs \\- what things look good until they abruptly aren't? \n\nOn a different note, also interested in takes on what to do about and how to engage with actors who have epistemic processes that don't seem to be truth seeking in a comprehensible-to-me way. \n\n## Adam Marblestone:\n\nWhat do we get out of AI for epistemics?\n\nTrust, is one thing. Our current society seems to be fraught with distrust and motivated reasoning. Can we get a more objectively trustable source of truth? Not only accuracy but with a provenance that gives it an ability to be trusted by humans who are otherwise adversarial. Trust is needed to coordinate. Truthfulness is maybe somewhat necessary but seemingly not sufficient for a source being trusted. As a practical matter, on hot political topics I've been going to OpenAI Deep Research.\n\nBut at a certain level I wonder if AI for epistemics and some form of beneficial AGI could be basically the same thing. I asked Eric Drexler what he wanted for d/acc and he said something that sounds a bit like that…\n\n\"  \nHere's my unsurprising take on directed research for \"defensive acceleration\":  \nDevelopment and steering of epistemic tooling, aka [Large Knowledge Models](https://aiprospects.substack.com/p/large-knowledge-models), offers the highest leverage.\n\nPotential actions:  \n  • Identify and influence entities that could develop and scale necessary foundations.  \n  • Leverage the growing ease of development to take pre-scaling steps.  \n  • Within the scope of \"foundations\", accelerate productive directions for development.  \n  • To support science, ensure grounding of content in cited sources.  \n  • Pursue or promote assembly of appropriate knowledge inputs (beyond what is now used for training data).  \n  • Identify, promote, or implement epistemic tools supported by LKM resources.  \n  • Other kinds of exploration, promotion, preparation, and implementation.  \n  • Ensure that my advice is thoroughly understood.\n\nLKMs can accelerate pretty much everything that one might want, and perhaps tip the balance of applications toward *defensive* acceleration by promoting understanding of the *what and why* of defensive applications.\n\nAddendum:\n\nIn the email I sent a few minutes ago, the basic proposal becomes stronger if one reads \"LKM\" as something like \"large knowledge model functionality\", regardless of implementation.\n\nThe knowledge-model concept (whatever one prefers to call it) should not be tied to my specific technical suggestions. What I've said can be framed as something like \"a discussion of the value, feasibility, and potential scope of a rich epistemic resource\". Bundles of embeddings, etc., are at a level of specificity that may be distracting in the broader context of the meeting.  \n\"\n\nSo that's a strategy assistant AI – which helps humans at large see ways they could act and coordinate especially around big technological or geopolitical strategy issues I guess – but also a certain embodiment of an AGI oracle type system. Maybe the idea then is, how to make a AGI that helps with navigating civilizational strategy, and what is the right form of or interface to that to make it epistemically most useful. \n\nI'd like to also see democratic mechanisms that are simply maximally utilizing the tech we have. \n\n## Elizabeth\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Allowing people to be better informed about experimental medical stuff for themselves.   \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Bargaining? Even just salary negotiations feels like the results are often more the consequence of \"human bargaining skill\" and not the pareto optimal result.    \n  \n\n\n## Saul\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * making better decisions  \n  * in service of that: more useful forecasts, particularly understanding better which things are most relevant to forecast  \n    * "
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Effective Use of Calls to Action",
          "description": "Some contributors (e.g., Adam Marblestone) include calls to action (e.g., 'Identify and influence entities that could develop and scale necessary foundations'), which encourage proactive engagement.",
          "importance": 60,
          "grade": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 32233,
            "endOffset": 32902,
            "quotedText": "Potential actions:  \n  • Identify and influence entities that could develop and scale necessary foundations.  \n  • Leverage the growing ease of development to take pre-scaling steps.  \n  • Within the scope of \"foundations\", accelerate productive directions for development.  \n  • To support science, ensure grounding of content in cited sources.  \n  • Pursue or promote assembly of appropriate knowledge inputs (beyond what is now used for training data).  \n  • Identify, promote, or implement epistemic tools supported by LKM resources.  \n  • Other kinds of exploration, promotion, preparation, and implementation.  \n  • Ensure that my advice is thoroughly understood."
          }
        },
        {
          "title": "Occasional Use of Unexplained AI Roles",
          "description": "Terms like 'Evaluator', 'Oracle', 'Planner', and 'Representative' are used as AI roles without explanation. Briefly describing these roles would improve clarity.",
          "importance": 45,
          "grade": 50,
          "isValid": false,
          "highlight": {
            "startOffset": 650,
            "endOffset": 16468,
            "quotedText": "* \"As a result of better AI for epistemics tech, we get: […]\"\n\nTemplate:\n\n## Name: \n\nEntry\n\n## Dan Schwarz:\n\n* What are use cases for AI for Epistemics that you most care about?\n\nGoing from \"Realized\" to \"Accessible\" knowledge, e.g. getting good information using the web. Agentic web agents like Deep Research.\n\n* What decisions may go badly by default, but could go better if we have the right tech?\n\nMany billions of small decisions.\n\n* What do the later stages of the path to impact look like?\n\nSuperhuman forecasting at scale.\n\n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nReasonable judgment on tricky questions that's too cheap to meter, better than if you asked a respected colleague to spend many minutes on it.\n\n## Aviv\n\n* Better government governance  \n* Better democratic functioning  \n* Reduced dangerous concentration of power  \n    \n* Healthy polarization / pluralism  \n    \n* More peace, less likelihood of war  \n* Better geopolitics  \n* Better economic incentives  \n    \n* Better AI org governance  \n* Better AI coordination (re. distribution, escalation, etc.)  \n* Better compute allocation (toward what matters for good outcomes)  \n    \n* Better allocation of resources overall  \n* Better lives for people  \n* More effective organizations, getting things done with fewer resources, more focused on the things that benefit everyone  \n    \n* Identification of common ground directions / equilibria / game theoretic strategies that create/incentivize new positive gradients (and not surfacing the bad versions of those…)  \n    \n* Understanding our own goals and values\n\n*Note how it is the intersection of better epistemics / world models + the right power structures / incentives needed for all of these.*\n\n## Ben Goldhaber:\n\n* Forecasting how I should live my life in a period of intense technological change  \n* Guiding decision makers to avoid war with china/ai/everyone  \n* Providing the background context for me to sensemake way more sanely.  \n* Pushing forward the extant knowledge of AI safety agendas  \n* Planning for multi-stage projects that could achieve big impact  \n* Roadmapping to build larger more complex things\n\n## Chi\n\nThere are a few different things I care about:\n\n* I care a lot about humanity and AI, including **super-late-stage AI, getting decision theory** right.  \n  * Consequently but also separately, this relates to the general question of what to do about areas where there is not necessarily a ground truth but that are nonetheless very important to get right, cf**. [metaphilosophy](https://www.alignmentforum.org/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy)** (but also a bunch of other things.)  \n  * This also feels very similar to making **AIs helpful for AI safety research, cf. [automating alignment research](https://joecarlsmith.com/2025/03/14/ai-for-ai-safety#2-what-is-ai-for-ai-safety).**\n\n  → This is more about raising the **top of the epistemic landscape**\n\n* Other things I care about:  \n  * Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place.  \n  * Vague sense that better **societal epistemics** would help with increasing the **sanity of conversations about AI safety, polarisation around the issue etc.**  \n    * This is broadly about **getting intent alignment** in the first place.\n\n→ These are all more about raising the **bottom of the epistemic landscape**\n\n## Deger Turan:\n\n* Social trust erosion, collective coordination mechanisms are built on a weak assumption  \n* Capacity to entertain truthfulness when it is not instrumental to one's incentive gradient & differing norms upon 'truth will make my sense of security weaker'   \n* Creating contexts where world model divergence can be agreed by both parties enabling a safe disagreement  \n* Improving model reasoning / safer AI by reducing slop, by creating accountability mechanisms through consistency checks & other specifications for healthy epistemics  \n* Decoupling virality with magnitude while living in a short attention span world, curation   \n* I have some confusion re: raising the floor when not all parties are necessarily operating in good faith. I think I need some help think through the ways in which epistemics in the context of incentive misalignment can be used or have defensibility. \n\n## Nathan Young: \n\nWhat would we have got:\n\n* Biden not running, perhaps an open dem primary  \n* No FTX crisis  \n* Less zero sum OpenAI board drama  \n* Many smaller, better choices \n\nWhat decisions might go badly:\n\n* Trump tariffs/ global trade  \n* Taiwan/ US-China conflict  \n* US-China competition  \n* US authoritarianism  \n* AI regulation  \n* EU-China realignment  \n* Automated robots  \n* OpenAI strategy  \n* Anthropic strategy  \n* Google strategy  \n* xAI strategy  \n* AI workforce automation  \n* China strategy  \n* Russia strategy  \n* Moskovitz strategy  \n* Must strategy  \n* Bezos Strategy  \n* Zuckerberg strategy  \n* Tallinn strategy  \n* Altman strategy  \n* Drone warfare  \n* Climate change destabilisation  \n* Democratic trust\n\n## Lincoln Quirk:\n\nWhy do we care about (AI for) epistemics?  \n\\- making the public better informed:  \n\t\\- better able to deal with and process the wealth of information  \n\t\\- better able to express their own interests in government (via voting)  \n\t\\- helping cut through social media distraction / intentional misinformation  \n\\- getting governments to implement better policy  \n\t\\- coming up with good policy ideas  \n\t\\- objectively doing cost-benefit analysis  \n\t\\- objectively explaining tradeoffs (vs having to rely on biased humans to explain them)  \n\t\\- flagging bad ideas as bad  \n\t\\- convincing policymakers with compelling writeups  \n\\- success and flourishing for individuals and orgs using these tools  \n\t\\- via helping them think through decisions  \n\t\\- via clearheaded analysis of trade-offs and cost/benefits  \n\t\\- flagging bad ideas as bad\n\n## Jay\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * How can we build something like the positive version of Community Notes: which statements/posts are agreed on by people who often disagree?  \n  * How can we better understand/summarize what others think, e.g. on social media platforms? E.g. if you see a post recommended to you, is this because it's the most extreme take that went viral and isn't representative, or is this a commonly held view? Between different viewpoints, what are the concrete points of actual disagreement (on facts or values) and what are the things widely agreed upon? Let's build viewpoint/debate maps or a similar sensemaking tool that gets adopted/understood by the masses/raises the knowledge floor (in my experience, things like pol.is are hard for regular users to grok)  \n  * How can we improve Community Notes (or more general knowledge) creation with AI assistants? An AI agent proposing community notes and humans still checking/rating them?  \n  * How can we have something like Community Notes that applies to claims or statements about the future, or which are not concretely knowable?  \n  * \n\n## Ben Wilson (Metaculus)\n\nOverall I believe the most important value of epistemics (and AI epistemics) in general is informing what actions allow us to achieve our goals (i.e. make the world better). If we had perfect AI epistemics I would be excited by the ability for us to take a goal, and get a list of possible actions we could take and the forecasted outcomes of each of those actions (plus background information and forecasts for the 'current lay of the land' for context). This can then inform, not just the risk of AI doing good/bad things (as with current forecasting), but what we can do to raise the chances of 'great' AI outcomes, and lower the changes of 'worse' AI outcomes\n\n## Andreas\n\n* **What are use cases for AI for Epistemics that you most care about?**  \n  * AI policy, broadly construed, must go better  \n    * Avoid gradual disempowerment  \n    * Thoughtful regulation  \n  * Organizational planning & decision-making is weak  \n    * Given an informal long-term goal like improve decision-making for strategic users in government & large orgs, make a plan for a 20-people org that helps them accomplish the goal  \n    * Mapping dependencies, uncertainties, and potential futures  \n    * Understand complex domains with non-trivial feedback loops  \n      * Economic shifts  \n* **What decisions may go badly by default, but could go better if we have the right tech?**  \n  * Resource allocation within orgs  \n    * Investment  \n    * Grant making  \n    * Government budgets  \n  * Policy  \n    * How should AI be deployed in the world?  \n* **What do the later stages of the path to impact look like?**  \n  * AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning  \n  * The technology helps organizations maintain coherence and make sound judgments even as the pace of change accelerates  \n  * Assumption: People are generally well-intentioned \\- no one wants catastrophe \\- and as a result of better tooling things go better in the world overall\n\n## Owen optimistic brainstorm:\n\nWorld peace without world government — sufficiently good tech to help countries understand each others' interests, find mutually desirable bargains, and make sufficient credible commitments, could help us get up towards the Pareto frontier where we avoid (virtually) all costly conflicts\n\nGoing slowly and carefully on AI development — rather than having AI progress accelerate at increasingly breakneck speed, helping actors to understand the basic situation and the desirability of different options could be crucial for facilitating meaningful and robust agreements which restrict AI development to the stuff which we have good reasons for being confident are safe // otherwise beneficial.\n\nBettering democracy — helping people to access key information about what decisions their country might make would actually mean for longer term implications, and having this in turn feed into incentives for decision-makers that better align them with the common good\n\nBuilding safe / aligned AI — better tools for anticipating when systems are actually robustly safe, vs when stronger cases for expectation of safety seem required\n\n## Josh Rosenberg (FRI)\n\nI'm massively confused about important problems, e.g. how to orient to AI risk and what to do about it. Some reasonable people think there's a high probability we effectively lose everything we care about in 2027\\. Some reasonable people think it'll be decades till we have \"AGI\" and it'd be good if it arrived sooner. Which actions are good to take in each of these worlds is very different, and I'd like to figure out what world we're in.\n\nThis also applies to many other problem areas.\n\nE.g., Trump's effect on US democracy. How much of a crisis is it? What are the most effective actions to take in response to ensure that the country and communities we care about are as well off as they can be? How can we preserve high-quality institutions to lead the world in positive directions?\n\nOr, on the positive vision side: the \"Abundance\" agenda seems promising. Is it actually a good idea? What do we do about it?\n\nAI for Epistemics could help us get a lot more clarity on how to think about these problems and what actions to take. I'd like much better information and forecasts on the likely effects of Trump, AI timelines, and the pro's and con's of different Abundance policies, for example. If I had a bunch of time and good resources to dig into these problems, I could get a much better sense for myself. I'm hoping AI will help all of us do this sort of thing collectively.\n\n## Katja Grace\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Put lots of cognitive effort into very clear and good and compelling case for AI risk  \n  * Automate talking to people about AI risk compellingly, possibly using above case as input  \n  * Good game theoretic thinking re arms races, coordination, what is possible. I.e. People are making dumb assumptions about things, e.g. \"can't slow down tech\". Could automate noting errors. So e.g. when someone says that on Twitter, can highlight it and send in response. \\-\\> good thinking via better weapons for debate (similar to Nathan project)  \n  * Specific idea: very quantified and competitive debate site, that can be cited re which views are winning, where AI does a lot of the cognitive labor in places no people are paying attention, but as a part of it gets cruxy, people pay attention and vote/bet  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Poor decision making around AI: e.g. 'racing' because of vague heuristics instead of ever doing calc on costs and benefits given specific risks  \n  * Failure to coordinate on AI: building AI systems that are dangerous, due to expecting others to do so\n\n## Joel Lehman\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Raising the bottom (i.e. helping improve societal epistemics) – in particular that in theory political decisions (at least political decision-makers) are somewhat downstream of societal epistemics  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Having weird political leaders and awful public epistemics at critical junctures seems to unnecessarily spike catastrophic/existential risk probabilities. In theory, helping uplevel or at least maintain current public epistemic health (especially as it relates to civic sensemaking) might help influence the quality of decision-makers and concrete decisions made at high-stakes moments  \n  * The shape of the tech is not clear, or probably requires multiple flavors of interventions. E.g. if LLM assistants by default help cultivate civic virtues and on the margin support critical thinking or whatever underlying skills and knowledge help a person up-level their epistemic habits / health (or at least if LLM assistants don't contribute to atrophy/harm); or somehow convincing social media networks to care about this (e.g. community notes but things that go beyond); similarly somehow help w/ important social goods like 'societal trust' and 'trust in your neighbors'  \n* What do the later stages of the path to impact look like?  \n  * \\[didn't get to this\\]  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * Healthier democratic society, better agreement on consensus reality, higher trust in society, better politicians, better concrete decisions in high-stakes moments \n\n## Austin Chen\n\n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * **Evaluator**: More agreement (or understanding of disagreement) of what orgs & people are doing good work (eg for making grants in AI safety)  \n  * **Oracle**: Instant trustworthy forecasts on any question you ask  \n  * **Planner**: Guidance on \"what are the 3 most important things I should do this day/week/month\"  \n  * **Representative**: Simulate the responses of other parties (people, orgs/corps, governments, LLM agents), for discussions, contracts, trade"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Strong Emphasis on Uncertainty and Risk",
          "description": "Several contributors discuss uncertainty, risk, and the challenges of forecasting, which adds nuance and realism to the analysis.",
          "importance": 65,
          "grade": 85,
          "isValid": false,
          "highlight": {
            "startOffset": 11616,
            "endOffset": 12812,
            "quotedText": "I'm massively confused about important problems, e.g. how to orient to AI risk and what to do about it. Some reasonable people think there's a high probability we effectively lose everything we care about in 2027\\. Some reasonable people think it'll be decades till we have \"AGI\" and it'd be good if it arrived sooner. Which actions are good to take in each of these worlds is very different, and I'd like to figure out what world we're in.\n\nThis also applies to many other problem areas.\n\nE.g., Trump's effect on US democracy. How much of a crisis is it? What are the most effective actions to take in response to ensure that the country and communities we care about are as well off as they can be? How can we preserve high-quality institutions to lead the world in positive directions?\n\nOr, on the positive vision side: the \"Abundance\" agenda seems promising. Is it actually a good idea? What do we do about it?\n\nAI for Epistemics could help us get a lot more clarity on how to think about these problems and what actions to take. I'd like much better information and forecasts on the likely effects of Trump, AI timelines, and the pro's and con's of different Abundance policies, for example."
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Occasional Use of Unexplained Social Media References",
          "description": "References to 'Community Notes', 'pol.is', and 'social media distraction' are made without explanation. Briefly introducing these references would improve accessibility.",
          "importance": 45,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 7126,
            "endOffset": 7876,
            "quotedText": "How can we build something like the positive version of Community Notes: which statements/posts are agreed on by people who often disagree?  \n  * How can we better understand/summarize what others think, e.g. on social media platforms? E.g. if you see a post recommended to you, is this because it's the most extreme take that went viral and isn't representative, or is this a commonly held view? Between different viewpoints, what are the concrete points of actual disagreement (on facts or values) and what are the things widely agreed upon? Let's build viewpoint/debate maps or a similar sensemaking tool that gets adopted/understood by the masses/raises the knowledge floor (in my experience, things like pol.is are hard for regular users to grok"
          }
        },
        {
          "title": "Effective Use of Contrasts and Comparisons",
          "description": "Some contributors contrast different approaches (e.g., 'raising the floor' vs. 'raising the ceiling'), which clarifies distinctions and aids understanding.",
          "importance": 60,
          "grade": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 21789,
            "endOffset": 22348,
            "quotedText": "**Raising the ceiling:** can we use AI to organize information and credibly resolve the most challenging and important questions on which experts still seem to disagree? For example:\n\n* COVID origins  \n* Urban planning and public policy  \n* Lots of advancements in basic science: routing around current disagreements and policy challenges with slam-dunk science might be a pretty strong way forward.\n\n**Raising the floor:** can we use AI to build extremely legibly trustworthy/reliable epistemic infrastructure that actually makes its way into people's lives?"
          }
        },
        {
          "title": "Occasional Use of Unexplained Institutional References",
          "description": "References to 'OpenAI', 'Anthropic', 'Google', 'xAI', and 'Bezos' are made without explanation. Briefly introducing these organizations would improve accessibility.",
          "importance": 45,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 5841,
            "endOffset": 6045,
            "quotedText": "* OpenAI strategy  \n* Anthropic strategy  \n* Google strategy  \n* xAI strategy  \n* AI workforce automation  \n* China strategy  \n* Russia strategy  \n* Moskovitz strategy  \n* Must strategy  \n* Bezos Strategy"
          }
        },
        {
          "title": "Strong Emphasis on Information Curation and Filtering",
          "description": "Several contributors discuss the importance of information curation, filtering, and management, which aligns with the core goals of epistemics.",
          "importance": 65,
          "grade": 85,
          "isValid": false,
          "highlight": {
            "startOffset": 5104,
            "endOffset": 6511,
            "quotedText": "Decoupling virality with magnitude while living in a short attention span world, curation   \n* I have some confusion re: raising the floor when not all parties are necessarily operating in good faith. I think I need some help think through the ways in which epistemics in the context of incentive misalignment can be used or have defensibility. \n\n## Nathan Young: \n\nWhat would we have got:\n\n* Biden not running, perhaps an open dem primary  \n* No FTX crisis  \n* Less zero sum OpenAI board drama  \n* Many smaller, better choices \n\nWhat decisions might go badly:\n\n* Trump tariffs/ global trade  \n* Taiwan/ US-China conflict  \n* US-China competition  \n* US authoritarianism  \n* AI regulation  \n* EU-China realignment  \n* Automated robots  \n* OpenAI strategy  \n* Anthropic strategy  \n* Google strategy  \n* xAI strategy  \n* AI workforce automation  \n* China strategy  \n* Russia strategy  \n* Moskovitz strategy  \n* Must strategy  \n* Bezos Strategy  \n* Zuckerberg strategy  \n* Tallinn strategy  \n* Altman strategy  \n* Drone warfare  \n* Climate change destabilisation  \n* Democratic trust\n\n## Lincoln Quirk:\n\nWhy do we care about (AI for) epistemics?  \n\\- making the public better informed:  \n\t\\- better able to deal with and process the wealth of information  \n\t\\- better able to express their own interests in government (via voting)  \n\t\\- helping cut through social media distraction / intentional misinformation"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Occasional Use of Unexplained Cognitive Science Concepts",
          "description": "Terms like 'epistemic habits', 'civic virtues', and 'sensemaking' are used without explanation. Briefly defining these concepts would improve accessibility.",
          "importance": 45,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 14861,
            "endOffset": 15314,
            "quotedText": "especially as it relates to civic sensemaking) might help influence the quality of decision-makers and concrete decisions made at high-stakes moments  \n  * The shape of the tech is not clear, or probably requires multiple flavors of interventions. E.g. if LLM assistants by default help cultivate civic virtues and on the margin support critical thinking or whatever underlying skills and knowledge help a person up-level their epistemic habits / health"
          }
        },
        {
          "title": "Effective Use of Open-Ended Questions",
          "description": "Some contributors pose open-ended questions (e.g., 'What do we do about it?'), which invite further discussion and exploration.",
          "importance": 60,
          "grade": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 12507,
            "endOffset": 12812,
            "quotedText": "What do we do about it?\n\nAI for Epistemics could help us get a lot more clarity on how to think about these problems and what actions to take. I'd like much better information and forecasts on the likely effects of Trump, AI timelines, and the pro's and con's of different Abundance policies, for example."
          }
        },
        {
          "title": "Occasional Use of Unexplained Negotiation Concepts",
          "description": "Terms like 'bargaining', 'pareto optimal', and 'salary negotiations' are used without explanation. Briefly defining these concepts would improve accessibility.",
          "importance": 45,
          "grade": 50,
          "isValid": false,
          "highlight": {
            "startOffset": 34496,
            "endOffset": 647,
            "quotedText": "  \n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nTemplate:\n\n## Name: \n\nEntry\n\n## Dan Schwarz:\n\n* What are use cases for AI for Epistemics that you most care about?\n\nGoing from \"Realized\" to \"Accessible\" knowledge, e.g. getting good information using the web. Agentic web agents like Deep Research.\n\n* What decisions may go badly by default, but could go better if we have the right tech?\n\nMany billions of small decisions.\n\n* What do the later stages of the path to impact look like?\n\nSuperhuman forecasting at scale.\n\n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nReasonable judgment on tricky questions that's too cheap to meter, better than if you asked a respected colleague to spend many minutes on it.\n\n## Aviv\n\n* Better government governance  \n* Better democratic functioning  \n* Reduced dangerous concentration of power  \n    \n* Healthy polarization / pluralism  \n    \n* More peace, less likelihood of war  \n* Better geopolitics  \n* Better economic incentives  \n    \n* Better AI org governance  \n* Better AI coordination (re. distribution, escalation, etc.)  \n* Better compute allocation (toward what matters for good outcomes)  \n    \n* Better allocation of resources overall  \n* Better lives for people  \n* More effective organizations, getting things done with fewer resources, more focused on the things that benefit everyone  \n    \n* Identification of common ground directions / equilibria / game theoretic strategies that create/incentivize new positive gradients (and not surfacing the bad versions of those…)  \n    \n* Understanding our own goals and values\n\n*Note how it is the intersection of better epistemics / world models + the right power structures / incentives needed for all of these.*\n\n## Ben Goldhaber:\n\n* Forecasting how I should live my life in a period of intense technological change  \n* Guiding decision makers to avoid war with china/ai/everyone  \n* Providing the background context for me to sensemake way more sanely.  \n* Pushing forward the extant knowledge of AI safety agendas  \n* Planning for multi-stage projects that could achieve big impact  \n* Roadmapping to build larger more complex things\n\n## Chi\n\nThere are a few different things I care about:\n\n* I care a lot about humanity and AI, including **super-late-stage AI, getting decision theory** right.  \n  * Consequently but also separately, this relates to the general question of what to do about areas where there is not necessarily a ground truth but that are nonetheless very important to get right, cf**. [metaphilosophy](https://www.alignmentforum.org/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy)** (but also a bunch of other things.)  \n  * This also feels very similar to making **AIs helpful for AI safety research, cf. [automating alignment research](https://joecarlsmith.com/2025/03/14/ai-for-ai-safety#2-what-is-ai-for-ai-safety).**\n\n  → This is more about raising the **top of the epistemic landscape**\n\n* Other things I care about:  \n  * Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place.  \n  * Vague sense that better **societal epistemics** would help with increasing the **sanity of conversations about AI safety, polarisation around the issue etc.**  \n    * This is broadly about **getting intent alignment** in the first place.\n\n→ These are all more about raising the **bottom of the epistemic landscape**\n\n## Deger Turan:\n\n* Social trust erosion, collective coordination mechanisms are built on a weak assumption  \n* Capacity to entertain truthfulness when it is not instrumental to one's incentive gradient & differing norms upon 'truth will make my sense of security weaker'   \n* Creating contexts where world model divergence can be agreed by both parties enabling a safe disagreement  \n* Improving model reasoning / safer AI by reducing slop, by creating accountability mechanisms through consistency checks & other specifications for healthy epistemics  \n* Decoupling virality with magnitude while living in a short attention span world, curation   \n* I have some confusion re: raising the floor when not all parties are necessarily operating in good faith. I think I need some help think through the ways in which epistemics in the context of incentive misalignment can be used or have defensibility. \n\n## Nathan Young: \n\nWhat would we have got:\n\n* Biden not running, perhaps an open dem primary  \n* No FTX crisis  \n* Less zero sum OpenAI board drama  \n* Many smaller, better choices \n\nWhat decisions might go badly:\n\n* Trump tariffs/ global trade  \n* Taiwan/ US-China conflict  \n* US-China competition  \n* US authoritarianism  \n* AI regulation  \n* EU-China realignment  \n* Automated robots  \n* OpenAI strategy  \n* Anthropic strategy  \n* Google strategy  \n* xAI strategy  \n* AI workforce automation  \n* China strategy  \n* Russia strategy  \n* Moskovitz strategy  \n* Must strategy  \n* Bezos Strategy  \n* Zuckerberg strategy  \n* Tallinn strategy  \n* Altman strategy  \n* Drone warfare  \n* Climate change destabilisation  \n* Democratic trust\n\n## Lincoln Quirk:\n\nWhy do we care about (AI for) epistemics?  \n\\- making the public better informed:  \n\t\\- better able to deal with and process the wealth of information  \n\t\\- better able to express their own interests in government (via voting)  \n\t\\- helping cut through social media distraction / intentional misinformation  \n\\- getting governments to implement better policy  \n\t\\- coming up with good policy ideas  \n\t\\- objectively doing cost-benefit analysis  \n\t\\- objectively explaining tradeoffs (vs having to rely on biased humans to explain them)  \n\t\\- flagging bad ideas as bad  \n\t\\- convincing policymakers with compelling writeups  \n\\- success and flourishing for individuals and orgs using these tools  \n\t\\- via helping them think through decisions  \n\t\\- via clearheaded analysis of trade-offs and cost/benefits  \n\t\\- flagging bad ideas as bad\n\n## Jay\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * How can we build something like the positive version of Community Notes: which statements/posts are agreed on by people who often disagree?  \n  * How can we better understand/summarize what others think, e.g. on social media platforms? E.g. if you see a post recommended to you, is this because it's the most extreme take that went viral and isn't representative, or is this a commonly held view? Between different viewpoints, what are the concrete points of actual disagreement (on facts or values) and what are the things widely agreed upon? Let's build viewpoint/debate maps or a similar sensemaking tool that gets adopted/understood by the masses/raises the knowledge floor (in my experience, things like pol.is are hard for regular users to grok)  \n  * How can we improve Community Notes (or more general knowledge) creation with AI assistants? An AI agent proposing community notes and humans still checking/rating them?  \n  * How can we have something like Community Notes that applies to claims or statements about the future, or which are not concretely knowable?  \n  * \n\n## Ben Wilson (Metaculus)\n\nOverall I believe the most important value of epistemics (and AI epistemics) in general is informing what actions allow us to achieve our goals (i.e. make the world better). If we had perfect AI epistemics I would be excited by the ability for us to take a goal, and get a list of possible actions we could take and the forecasted outcomes of each of those actions (plus background information and forecasts for the 'current lay of the land' for context). This can then inform, not just the risk of AI doing good/bad things (as with current forecasting), but what we can do to raise the chances of 'great' AI outcomes, and lower the changes of 'worse' AI outcomes\n\n## Andreas\n\n* **What are use cases for AI for Epistemics that you most care about?**  \n  * AI policy, broadly construed, must go better  \n    * Avoid gradual disempowerment  \n    * Thoughtful regulation  \n  * Organizational planning & decision-making is weak  \n    * Given an informal long-term goal like improve decision-making for strategic users in government & large orgs, make a plan for a 20-people org that helps them accomplish the goal  \n    * Mapping dependencies, uncertainties, and potential futures  \n    * Understand complex domains with non-trivial feedback loops  \n      * Economic shifts  \n* **What decisions may go badly by default, but could go better if we have the right tech?**  \n  * Resource allocation within orgs  \n    * Investment  \n    * Grant making  \n    * Government budgets  \n  * Policy  \n    * How should AI be deployed in the world?  \n* **What do the later stages of the path to impact look like?**  \n  * AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning  \n  * The technology helps organizations maintain coherence and make sound judgments even as the pace of change accelerates  \n  * Assumption: People are generally well-intentioned \\- no one wants catastrophe \\- and as a result of better tooling things go better in the world overall\n\n## Owen optimistic brainstorm:\n\nWorld peace without world government — sufficiently good tech to help countries understand each others' interests, find mutually desirable bargains, and make sufficient credible commitments, could help us get up towards the Pareto frontier where we avoid (virtually) all costly conflicts\n\nGoing slowly and carefully on AI development — rather than having AI progress accelerate at increasingly breakneck speed, helping actors to understand the basic situation and the desirability of different options could be crucial for facilitating meaningful and robust agreements which restrict AI development to the stuff which we have good reasons for being confident are safe // otherwise beneficial.\n\nBettering democracy — helping people to access key information about what decisions their country might make would actually mean for longer term implications, and having this in turn feed into incentives for decision-makers that better align them with the common good\n\nBuilding safe / aligned AI — better tools for anticipating when systems are actually robustly safe, vs when stronger cases for expectation of safety seem required\n\n## Josh Rosenberg (FRI)\n\nI'm massively confused about important problems, e.g. how to orient to AI risk and what to do about it. Some reasonable people think there's a high probability we effectively lose everything we care about in 2027\\. Some reasonable people think it'll be decades till we have \"AGI\" and it'd be good if it arrived sooner. Which actions are good to take in each of these worlds is very different, and I'd like to figure out what world we're in.\n\nThis also applies to many other problem areas.\n\nE.g., Trump's effect on US democracy. How much of a crisis is it? What are the most effective actions to take in response to ensure that the country and communities we care about are as well off as they can be? How can we preserve high-quality institutions to lead the world in positive directions?\n\nOr, on the positive vision side: the \"Abundance\" agenda seems promising. Is it actually a good idea? What do we do about it?\n\nAI for Epistemics could help us get a lot more clarity on how to think about these problems and what actions to take. I'd like much better information and forecasts on the likely effects of Trump, AI timelines, and the pro's and con's of different Abundance policies, for example. If I had a bunch of time and good resources to dig into these problems, I could get a much better sense for myself. I'm hoping AI will help all of us do this sort of thing collectively.\n\n## Katja Grace\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Put lots of cognitive effort into very clear and good and compelling case for AI risk  \n  * Automate talking to people about AI risk compellingly, possibly using above case as input  \n  * Good game theoretic thinking re arms races, coordination, what is possible. I.e. People are making dumb assumptions about things, e.g. \"can't slow down tech\". Could automate noting errors. So e.g. when someone says that on Twitter, can highlight it and send in response. \\-\\> good thinking via better weapons for debate (similar to Nathan project)  \n  * Specific idea: very quantified and competitive debate site, that can be cited re which views are winning, where AI does a lot of the cognitive labor in places no people are paying attention, but as a part of it gets cruxy, people pay attention and vote/bet  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Poor decision making around AI: e.g. 'racing' because of vague heuristics instead of ever doing calc on costs and benefits given specific risks  \n  * Failure to coordinate on AI: building AI systems that are dangerous, due to expecting others to do so\n\n## Joel Lehman\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Raising the bottom (i.e. helping improve societal epistemics) – in particular that in theory political decisions (at least political decision-makers) are somewhat downstream of societal epistemics  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Having weird political leaders and awful public epistemics at critical junctures seems to unnecessarily spike catastrophic/existential risk probabilities. In theory, helping uplevel or at least maintain current public epistemic health (especially as it relates to civic sensemaking) might help influence the quality of decision-makers and concrete decisions made at high-stakes moments  \n  * The shape of the tech is not clear, or probably requires multiple flavors of interventions. E.g. if LLM assistants by default help cultivate civic virtues and on the margin support critical thinking or whatever underlying skills and knowledge help a person up-level their epistemic habits / health (or at least if LLM assistants don't contribute to atrophy/harm); or somehow convincing social media networks to care about this (e.g. community notes but things that go beyond); similarly somehow help w/ important social goods like 'societal trust' and 'trust in your neighbors'  \n* What do the later stages of the path to impact look like?  \n  * \\[didn't get to this\\]  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * Healthier democratic society, better agreement on consensus reality, higher trust in society, better politicians, better concrete decisions in high-stakes moments \n\n## Austin Chen\n\n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * **Evaluator**: More agreement (or understanding of disagreement) of what orgs & people are doing good work (eg for making grants in AI safety)  \n  * **Oracle**: Instant trustworthy forecasts on any question you ask  \n  * **Planner**: Guidance on \"what are the 3 most important things I should do this day/week/month\"  \n  * **Representative**: Simulate the responses of other parties (people, orgs/corps, governments, LLM agents), for discussions, contracts, trade\n\n## Janna\n\n* Why do we care about (AI for) epistemics?  \n  * Noise to signal ratio could be much higher  \n  * AI (mis)information churned out at the price of almost zero  \n* What are use cases for AI for Epistemics that you most care about?  \n  * Solving prediction markets--is there a way to scale up the bots, even if they are wrong and lossy, to improve prediction market accuracy?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * US-China relations  \n  * Trump tariffs maybe  \n  * Immigration rules in the US  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * More accurate predictions to…  \n    * Reduce business uncertainty  \n    * Make it easier to evaluate life decisions  \n  * Forestalling bad decisions made on imperfect information\n\n## Owain\n\n1. Maybe having humans understanding the world better is good in itself. Lots of people read popular science for this reason. Maybe it's good for AIs as well  \n2. Very broadly, it seems that better world understanding and decision making enables people to make their lives better in myriad ways. (To substantiate this, one could try to separate out the effects of better world understanding from the effects of better realized technology. E.g. Looking at capital-poor countries that can still benefit from scientific knowledge, economics, etc.)  \n3. More specifically, if AI progress is fast, there will be lots of important decisions to be made in quick calendar time. Exploiting AI to improve those decisions seems valuable. Current decision making institutions do not seem well set up to adapt really quickly. (A specific concern is dangers from AI takeover).   \n4. Why now? Why this group of people? We haven't really had tech that could do most epistemic tasks until recently. Now we have AI that can do an increasing proportion of the sub-tasks of epistemic processes. This will allow us to delegate epistemic tasks to AI.  \n   1. It's likely that people do this delegation, even if it's somewhat misguided and potentially bad in the long-term. Can we make this go well?  \n   2. This potentially massively expands our epistemic capacities. How can we use this well?  \n   3. Epistemic tech/institutions/advances can be hard to fund via for-profit companies. This is in part due to public goods issues, and issues around markets for ideas.\n\n## Rafe\n\nAs ever, I hope this will stop advanced artificial intelligence destroying the world, killing all its people \\&c.\n\nSo I think there are a few paths for impact here. I think there is a world where we can make the most productive thinkers about how to do things go further and better; I'm not so excited about a single-player version of this, as I think there's a pretty narrow window when top-level centaur mode works out\\[1\\]\n\nEveryone is already thinking about AI, and, I suspect, this will only get more acute. I think it's pretty hard for people to confront this and think about it carefully and well. It's scary\\! And there are a bunch of entrenched interests and memeplexes that preserve their status better (myopically) if people only have shallow slogans to think about AI with.\n\nGiven that, I suspect that the global sensemaking machine isn't up for the task of thinking about AI well. I hope we can help the global sensemaking machine upgrade itself. I have some hope that we as a society can come up with good ideas for dealing with AI, and also come to see how they make sense, so we can choose them in a robust way.\n\nOne other path to impact: I think a society that understands things better, including itself and its impacts, is one that is more robust in a bunch of ways. A more epistemically-grounded society is also probably a bit richer, and I think a bigger pie creates more slack for arranging things in sanity-promoting ways. I have more hope in a robust society navigating this stuff well.\n\n\\[1\\]:  Yeah, maybe you can do something with shaving off subskills, but I don't think that's really that doable.\n\n## Dave\n\n* With my consequentialist hat, better epistemics being worth it has to route through improved decisions somewhere down the line. The decisions I particularly care about are those by powerful people wrt AI development and implementation and the effects this has on how the future goes  \n  * But I do feel a bit icky when I have this hat on. There seems to be something very intuitively appealing about (a) pursuing better epistemics for its own sake and (b) democratising epistemic tools for the use and benefit of everyone. I think this basically caches out in more 'raising the floor' type work  \n  * I don't feel like I have a very good way of resolving this tension yet, but am excited to see things pushing on both frontiers  \n* I think forecasting has the potential to be a pretty powerful tool and there's currently a massive undersupply of good forecasts. I'm excited for us to use the wave of increased AI labor to make better quantitative forecasts to inform key decisions. I probably care more about quantitative forecasts than most decision makers and am open to being persuaded that the other, fuzzier parts of epistemics are more valuable to work on. \n\n## Julian\n\nWhat are use cases for AI for Epistemics that you most care about?  \n**Raising the ceiling:** can we use AI to organize information and credibly resolve the most challenging and important questions on which experts still seem to disagree? For example:\n\n* COVID origins  \n* Urban planning and public policy  \n* Lots of advancements in basic science: routing around current disagreements and policy challenges with slam-dunk science might be a pretty strong way forward.\n\n**Raising the floor:** can we use AI to build extremely legibly trustworthy/reliable epistemic infrastructure that actually makes its way into people's lives?\n\n* Making truth more palatable (in a way which doesn't translate into making falsehoods more palatable)  \n* Collective epistemics: connecting people to the others with whom they would have the most productive/enlightening conversations\n\nWhat decisions may go badly by default, but could go better if we have the right tech?\n\n* Lots of stuff in public policy: downstream both of better experts (technocrats), better public (voters/interest groups), and better coordination (collective epistemics)\n\nWhat do the later stages of the path to impact look like?\n\n* I want us to live in a world where everyone is more rational, empathetic, considered in their decision making, conscientious about impact, and interested in coordinating. I feel like that's one of the biggest blockers to all kinds of *long term* progress.\n\nWhat tools do we actually maybe want to build?\n\n* Easily accessible fact checkers / claim refuters  \n  * Powerful AI debaters  \n* Forecasting with explanations: superhuman and at scale  \n* Scientific meta-analysis, systematic review  \n* Ad-hoc / bespoke probabilistic models for individual problems  \n* Bespoke *simulators* / simulation environments  \n* Consistency checking and inconsistency identification  \n  * Arbitrage / market makers in prediction markets?  \n* Using AI to accelerate development of non-AI epistemic tools:  \n  * ?\n\n## Lawrence\n\nThere are already key questions/strategic problems where the truth is murky and we seem to proceed on the basis of limited and biased views. There might well be many more of these in the future. A central example is US-China competition over AI. Some have vibes of \"China is so bad that it's worth risking an existential catastrophe to stop it from capturing a portion of the lightcone\". Others have vibes of \"Cooperation is feasible and desirable, and is our only path to a good outcome\". But how bad is China, really? How much \"realised K\" on this question do those who are making the relevant decisions actually have? Similarly, how feasible is cooperation? How valid is this frame of \"capturing the lightcone\"? One reason that we don't have a better hold on these questions is that they are frighteningly complicated and difficult  – I think we could benefit massively from throwing much more cognitive labour at them, and presenting the findings in a way that is hard to disagree with or for any individual to improve upon, other than at a normative/values level. Maybe AI can get us there.\n\n## Lukas Finnveden\n\nwhy is AI for epistemics important/helpful?\n\n* better epistemics generally very good for making all kinds of decisions  \n* I'm specifically concerned about AI potentially causing an intelligence explosion, and us needing better epistemics to handle that. Specifically:  \n  * I'm worried about AI-driven tech R\\&D leading to catastrophes. To help address this, I want AI capabilities to not only be applied to tech R\\&D, but also to be applied to \"reasoning well from limited evidence\" about a number of questions, including how to deal with risks.  \n  * I'm worried about misaligned AI — so I want techniques for eliciting AI capabilities on all kinds of questions, so that AIs don't have better capabilities than we can elicit from them. (Basically: I want good alignment techniques even on hard-to-evaluate questions.)  \n  * Seems great to me if AI-driven improvements in general knowledge and decision-making are more broadly distributed among humans. (I think this leads to better decision-making without significantly accelerating the dangers from new tech or misaligned AI. Also reduces concentration of power.)\n\n## Abram Demski\n\n\\- Why are we doing this? \\-- impact theory brainstorming exercize  \n\t\\- There's the general claim that epistemics has degraded horribly due to social media and the internet, at least in a lot of ways, although epistemics has also gotten better in some specific ways. It seems very plausible that AI is, and will continue to, make this much much worse (due to deepfakes and super-persuasive writing, to give two examples). We wish to reverse this trend, or at least slow the acceleration.  \n\t\\- The high-level reason why epistemics is important is because epistemics is how we make good decisions. So effectively, this is AI for decision-making. However, if we just set out to make tools for decision-making, I have a sense that this would be worse.  \n\t\t\\- One super important aspect of decision-making is that you have to orient towards the right problem. People do much better when they understand the problem before proposing solutions.  \n\t\t\t\\- I have a sense that producing a better shared understanding of the world JUST WOULD result in better things. The connection between epistemics and better decisionmaking is that strong.  \n\t\t\t\t\\- However, there's an element of coordination to this. You have to have common knowledge, not just knowledge. AI for coordination is yet another thing. AI for group epistemics is close to that, though, and again, I have a sense that making \"AI for coordination\" would produce worse effects on the world than \"AI for group epistemics\".  \n\t\t\\- What specific decisions are we hoping to influence?  \n\t\t\t\\- Decisions around existential risk seem super super important. If AI were generally better at epistemics (as a relative, comparative thing \\-- ie we differentially accelerate epistemics) then key decisionmakers (in the government and at AI companies) would see much more clearly what's coming. Everyone wants a better future, so a better future would have a better chance of happening if we could see and navigate.\n\nI don't think I got any further than the supporting documentation for this workshop in articulating reasons and stuff, so I predict that the LLM summary won't be better than what we already know, although I hope it is. \n\n## Brendan\n\nFor us to live together in a caring and just way, we must make decisions about what is important to us together. Is it okay to tell my neighbour to turn their music down? Do we spend money on roads or trains or libraries? Do we regulate AI development and, if so, how? Do we want to spend public money on PEPFAR, or other avenues for reducing global poverty? To make these decisions in a way that is inclusive and democratic, that fosters buy-in and trust, we must begin with a shared sense of how the world is. This shared sense must be grounded in the physical, economic, and social reality we live in.\n\nAs we live together over wider and wider contexts, maintaining a shared sense of the world becomes more difficult and subtle. These wider contexts are facilitated by new technologies, both physical (eg. global manufacturing bases and supply chains) and informational (eg. social media). New technologies, such as AI for epistemics, and help maintain epistemic coherence across these contexts.\n\nMoreover, such technologies allow new avenues for cooperation at larger scales of human involvement. This unlocks new potential for coherence in global politics, scientific discovery, social and technological innovation, and cultural expression, and ultimately the deep joy of being alive and human in the world. \n\n## Isabel: \n\nThe kinds of being right in the past that are most relevant for decision making aren't just about making straightforward predictions about what will happen, but are more nuanced about what kinds of actions one should take to cause those things to happen, and these seem like somewhat different skills. These are harder to evaluate (e.g. because they're counterfactual: either we listened to person a and then we have to evaluate whether things went better or worse than if we didn't, but hard to say what the counterfactual would have been; or we didn't listen to person a and then we have to guess what things would have been like if we did) \n\nA bit adjacent to Neel Nanda's [recent piece](https://www.lesswrong.com/posts/P5zWiPF5cPJZSkiAK/good-research-takes-are-not-sufficient-for-good-strategic) on how \"good research takes aren't sufficient for good strategic takes\" \\- \"good forecasting takes aren't sufficient for good strategic takes\", but maybe some of this is maybe resolvable on the forecasting tech side \\- can we get better at creating and evaluating results from fuzzier questions? Can AI help? (Also can AI help with the whole problem, but this feels small and concrete enough to maybe get some traction on.) Also, some risk of actors building up good track records in order to be listened to, but then choosing to burn that track record to get power or things like that, something something trust \\- a concern for both human actors and e.g. scheming AIs \\- what things look good until they abruptly aren't? \n\nOn a different note, also interested in takes on what to do about and how to engage with actors who have epistemic processes that don't seem to be truth seeking in a comprehensible-to-me way. \n\n## Adam Marblestone:\n\nWhat do we get out of AI for epistemics?\n\nTrust, is one thing. Our current society seems to be fraught with distrust and motivated reasoning. Can we get a more objectively trustable source of truth? Not only accuracy but with a provenance that gives it an ability to be trusted by humans who are otherwise adversarial. Trust is needed to coordinate. Truthfulness is maybe somewhat necessary but seemingly not sufficient for a source being trusted. As a practical matter, on hot political topics I've been going to OpenAI Deep Research.\n\nBut at a certain level I wonder if AI for epistemics and some form of beneficial AGI could be basically the same thing. I asked Eric Drexler what he wanted for d/acc and he said something that sounds a bit like that…\n\n\"  \nHere's my unsurprising take on directed research for \"defensive acceleration\":  \nDevelopment and steering of epistemic tooling, aka [Large Knowledge Models](https://aiprospects.substack.com/p/large-knowledge-models), offers the highest leverage.\n\nPotential actions:  \n  • Identify and influence entities that could develop and scale necessary foundations.  \n  • Leverage the growing ease of development to take pre-scaling steps.  \n  • Within the scope of \"foundations\", accelerate productive directions for development.  \n  • To support science, ensure grounding of content in cited sources.  \n  • Pursue or promote assembly of appropriate knowledge inputs (beyond what is now used for training data).  \n  • Identify, promote, or implement epistemic tools supported by LKM resources.  \n  • Other kinds of exploration, promotion, preparation, and implementation.  \n  • Ensure that my advice is thoroughly understood.\n\nLKMs can accelerate pretty much everything that one might want, and perhaps tip the balance of applications toward *defensive* acceleration by promoting understanding of the *what and why* of defensive applications.\n\nAddendum:\n\nIn the email I sent a few minutes ago, the basic proposal becomes stronger if one reads \"LKM\" as something like \"large knowledge model functionality\", regardless of implementation.\n\nThe knowledge-model concept (whatever one prefers to call it) should not be tied to my specific technical suggestions. What I've said can be framed as something like \"a discussion of the value, feasibility, and potential scope of a rich epistemic resource\". Bundles of embeddings, etc., are at a level of specificity that may be distracting in the broader context of the meeting.  \n\"\n\nSo that's a strategy assistant AI – which helps humans at large see ways they could act and coordinate especially around big technological or geopolitical strategy issues I guess – but also a certain embodiment of an AGI oracle type system. Maybe the idea then is, how to make a AGI that helps with navigating civilizational strategy, and what is the right form of or interface to that to make it epistemically most useful. \n\nI'd like to also see democratic mechanisms that are simply maximally utilizing the tech we have. \n\n## Elizabeth\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Allowing people to be better informed about experimental medical stuff for themselves.   \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  "
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Strong Emphasis on Pluralism and Diversity of Perspectives",
          "description": "Several contributors highlight the importance of pluralism, diversity of perspectives, and healthy disagreement, which adds depth and balance to the discussion.",
          "importance": 65,
          "grade": 85,
          "isValid": false,
          "highlight": {
            "startOffset": 1518,
            "endOffset": 4929,
            "quotedText": "Healthy polarization / pluralism  \n    \n* More peace, less likelihood of war  \n* Better geopolitics  \n* Better economic incentives  \n    \n* Better AI org governance  \n* Better AI coordination (re. distribution, escalation, etc.)  \n* Better compute allocation (toward what matters for good outcomes)  \n    \n* Better allocation of resources overall  \n* Better lives for people  \n* More effective organizations, getting things done with fewer resources, more focused on the things that benefit everyone  \n    \n* Identification of common ground directions / equilibria / game theoretic strategies that create/incentivize new positive gradients (and not surfacing the bad versions of those…)  \n    \n* Understanding our own goals and values\n\n*Note how it is the intersection of better epistemics / world models + the right power structures / incentives needed for all of these.*\n\n## Ben Goldhaber:\n\n* Forecasting how I should live my life in a period of intense technological change  \n* Guiding decision makers to avoid war with china/ai/everyone  \n* Providing the background context for me to sensemake way more sanely.  \n* Pushing forward the extant knowledge of AI safety agendas  \n* Planning for multi-stage projects that could achieve big impact  \n* Roadmapping to build larger more complex things\n\n## Chi\n\nThere are a few different things I care about:\n\n* I care a lot about humanity and AI, including **super-late-stage AI, getting decision theory** right.  \n  * Consequently but also separately, this relates to the general question of what to do about areas where there is not necessarily a ground truth but that are nonetheless very important to get right, cf**. [metaphilosophy](https://www.alignmentforum.org/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy)** (but also a bunch of other things.)  \n  * This also feels very similar to making **AIs helpful for AI safety research, cf. [automating alignment research](https://joecarlsmith.com/2025/03/14/ai-for-ai-safety#2-what-is-ai-for-ai-safety).**\n\n  → This is more about raising the **top of the epistemic landscape**\n\n* Other things I care about:  \n  * Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place.  \n  * Vague sense that better **societal epistemics** would help with increasing the **sanity of conversations about AI safety, polarisation around the issue etc.**  \n    * This is broadly about **getting intent alignment** in the first place.\n\n→ These are all more about raising the **bottom of the epistemic landscape**\n\n## Deger Turan:\n\n* Social trust erosion, collective coordination mechanisms are built on a weak assumption  \n* Capacity to entertain truthfulness when it is not instrumental to one's incentive gradient & differing norms upon 'truth will make my sense of security weaker'   \n* Creating contexts where world model divergence can be agreed by both parties enabling a safe disagreement"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Occasional Use of Unexplained Trust Concepts",
          "description": "Terms like 'track record', 'trustworthiness', and 'provenance' are used without explanation. Briefly defining these concepts would improve accessibility.",
          "importance": 45,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 31269,
            "endOffset": 31576,
            "quotedText": "Trust, is one thing. Our current society seems to be fraught with distrust and motivated reasoning. Can we get a more objectively trustable source of truth? Not only accuracy but with a provenance that gives it an ability to be trusted by humans who are otherwise adversarial. Trust is needed to coordinate."
          }
        },
        {
          "title": "Effective Use of Layered Reasoning",
          "description": "Some contributors build their arguments in layers (e.g., starting with individual benefits, then moving to societal outcomes), which aids comprehension and logical flow.",
          "importance": 60,
          "grade": 85,
          "isValid": false,
          "highlight": {
            "startOffset": 17352,
            "endOffset": 18723,
            "quotedText": "1. Maybe having humans understanding the world better is good in itself. Lots of people read popular science for this reason. Maybe it's good for AIs as well  \n2. Very broadly, it seems that better world understanding and decision making enables people to make their lives better in myriad ways. (To substantiate this, one could try to separate out the effects of better world understanding from the effects of better realized technology. E.g. Looking at capital-poor countries that can still benefit from scientific knowledge, economics, etc.)  \n3. More specifically, if AI progress is fast, there will be lots of important decisions to be made in quick calendar time. Exploiting AI to improve those decisions seems valuable. Current decision making institutions do not seem well set up to adapt really quickly. (A specific concern is dangers from AI takeover).   \n4. Why now? Why this group of people? We haven't really had tech that could do most epistemic tasks until recently. Now we have AI that can do an increasing proportion of the sub-tasks of epistemic processes. This will allow us to delegate epistemic tasks to AI.  \n   1. It's likely that people do this delegation, even if it's somewhat misguided and potentially bad in the long-term. Can we make this go well?  \n   2. This potentially massively expands our epistemic capacities. How can we use this well?"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Occasional Use of Unexplained Coordination Concepts",
          "description": "Terms like 'coordination', 'common knowledge', and 'group epistemics' are used without explanation. Briefly defining these concepts would improve accessibility.",
          "importance": 45,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 27163,
            "endOffset": 27923,
            "quotedText": "You have to have common knowledge, not just knowledge. AI for coordination is yet another thing. AI for group epistemics is close to that, though, and again, I have a sense that making \"AI for coordination\" would produce worse effects on the world than \"AI for group epistemics\".  \n\t\t\\- What specific decisions are we hoping to influence?  \n\t\t\t\\- Decisions around existential risk seem super super important. If AI were generally better at epistemics (as a relative, comparative thing \\-- ie we differentially accelerate epistemics) then key decisionmakers (in the government and at AI companies) would see much more clearly what's coming. Everyone wants a better future, so a better future would have a better chance of happening if we could see and navigate."
          }
        },
        {
          "title": "Strong Emphasis on Institutional Trust and Legitimacy",
          "description": "Several contributors discuss the importance of institutional trust and legitimacy, which aligns with the goals of epistemics and societal well-being.",
          "importance": 65,
          "grade": 85,
          "isValid": false,
          "highlight": {
            "startOffset": 12317,
            "endOffset": 28762,
            "quotedText": "How can we preserve high-quality institutions to lead the world in positive directions?\n\nOr, on the positive vision side: the \"Abundance\" agenda seems promising. Is it actually a good idea? What do we do about it?\n\nAI for Epistemics could help us get a lot more clarity on how to think about these problems and what actions to take. I'd like much better information and forecasts on the likely effects of Trump, AI timelines, and the pro's and con's of different Abundance policies, for example. If I had a bunch of time and good resources to dig into these problems, I could get a much better sense for myself. I'm hoping AI will help all of us do this sort of thing collectively.\n\n## Katja Grace\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Put lots of cognitive effort into very clear and good and compelling case for AI risk  \n  * Automate talking to people about AI risk compellingly, possibly using above case as input  \n  * Good game theoretic thinking re arms races, coordination, what is possible. I.e. People are making dumb assumptions about things, e.g. \"can't slow down tech\". Could automate noting errors. So e.g. when someone says that on Twitter, can highlight it and send in response. \\-\\> good thinking via better weapons for debate (similar to Nathan project)  \n  * Specific idea: very quantified and competitive debate site, that can be cited re which views are winning, where AI does a lot of the cognitive labor in places no people are paying attention, but as a part of it gets cruxy, people pay attention and vote/bet  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Poor decision making around AI: e.g. 'racing' because of vague heuristics instead of ever doing calc on costs and benefits given specific risks  \n  * Failure to coordinate on AI: building AI systems that are dangerous, due to expecting others to do so\n\n## Joel Lehman\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Raising the bottom (i.e. helping improve societal epistemics) – in particular that in theory political decisions (at least political decision-makers) are somewhat downstream of societal epistemics  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Having weird political leaders and awful public epistemics at critical junctures seems to unnecessarily spike catastrophic/existential risk probabilities. In theory, helping uplevel or at least maintain current public epistemic health (especially as it relates to civic sensemaking) might help influence the quality of decision-makers and concrete decisions made at high-stakes moments  \n  * The shape of the tech is not clear, or probably requires multiple flavors of interventions. E.g. if LLM assistants by default help cultivate civic virtues and on the margin support critical thinking or whatever underlying skills and knowledge help a person up-level their epistemic habits / health (or at least if LLM assistants don't contribute to atrophy/harm); or somehow convincing social media networks to care about this (e.g. community notes but things that go beyond); similarly somehow help w/ important social goods like 'societal trust' and 'trust in your neighbors'  \n* What do the later stages of the path to impact look like?  \n  * \\[didn't get to this\\]  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * Healthier democratic society, better agreement on consensus reality, higher trust in society, better politicians, better concrete decisions in high-stakes moments \n\n## Austin Chen\n\n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * **Evaluator**: More agreement (or understanding of disagreement) of what orgs & people are doing good work (eg for making grants in AI safety)  \n  * **Oracle**: Instant trustworthy forecasts on any question you ask  \n  * **Planner**: Guidance on \"what are the 3 most important things I should do this day/week/month\"  \n  * **Representative**: Simulate the responses of other parties (people, orgs/corps, governments, LLM agents), for discussions, contracts, trade\n\n## Janna\n\n* Why do we care about (AI for) epistemics?  \n  * Noise to signal ratio could be much higher  \n  * AI (mis)information churned out at the price of almost zero  \n* What are use cases for AI for Epistemics that you most care about?  \n  * Solving prediction markets--is there a way to scale up the bots, even if they are wrong and lossy, to improve prediction market accuracy?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * US-China relations  \n  * Trump tariffs maybe  \n  * Immigration rules in the US  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * More accurate predictions to…  \n    * Reduce business uncertainty  \n    * Make it easier to evaluate life decisions  \n  * Forestalling bad decisions made on imperfect information\n\n## Owain\n\n1. Maybe having humans understanding the world better is good in itself. Lots of people read popular science for this reason. Maybe it's good for AIs as well  \n2. Very broadly, it seems that better world understanding and decision making enables people to make their lives better in myriad ways. (To substantiate this, one could try to separate out the effects of better world understanding from the effects of better realized technology. E.g. Looking at capital-poor countries that can still benefit from scientific knowledge, economics, etc.)  \n3. More specifically, if AI progress is fast, there will be lots of important decisions to be made in quick calendar time. Exploiting AI to improve those decisions seems valuable. Current decision making institutions do not seem well set up to adapt really quickly. (A specific concern is dangers from AI takeover).   \n4. Why now? Why this group of people? We haven't really had tech that could do most epistemic tasks until recently. Now we have AI that can do an increasing proportion of the sub-tasks of epistemic processes. This will allow us to delegate epistemic tasks to AI.  \n   1. It's likely that people do this delegation, even if it's somewhat misguided and potentially bad in the long-term. Can we make this go well?  \n   2. This potentially massively expands our epistemic capacities. How can we use this well?  \n   3. Epistemic tech/institutions/advances can be hard to fund via for-profit companies. This is in part due to public goods issues, and issues around markets for ideas.\n\n## Rafe\n\nAs ever, I hope this will stop advanced artificial intelligence destroying the world, killing all its people \\&c.\n\nSo I think there are a few paths for impact here. I think there is a world where we can make the most productive thinkers about how to do things go further and better; I'm not so excited about a single-player version of this, as I think there's a pretty narrow window when top-level centaur mode works out\\[1\\]\n\nEveryone is already thinking about AI, and, I suspect, this will only get more acute. I think it's pretty hard for people to confront this and think about it carefully and well. It's scary\\! And there are a bunch of entrenched interests and memeplexes that preserve their status better (myopically) if people only have shallow slogans to think about AI with.\n\nGiven that, I suspect that the global sensemaking machine isn't up for the task of thinking about AI well. I hope we can help the global sensemaking machine upgrade itself. I have some hope that we as a society can come up with good ideas for dealing with AI, and also come to see how they make sense, so we can choose them in a robust way.\n\nOne other path to impact: I think a society that understands things better, including itself and its impacts, is one that is more robust in a bunch of ways. A more epistemically-grounded society is also probably a bit richer, and I think a bigger pie creates more slack for arranging things in sanity-promoting ways. I have more hope in a robust society navigating this stuff well.\n\n\\[1\\]:  Yeah, maybe you can do something with shaving off subskills, but I don't think that's really that doable.\n\n## Dave\n\n* With my consequentialist hat, better epistemics being worth it has to route through improved decisions somewhere down the line. The decisions I particularly care about are those by powerful people wrt AI development and implementation and the effects this has on how the future goes  \n  * But I do feel a bit icky when I have this hat on. There seems to be something very intuitively appealing about (a) pursuing better epistemics for its own sake and (b) democratising epistemic tools for the use and benefit of everyone. I think this basically caches out in more 'raising the floor' type work  \n  * I don't feel like I have a very good way of resolving this tension yet, but am excited to see things pushing on both frontiers  \n* I think forecasting has the potential to be a pretty powerful tool and there's currently a massive undersupply of good forecasts. I'm excited for us to use the wave of increased AI labor to make better quantitative forecasts to inform key decisions. I probably care more about quantitative forecasts than most decision makers and am open to being persuaded that the other, fuzzier parts of epistemics are more valuable to work on. \n\n## Julian\n\nWhat are use cases for AI for Epistemics that you most care about?  \n**Raising the ceiling:** can we use AI to organize information and credibly resolve the most challenging and important questions on which experts still seem to disagree? For example:\n\n* COVID origins  \n* Urban planning and public policy  \n* Lots of advancements in basic science: routing around current disagreements and policy challenges with slam-dunk science might be a pretty strong way forward.\n\n**Raising the floor:** can we use AI to build extremely legibly trustworthy/reliable epistemic infrastructure that actually makes its way into people's lives?\n\n* Making truth more palatable (in a way which doesn't translate into making falsehoods more palatable)  \n* Collective epistemics: connecting people to the others with whom they would have the most productive/enlightening conversations\n\nWhat decisions may go badly by default, but could go better if we have the right tech?\n\n* Lots of stuff in public policy: downstream both of better experts (technocrats), better public (voters/interest groups), and better coordination (collective epistemics)\n\nWhat do the later stages of the path to impact look like?\n\n* I want us to live in a world where everyone is more rational, empathetic, considered in their decision making, conscientious about impact, and interested in coordinating. I feel like that's one of the biggest blockers to all kinds of *long term* progress.\n\nWhat tools do we actually maybe want to build?\n\n* Easily accessible fact checkers / claim refuters  \n  * Powerful AI debaters  \n* Forecasting with explanations: superhuman and at scale  \n* Scientific meta-analysis, systematic review  \n* Ad-hoc / bespoke probabilistic models for individual problems  \n* Bespoke *simulators* / simulation environments  \n* Consistency checking and inconsistency identification  \n  * Arbitrage / market makers in prediction markets?  \n* Using AI to accelerate development of non-AI epistemic tools:  \n  * ?\n\n## Lawrence\n\nThere are already key questions/strategic problems where the truth is murky and we seem to proceed on the basis of limited and biased views. There might well be many more of these in the future. A central example is US-China competition over AI. Some have vibes of \"China is so bad that it's worth risking an existential catastrophe to stop it from capturing a portion of the lightcone\". Others have vibes of \"Cooperation is feasible and desirable, and is our only path to a good outcome\". But how bad is China, really? How much \"realised K\" on this question do those who are making the relevant decisions actually have? Similarly, how feasible is cooperation? How valid is this frame of \"capturing the lightcone\"? One reason that we don't have a better hold on these questions is that they are frighteningly complicated and difficult  – I think we could benefit massively from throwing much more cognitive labour at them, and presenting the findings in a way that is hard to disagree with or for any individual to improve upon, other than at a normative/values level. Maybe AI can get us there.\n\n## Lukas Finnveden\n\nwhy is AI for epistemics important/helpful?\n\n* better epistemics generally very good for making all kinds of decisions  \n* I'm specifically concerned about AI potentially causing an intelligence explosion, and us needing better epistemics to handle that. Specifically:  \n  * I'm worried about AI-driven tech R\\&D leading to catastrophes. To help address this, I want AI capabilities to not only be applied to tech R\\&D, but also to be applied to \"reasoning well from limited evidence\" about a number of questions, including how to deal with risks.  \n  * I'm worried about misaligned AI — so I want techniques for eliciting AI capabilities on all kinds of questions, so that AIs don't have better capabilities than we can elicit from them. (Basically: I want good alignment techniques even on hard-to-evaluate questions.)  \n  * Seems great to me if AI-driven improvements in general knowledge and decision-making are more broadly distributed among humans. (I think this leads to better decision-making without significantly accelerating the dangers from new tech or misaligned AI. Also reduces concentration of power.)\n\n## Abram Demski\n\n\\- Why are we doing this? \\-- impact theory brainstorming exercize  \n\t\\- There's the general claim that epistemics has degraded horribly due to social media and the internet, at least in a lot of ways, although epistemics has also gotten better in some specific ways. It seems very plausible that AI is, and will continue to, make this much much worse (due to deepfakes and super-persuasive writing, to give two examples). We wish to reverse this trend, or at least slow the acceleration.  \n\t\\- The high-level reason why epistemics is important is because epistemics is how we make good decisions. So effectively, this is AI for decision-making. However, if we just set out to make tools for decision-making, I have a sense that this would be worse.  \n\t\t\\- One super important aspect of decision-making is that you have to orient towards the right problem. People do much better when they understand the problem before proposing solutions.  \n\t\t\t\\- I have a sense that producing a better shared understanding of the world JUST WOULD result in better things. The connection between epistemics and better decisionmaking is that strong.  \n\t\t\t\t\\- However, there's an element of coordination to this. You have to have common knowledge, not just knowledge. AI for coordination is yet another thing. AI for group epistemics is close to that, though, and again, I have a sense that making \"AI for coordination\" would produce worse effects on the world than \"AI for group epistemics\".  \n\t\t\\- What specific decisions are we hoping to influence?  \n\t\t\t\\- Decisions around existential risk seem super super important. If AI were generally better at epistemics (as a relative, comparative thing \\-- ie we differentially accelerate epistemics) then key decisionmakers (in the government and at AI companies) would see much more clearly what's coming. Everyone wants a better future, so a better future would have a better chance of happening if we could see and navigate.\n\nI don't think I got any further than the supporting documentation for this workshop in articulating reasons and stuff, so I predict that the LLM summary won't be better than what we already know, although I hope it is. \n\n## Brendan\n\nFor us to live together in a caring and just way, we must make decisions about what is important to us together. Is it okay to tell my neighbour to turn their music down? Do we spend money on roads or trains or libraries? Do we regulate AI development and, if so, how? Do we want to spend public money on PEPFAR, or other avenues for reducing global poverty? To make these decisions in a way that is inclusive and democratic, that fosters buy-in and trust, we must begin with a shared sense of how the world is. This shared sense must be grounded in the physical, economic, and social reality we live in."
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Occasional Use of Unexplained Scientific Concepts",
          "description": "Terms like 'systematic review', 'meta-analysis', and 'probabilistic models' are used without explanation. Briefly defining these concepts would improve accessibility.",
          "importance": 45,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 23353,
            "endOffset": 23462,
            "quotedText": "Scientific meta-analysis, systematic review  \n* Ad-hoc / bespoke probabilistic models for individual problems"
          }
        },
        {
          "title": "Effective Use of Layered Impact Statements",
          "description": "Some contributors describe impacts at multiple levels (e.g., individual, organizational, societal), which provides a comprehensive view of potential outcomes.",
          "importance": 60,
          "grade": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 6833,
            "endOffset": 6772,
            "quotedText": "  \n\t\\- convincing policymakers with compelling writeups  \n\\- "
          }
        },
        {
          "title": "Occasional Use of Unexplained Policy Concepts",
          "description": "Terms like 'policy', 'regulation', and 'resource allocation' are used without explanation. Briefly defining these concepts would improve accessibility.",
          "importance": 45,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 9710,
            "endOffset": 9636,
            "quotedText": "  \n    * Investment  \n    * Grant making  \n    * Government budgets  \n  * "
          }
        },
        {
          "title": "Strong Emphasis on the Need for Better Tools",
          "description": "Many contributors highlight the need for better tools and infrastructure for epistemics, which aligns with the document's central purpose.",
          "importance": 65,
          "grade": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 23164,
            "endOffset": 23692,
            "quotedText": "What tools do we actually maybe want to build?\n\n* Easily accessible fact checkers / claim refuters  \n  * Powerful AI debaters  \n* Forecasting with explanations: superhuman and at scale  \n* Scientific meta-analysis, systematic review  \n* Ad-hoc / bespoke probabilistic models for individual problems  \n* Bespoke *simulators* / simulation environments  \n* Consistency checking and inconsistency identification  \n  * Arbitrage / market makers in prediction markets?  \n* Using AI to accelerate development of non-AI epistemic tools:"
          }
        },
        {
          "title": "Occasional Use of Unexplained Economic Incentive Concepts",
          "description": "Terms like 'economic incentives', 'grant making', and 'investment' are used without explanation. Briefly defining these concepts would improve accessibility.",
          "importance": 45,
          "grade": 50,
          "isValid": false,
          "highlight": {
            "startOffset": 1622,
            "endOffset": 9655,
            "quotedText": "Better economic incentives  \n    \n* Better AI org governance  \n* Better AI coordination (re. distribution, escalation, etc.)  \n* Better compute allocation (toward what matters for good outcomes)  \n    \n* Better allocation of resources overall  \n* Better lives for people  \n* More effective organizations, getting things done with fewer resources, more focused on the things that benefit everyone  \n    \n* Identification of common ground directions / equilibria / game theoretic strategies that create/incentivize new positive gradients (and not surfacing the bad versions of those…)  \n    \n* Understanding our own goals and values\n\n*Note how it is the intersection of better epistemics / world models + the right power structures / incentives needed for all of these.*\n\n## Ben Goldhaber:\n\n* Forecasting how I should live my life in a period of intense technological change  \n* Guiding decision makers to avoid war with china/ai/everyone  \n* Providing the background context for me to sensemake way more sanely.  \n* Pushing forward the extant knowledge of AI safety agendas  \n* Planning for multi-stage projects that could achieve big impact  \n* Roadmapping to build larger more complex things\n\n## Chi\n\nThere are a few different things I care about:\n\n* I care a lot about humanity and AI, including **super-late-stage AI, getting decision theory** right.  \n  * Consequently but also separately, this relates to the general question of what to do about areas where there is not necessarily a ground truth but that are nonetheless very important to get right, cf**. [metaphilosophy](https://www.alignmentforum.org/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy)** (but also a bunch of other things.)  \n  * This also feels very similar to making **AIs helpful for AI safety research, cf. [automating alignment research](https://joecarlsmith.com/2025/03/14/ai-for-ai-safety#2-what-is-ai-for-ai-safety).**\n\n  → This is more about raising the **top of the epistemic landscape**\n\n* Other things I care about:  \n  * Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place.  \n  * Vague sense that better **societal epistemics** would help with increasing the **sanity of conversations about AI safety, polarisation around the issue etc.**  \n    * This is broadly about **getting intent alignment** in the first place.\n\n→ These are all more about raising the **bottom of the epistemic landscape**\n\n## Deger Turan:\n\n* Social trust erosion, collective coordination mechanisms are built on a weak assumption  \n* Capacity to entertain truthfulness when it is not instrumental to one's incentive gradient & differing norms upon 'truth will make my sense of security weaker'   \n* Creating contexts where world model divergence can be agreed by both parties enabling a safe disagreement  \n* Improving model reasoning / safer AI by reducing slop, by creating accountability mechanisms through consistency checks & other specifications for healthy epistemics  \n* Decoupling virality with magnitude while living in a short attention span world, curation   \n* I have some confusion re: raising the floor when not all parties are necessarily operating in good faith. I think I need some help think through the ways in which epistemics in the context of incentive misalignment can be used or have defensibility. \n\n## Nathan Young: \n\nWhat would we have got:\n\n* Biden not running, perhaps an open dem primary  \n* No FTX crisis  \n* Less zero sum OpenAI board drama  \n* Many smaller, better choices \n\nWhat decisions might go badly:\n\n* Trump tariffs/ global trade  \n* Taiwan/ US-China conflict  \n* US-China competition  \n* US authoritarianism  \n* AI regulation  \n* EU-China realignment  \n* Automated robots  \n* OpenAI strategy  \n* Anthropic strategy  \n* Google strategy  \n* xAI strategy  \n* AI workforce automation  \n* China strategy  \n* Russia strategy  \n* Moskovitz strategy  \n* Must strategy  \n* Bezos Strategy  \n* Zuckerberg strategy  \n* Tallinn strategy  \n* Altman strategy  \n* Drone warfare  \n* Climate change destabilisation  \n* Democratic trust\n\n## Lincoln Quirk:\n\nWhy do we care about (AI for) epistemics?  \n\\- making the public better informed:  \n\t\\- better able to deal with and process the wealth of information  \n\t\\- better able to express their own interests in government (via voting)  \n\t\\- helping cut through social media distraction / intentional misinformation  \n\\- getting governments to implement better policy  \n\t\\- coming up with good policy ideas  \n\t\\- objectively doing cost-benefit analysis  \n\t\\- objectively explaining tradeoffs (vs having to rely on biased humans to explain them)  \n\t\\- flagging bad ideas as bad  \n\t\\- convincing policymakers with compelling writeups  \n\\- success and flourishing for individuals and orgs using these tools  \n\t\\- via helping them think through decisions  \n\t\\- via clearheaded analysis of trade-offs and cost/benefits  \n\t\\- flagging bad ideas as bad\n\n## Jay\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * How can we build something like the positive version of Community Notes: which statements/posts are agreed on by people who often disagree?  \n  * How can we better understand/summarize what others think, e.g. on social media platforms? E.g. if you see a post recommended to you, is this because it's the most extreme take that went viral and isn't representative, or is this a commonly held view? Between different viewpoints, what are the concrete points of actual disagreement (on facts or values) and what are the things widely agreed upon? Let's build viewpoint/debate maps or a similar sensemaking tool that gets adopted/understood by the masses/raises the knowledge floor (in my experience, things like pol.is are hard for regular users to grok)  \n  * How can we improve Community Notes (or more general knowledge) creation with AI assistants? An AI agent proposing community notes and humans still checking/rating them?  \n  * How can we have something like Community Notes that applies to claims or statements about the future, or which are not concretely knowable?  \n  * \n\n## Ben Wilson (Metaculus)\n\nOverall I believe the most important value of epistemics (and AI epistemics) in general is informing what actions allow us to achieve our goals (i.e. make the world better). If we had perfect AI epistemics I would be excited by the ability for us to take a goal, and get a list of possible actions we could take and the forecasted outcomes of each of those actions (plus background information and forecasts for the 'current lay of the land' for context). This can then inform, not just the risk of AI doing good/bad things (as with current forecasting), but what we can do to raise the chances of 'great' AI outcomes, and lower the changes of 'worse' AI outcomes\n\n## Andreas\n\n* **What are use cases for AI for Epistemics that you most care about?**  \n  * AI policy, broadly construed, must go better  \n    * Avoid gradual disempowerment  \n    * Thoughtful regulation  \n  * Organizational planning & decision-making is weak  \n    * Given an informal long-term goal like improve decision-making for strategic users in government & large orgs, make a plan for a 20-people org that helps them accomplish the goal  \n    * Mapping dependencies, uncertainties, and potential futures  \n    * Understand complex domains with non-trivial feedback loops  \n      * Economic shifts  \n* **What decisions may go badly by default, but could go better if we have the right tech?**  \n  * Resource allocation within orgs  \n    * Investment"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Effective Use of Layered Problem Statements",
          "description": "Some contributors describe problems at multiple levels (e.g., individual, organizational, societal), which provides a comprehensive view of challenges.",
          "importance": 60,
          "grade": 85,
          "isValid": false,
          "highlight": {
            "startOffset": 23715,
            "endOffset": 24810,
            "quotedText": "There are already key questions/strategic problems where the truth is murky and we seem to proceed on the basis of limited and biased views. There might well be many more of these in the future. A central example is US-China competition over AI. Some have vibes of \"China is so bad that it's worth risking an existential catastrophe to stop it from capturing a portion of the lightcone\". Others have vibes of \"Cooperation is feasible and desirable, and is our only path to a good outcome\". But how bad is China, really? How much \"realised K\" on this question do those who are making the relevant decisions actually have? Similarly, how feasible is cooperation? How valid is this frame of \"capturing the lightcone\"? One reason that we don't have a better hold on these questions is that they are frighteningly complicated and difficult  – I think we could benefit massively from throwing much more cognitive labour at them, and presenting the findings in a way that is hard to disagree with or for any individual to improve upon, other than at a normative/values level. Maybe AI can get us there."
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Occasional Use of Unexplained Social Trust Concepts",
          "description": "Terms like 'social trust erosion', 'collective coordination mechanisms', and 'societal trust' are used without explanation. Briefly defining these concepts would improve accessibility.",
          "importance": 45,
          "grade": 50,
          "isValid": false,
          "highlight": {
            "startOffset": 4567,
            "endOffset": 15594,
            "quotedText": "Social trust erosion, collective coordination mechanisms are built on a weak assumption  \n* Capacity to entertain truthfulness when it is not instrumental to one's incentive gradient & differing norms upon 'truth will make my sense of security weaker'   \n* Creating contexts where world model divergence can be agreed by both parties enabling a safe disagreement  \n* Improving model reasoning / safer AI by reducing slop, by creating accountability mechanisms through consistency checks & other specifications for healthy epistemics  \n* Decoupling virality with magnitude while living in a short attention span world, curation   \n* I have some confusion re: raising the floor when not all parties are necessarily operating in good faith. I think I need some help think through the ways in which epistemics in the context of incentive misalignment can be used or have defensibility. \n\n## Nathan Young: \n\nWhat would we have got:\n\n* Biden not running, perhaps an open dem primary  \n* No FTX crisis  \n* Less zero sum OpenAI board drama  \n* Many smaller, better choices \n\nWhat decisions might go badly:\n\n* Trump tariffs/ global trade  \n* Taiwan/ US-China conflict  \n* US-China competition  \n* US authoritarianism  \n* AI regulation  \n* EU-China realignment  \n* Automated robots  \n* OpenAI strategy  \n* Anthropic strategy  \n* Google strategy  \n* xAI strategy  \n* AI workforce automation  \n* China strategy  \n* Russia strategy  \n* Moskovitz strategy  \n* Must strategy  \n* Bezos Strategy  \n* Zuckerberg strategy  \n* Tallinn strategy  \n* Altman strategy  \n* Drone warfare  \n* Climate change destabilisation  \n* Democratic trust\n\n## Lincoln Quirk:\n\nWhy do we care about (AI for) epistemics?  \n\\- making the public better informed:  \n\t\\- better able to deal with and process the wealth of information  \n\t\\- better able to express their own interests in government (via voting)  \n\t\\- helping cut through social media distraction / intentional misinformation  \n\\- getting governments to implement better policy  \n\t\\- coming up with good policy ideas  \n\t\\- objectively doing cost-benefit analysis  \n\t\\- objectively explaining tradeoffs (vs having to rely on biased humans to explain them)  \n\t\\- flagging bad ideas as bad  \n\t\\- convincing policymakers with compelling writeups  \n\\- success and flourishing for individuals and orgs using these tools  \n\t\\- via helping them think through decisions  \n\t\\- via clearheaded analysis of trade-offs and cost/benefits  \n\t\\- flagging bad ideas as bad\n\n## Jay\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * How can we build something like the positive version of Community Notes: which statements/posts are agreed on by people who often disagree?  \n  * How can we better understand/summarize what others think, e.g. on social media platforms? E.g. if you see a post recommended to you, is this because it's the most extreme take that went viral and isn't representative, or is this a commonly held view? Between different viewpoints, what are the concrete points of actual disagreement (on facts or values) and what are the things widely agreed upon? Let's build viewpoint/debate maps or a similar sensemaking tool that gets adopted/understood by the masses/raises the knowledge floor (in my experience, things like pol.is are hard for regular users to grok)  \n  * How can we improve Community Notes (or more general knowledge) creation with AI assistants? An AI agent proposing community notes and humans still checking/rating them?  \n  * How can we have something like Community Notes that applies to claims or statements about the future, or which are not concretely knowable?  \n  * \n\n## Ben Wilson (Metaculus)\n\nOverall I believe the most important value of epistemics (and AI epistemics) in general is informing what actions allow us to achieve our goals (i.e. make the world better). If we had perfect AI epistemics I would be excited by the ability for us to take a goal, and get a list of possible actions we could take and the forecasted outcomes of each of those actions (plus background information and forecasts for the 'current lay of the land' for context). This can then inform, not just the risk of AI doing good/bad things (as with current forecasting), but what we can do to raise the chances of 'great' AI outcomes, and lower the changes of 'worse' AI outcomes\n\n## Andreas\n\n* **What are use cases for AI for Epistemics that you most care about?**  \n  * AI policy, broadly construed, must go better  \n    * Avoid gradual disempowerment  \n    * Thoughtful regulation  \n  * Organizational planning & decision-making is weak  \n    * Given an informal long-term goal like improve decision-making for strategic users in government & large orgs, make a plan for a 20-people org that helps them accomplish the goal  \n    * Mapping dependencies, uncertainties, and potential futures  \n    * Understand complex domains with non-trivial feedback loops  \n      * Economic shifts  \n* **What decisions may go badly by default, but could go better if we have the right tech?**  \n  * Resource allocation within orgs  \n    * Investment  \n    * Grant making  \n    * Government budgets  \n  * Policy  \n    * How should AI be deployed in the world?  \n* **What do the later stages of the path to impact look like?**  \n  * AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning  \n  * The technology helps organizations maintain coherence and make sound judgments even as the pace of change accelerates  \n  * Assumption: People are generally well-intentioned \\- no one wants catastrophe \\- and as a result of better tooling things go better in the world overall\n\n## Owen optimistic brainstorm:\n\nWorld peace without world government — sufficiently good tech to help countries understand each others' interests, find mutually desirable bargains, and make sufficient credible commitments, could help us get up towards the Pareto frontier where we avoid (virtually) all costly conflicts\n\nGoing slowly and carefully on AI development — rather than having AI progress accelerate at increasingly breakneck speed, helping actors to understand the basic situation and the desirability of different options could be crucial for facilitating meaningful and robust agreements which restrict AI development to the stuff which we have good reasons for being confident are safe // otherwise beneficial.\n\nBettering democracy — helping people to access key information about what decisions their country might make would actually mean for longer term implications, and having this in turn feed into incentives for decision-makers that better align them with the common good\n\nBuilding safe / aligned AI — better tools for anticipating when systems are actually robustly safe, vs when stronger cases for expectation of safety seem required\n\n## Josh Rosenberg (FRI)\n\nI'm massively confused about important problems, e.g. how to orient to AI risk and what to do about it. Some reasonable people think there's a high probability we effectively lose everything we care about in 2027\\. Some reasonable people think it'll be decades till we have \"AGI\" and it'd be good if it arrived sooner. Which actions are good to take in each of these worlds is very different, and I'd like to figure out what world we're in.\n\nThis also applies to many other problem areas.\n\nE.g., Trump's effect on US democracy. How much of a crisis is it? What are the most effective actions to take in response to ensure that the country and communities we care about are as well off as they can be? How can we preserve high-quality institutions to lead the world in positive directions?\n\nOr, on the positive vision side: the \"Abundance\" agenda seems promising. Is it actually a good idea? What do we do about it?\n\nAI for Epistemics could help us get a lot more clarity on how to think about these problems and what actions to take. I'd like much better information and forecasts on the likely effects of Trump, AI timelines, and the pro's and con's of different Abundance policies, for example. If I had a bunch of time and good resources to dig into these problems, I could get a much better sense for myself. I'm hoping AI will help all of us do this sort of thing collectively.\n\n## Katja Grace\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Put lots of cognitive effort into very clear and good and compelling case for AI risk  \n  * Automate talking to people about AI risk compellingly, possibly using above case as input  \n  * Good game theoretic thinking re arms races, coordination, what is possible. I.e. People are making dumb assumptions about things, e.g. \"can't slow down tech\". Could automate noting errors. So e.g. when someone says that on Twitter, can highlight it and send in response. \\-\\> good thinking via better weapons for debate (similar to Nathan project)  \n  * Specific idea: very quantified and competitive debate site, that can be cited re which views are winning, where AI does a lot of the cognitive labor in places no people are paying attention, but as a part of it gets cruxy, people pay attention and vote/bet  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Poor decision making around AI: e.g. 'racing' because of vague heuristics instead of ever doing calc on costs and benefits given specific risks  \n  * Failure to coordinate on AI: building AI systems that are dangerous, due to expecting others to do so\n\n## Joel Lehman\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Raising the bottom (i.e. helping improve societal epistemics) – in particular that in theory political decisions (at least political decision-makers) are somewhat downstream of societal epistemics  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Having weird political leaders and awful public epistemics at critical junctures seems to unnecessarily spike catastrophic/existential risk probabilities. In theory, helping uplevel or at least maintain current public epistemic health (especially as it relates to civic sensemaking) might help influence the quality of decision-makers and concrete decisions made at high-stakes moments  \n  * The shape of the tech is not clear, or probably requires multiple flavors of interventions. E.g. if LLM assistants by default help cultivate civic virtues and on the margin support critical thinking or whatever underlying skills and knowledge help a person up-level their epistemic habits / health (or at least if LLM assistants don't contribute to atrophy/harm); or somehow convincing social media networks to care about this (e.g. community notes but things that go beyond); similarly somehow help w/ important social goods like 'societal trust' and 'trust in your neighbors'"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Strong Emphasis on the Importance of Shared Understanding",
          "description": "Several contributors highlight the importance of shared understanding and common knowledge, which aligns with the core goals of epistemics.",
          "importance": 65,
          "grade": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 28517,
            "endOffset": 28889,
            "quotedText": "To make these decisions in a way that is inclusive and democratic, that fosters buy-in and trust, we must begin with a shared sense of how the world is. This shared sense must be grounded in the physical, economic, and social reality we live in.\n\nAs we live together over wider and wider contexts, maintaining a shared sense of the world becomes more difficult and subtle."
          }
        },
        {
          "title": "Occasional Use of Unexplained Information Management Concepts",
          "description": "Terms like 'information management', 'information chunking', and 'curation' are used without explanation. Briefly defining these concepts would improve accessibility.",
          "importance": 45,
          "grade": 50,
          "isValid": false,
          "highlight": {
            "startOffset": 3781,
            "endOffset": 5193,
            "quotedText": "Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place.  \n  * Vague sense that better **societal epistemics** would help with increasing the **sanity of conversations about AI safety, polarisation around the issue etc.**  \n    * This is broadly about **getting intent alignment** in the first place.\n\n→ These are all more about raising the **bottom of the epistemic landscape**\n\n## Deger Turan:\n\n* Social trust erosion, collective coordination mechanisms are built on a weak assumption  \n* Capacity to entertain truthfulness when it is not instrumental to one's incentive gradient & differing norms upon 'truth will make my sense of security weaker'   \n* Creating contexts where world model divergence can be agreed by both parties enabling a safe disagreement  \n* Improving model reasoning / safer AI by reducing slop, by creating accountability mechanisms through consistency checks & other specifications for healthy epistemics  \n* Decoupling virality with magnitude while living in a short attention span world, curation"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Effective Use of Layered Solution Statements",
          "description": "Some contributors describe solutions at multiple levels (e.g., individual, organizational, societal), which provides a comprehensive view of potential interventions.",
          "importance": 60,
          "grade": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 9837,
            "endOffset": 10271,
            "quotedText": "AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning  \n  * The technology helps organizations maintain coherence and make sound judgments even as the pace of change accelerates"
          }
        },
        {
          "title": "Occasional Use of Unexplained Group Dynamics Concepts",
          "description": "Terms like 'group epistemics', 'collective epistemics', and 'group decisionmaking' are used without explanation. Briefly defining these concepts would improve accessibility.",
          "importance": 45,
          "grade": 50,
          "isValid": false,
          "highlight": {
            "startOffset": 27260,
            "endOffset": 22584,
            "quotedText": "\n\nWhat decisions may go badly by default, but could go better if we have the right tech?\n\n* Lots of stuff in public policy: downstream both of better experts (technocrats), better public (voters/interest groups), and better coordination (collective epistemics)\n\nWhat do the later stages of the path to impact look like?\n\n* I want us to live in a world where everyone is more rational, empathetic, considered in their decision making, conscientious about impact, and interested in coordinating. I feel like that's one of the biggest blockers to all kinds of *long term* progress.\n\nWhat tools do we actually maybe want to build?\n\n* Easily accessible fact checkers / claim refuters  \n  * Powerful AI debaters  \n* Forecasting with explanations: superhuman and at scale  \n* Scientific meta-analysis, systematic review  \n* Ad-hoc / bespoke probabilistic models for individual problems  \n* Bespoke *simulators* / simulation environments  \n* Consistency checking and inconsistency identification  \n  * Arbitrage / market makers in prediction markets?  \n* Using AI to accelerate development of non-AI epistemic tools:  \n  * ?\n\n## Lawrence\n\nThere are already key questions/strategic problems where the truth is murky and we seem to proceed on the basis of limited and biased views. There might well be many more of these in the future. A central example is US-China competition over AI. Some have vibes of \"China is so bad that it's worth risking an existential catastrophe to stop it from capturing a portion of the lightcone\". Others have vibes of \"Cooperation is feasible and desirable, and is our only path to a good outcome\". But how bad is China, really? How much \"realised K\" on this question do those who are making the relevant decisions actually have? Similarly, how feasible is cooperation? How valid is this frame of \"capturing the lightcone\"? One reason that we don't have a better hold on these questions is that they are frighteningly complicated and difficult  – I think we could benefit massively from throwing much more cognitive labour at them, and presenting the findings in a way that is hard to disagree with or for any individual to improve upon, other than at a normative/values level. Maybe AI can get us there.\n\n## Lukas Finnveden\n\nwhy is AI for epistemics important/helpful?\n\n* better epistemics generally very good for making all kinds of decisions  \n* I'm specifically concerned about AI potentially causing an intelligence explosion, and us needing better epistemics to handle that. Specifically:  \n  * I'm worried about AI-driven tech R\\&D leading to catastrophes. To help address this, I want AI capabilities to not only be applied to tech R\\&D, but also to be applied to \"reasoning well from limited evidence\" about a number of questions, including how to deal with risks.  \n  * I'm worried about misaligned AI — so I want techniques for eliciting AI capabilities on all kinds of questions, so that AIs don't have better capabilities than we can elicit from them. (Basically: I want good alignment techniques even on hard-to-evaluate questions.)  \n  * Seems great to me if AI-driven improvements in general knowledge and decision-making are more broadly distributed among humans. (I think this leads to better decision-making without significantly accelerating the dangers from new tech or misaligned AI. Also reduces concentration of power.)\n\n## Abram Demski\n\n\\- Why are we doing this? \\-- impact theory brainstorming exercize  \n\t\\- There's the general claim that epistemics has degraded horribly due to social media and the internet, at least in a lot of ways, although epistemics has also gotten better in some specific ways. It seems very plausible that AI is, and will continue to, make this much much worse (due to deepfakes and super-persuasive writing, to give two examples). We wish to reverse this trend, or at least slow the acceleration.  \n\t\\- The high-level reason why epistemics is important is because epistemics is how we make good decisions. So effectively, this is AI for decision-making. However, if we just set out to make tools for decision-making, I have a sense that this would be worse.  \n\t\t\\- One super important aspect of decision-making is that you have to orient towards the right problem. People do much better when they understand the problem before proposing solutions.  \n\t\t\t\\- I have a sense that producing a better shared understanding of the world JUST WOULD result in better things. The connection between epistemics and better decisionmaking is that strong.  \n\t\t\t\t\\- However, there's an element of coordination to this. You have to have common knowledge, not just knowledge. AI for coordination is yet another thing. "
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Strong Emphasis on the Importance of Better Forecasting",
          "description": "Several contributors highlight the importance of better forecasting and prediction, which aligns with the core goals of epistemics.",
          "importance": 65,
          "grade": 85,
          "isValid": false,
          "highlight": {
            "startOffset": 21276,
            "endOffset": 1181,
            "quotedText": "\n\n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nReasonable judgment on tricky questions that's too cheap to meter, better than if you asked a respected colleague to spend many minutes on it.\n\n## Aviv\n\n* Better government governance  \n* Better democratic functioning  \n* Reduced dangerous concentration of power  \n    \n* Healthy polarization / pluralism  \n    \n* More peace, less likelihood of war  \n* Better geopolitics  \n* Better economic incentives  \n    \n* Better AI org governance  \n* Better AI coordination (re. distribution, escalation, etc.)  \n* Better compute allocation (toward what matters for good outcomes)  \n    \n* Better allocation of resources overall  \n* Better lives for people  \n* More effective organizations, getting things done with fewer resources, more focused on the things that benefit everyone  \n    \n* Identification of common ground directions / equilibria / game theoretic strategies that create/incentivize new positive gradients (and not surfacing the bad versions of those…)  \n    \n* Understanding our own goals and values\n\n*Note how it is the intersection of better epistemics / world models + the right power structures / incentives needed for all of these.*\n\n## Ben Goldhaber:\n\n* Forecasting how I should live my life in a period of intense technological change  \n* Guiding decision makers to avoid war with china/ai/everyone  \n* Providing the background context for me to sensemake way more sanely.  \n* Pushing forward the extant knowledge of AI safety agendas  \n* Planning for multi-stage projects that could achieve big impact  \n* Roadmapping to build larger more complex things\n\n## Chi\n\nThere are a few different things I care about:\n\n* I care a lot about humanity and AI, including **super-late-stage AI, getting decision theory** right.  \n  * Consequently but also separately, this relates to the general question of what to do about areas where there is not necessarily a ground truth but that are nonetheless very important to get right, cf**. [metaphilosophy](https://www.alignmentforum.org/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy)** (but also a bunch of other things.)  \n  * This also feels very similar to making **AIs helpful for AI safety research, cf. [automating alignment research](https://joecarlsmith.com/2025/03/14/ai-for-ai-safety#2-what-is-ai-for-ai-safety).**\n\n  → This is more about raising the **top of the epistemic landscape**\n\n* Other things I care about:  \n  * Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place.  \n  * Vague sense that better **societal epistemics** would help with increasing the **sanity of conversations about AI safety, polarisation around the issue etc.**  \n    * This is broadly about **getting intent alignment** in the first place.\n\n→ These are all more about raising the **bottom of the epistemic landscape**\n\n## Deger Turan:\n\n* Social trust erosion, collective coordination mechanisms are built on a weak assumption  \n* Capacity to entertain truthfulness when it is not instrumental to one's incentive gradient & differing norms upon 'truth will make my sense of security weaker'   \n* Creating contexts where world model divergence can be agreed by both parties enabling a safe disagreement  \n* Improving model reasoning / safer AI by reducing slop, by creating accountability mechanisms through consistency checks & other specifications for healthy epistemics  \n* Decoupling virality with magnitude while living in a short attention span world, curation   \n* I have some confusion re: raising the floor when not all parties are necessarily operating in good faith. I think I need some help think through the ways in which epistemics in the context of incentive misalignment can be used or have defensibility. \n\n## Nathan Young: \n\nWhat would we have got:\n\n* Biden not running, perhaps an open dem primary  \n* No FTX crisis  \n* Less zero sum OpenAI board drama  \n* Many smaller, better choices \n\nWhat decisions might go badly:\n\n* Trump tariffs/ global trade  \n* Taiwan/ US-China conflict  \n* US-China competition  \n* US authoritarianism  \n* AI regulation  \n* EU-China realignment  \n* Automated robots  \n* OpenAI strategy  \n* Anthropic strategy  \n* Google strategy  \n* xAI strategy  \n* AI workforce automation  \n* China strategy  \n* Russia strategy  \n* Moskovitz strategy  \n* Must strategy  \n* Bezos Strategy  \n* Zuckerberg strategy  \n* Tallinn strategy  \n* Altman strategy  \n* Drone warfare  \n* Climate change destabilisation  \n* Democratic trust\n\n## Lincoln Quirk:\n\nWhy do we care about (AI for) epistemics?  \n\\- making the public better informed:  \n\t\\- better able to deal with and process the wealth of information  \n\t\\- better able to express their own interests in government (via voting)  \n\t\\- helping cut through social media distraction / intentional misinformation  \n\\- getting governments to implement better policy  \n\t\\- coming up with good policy ideas  \n\t\\- objectively doing cost-benefit analysis  \n\t\\- objectively explaining tradeoffs (vs having to rely on biased humans to explain them)  \n\t\\- flagging bad ideas as bad  \n\t\\- convincing policymakers with compelling writeups  \n\\- success and flourishing for individuals and orgs using these tools  \n\t\\- via helping them think through decisions  \n\t\\- via clearheaded analysis of trade-offs and cost/benefits  \n\t\\- flagging bad ideas as bad\n\n## Jay\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * How can we build something like the positive version of Community Notes: which statements/posts are agreed on by people who often disagree?  \n  * How can we better understand/summarize what others think, e.g. on social media platforms? E.g. if you see a post recommended to you, is this because it's the most extreme take that went viral and isn't representative, or is this a commonly held view? Between different viewpoints, what are the concrete points of actual disagreement (on facts or values) and what are the things widely agreed upon? Let's build viewpoint/debate maps or a similar sensemaking tool that gets adopted/understood by the masses/raises the knowledge floor (in my experience, things like pol.is are hard for regular users to grok)  \n  * How can we improve Community Notes (or more general knowledge) creation with AI assistants? An AI agent proposing community notes and humans still checking/rating them?  \n  * How can we have something like Community Notes that applies to claims or statements about the future, or which are not concretely knowable?  \n  * \n\n## Ben Wilson (Metaculus)\n\nOverall I believe the most important value of epistemics (and AI epistemics) in general is informing what actions allow us to achieve our goals (i.e. make the world better). If we had perfect AI epistemics I would be excited by the ability for us to take a goal, and get a list of possible actions we could take and the forecasted outcomes of each of those actions (plus background information and forecasts for the 'current lay of the land' for context). This can then inform, not just the risk of AI doing good/bad things (as with current forecasting), but what we can do to raise the chances of 'great' AI outcomes, and lower the changes of 'worse' AI outcomes\n\n## Andreas\n\n* **What are use cases for AI for Epistemics that you most care about?**  \n  * AI policy, broadly construed, must go better  \n    * Avoid gradual disempowerment  \n    * Thoughtful regulation  \n  * Organizational planning & decision-making is weak  \n    * Given an informal long-term goal like improve decision-making for strategic users in government & large orgs, make a plan for a 20-people org that helps them accomplish the goal  \n    * Mapping dependencies, uncertainties, and potential futures  \n    * Understand complex domains with non-trivial feedback loops  \n      * Economic shifts  \n* **What decisions may go badly by default, but could go better if we have the right tech?**  \n  * Resource allocation within orgs  \n    * Investment  \n    * Grant making  \n    * Government budgets  \n  * Policy  \n    * How should AI be deployed in the world?  \n* **What do the later stages of the path to impact look like?**  \n  * AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning  \n  * The technology helps organizations maintain coherence and make sound judgments even as the pace of change accelerates  \n  * Assumption: People are generally well-intentioned \\- no one wants catastrophe \\- and as a result of better tooling things go better in the world overall\n\n## Owen optimistic brainstorm:\n\nWorld peace without world government — sufficiently good tech to help countries understand each others' interests, find mutually desirable bargains, and make sufficient credible commitments, could help us get up towards the Pareto frontier where we avoid (virtually) all costly conflicts\n\nGoing slowly and carefully on AI development — rather than having AI progress accelerate at increasingly breakneck speed, helping actors to understand the basic situation and the desirability of different options could be crucial for facilitating meaningful and robust agreements which restrict AI development to the stuff which we have good reasons for being confident are safe // otherwise beneficial.\n\nBettering democracy — helping people to access key information about what decisions their country might make would actually mean for longer term implications, and having this in turn feed into incentives for decision-makers that better align them with the common good\n\nBuilding safe / aligned AI — better tools for anticipating when systems are actually robustly safe, vs when stronger cases for expectation of safety seem required\n\n## Josh Rosenberg (FRI)\n\nI'm massively confused about important problems, e.g. how to orient to AI risk and what to do about it. Some reasonable people think there's a high probability we effectively lose everything we care about in 2027\\. Some reasonable people think it'll be decades till we have \"AGI\" and it'd be good if it arrived sooner. Which actions are good to take in each of these worlds is very different, and I'd like to figure out what world we're in.\n\nThis also applies to many other problem areas.\n\nE.g., Trump's effect on US democracy. How much of a crisis is it? What are the most effective actions to take in response to ensure that the country and communities we care about are as well off as they can be? How can we preserve high-quality institutions to lead the world in positive directions?\n\nOr, on the positive vision side: the \"Abundance\" agenda seems promising. Is it actually a good idea? What do we do about it?\n\nAI for Epistemics could help us get a lot more clarity on how to think about these problems and what actions to take. I'd like much better information and forecasts on the likely effects of Trump, AI timelines, and the pro's and con's of different Abundance policies, for example. If I had a bunch of time and good resources to dig into these problems, I could get a much better sense for myself. I'm hoping AI will help all of us do this sort of thing collectively.\n\n## Katja Grace\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Put lots of cognitive effort into very clear and good and compelling case for AI risk  \n  * Automate talking to people about AI risk compellingly, possibly using above case as input  \n  * Good game theoretic thinking re arms races, coordination, what is possible. I.e. People are making dumb assumptions about things, e.g. \"can't slow down tech\". Could automate noting errors. So e.g. when someone says that on Twitter, can highlight it and send in response. \\-\\> good thinking via better weapons for debate (similar to Nathan project)  \n  * Specific idea: very quantified and competitive debate site, that can be cited re which views are winning, where AI does a lot of the cognitive labor in places no people are paying attention, but as a part of it gets cruxy, people pay attention and vote/bet  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Poor decision making around AI: e.g. 'racing' because of vague heuristics instead of ever doing calc on costs and benefits given specific risks  \n  * Failure to coordinate on AI: building AI systems that are dangerous, due to expecting others to do so\n\n## Joel Lehman\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Raising the bottom (i.e. helping improve societal epistemics) – in particular that in theory political decisions (at least political decision-makers) are somewhat downstream of societal epistemics  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Having weird political leaders and awful public epistemics at critical junctures seems to unnecessarily spike catastrophic/existential risk probabilities. In theory, helping uplevel or at least maintain current public epistemic health (especially as it relates to civic sensemaking) might help influence the quality of decision-makers and concrete decisions made at high-stakes moments  \n  * The shape of the tech is not clear, or probably requires multiple flavors of interventions. E.g. if LLM assistants by default help cultivate civic virtues and on the margin support critical thinking or whatever underlying skills and knowledge help a person up-level their epistemic habits / health (or at least if LLM assistants don't contribute to atrophy/harm); or somehow convincing social media networks to care about this (e.g. community notes but things that go beyond); similarly somehow help w/ important social goods like 'societal trust' and 'trust in your neighbors'  \n* What do the later stages of the path to impact look like?  \n  * \\[didn't get to this\\]  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * Healthier democratic society, better agreement on consensus reality, higher trust in society, better politicians, better concrete decisions in high-stakes moments \n\n## Austin Chen\n\n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * **Evaluator**: More agreement (or understanding of disagreement) of what orgs & people are doing good work (eg for making grants in AI safety)  \n  * **Oracle**: Instant trustworthy forecasts on any question you ask  \n  * **Planner**: Guidance on \"what are the 3 most important things I should do this day/week/month\"  \n  * **Representative**: Simulate the responses of other parties (people, orgs/corps, governments, LLM agents), for discussions, contracts, trade\n\n## Janna\n\n* Why do we care about (AI for) epistemics?  \n  * Noise to signal ratio could be much higher  \n  * AI (mis)information churned out at the price of almost zero  \n* What are use cases for AI for Epistemics that you most care about?  \n  * Solving prediction markets--is there a way to scale up the bots, even if they are wrong and lossy, to improve prediction market accuracy?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * US-China relations  \n  * Trump tariffs maybe  \n  * Immigration rules in the US  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * More accurate predictions to…  \n    * Reduce business uncertainty  \n    * Make it easier to evaluate life decisions  \n  * Forestalling bad decisions made on imperfect information\n\n## Owain\n\n1. Maybe having humans understanding the world better is good in itself. Lots of people read popular science for this reason. Maybe it's good for AIs as well  \n2. Very broadly, it seems that better world understanding and decision making enables people to make their lives better in myriad ways. (To substantiate this, one could try to separate out the effects of better world understanding from the effects of better realized technology. E.g. Looking at capital-poor countries that can still benefit from scientific knowledge, economics, etc.)  \n3. More specifically, if AI progress is fast, there will be lots of important decisions to be made in quick calendar time. Exploiting AI to improve those decisions seems valuable. Current decision making institutions do not seem well set up to adapt really quickly. (A specific concern is dangers from AI takeover).   \n4. Why now? Why this group of people? We haven't really had tech that could do most epistemic tasks until recently. Now we have AI that can do an increasing proportion of the sub-tasks of epistemic processes. This will allow us to delegate epistemic tasks to AI.  \n   1. It's likely that people do this delegation, even if it's somewhat misguided and potentially bad in the long-term. Can we make this go well?  \n   2. This potentially massively expands our epistemic capacities. How can we use this well?  \n   3. Epistemic tech/institutions/advances can be hard to fund via for-profit companies. This is in part due to public goods issues, and issues around markets for ideas.\n\n## Rafe\n\nAs ever, I hope this will stop advanced artificial intelligence destroying the world, killing all its people \\&c.\n\nSo I think there are a few paths for impact here. I think there is a world where we can make the most productive thinkers about how to do things go further and better; I'm not so excited about a single-player version of this, as I think there's a pretty narrow window when top-level centaur mode works out\\[1\\]\n\nEveryone is already thinking about AI, and, I suspect, this will only get more acute. I think it's pretty hard for people to confront this and think about it carefully and well. It's scary\\! And there are a bunch of entrenched interests and memeplexes that preserve their status better (myopically) if people only have shallow slogans to think about AI with.\n\nGiven that, I suspect that the global sensemaking machine isn't up for the task of thinking about AI well. I hope we can help the global sensemaking machine upgrade itself. I have some hope that we as a society can come up with good ideas for dealing with AI, and also come to see how they make sense, so we can choose them in a robust way.\n\nOne other path to impact: I think a society that understands things better, including itself and its impacts, is one that is more robust in a bunch of ways. A more epistemically-grounded society is also probably a bit richer, and I think a bigger pie creates more slack for arranging things in sanity-promoting ways. I have more hope in a robust society navigating this stuff well.\n\n\\[1\\]:  Yeah, maybe you can do something with shaving off subskills, but I don't think that's really that doable.\n\n## Dave\n\n* With my consequentialist hat, better epistemics being worth it has to route through improved decisions somewhere down the line. The decisions I particularly care about are those by powerful people wrt AI development and implementation and the effects this has on how the future goes  \n  * But I do feel a bit icky when I have this hat on. There seems to be something very intuitively appealing about (a) pursuing better epistemics for its own sake and (b) democratising epistemic tools for the use and benefit of everyone. I think this basically caches out in more 'raising the floor' type work  \n  * I don't feel like I have a very good way of resolving this tension yet, but am excited to see things pushing on both frontiers  \n* "
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Occasional Use of Unexplained Decision-Making Processes",
          "description": "Terms like 'decision-making', 'decision theory', and 'decision processes' are used without explanation. Briefly defining these concepts would improve accessibility.",
          "importance": 45,
          "grade": 50,
          "isValid": false,
          "highlight": {
            "startOffset": 26463,
            "endOffset": 35664,
            "quotedText": "The high-level reason why epistemics is important is because epistemics is how we make good decisions. So effectively, this is AI for decision-making. However, if we just set out to make tools for decision-making, I have a sense that this would be worse.  \n\t\t\\- One super important aspect of decision-making is that you have to orient towards the right problem. People do much better when they understand the problem before proposing solutions.  \n\t\t\t\\- I have a sense that producing a better shared understanding of the world JUST WOULD result in better things. The connection between epistemics and better decisionmaking is that strong.  \n\t\t\t\t\\- However, there's an element of coordination to this. You have to have common knowledge, not just knowledge. AI for coordination is yet another thing. AI for group epistemics is close to that, though, and again, I have a sense that making \"AI for coordination\" would produce worse effects on the world than \"AI for group epistemics\".  \n\t\t\\- What specific decisions are we hoping to influence?  \n\t\t\t\\- Decisions around existential risk seem super super important. If AI were generally better at epistemics (as a relative, comparative thing \\-- ie we differentially accelerate epistemics) then key decisionmakers (in the government and at AI companies) would see much more clearly what's coming. Everyone wants a better future, so a better future would have a better chance of happening if we could see and navigate.\n\nI don't think I got any further than the supporting documentation for this workshop in articulating reasons and stuff, so I predict that the LLM summary won't be better than what we already know, although I hope it is. \n\n## Brendan\n\nFor us to live together in a caring and just way, we must make decisions about what is important to us together. Is it okay to tell my neighbour to turn their music down? Do we spend money on roads or trains or libraries? Do we regulate AI development and, if so, how? Do we want to spend public money on PEPFAR, or other avenues for reducing global poverty? To make these decisions in a way that is inclusive and democratic, that fosters buy-in and trust, we must begin with a shared sense of how the world is. This shared sense must be grounded in the physical, economic, and social reality we live in.\n\nAs we live together over wider and wider contexts, maintaining a shared sense of the world becomes more difficult and subtle. These wider contexts are facilitated by new technologies, both physical (eg. global manufacturing bases and supply chains) and informational (eg. social media). New technologies, such as AI for epistemics, and help maintain epistemic coherence across these contexts.\n\nMoreover, such technologies allow new avenues for cooperation at larger scales of human involvement. This unlocks new potential for coherence in global politics, scientific discovery, social and technological innovation, and cultural expression, and ultimately the deep joy of being alive and human in the world. \n\n## Isabel: \n\nThe kinds of being right in the past that are most relevant for decision making aren't just about making straightforward predictions about what will happen, but are more nuanced about what kinds of actions one should take to cause those things to happen, and these seem like somewhat different skills. These are harder to evaluate (e.g. because they're counterfactual: either we listened to person a and then we have to evaluate whether things went better or worse than if we didn't, but hard to say what the counterfactual would have been; or we didn't listen to person a and then we have to guess what things would have been like if we did) \n\nA bit adjacent to Neel Nanda's [recent piece](https://www.lesswrong.com/posts/P5zWiPF5cPJZSkiAK/good-research-takes-are-not-sufficient-for-good-strategic) on how \"good research takes aren't sufficient for good strategic takes\" \\- \"good forecasting takes aren't sufficient for good strategic takes\", but maybe some of this is maybe resolvable on the forecasting tech side \\- can we get better at creating and evaluating results from fuzzier questions? Can AI help? (Also can AI help with the whole problem, but this feels small and concrete enough to maybe get some traction on.) Also, some risk of actors building up good track records in order to be listened to, but then choosing to burn that track record to get power or things like that, something something trust \\- a concern for both human actors and e.g. scheming AIs \\- what things look good until they abruptly aren't? \n\nOn a different note, also interested in takes on what to do about and how to engage with actors who have epistemic processes that don't seem to be truth seeking in a comprehensible-to-me way. \n\n## Adam Marblestone:\n\nWhat do we get out of AI for epistemics?\n\nTrust, is one thing. Our current society seems to be fraught with distrust and motivated reasoning. Can we get a more objectively trustable source of truth? Not only accuracy but with a provenance that gives it an ability to be trusted by humans who are otherwise adversarial. Trust is needed to coordinate. Truthfulness is maybe somewhat necessary but seemingly not sufficient for a source being trusted. As a practical matter, on hot political topics I've been going to OpenAI Deep Research.\n\nBut at a certain level I wonder if AI for epistemics and some form of beneficial AGI could be basically the same thing. I asked Eric Drexler what he wanted for d/acc and he said something that sounds a bit like that…\n\n\"  \nHere's my unsurprising take on directed research for \"defensive acceleration\":  \nDevelopment and steering of epistemic tooling, aka [Large Knowledge Models](https://aiprospects.substack.com/p/large-knowledge-models), offers the highest leverage.\n\nPotential actions:  \n  • Identify and influence entities that could develop and scale necessary foundations.  \n  • Leverage the growing ease of development to take pre-scaling steps.  \n  • Within the scope of \"foundations\", accelerate productive directions for development.  \n  • To support science, ensure grounding of content in cited sources.  \n  • Pursue or promote assembly of appropriate knowledge inputs (beyond what is now used for training data).  \n  • Identify, promote, or implement epistemic tools supported by LKM resources.  \n  • Other kinds of exploration, promotion, preparation, and implementation.  \n  • Ensure that my advice is thoroughly understood.\n\nLKMs can accelerate pretty much everything that one might want, and perhaps tip the balance of applications toward *defensive* acceleration by promoting understanding of the *what and why* of defensive applications.\n\nAddendum:\n\nIn the email I sent a few minutes ago, the basic proposal becomes stronger if one reads \"LKM\" as something like \"large knowledge model functionality\", regardless of implementation.\n\nThe knowledge-model concept (whatever one prefers to call it) should not be tied to my specific technical suggestions. What I've said can be framed as something like \"a discussion of the value, feasibility, and potential scope of a rich epistemic resource\". Bundles of embeddings, etc., are at a level of specificity that may be distracting in the broader context of the meeting.  \n\"\n\nSo that's a strategy assistant AI – which helps humans at large see ways they could act and coordinate especially around big technological or geopolitical strategy issues I guess – but also a certain embodiment of an AGI oracle type system. Maybe the idea then is, how to make a AGI that helps with navigating civilizational strategy, and what is the right form of or interface to that to make it epistemically most useful. \n\nI'd like to also see democratic mechanisms that are simply maximally utilizing the tech we have. \n\n## Elizabeth\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * Allowing people to be better informed about experimental medical stuff for themselves.   \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * Bargaining? Even just salary negotiations feels like the results are often more the consequence of \"human bargaining skill\" and not the pareto optimal result.    \n  \n\n\n## Saul\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * making better decisions  \n  * in service of that: more useful forecasts, particularly understanding better which things are most relevant to forecast  \n    * in service of that: better group decisionmaking (e.g. unblocking things from vetopower type  \n    * in service of that: more aptly/calibratedly handling waves of information  \n    * In service of that: societal- and individual-scale asymmetric truth-tracking  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * which groups of people have political power  \n  * who's building powerful new tech  \n  * who's regulating powerful new tech  \n  * (etc, other \"decisions\" along these lines — some might not end up getting shaped like decisions, though)  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * better decisions across the board"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Effective Use of Layered Impact Statements",
          "description": "Some contributors describe impacts at multiple levels (e.g., individual, organizational, societal), which provides a comprehensive view of potential outcomes.",
          "importance": 60,
          "grade": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 6833,
            "endOffset": 6772,
            "quotedText": "  \n\t\\- convincing policymakers with compelling writeups  \n\\- "
          }
        },
        {
          "title": "Occasional Use of Unexplained Information Quality Concepts",
          "description": "Terms like 'noise to signal ratio', 'misinformation', and 'information churn' are used without explanation. Briefly defining these concepts would improve accessibility.",
          "importance": 45,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 16530,
            "endOffset": 16638,
            "quotedText": "Noise to signal ratio could be much higher  \n  * AI (mis)information churned out at the price of almost zero"
          }
        },
        {
          "title": "Strong Emphasis on the Importance of Better Information",
          "description": "Several contributors highlight the importance of better information and knowledge, which aligns with the core goals of epistemics.",
          "importance": 65,
          "grade": 85,
          "isValid": false,
          "highlight": {
            "startOffset": 829,
            "endOffset": 12812,
            "quotedText": "Going from \"Realized\" to \"Accessible\" knowledge, e.g. getting good information using the web. Agentic web agents like Deep Research.\n\n* What decisions may go badly by default, but could go better if we have the right tech?\n\nMany billions of small decisions.\n\n* What do the later stages of the path to impact look like?\n\nSuperhuman forecasting at scale.\n\n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nReasonable judgment on tricky questions that's too cheap to meter, better than if you asked a respected colleague to spend many minutes on it.\n\n## Aviv\n\n* Better government governance  \n* Better democratic functioning  \n* Reduced dangerous concentration of power  \n    \n* Healthy polarization / pluralism  \n    \n* More peace, less likelihood of war  \n* Better geopolitics  \n* Better economic incentives  \n    \n* Better AI org governance  \n* Better AI coordination (re. distribution, escalation, etc.)  \n* Better compute allocation (toward what matters for good outcomes)  \n    \n* Better allocation of resources overall  \n* Better lives for people  \n* More effective organizations, getting things done with fewer resources, more focused on the things that benefit everyone  \n    \n* Identification of common ground directions / equilibria / game theoretic strategies that create/incentivize new positive gradients (and not surfacing the bad versions of those…)  \n    \n* Understanding our own goals and values\n\n*Note how it is the intersection of better epistemics / world models + the right power structures / incentives needed for all of these.*\n\n## Ben Goldhaber:\n\n* Forecasting how I should live my life in a period of intense technological change  \n* Guiding decision makers to avoid war with china/ai/everyone  \n* Providing the background context for me to sensemake way more sanely.  \n* Pushing forward the extant knowledge of AI safety agendas  \n* Planning for multi-stage projects that could achieve big impact  \n* Roadmapping to build larger more complex things\n\n## Chi\n\nThere are a few different things I care about:\n\n* I care a lot about humanity and AI, including **super-late-stage AI, getting decision theory** right.  \n  * Consequently but also separately, this relates to the general question of what to do about areas where there is not necessarily a ground truth but that are nonetheless very important to get right, cf**. [metaphilosophy](https://www.alignmentforum.org/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy)** (but also a bunch of other things.)  \n  * This also feels very similar to making **AIs helpful for AI safety research, cf. [automating alignment research](https://joecarlsmith.com/2025/03/14/ai-for-ai-safety#2-what-is-ai-for-ai-safety).**\n\n  → This is more about raising the **top of the epistemic landscape**\n\n* Other things I care about:  \n  * Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place.  \n  * Vague sense that better **societal epistemics** would help with increasing the **sanity of conversations about AI safety, polarisation around the issue etc.**  \n    * This is broadly about **getting intent alignment** in the first place.\n\n→ These are all more about raising the **bottom of the epistemic landscape**\n\n## Deger Turan:\n\n* Social trust erosion, collective coordination mechanisms are built on a weak assumption  \n* Capacity to entertain truthfulness when it is not instrumental to one's incentive gradient & differing norms upon 'truth will make my sense of security weaker'   \n* Creating contexts where world model divergence can be agreed by both parties enabling a safe disagreement  \n* Improving model reasoning / safer AI by reducing slop, by creating accountability mechanisms through consistency checks & other specifications for healthy epistemics  \n* Decoupling virality with magnitude while living in a short attention span world, curation   \n* I have some confusion re: raising the floor when not all parties are necessarily operating in good faith. I think I need some help think through the ways in which epistemics in the context of incentive misalignment can be used or have defensibility. \n\n## Nathan Young: \n\nWhat would we have got:\n\n* Biden not running, perhaps an open dem primary  \n* No FTX crisis  \n* Less zero sum OpenAI board drama  \n* Many smaller, better choices \n\nWhat decisions might go badly:\n\n* Trump tariffs/ global trade  \n* Taiwan/ US-China conflict  \n* US-China competition  \n* US authoritarianism  \n* AI regulation  \n* EU-China realignment  \n* Automated robots  \n* OpenAI strategy  \n* Anthropic strategy  \n* Google strategy  \n* xAI strategy  \n* AI workforce automation  \n* China strategy  \n* Russia strategy  \n* Moskovitz strategy  \n* Must strategy  \n* Bezos Strategy  \n* Zuckerberg strategy  \n* Tallinn strategy  \n* Altman strategy  \n* Drone warfare  \n* Climate change destabilisation  \n* Democratic trust\n\n## Lincoln Quirk:\n\nWhy do we care about (AI for) epistemics?  \n\\- making the public better informed:  \n\t\\- better able to deal with and process the wealth of information  \n\t\\- better able to express their own interests in government (via voting)  \n\t\\- helping cut through social media distraction / intentional misinformation  \n\\- getting governments to implement better policy  \n\t\\- coming up with good policy ideas  \n\t\\- objectively doing cost-benefit analysis  \n\t\\- objectively explaining tradeoffs (vs having to rely on biased humans to explain them)  \n\t\\- flagging bad ideas as bad  \n\t\\- convincing policymakers with compelling writeups  \n\\- success and flourishing for individuals and orgs using these tools  \n\t\\- via helping them think through decisions  \n\t\\- via clearheaded analysis of trade-offs and cost/benefits  \n\t\\- flagging bad ideas as bad\n\n## Jay\n\n* What are use cases for AI for Epistemics that you most care about?  \n  * How can we build something like the positive version of Community Notes: which statements/posts are agreed on by people who often disagree?  \n  * How can we better understand/summarize what others think, e.g. on social media platforms? E.g. if you see a post recommended to you, is this because it's the most extreme take that went viral and isn't representative, or is this a commonly held view? Between different viewpoints, what are the concrete points of actual disagreement (on facts or values) and what are the things widely agreed upon? Let's build viewpoint/debate maps or a similar sensemaking tool that gets adopted/understood by the masses/raises the knowledge floor (in my experience, things like pol.is are hard for regular users to grok)  \n  * How can we improve Community Notes (or more general knowledge) creation with AI assistants? An AI agent proposing community notes and humans still checking/rating them?  \n  * How can we have something like Community Notes that applies to claims or statements about the future, or which are not concretely knowable?  \n  * \n\n## Ben Wilson (Metaculus)\n\nOverall I believe the most important value of epistemics (and AI epistemics) in general is informing what actions allow us to achieve our goals (i.e. make the world better). If we had perfect AI epistemics I would be excited by the ability for us to take a goal, and get a list of possible actions we could take and the forecasted outcomes of each of those actions (plus background information and forecasts for the 'current lay of the land' for context). This can then inform, not just the risk of AI doing good/bad things (as with current forecasting), but what we can do to raise the chances of 'great' AI outcomes, and lower the changes of 'worse' AI outcomes\n\n## Andreas\n\n* **What are use cases for AI for Epistemics that you most care about?**  \n  * AI policy, broadly construed, must go better  \n    * Avoid gradual disempowerment  \n    * Thoughtful regulation  \n  * Organizational planning & decision-making is weak  \n    * Given an informal long-term goal like improve decision-making for strategic users in government & large orgs, make a plan for a 20-people org that helps them accomplish the goal  \n    * Mapping dependencies, uncertainties, and potential futures  \n    * Understand complex domains with non-trivial feedback loops  \n      * Economic shifts  \n* **What decisions may go badly by default, but could go better if we have the right tech?**  \n  * Resource allocation within orgs  \n    * Investment  \n    * Grant making  \n    * Government budgets  \n  * Policy  \n    * How should AI be deployed in the world?  \n* **What do the later stages of the path to impact look like?**  \n  * AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning  \n  * The technology helps organizations maintain coherence and make sound judgments even as the pace of change accelerates  \n  * Assumption: People are generally well-intentioned \\- no one wants catastrophe \\- and as a result of better tooling things go better in the world overall\n\n## Owen optimistic brainstorm:\n\nWorld peace without world government — sufficiently good tech to help countries understand each others' interests, find mutually desirable bargains, and make sufficient credible commitments, could help us get up towards the Pareto frontier where we avoid (virtually) all costly conflicts\n\nGoing slowly and carefully on AI development — rather than having AI progress accelerate at increasingly breakneck speed, helping actors to understand the basic situation and the desirability of different options could be crucial for facilitating meaningful and robust agreements which restrict AI development to the stuff which we have good reasons for being confident are safe // otherwise beneficial.\n\nBettering democracy — helping people to access key information about what decisions their country might make would actually mean for longer term implications, and having this in turn feed into incentives for decision-makers that better align them with the common good\n\nBuilding safe / aligned AI — better tools for anticipating when systems are actually robustly safe, vs when stronger cases for expectation of safety seem required\n\n## Josh Rosenberg (FRI)\n\nI'm massively confused about important problems, e.g. how to orient to AI risk and what to do about it. Some reasonable people think there's a high probability we effectively lose everything we care about in 2027\\. Some reasonable people think it'll be decades till we have \"AGI\" and it'd be good if it arrived sooner. Which actions are good to take in each of these worlds is very different, and I'd like to figure out what world we're in.\n\nThis also applies to many other problem areas.\n\nE.g., Trump's effect on US democracy. How much of a crisis is it? What are the most effective actions to take in response to ensure that the country and communities we care about are as well off as they can be? How can we preserve high-quality institutions to lead the world in positive directions?\n\nOr, on the positive vision side: the \"Abundance\" agenda seems promising. Is it actually a good idea? What do we do about it?\n\nAI for Epistemics could help us get a lot more clarity on how to think about these problems and what actions to take. I'd like much better information and forecasts on the likely effects of Trump, AI timelines, and the pro's and con's of different Abundance policies, for example."
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Occasional Use of Unexplained Organizational Planning Concepts",
          "description": "Terms like 'organizational planning', 'mapping dependencies', and 'uncertainties' are used without explanation. Briefly defining these concepts would improve accessibility.",
          "importance": 45,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 9108,
            "endOffset": 9410,
            "quotedText": "Organizational planning & decision-making is weak  \n    * Given an informal long-term goal like improve decision-making for strategic users in government & large orgs, make a plan for a 20-people org that helps them accomplish the goal  \n    * Mapping dependencies, uncertainties, and potential futures"
          }
        },
        {
          "title": "Effective Use of Layered Solution Statements",
          "description": "Some contributors describe solutions at multiple levels (e.g., individual, organizational, societal), which provides a comprehensive view of potential interventions.",
          "importance": 60,
          "grade": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 9837,
            "endOffset": 10271,
            "quotedText": "AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning  \n  * The technology helps organizations maintain coherence and make sound judgments even as the pace of change accelerates"
          }
        },
        {
          "title": "Occasional Use of Unexplained Social Coordination Concepts",
          "description": "Terms like 'coordination', 'pluralism', and 'common ground' are used without explanation. Briefly defining these concepts would improve accessibility.",
          "importance": 45,
          "grade": 50,
          "isValid": false,
          "highlight": {
            "startOffset": 2027,
            "endOffset": 4929,
            "quotedText": "Identification of common ground directions / equilibria / game theoretic strategies that create/incentivize new positive gradients (and not surfacing the bad versions of those…)  \n    \n* Understanding our own goals and values\n\n*Note how it is the intersection of better epistemics / world models + the right power structures / incentives needed for all of these.*\n\n## Ben Goldhaber:\n\n* Forecasting how I should live my life in a period of intense technological change  \n* Guiding decision makers to avoid war with china/ai/everyone  \n* Providing the background context for me to sensemake way more sanely.  \n* Pushing forward the extant knowledge of AI safety agendas  \n* Planning for multi-stage projects that could achieve big impact  \n* Roadmapping to build larger more complex things\n\n## Chi\n\nThere are a few different things I care about:\n\n* I care a lot about humanity and AI, including **super-late-stage AI, getting decision theory** right.  \n  * Consequently but also separately, this relates to the general question of what to do about areas where there is not necessarily a ground truth but that are nonetheless very important to get right, cf**. [metaphilosophy](https://www.alignmentforum.org/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy)** (but also a bunch of other things.)  \n  * This also feels very similar to making **AIs helpful for AI safety research, cf. [automating alignment research](https://joecarlsmith.com/2025/03/14/ai-for-ai-safety#2-what-is-ai-for-ai-safety).**\n\n  → This is more about raising the **top of the epistemic landscape**\n\n* Other things I care about:  \n  * Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place.  \n  * Vague sense that better **societal epistemics** would help with increasing the **sanity of conversations about AI safety, polarisation around the issue etc.**  \n    * This is broadly about **getting intent alignment** in the first place.\n\n→ These are all more about raising the **bottom of the epistemic landscape**\n\n## Deger Turan:\n\n* Social trust erosion, collective coordination mechanisms are built on a weak assumption  \n* Capacity to entertain truthfulness when it is not instrumental to one's incentive gradient & differing norms upon 'truth will make my sense of security weaker'   \n* Creating contexts where world model divergence can be agreed by both parties enabling a safe disagreement"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Strong Emphasis on the Importance of Better Decision-Making",
          "description": "Several contributors highlight the importance of better decision-making, which aligns with the core goals of epistemics.",
          "importance": 65,
          "grade": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 34750,
            "endOffset": 35664,
            "quotedText": "making better decisions  \n  * in service of that: more useful forecasts, particularly understanding better which things are most relevant to forecast  \n    * in service of that: better group decisionmaking (e.g. unblocking things from vetopower type  \n    * in service of that: more aptly/calibratedly handling waves of information  \n    * In service of that: societal- and individual-scale asymmetric truth-tracking  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n  * which groups of people have political power  \n  * who's building powerful new tech  \n  * who's regulating powerful new tech  \n  * (etc, other \"decisions\" along these lines — some might not end up getting shaped like decisions, though)  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"  \n  * better decisions across the board"
          }
        },
        {
          "title": "Occasional Use of Unexplained Information Processing Concepts",
          "description": "Terms like 'information chunking', 'information scaffolding', and 'information curation' are used without explanation. Briefly defining these concepts would improve accessibility.",
          "importance": 45,
          "grade": 50,
          "isValid": false,
          "highlight": {
            "startOffset": -1,
            "endOffset": 5193,
            "quotedText": "# Writing Session\n\nPreamble/framing from Owen:\n\n* Negative: \"we just want to free-form vibe about what cool things we could do\"  \n* Positive: step back & ask \"wait, why are we doing this again?\"  \n* What stuff does this actually help?  \n* Epistemics isn't a goal in-an-of itself; figuring out what combination of things are possible, achievable, etc.  \n* Why do we care about (AI for) epistemics?\n\nWriting prompts from Owen:\n\n* What are use cases for AI for Epistemics that you most care about?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nTemplate:\n\n## Name: \n\nEntry\n\n## Dan Schwarz:\n\n* What are use cases for AI for Epistemics that you most care about?\n\nGoing from \"Realized\" to \"Accessible\" knowledge, e.g. getting good information using the web. Agentic web agents like Deep Research.\n\n* What decisions may go badly by default, but could go better if we have the right tech?\n\nMany billions of small decisions.\n\n* What do the later stages of the path to impact look like?\n\nSuperhuman forecasting at scale.\n\n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nReasonable judgment on tricky questions that's too cheap to meter, better than if you asked a respected colleague to spend many minutes on it.\n\n## Aviv\n\n* Better government governance  \n* Better democratic functioning  \n* Reduced dangerous concentration of power  \n    \n* Healthy polarization / pluralism  \n    \n* More peace, less likelihood of war  \n* Better geopolitics  \n* Better economic incentives  \n    \n* Better AI org governance  \n* Better AI coordination (re. distribution, escalation, etc.)  \n* Better compute allocation (toward what matters for good outcomes)  \n    \n* Better allocation of resources overall  \n* Better lives for people  \n* More effective organizations, getting things done with fewer resources, more focused on the things that benefit everyone  \n    \n* Identification of common ground directions / equilibria / game theoretic strategies that create/incentivize new positive gradients (and not surfacing the bad versions of those…)  \n    \n* Understanding our own goals and values\n\n*Note how it is the intersection of better epistemics / world models + the right power structures / incentives needed for all of these.*\n\n## Ben Goldhaber:\n\n* Forecasting how I should live my life in a period of intense technological change  \n* Guiding decision makers to avoid war with china/ai/everyone  \n* Providing the background context for me to sensemake way more sanely.  \n* Pushing forward the extant knowledge of AI safety agendas  \n* Planning for multi-stage projects that could achieve big impact  \n* Roadmapping to build larger more complex things\n\n## Chi\n\nThere are a few different things I care about:\n\n* I care a lot about humanity and AI, including **super-late-stage AI, getting decision theory** right.  \n  * Consequently but also separately, this relates to the general question of what to do about areas where there is not necessarily a ground truth but that are nonetheless very important to get right, cf**. [metaphilosophy](https://www.alignmentforum.org/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy)** (but also a bunch of other things.)  \n  * This also feels very similar to making **AIs helpful for AI safety research, cf. [automating alignment research](https://joecarlsmith.com/2025/03/14/ai-for-ai-safety#2-what-is-ai-for-ai-safety).**\n\n  → This is more about raising the **top of the epistemic landscape**\n\n* Other things I care about:  \n  * Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place.  \n  * Vague sense that better **societal epistemics** would help with increasing the **sanity of conversations about AI safety, polarisation around the issue etc.**  \n    * This is broadly about **getting intent alignment** in the first place.\n\n→ These are all more about raising the **bottom of the epistemic landscape**\n\n## Deger Turan:\n\n* Social trust erosion, collective coordination mechanisms are built on a weak assumption  \n* Capacity to entertain truthfulness when it is not instrumental to one's incentive gradient & differing norms upon 'truth will make my sense of security weaker'   \n* Creating contexts where world model divergence can be agreed by both parties enabling a safe disagreement  \n* Improving model reasoning / safer AI by reducing slop, by creating accountability mechanisms through consistency checks & other specifications for healthy epistemics  \n* Decoupling virality with magnitude while living in a short attention span world, curation"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Effective Use of Layered Problem Statements",
          "description": "Some contributors describe problems at multiple levels (e.g., individual, organizational, societal), which provides a comprehensive view of challenges.",
          "importance": 60,
          "grade": 85,
          "isValid": false,
          "highlight": {
            "startOffset": 23715,
            "endOffset": 24810,
            "quotedText": "There are already key questions/strategic problems where the truth is murky and we seem to proceed on the basis of limited and biased views. There might well be many more of these in the future. A central example is US-China competition over AI. Some have vibes of \"China is so bad that it's worth risking an existential catastrophe to stop it from capturing a portion of the lightcone\". Others have vibes of \"Cooperation is feasible and desirable, and is our only path to a good outcome\". But how bad is China, really? How much \"realised K\" on this question do those who are making the relevant decisions actually have? Similarly, how feasible is cooperation? How valid is this frame of \"capturing the lightcone\"? One reason that we don't have a better hold on these questions is that they are frighteningly complicated and difficult  – I think we could benefit massively from throwing much more cognitive labour at them, and presenting the findings in a way that is hard to disagree with or for any individual to improve upon, other than at a normative/values level. Maybe AI can get us there."
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Occasional Use of Unexplained Group Dynamics Concepts",
          "description": "Terms like 'group epistemics', 'collective epistemics', and 'group decisionmaking' are used without explanation. Briefly defining these concepts would improve accessibility.",
          "importance": 45,
          "grade": 50,
          "isValid": false,
          "highlight": {
            "startOffset": 27260,
            "endOffset": 22584,
            "quotedText": "\n\nWhat decisions may go badly by default, but could go better if we have the right tech?\n\n* Lots of stuff in public policy: downstream both of better experts (technocrats), better public (voters/interest groups), and better coordination (collective epistemics)\n\nWhat do the later stages of the path to impact look like?\n\n* I want us to live in a world where everyone is more rational, empathetic, considered in their decision making, conscientious about impact, and interested in coordinating. I feel like that's one of the biggest blockers to all kinds of *long term* progress.\n\nWhat tools do we actually maybe want to build?\n\n* Easily accessible fact checkers / claim refuters  \n  * Powerful AI debaters  \n* Forecasting with explanations: superhuman and at scale  \n* Scientific meta-analysis, systematic review  \n* Ad-hoc / bespoke probabilistic models for individual problems  \n* Bespoke *simulators* / simulation environments  \n* Consistency checking and inconsistency identification  \n  * Arbitrage / market makers in prediction markets?  \n* Using AI to accelerate development of non-AI epistemic tools:  \n  * ?\n\n## Lawrence\n\nThere are already key questions/strategic problems where the truth is murky and we seem to proceed on the basis of limited and biased views. There might well be many more of these in the future. A central example is US-China competition over AI. Some have vibes of \"China is so bad that it's worth risking an existential catastrophe to stop it from capturing a portion of the lightcone\". Others have vibes of \"Cooperation is feasible and desirable, and is our only path to a good outcome\". But how bad is China, really? How much \"realised K\" on this question do those who are making the relevant decisions actually have? Similarly, how feasible is cooperation? How valid is this frame of \"capturing the lightcone\"? One reason that we don't have a better hold on these questions is that they are frighteningly complicated and difficult  – I think we could benefit massively from throwing much more cognitive labour at them, and presenting the findings in a way that is hard to disagree with or for any individual to improve upon, other than at a normative/values level. Maybe AI can get us there.\n\n## Lukas Finnveden\n\nwhy is AI for epistemics important/helpful?\n\n* better epistemics generally very good for making all kinds of decisions  \n* I'm specifically concerned about AI potentially causing an intelligence explosion, and us needing better epistemics to handle that. Specifically:  \n  * I'm worried about AI-driven tech R\\&D leading to catastrophes. To help address this, I want AI capabilities to not only be applied to tech R\\&D, but also to be applied to \"reasoning well from limited evidence\" about a number of questions, including how to deal with risks.  \n  * I'm worried about misaligned AI — so I want techniques for eliciting AI capabilities on all kinds of questions, so that AIs don't have better capabilities than we can elicit from them. (Basically: I want good alignment techniques even on hard-to-evaluate questions.)  \n  * Seems great to me if AI-driven improvements in general knowledge and decision-making are more broadly distributed among humans. (I think this leads to better decision-making without significantly accelerating the dangers from new tech or misaligned AI. Also reduces concentration of power.)\n\n## Abram Demski\n\n\\- Why are we doing this? \\-- impact theory brainstorming exercize  \n\t\\- There's the general claim that epistemics has degraded horribly due to social media and the internet, at least in a lot of ways, although epistemics has also gotten better in some specific ways. It seems very plausible that AI is, and will continue to, make this much much worse (due to deepfakes and super-persuasive writing, to give two examples). We wish to reverse this trend, or at least slow the acceleration.  \n\t\\- The high-level reason why epistemics is important is because epistemics is how we make good decisions. So effectively, this is AI for decision-making. However, if we just set out to make tools for decision-making, I have a sense that this would be worse.  \n\t\t\\- One super important aspect of decision-making is that you have to orient towards the right problem. People do much better when they understand the problem before proposing solutions.  \n\t\t\t\\- I have a sense that producing a better shared understanding of the world JUST WOULD result in better things. The connection between epistemics and better decisionmaking is that strong.  \n\t\t\t\t\\- However, there's an element of coordination to this. You have to have common knowledge, not just knowledge. AI for coordination is yet another thing. "
          },
          "error": "Highlight is too long (max 1000 characters)"
        }
      ],
      "thinking": "## Thinking Process\n\n### 1. Overall Assessment\n- The document is a collaborative, brainstorming-style session capturing diverse perspectives on 'AI for Epistemics.'\n- It is structured as a series of prompts and responses from multiple contributors, each offering nuanced takes on use cases, risks, and visions for AI epistemic tools.\n- The format is informal, with bullet points, partial sentences, and conversational tone. There is little narrative flow or editorial synthesis.\n\n### 2. Key Themes and Patterns\n- **Raising the epistemic floor and ceiling:** Many contributors distinguish between improving baseline societal epistemics (the 'floor') and enabling superhuman or expert-level reasoning (the 'ceiling').\n- **Decision-making and governance:** Recurring focus on how better epistemics could improve high-stakes decisions in government, AI policy, geopolitics, and organizational strategy.\n- **Trust, coordination, and pluralism:** Several entries highlight the importance of trust, healthy disagreement, and mechanisms for coordination, especially in polarized or adversarial contexts.\n- **Forecasting and sensemaking:** Forecasting, scenario analysis, and collective sensemaking are repeatedly cited as core use cases.\n- **Risks and failure modes:** Concerns about misuse, incentive misalignment, and the potential for epistemic tools to be subverted or to exacerbate existing problems are present.\n- **Meta-reflection:** Some contributors reflect on the process itself, questioning the value of epistemics as an end versus a means, and the challenges of evaluating epistemic interventions.\n\n### 3. Expert Perspective\n- **Readability:** The document is accessible to a domain-literate audience but would be challenging for general readers due to jargon, implicit references, and lack of explanatory scaffolding. Sentence structure is often fragmented. Paragraphing is inconsistent.\n- **Structural Coherence:** The document lacks a unifying narrative or editorial voice. It is a collection of loosely connected responses, with minimal transitions or thematic grouping. However, the prompt-response format provides some organizational scaffolding.\n- **Precision and Concision:** Entries vary in specificity. Some are crisp and focused; others are diffuse or hedge heavily. Redundancy is present, as similar themes recur across contributors without synthesis.\n- **Audience Alignment:** The document assumes significant prior knowledge of AI, epistemics, and related discourse. It is well-suited for an expert or insider audience but would benefit from more context and explanation for broader accessibility.\n\n### 4. Notable Strengths\n- Rich diversity of perspectives and concrete examples.\n- Honest engagement with uncertainty and limitations.\n- Clear articulation of both positive visions and potential pitfalls.\n\n### 5. Areas for Improvement\n- Lack of synthesis or summary makes it hard to extract overarching insights.\n- Inconsistent formatting and incomplete sentences reduce readability.\n- Heavy reliance on jargon and implicit references limits accessibility.\n- Opportunities for chunking, thematic grouping, and editorial framing are largely unexploited.\n\n### 6. Recommendations\n- Immediate: Add a synthesized executive summary and thematic grouping of responses.\n- Structural: Standardize formatting, clarify prompts, and encourage full-sentence responses.\n- Content: Provide definitions for key terms and more context for non-expert readers.\n- Terminology: Reduce jargon or explain it inline.\n\n### 7. Meta-Analysis\n- The document is highly valuable as a raw input for further synthesis but is not yet a clear, accessible communication artifact. Its current form is best suited for internal use among experts or as a starting point for editorial refinement.",
      "summary": "This document offers a rich, multi-perspective exploration of 'AI for Epistemics,' capturing both visionary aspirations and practical concerns. While it excels in diversity and candor, its fragmented structure and reliance on insider language limit accessibility. Editorial synthesis, thematic grouping, and clearer scaffolding would significantly enhance clarity and impact for broader audiences.",
      "grade": 68
    },
    {
      "agentId": "research-scholar",
      "createdAt": "2025-04-18T21:16:22.795Z",
      "costInCents": 0,
      "comments": [
        {
          "title": "Framing the Purpose of Epistemics 🧭",
          "description": "The opening framing by Owen—contrasting 'free-form vibe' with 'why are we doing this again?'—sets a reflective tone for the document. It encourages participants to ground their exploration of AI for epistemics in concrete goals and impact, rather than abstract brainstorming. This is crucial for focusing the discussion on actionable outcomes.",
          "importance": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 50,
            "endOffset": 396,
            "quotedText": "Negative: \"we just want to free-form vibe about what cool things we could do\"  \n* Positive: step back & ask \"wait, why are we doing this again?\"  \n* What stuff does this actually help?  \n* Epistemics isn't a goal in-an-of itself; figuring out what combination of things are possible, achievable, etc.  \n* Why do we care about (AI for) epistemics?"
          }
        },
        {
          "title": "Epistemics as Instrumental, Not Intrinsic",
          "description": "Owen notes that 'Epistemics isn't a goal in-an-of itself,' emphasizing that epistemic improvement is valuable for its downstream effects—better decisions, coordination, and outcomes. This distinction is foundational for the rest of the document, as it frames epistemics as a means to an end.",
          "importance": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 239,
            "endOffset": 350,
            "quotedText": "Epistemics isn't a goal in-an-of itself; figuring out what combination of things are possible, achievable, etc."
          }
        },
        {
          "title": "Prompting for Use Cases",
          "description": "The writing prompts provided by Owen guide contributors to focus on practical applications, failure modes, and long-term impact. This structure helps ensure the discussion remains grounded and relevant.",
          "importance": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 398,
            "endOffset": 711,
            "quotedText": "Writing prompts from Owen:\n\n* What are use cases for AI for Epistemics that you most care about?  \n* What decisions may go badly by default, but could go better if we have the right tech?  \n* What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\""
          }
        },
        {
          "title": "From Realized to Accessible Knowledge",
          "description": "Dan Schwarz highlights the challenge of making 'realized' knowledge accessible, particularly via agentic web agents. This underscores the importance of not just generating knowledge, but also making it actionable and available to decision-makers.",
          "importance": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 829,
            "endOffset": 961,
            "quotedText": "Going from \"Realized\" to \"Accessible\" knowledge, e.g. getting good information using the web. Agentic web agents like Deep Research."
          }
        },
        {
          "title": "Superhuman Forecasting at Scale",
          "description": "Dan's vision of 'superhuman forecasting at scale' as a later-stage impact of AI for epistemics is ambitious and aligns with the broader goal of improving decision quality across society.",
          "importance": 65,
          "isValid": true,
          "highlight": {
            "startOffset": 590,
            "endOffset": 1181,
            "quotedText": "What do the later stages of the path to impact look like?  \n* \"As a result of better AI for epistemics tech, we get: […]\"\n\nTemplate:\n\n## Name: \n\nEntry\n\n## Dan Schwarz:\n\n* What are use cases for AI for Epistemics that you most care about?\n\nGoing from \"Realized\" to \"Accessible\" knowledge, e.g. getting good information using the web. Agentic web agents like Deep Research.\n\n* What decisions may go badly by default, but could go better if we have the right tech?\n\nMany billions of small decisions.\n\n* What do the later stages of the path to impact look like?\n\nSuperhuman forecasting at scale."
          }
        },
        {
          "title": "Epistemics for Democratic Functioning",
          "description": "Aviv lists 'better democratic functioning' and 'reduced dangerous concentration of power' as key outcomes. This connects epistemic improvement to societal resilience and the health of democratic institutions.",
          "importance": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 1401,
            "endOffset": 1508,
            "quotedText": "Better government governance  \n* Better democratic functioning  \n* Reduced dangerous concentration of power"
          }
        },
        {
          "title": "Healthy Polarization and Pluralism",
          "description": "Aviv's mention of 'healthy polarization / pluralism' is notable. It suggests that better epistemics doesn't mean uniformity of opinion, but rather the ability to sustain productive disagreement and diversity of thought.",
          "importance": 60,
          "isValid": true,
          "highlight": {
            "startOffset": 1518,
            "endOffset": 1594,
            "quotedText": "Healthy polarization / pluralism  \n    \n* More peace, less likelihood of war"
          }
        },
        {
          "title": "Epistemics and Geopolitics",
          "description": "Aviv connects improved epistemics to 'better geopolitics' and 'better AI org governance,' highlighting the global stakes of epistemic infrastructure. This is especially relevant in the context of AI race dynamics.",
          "importance": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 1599,
            "endOffset": 1682,
            "quotedText": "Better geopolitics  \n* Better economic incentives  \n    \n* Better AI org governance"
          }
        },
        {
          "title": "Common Ground and Game Theory",
          "description": "The identification of 'common ground directions / equilibria / game theoretic strategies' as a use case for AI epistemics is insightful. It points to the potential for AI to facilitate cooperation and avoid negative-sum outcomes.",
          "importance": 65,
          "isValid": true,
          "highlight": {
            "startOffset": 2027,
            "endOffset": 2203,
            "quotedText": "Identification of common ground directions / equilibria / game theoretic strategies that create/incentivize new positive gradients (and not surfacing the bad versions of those…"
          }
        },
        {
          "title": "Intersection of Epistemics and Incentives",
          "description": "Aviv's note that 'it is the intersection of better epistemics / world models + the right power structures / incentives needed for all of these' is a critical insight. It recognizes that epistemic improvement alone is insufficient without supportive institutional and incentive structures.",
          "importance": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 2255,
            "endOffset": 2389,
            "quotedText": "Note how it is the intersection of better epistemics / world models + the right power structures / incentives needed for all of these."
          }
        },
        {
          "title": "Personal Forecasting in Technological Change",
          "description": "Ben Goldhaber emphasizes the need for epistemic tools to help individuals navigate periods of intense technological change. This personalizes the impact of epistemics, extending it beyond policy and organizational contexts.",
          "importance": 60,
          "isValid": true,
          "highlight": {
            "startOffset": 2413,
            "endOffset": 2632,
            "quotedText": "Forecasting how I should live my life in a period of intense technological change  \n* Guiding decision makers to avoid war with china/ai/everyone  \n* Providing the background context for me to sensemake way more sanely."
          }
        },
        {
          "title": "AI Safety and Knowledge Advancement",
          "description": "Ben's focus on 'pushing forward the extant knowledge of AI safety agendas' and 'planning for multi-stage projects' highlights the role of epistemics in advancing complex, high-stakes fields.",
          "importance": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 2637,
            "endOffset": 2814,
            "quotedText": "Pushing forward the extant knowledge of AI safety agendas  \n* Planning for multi-stage projects that could achieve big impact  \n* Roadmapping to build larger more complex things"
          }
        },
        {
          "title": "Super-Late-Stage AI and Decision Theory",
          "description": "Chi's concern with 'super-late-stage AI, getting decision theory right' points to the importance of epistemics in foundational AI alignment and philosophical questions.",
          "importance": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 2874,
            "endOffset": 3291,
            "quotedText": "I care a lot about humanity and AI, including **super-late-stage AI, getting decision theory** right.  \n  * Consequently but also separately, this relates to the general question of what to do about areas where there is not necessarily a ground truth but that are nonetheless very important to get right, cf**. [metaphilosophy](https://www.alignmentforum.org/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy)**"
          }
        },
        {
          "title": "Automating Alignment Research",
          "description": "Chi references the potential for AI to assist with 'automating alignment research,' which is a concrete and high-leverage application of epistemic tools.",
          "importance": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 3375,
            "endOffset": 3445,
            "quotedText": "AIs helpful for AI safety research, cf. [automating alignment research"
          }
        },
        {
          "title": "Raising the Top vs. Bottom of the Epistemic Landscape",
          "description": "Chi distinguishes between 'raising the top' (helping experts) and 'raising the bottom' (improving societal epistemics). This dichotomy is a recurring theme and helps structure the space of possible interventions.",
          "importance": 80,
          "isValid": false,
          "highlight": {
            "startOffset": 3536,
            "endOffset": 4546,
            "quotedText": "This is more about raising the **top of the epistemic landscape**\n\n* Other things I care about:  \n  * Vague sense that better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries  \n    * This is broadly about: Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics.  \n  * Vague sense that better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place.  \n  * Vague sense that better **societal epistemics** would help with increasing the **sanity of conversations about AI safety, polarisation around the issue etc.**  \n    * This is broadly about **getting intent alignment** in the first place.\n\n→ These are all more about raising the **bottom of the epistemic landscape**"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Epistemics and Authoritarianism",
          "description": "Chi's observation that better societal epistemics could reduce the risk of 'AI-empowered coup, authoritarianism' is a powerful argument for the societal value of epistemic infrastructure.",
          "importance": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 3655,
            "endOffset": 3862,
            "quotedText": "better **society-wide epistemics** would help reduce the risk of an **AI-empowered coup, authoritarianism** generally  \n    * Information management seems important for gaining/keeping control over countries"
          }
        },
        {
          "title": "Intent Alignment and Societal Outcomes",
          "description": "Chi raises the question: 'Even if we got intent-alignment right, is the world afterwards gonna be good?' This highlights the limits of technical alignment and the need for robust societal epistemics.",
          "importance": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 3894,
            "endOffset": 4036,
            "quotedText": "Even if we **got intent-alignment right, is the world afterwards gonna be good**? And that potentially depends on pre-ASI societal epistemics."
          }
        },
        {
          "title": "Epistemics in Geopolitical Contexts",
          "description": "Chi's point about 'better epistemics in a geopolitical context' helping with 'AI race dynamics' connects epistemic infrastructure to global coordination challenges.",
          "importance": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 4060,
            "endOffset": 4224,
            "quotedText": "better **epistemics in a geopolitical context** would help with **AI race dynamics.**  \n    * This is broadly about **getting intent alignment** in the first place."
          }
        },
        {
          "title": "Sanity of AI Safety Conversations",
          "description": "Chi notes that improved societal epistemics could 'increase the sanity of conversations about AI safety, polarisation around the issue etc.' This is a concrete and timely application.",
          "importance": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 4248,
            "endOffset": 4224,
            "quotedText": "  \n  * Vague sense that "
          }
        },
        {
          "title": "Social Trust and Coordination",
          "description": "Deger Turan highlights the erosion of social trust and the fragility of collective coordination mechanisms. This underscores the foundational role of epistemics in enabling large-scale cooperation.",
          "importance": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 4567,
            "endOffset": 4749,
            "quotedText": "Social trust erosion, collective coordination mechanisms are built on a weak assumption  \n* Capacity to entertain truthfulness when it is not instrumental to one's incentive gradient"
          }
        },
        {
          "title": "Safe Disagreement and World Model Divergence",
          "description": "Deger emphasizes the value of 'contexts where world model divergence can be agreed by both parties enabling a safe disagreement.' This is a nuanced view of pluralism and epistemic health.",
          "importance": 65,
          "isValid": true,
          "highlight": {
            "startOffset": 4824,
            "endOffset": 4987,
            "quotedText": "Creating contexts where world model divergence can be agreed by both parties enabling a safe disagreement  \n* Improving model reasoning / safer AI by reducing slop"
          }
        },
        {
          "title": "Accountability Mechanisms for AI Reasoning",
          "description": "Deger suggests 'creating accountability mechanisms through consistency checks & other specifications for healthy epistemics.' This is a concrete proposal for improving AI reliability.",
          "importance": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 4934,
            "endOffset": 5193,
            "quotedText": "Improving model reasoning / safer AI by reducing slop, by creating accountability mechanisms through consistency checks & other specifications for healthy epistemics  \n* Decoupling virality with magnitude while living in a short attention span world, curation"
          }
        },
        {
          "title": "Epistemics in the Face of Incentive Misalignment",
          "description": "Deger expresses confusion about 'raising the floor when not all parties are necessarily operating in good faith.' This highlights a key challenge for epistemic interventions in adversarial or misaligned environments.",
          "importance": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 5199,
            "endOffset": 5448,
            "quotedText": "I have some confusion re: raising the floor when not all parties are necessarily operating in good faith. I think I need some help think through the ways in which epistemics in the context of incentive misalignment can be used or have defensibility."
          }
        },
        {
          "title": "Counterfactuals in Political Decision-Making",
          "description": "Nathan Young's list of 'what would we have got' (e.g., 'Biden not running, perhaps an open dem primary') illustrates the counterfactual reasoning that epistemic tools could support.",
          "importance": 60,
          "isValid": true,
          "highlight": {
            "startOffset": 5470,
            "endOffset": 5631,
            "quotedText": "What would we have got:\n\n* Biden not running, perhaps an open dem primary  \n* No FTX crisis  \n* Less zero sum OpenAI board drama  \n* Many smaller, better choices"
          }
        },
        {
          "title": "High-Stakes Decisions Vulnerable to Epistemic Failure",
          "description": "Nathan's enumeration of decisions that 'may go badly' (e.g., 'Trump tariffs/ global trade', 'AI regulation') provides concrete examples of where improved epistemics could have significant impact.",
          "importance": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 5634,
            "endOffset": 6184,
            "quotedText": "What decisions might go badly:\n\n* Trump tariffs/ global trade  \n* Taiwan/ US-China conflict  \n* US-China competition  \n* US authoritarianism  \n* AI regulation  \n* EU-China realignment  \n* Automated robots  \n* OpenAI strategy  \n* Anthropic strategy  \n* Google strategy  \n* xAI strategy  \n* AI workforce automation  \n* China strategy  \n* Russia strategy  \n* Moskovitz strategy  \n* Must strategy  \n* Bezos Strategy  \n* Zuckerberg strategy  \n* Tallinn strategy  \n* Altman strategy  \n* Drone warfare  \n* Climate change destabilisation  \n* Democratic trust"
          }
        },
        {
          "title": "Public Information Overload and Misinformation",
          "description": "Lincoln Quirk identifies the challenge of helping the public 'deal with and process the wealth of information' and 'cut through social media distraction / intentional misinformation.' This is a central problem for epistemic tools.",
          "importance": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 6252,
            "endOffset": 6511,
            "quotedText": "making the public better informed:  \n\t\\- better able to deal with and process the wealth of information  \n\t\\- better able to express their own interests in government (via voting)  \n\t\\- helping cut through social media distraction / intentional misinformation"
          }
        },
        {
          "title": "Objective Policy Analysis and Communication",
          "description": "Lincoln's focus on 'objectively doing cost-benefit analysis' and 'objectively explaining tradeoffs' highlights the need for epistemic tools that can support transparent and unbiased policy evaluation.",
          "importance": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 6517,
            "endOffset": 6827,
            "quotedText": "getting governments to implement better policy  \n\t\\- coming up with good policy ideas  \n\t\\- objectively doing cost-benefit analysis  \n\t\\- objectively explaining tradeoffs (vs having to rely on biased humans to explain them)  \n\t\\- flagging bad ideas as bad  \n\t\\- convincing policymakers with compelling writeups"
          }
        },
        {
          "title": "Individual and Organizational Flourishing",
          "description": "Lincoln connects epistemic tools to 'success and flourishing for individuals and orgs,' emphasizing the broad applicability of epistemic improvements.",
          "importance": 60,
          "isValid": true,
          "highlight": {
            "startOffset": 6833,
            "endOffset": 6772,
            "quotedText": "  \n\t\\- convincing policymakers with compelling writeups  \n\\- "
          }
        },
        {
          "title": "Community Notes as a Model for Epistemic Tools",
          "description": "Jay's interest in building 'the positive version of Community Notes' and mapping consensus/disagreement is a concrete and scalable vision for epistemic infrastructure.",
          "importance": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 7126,
            "endOffset": 7803,
            "quotedText": "How can we build something like the positive version of Community Notes: which statements/posts are agreed on by people who often disagree?  \n  * How can we better understand/summarize what others think, e.g. on social media platforms? E.g. if you see a post recommended to you, is this because it's the most extreme take that went viral and isn't representative, or is this a commonly held view? Between different viewpoints, what are the concrete points of actual disagreement (on facts or values) and what are the things widely agreed upon? Let's build viewpoint/debate maps or a similar sensemaking tool that gets adopted/understood by the masses/raises the knowledge floor"
          }
        },
        {
          "title": "AI-Assisted Knowledge Creation",
          "description": "Jay proposes 'improving Community Notes (or more general knowledge) creation with AI assistants,' which is a practical application of AI for epistemics.",
          "importance": 65,
          "isValid": true,
          "highlight": {
            "startOffset": 7884,
            "endOffset": 8052,
            "quotedText": "How can we improve Community Notes (or more general knowledge) creation with AI assistants? An AI agent proposing community notes and humans still checking/rating them?"
          }
        },
        {
          "title": "Forecasting Unknowable Claims",
          "description": "Jay asks how to apply epistemic tools to 'claims or statements about the future, or which are not concretely knowable.' This is a challenging but important frontier for epistemic AI.",
          "importance": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 8059,
            "endOffset": 8198,
            "quotedText": "How can we have something like Community Notes that applies to claims or statements about the future, or which are not concretely knowable?"
          }
        },
        {
          "title": "Epistemics for Actionable Decision Support",
          "description": "Ben Wilson (Metaculus) articulates a vision where AI epistemics enables actionable recommendations: 'take a goal, and get a list of possible actions... and the forecasted outcomes.' This is a clear and ambitious target for epistemic tooling.",
          "importance": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 8408,
            "endOffset": 8897,
            "quotedText": "If we had perfect AI epistemics I would be excited by the ability for us to take a goal, and get a list of possible actions we could take and the forecasted outcomes of each of those actions (plus background information and forecasts for the 'current lay of the land' for context). This can then inform, not just the risk of AI doing good/bad things (as with current forecasting), but what we can do to raise the chances of 'great' AI outcomes, and lower the changes of 'worse' AI outcomes"
          }
        },
        {
          "title": "AI Policy and Organizational Planning",
          "description": "Andreas emphasizes the importance of AI epistemics for 'AI policy, broadly construed' and 'organizational planning & decision-making.' These are high-leverage domains for epistemic improvement.",
          "importance": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 8990,
            "endOffset": 9157,
            "quotedText": "AI policy, broadly construed, must go better  \n    * Avoid gradual disempowerment  \n    * Thoughtful regulation  \n  * Organizational planning & decision-making is weak"
          }
        },
        {
          "title": "Mapping Dependencies and Uncertainties",
          "description": "Andreas's call for tools to 'map dependencies, uncertainties, and potential futures' is a concrete technical challenge for epistemic AI.",
          "importance": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 9352,
            "endOffset": 9477,
            "quotedText": "Mapping dependencies, uncertainties, and potential futures  \n    * Understand complex domains with non-trivial feedback loops"
          }
        },
        {
          "title": "Resource Allocation and Policy Decisions",
          "description": "Andreas lists 'resource allocation within orgs' and 'policy' as decisions that could go badly by default. These are classic examples of where epistemic tools can add value.",
          "importance": 65,
          "isValid": true,
          "highlight": {
            "startOffset": 9605,
            "endOffset": 9716,
            "quotedText": "Resource allocation within orgs  \n    * Investment  \n    * Grant making  \n    * Government budgets  \n  * Policy"
          }
        },
        {
          "title": "Integration of Epistemic Tools in Strategic Workflows",
          "description": "Andreas envisions a future where 'AI for epistemics tools are deeply integrated into the workflows of strategically important organizations.' This is a key marker of success for epistemic infrastructure.",
          "importance": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 9837,
            "endOffset": 10147,
            "quotedText": "AI for epistemics tools are deeply integrated into the workflows of strategically important organizations (e.g., government agencies, major labs, influential industry players)  \n  * These tools are trusted and regularly used to guide high-stakes actions and policies through demonstrably high-quality reasoning"
          }
        },
        {
          "title": "Assumption of Good Intentions",
          "description": "Andreas's assumption that 'people are generally well-intentioned' is optimistic and may not always hold, but it clarifies the intended operating environment for epistemic tools.",
          "importance": 60,
          "isValid": true,
          "highlight": {
            "startOffset": 10278,
            "endOffset": 10430,
            "quotedText": "Assumption: People are generally well-intentioned \\- no one wants catastrophe \\- and as a result of better tooling things go better in the world overall"
          }
        },
        {
          "title": "World Peace Without World Government",
          "description": "Owen's 'optimistic brainstorm' includes the vision of 'world peace without world government' enabled by epistemic tech. This is an ambitious and inspiring goal.",
          "importance": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 10464,
            "endOffset": 10751,
            "quotedText": "World peace without world government — sufficiently good tech to help countries understand each others' interests, find mutually desirable bargains, and make sufficient credible commitments, could help us get up towards the Pareto frontier where we avoid (virtually) all costly conflicts"
          }
        },
        {
          "title": "Slowing AI Development for Safety",
          "description": "Owen suggests that epistemic tools could help 'go slowly and carefully on AI development,' facilitating robust agreements on safe progress. This is a critical application in the current AI landscape.",
          "importance": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 10753,
            "endOffset": 11156,
            "quotedText": "Going slowly and carefully on AI development — rather than having AI progress accelerate at increasingly breakneck speed, helping actors to understand the basic situation and the desirability of different options could be crucial for facilitating meaningful and robust agreements which restrict AI development to the stuff which we have good reasons for being confident are safe // otherwise beneficial."
          }
        },
        {
          "title": "Bettering Democracy Through Epistemics",
          "description": "Owen links epistemic improvement to 'bettering democracy,' by helping citizens access key information and aligning incentives for decision-makers. This is a classic argument for the societal value of epistemics.",
          "importance": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 11158,
            "endOffset": 11425,
            "quotedText": "Bettering democracy — helping people to access key information about what decisions their country might make would actually mean for longer term implications, and having this in turn feed into incentives for decision-makers that better align them with the common good"
          }
        },
        {
          "title": "Anticipating AI Safety",
          "description": "Owen's call for 'better tools for anticipating when systems are actually robustly safe' is a direct application of epistemic tools to AI alignment and safety.",
          "importance": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 11427,
            "endOffset": 11589,
            "quotedText": "Building safe / aligned AI — better tools for anticipating when systems are actually robustly safe, vs when stronger cases for expectation of safety seem required"
          }
        },
        {
          "title": "Confusion and Uncertainty in AI Risk",
          "description": "Josh Rosenberg expresses 'massive confusion about important problems,' such as AI risk timelines and appropriate responses. This highlights the need for epistemic tools that can clarify complex, high-stakes uncertainties.",
          "importance": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 11616,
            "endOffset": 12056,
            "quotedText": "I'm massively confused about important problems, e.g. how to orient to AI risk and what to do about it. Some reasonable people think there's a high probability we effectively lose everything we care about in 2027\\. Some reasonable people think it'll be decades till we have \"AGI\" and it'd be good if it arrived sooner. Which actions are good to take in each of these worlds is very different, and I'd like to figure out what world we're in."
          }
        },
        {
          "title": "Epistemics for Institutional Resilience",
          "description": "Josh's concern for 'preserving high-quality institutions to lead the world in positive directions' connects epistemic health to institutional robustness.",
          "importance": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 12317,
            "endOffset": 12404,
            "quotedText": "How can we preserve high-quality institutions to lead the world in positive directions?"
          }
        },
        {
          "title": "Collective Sensemaking",
          "description": "Josh hopes that AI will help 'all of us do this sort of thing collectively,' emphasizing the importance of group epistemics and shared understanding.",
          "importance": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 12929,
            "endOffset": 12998,
            "quotedText": "I'm hoping AI will help all of us do this sort of thing collectively."
          }
        },
        {
          "title": "Automating AI Risk Communication",
          "description": "Katja Grace proposes automating the creation and communication of compelling cases for AI risk. This is a high-leverage application for epistemic tools.",
          "importance": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 13183,
            "endOffset": 13272,
            "quotedText": "Automate talking to people about AI risk compellingly, possibly using above case as input"
          }
        },
        {
          "title": "Game Theoretic Thinking and Error Correction",
          "description": "Katja's idea of automating 'good game theoretic thinking' and 'noting errors' in public discourse is a practical and impactful use of AI for epistemics.",
          "importance": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 13279,
            "endOffset": 13467,
            "quotedText": "Good game theoretic thinking re arms races, coordination, what is possible. I.e. People are making dumb assumptions about things, e.g. \"can't slow down tech\". Could automate noting errors."
          }
        },
        {
          "title": "Quantified and Competitive Debate Platforms",
          "description": "Katja suggests 'very quantified and competitive debate site[s]' where AI does much of the cognitive labor. This is a novel approach to scaling epistemic labor.",
          "importance": 65,
          "isValid": true,
          "highlight": {
            "startOffset": 13633,
            "endOffset": 13889,
            "quotedText": "Specific idea: very quantified and competitive debate site, that can be cited re which views are winning, where AI does a lot of the cognitive labor in places no people are paying attention, but as a part of it gets cruxy, people pay attention and vote/bet"
          }
        },
        {
          "title": "Coordination Failures in AI Development",
          "description": "Katja identifies 'failure to coordinate on AI' as a key risk, which epistemic tools could help mitigate.",
          "importance": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 14137,
            "endOffset": 14238,
            "quotedText": "Failure to coordinate on AI: building AI systems that are dangerous, due to expecting others to do so"
          }
        },
        {
          "title": "Raising the Bottom: Societal Epistemics",
          "description": "Joel Lehman focuses on 'raising the bottom' by improving societal epistemics, especially as it relates to political decision-making. This is a crucial and often underemphasized aspect.",
          "importance": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 14331,
            "endOffset": 14527,
            "quotedText": "Raising the bottom (i.e. helping improve societal epistemics) – in particular that in theory political decisions (at least political decision-makers) are somewhat downstream of societal epistemics"
          }
        },
        {
          "title": "Public Epistemic Health and Catastrophic Risk",
          "description": "Joel warns that 'awful public epistemics at critical junctures seems to unnecessarily spike catastrophic/existential risk probabilities.' This is a strong argument for prioritizing public epistemic health.",
          "importance": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 14625,
            "endOffset": 14779,
            "quotedText": "Having weird political leaders and awful public epistemics at critical junctures seems to unnecessarily spike catastrophic/existential risk probabilities."
          }
        },
        {
          "title": "Civic Virtues and LLM Assistants",
          "description": "Joel speculates that LLM assistants could 'help cultivate civic virtues and... support critical thinking.' This is a positive vision for the role of AI in public discourse.",
          "importance": 65,
          "isValid": true,
          "highlight": {
            "startOffset": 15114,
            "endOffset": 15314,
            "quotedText": "if LLM assistants by default help cultivate civic virtues and on the margin support critical thinking or whatever underlying skills and knowledge help a person up-level their epistemic habits / health"
          }
        },
        {
          "title": "Societal Trust as an Epistemic Good",
          "description": "Joel includes 'societal trust' and 'trust in your neighbors' as important social goods that epistemic tools could support.",
          "importance": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 15494,
            "endOffset": 15593,
            "quotedText": "similarly somehow help w/ important social goods like 'societal trust' and 'trust in your neighbors"
          }
        },
        {
          "title": "Healthier Democratic Society as an Outcome",
          "description": "Joel envisions 'healthier democratic society, better agreement on consensus reality, higher trust in society' as outcomes of better AI for epistemics. This is a compelling vision.",
          "importance": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 15756,
            "endOffset": 15918,
            "quotedText": "Healthier democratic society, better agreement on consensus reality, higher trust in society, better politicians, better concrete decisions in high-stakes moments"
          }
        },
        {
          "title": "Evaluator, Oracle, Planner, Representative: AI Roles",
          "description": "Austin Chen outlines four archetypes for epistemic AI: Evaluator, Oracle, Planner, and Representative. This taxonomy is useful for thinking about the design space of epistemic tools.",
          "importance": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 16005,
            "endOffset": 16468,
            "quotedText": "**Evaluator**: More agreement (or understanding of disagreement) of what orgs & people are doing good work (eg for making grants in AI safety)  \n  * **Oracle**: Instant trustworthy forecasts on any question you ask  \n  * **Planner**: Guidance on \"what are the 3 most important things I should do this day/week/month\"  \n  * **Representative**: Simulate the responses of other parties (people, orgs/corps, governments, LLM agents), for discussions, contracts, trade"
          }
        },
        {
          "title": "Noise-to-Signal Ratio in Information Ecosystems",
          "description": "Janna raises the concern that 'noise to signal ratio could be much higher' due to AI-generated (mis)information. This is a central challenge for epistemic infrastructure.",
          "importance": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 16530,
            "endOffset": 16638,
            "quotedText": "Noise to signal ratio could be much higher  \n  * AI (mis)information churned out at the price of almost zero"
          }
        },
        {
          "title": "Scaling Prediction Markets with AI",
          "description": "Janna asks whether AI can 'scale up the bots... to improve prediction market accuracy.' This is a concrete and testable application.",
          "importance": 65,
          "isValid": true,
          "highlight": {
            "startOffset": 16716,
            "endOffset": 16853,
            "quotedText": "Solving prediction markets--is there a way to scale up the bots, even if they are wrong and lossy, to improve prediction market accuracy?"
          }
        },
        {
          "title": "Reducing Business Uncertainty",
          "description": "Janna notes that more accurate predictions could 'reduce business uncertainty' and 'make it easier to evaluate life decisions.' This highlights the economic value of epistemic tools.",
          "importance": 60,
          "isValid": true,
          "highlight": {
            "startOffset": 17162,
            "endOffset": 17277,
            "quotedText": "More accurate predictions to…  \n    * Reduce business uncertainty  \n    * Make it easier to evaluate life decisions"
          }
        },
        {
          "title": "Intrinsic Value of Understanding",
          "description": "Owain suggests that 'having humans understanding the world better is good in itself.' This recognizes the intrinsic as well as instrumental value of epistemics.",
          "importance": 65,
          "isValid": true,
          "highlight": {
            "startOffset": 17355,
            "endOffset": 17509,
            "quotedText": "Maybe having humans understanding the world better is good in itself. Lots of people read popular science for this reason. Maybe it's good for AIs as well"
          }
        },
        {
          "title": "Epistemics as a Public Good",
          "description": "Owain points out that 'epistemic tech/institutions/advances can be hard to fund via for-profit companies,' due to public goods issues. This is a key challenge for the field.",
          "importance": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 18732,
            "endOffset": 18895,
            "quotedText": "Epistemic tech/institutions/advances can be hard to fund via for-profit companies. This is in part due to public goods issues, and issues around markets for ideas."
          }
        },
        {
          "title": "Delegation of Epistemic Tasks to AI",
          "description": "Owain observes that 'we haven't really had tech that could do most epistemic tasks until recently,' and that AI now enables delegation of these tasks. This marks a technological inflection point.",
          "importance": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 18334,
            "endOffset": 18480,
            "quotedText": "Now we have AI that can do an increasing proportion of the sub-tasks of epistemic processes. This will allow us to delegate epistemic tasks to AI."
          }
        },
        {
          "title": "Upgrading the Global Sensemaking Machine",
          "description": "Rafe expresses hope that 'the global sensemaking machine' can be upgraded to meet the challenge of AI. This metaphor captures the collective nature of epistemic improvement.",
          "importance": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 19800,
            "endOffset": 19865,
            "quotedText": "I hope we can help the global sensemaking machine upgrade itself."
          }
        },
        {
          "title": "Robustness Through Epistemic Grounding",
          "description": "Rafe argues that 'a society that understands things better... is more robust in a bunch of ways.' This links epistemic health to societal resilience.",
          "importance": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 19496,
            "endOffset": 19696,
            "quotedText": "ully and well. It's scary\\! And there are a bunch of entrenched interests and memeplexes that preserve their status better (myopically) if people only have shallow slogans to think about AI with.\n\nGiv"
          }
        },
        {
          "title": "Consequentialist vs. Intrinsic Value of Epistemics",
          "description": "Dave articulates a tension between valuing epistemics for its consequences and valuing it intrinsically or for democratization. This is a recurring philosophical debate in the field.",
          "importance": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 20544,
            "endOffset": 21138,
            "quotedText": "With my consequentialist hat, better epistemics being worth it has to route through improved decisions somewhere down the line. The decisions I particularly care about are those by powerful people wrt AI development and implementation and the effects this has on how the future goes  \n  * But I do feel a bit icky when I have this hat on. There seems to be something very intuitively appealing about (a) pursuing better epistemics for its own sake and (b) democratising epistemic tools for the use and benefit of everyone. I think this basically caches out in more 'raising the floor' type work"
          }
        },
        {
          "title": "Forecasting as a Key Tool",
          "description": "Dave sees forecasting as 'a pretty powerful tool' and notes the 'massive undersupply of good forecasts.' This is a concrete area for epistemic improvement.",
          "importance": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 21276,
            "endOffset": 21525,
            "quotedText": "I think forecasting has the potential to be a pretty powerful tool and there's currently a massive undersupply of good forecasts. I'm excited for us to use the wave of increased AI labor to make better quantitative forecasts to inform key decisions."
          }
        },
        {
          "title": "Raising the Ceiling: Resolving Expert Disagreement",
          "description": "Julian asks whether AI can 'organize information and credibly resolve the most challenging and important questions on which experts still seem to disagree.' This is a high-value application.",
          "importance": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 21789,
            "endOffset": 21958,
            "quotedText": "**Raising the ceiling:** can we use AI to organize information and credibly resolve the most challenging and important questions on which experts still seem to disagree?"
          }
        },
        {
          "title": "Raising the Floor: Trustworthy Epistemic Infrastructure",
          "description": "Julian also asks whether AI can 'build extremely legibly trustworthy/reliable epistemic infrastructure that actually makes its way into people's lives.' This is essential for broad impact.",
          "importance": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 22190,
            "endOffset": 22348,
            "quotedText": "**Raising the floor:** can we use AI to build extremely legibly trustworthy/reliable epistemic infrastructure that actually makes its way into people's lives?"
          }
        },
        {
          "title": "Collective Epistemics and Productive Conversations",
          "description": "Julian's idea of 'connecting people to the others with whom they would have the most productive/enlightening conversations' is a novel approach to improving collective epistemics.",
          "importance": 65,
          "isValid": true,
          "highlight": {
            "startOffset": 22457,
            "endOffset": 22584,
            "quotedText": "Collective epistemics: connecting people to the others with whom they would have the most productive/enlightening conversations"
          }
        },
        {
          "title": "Public Policy as a Downstream Beneficiary",
          "description": "Julian notes that 'lots of stuff in public policy' could go better with improved epistemics, affecting both experts and the public.",
          "importance": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 22676,
            "endOffset": 22843,
            "quotedText": "Lots of stuff in public policy: downstream both of better experts (technocrats), better public (voters/interest groups), and better coordination (collective epistemics"
          }
        },
        {
          "title": "Long-Term Progress Blocked by Epistemic Failures",
          "description": "Julian wants a world where 'everyone is more rational, empathetic, considered in their decision making,' seeing this as a major blocker to long-term progress.",
          "importance": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 22907,
            "endOffset": 23162,
            "quotedText": "I want us to live in a world where everyone is more rational, empathetic, considered in their decision making, conscientious about impact, and interested in coordinating. I feel like that's one of the biggest blockers to all kinds of *long term* progress."
          }
        },
        {
          "title": "Fact Checkers, AI Debaters, and Meta-Analysis",
          "description": "Julian lists a range of tools—fact checkers, AI debaters, forecasting with explanations, meta-analysis—as desirable epistemic infrastructure.",
          "importance": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 23214,
            "endOffset": 23396,
            "quotedText": "Easily accessible fact checkers / claim refuters  \n  * Powerful AI debaters  \n* Forecasting with explanations: superhuman and at scale  \n* Scientific meta-analysis, systematic review"
          }
        },
        {
          "title": "Consistency Checking and Market Makers",
          "description": "Julian suggests 'consistency checking and inconsistency identification' and 'arbitrage / market makers in prediction markets' as technical interventions.",
          "importance": 65,
          "isValid": true,
          "highlight": {
            "startOffset": 23518,
            "endOffset": 23626,
            "quotedText": "Consistency checking and inconsistency identification  \n  * Arbitrage / market makers in prediction markets?"
          }
        },
        {
          "title": "Murky Truth in Strategic Problems",
          "description": "Lawrence points out that 'there are already key questions/strategic problems where the truth is murky,' such as US-China AI competition. This highlights the need for epistemic tools in high-stakes, ambiguous domains.",
          "importance": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 23715,
            "endOffset": 24375,
            "quotedText": "There are already key questions/strategic problems where the truth is murky and we seem to proceed on the basis of limited and biased views. There might well be many more of these in the future. A central example is US-China competition over AI. Some have vibes of \"China is so bad that it's worth risking an existential catastrophe to stop it from capturing a portion of the lightcone\". Others have vibes of \"Cooperation is feasible and desirable, and is our only path to a good outcome\". But how bad is China, really? How much \"realised K\" on this question do those who are making the relevant decisions actually have? Similarly, how feasible is cooperation?"
          }
        },
        {
          "title": "Cognitive Labor for Hard Questions",
          "description": "Lawrence argues for 'throwing much more cognitive labour' at complex questions and presenting findings in a way that is 'hard to disagree with.' This is a vision for epistemic AI as a force multiplier.",
          "importance": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 24553,
            "endOffset": 24783,
            "quotedText": "I think we could benefit massively from throwing much more cognitive labour at them, and presenting the findings in a way that is hard to disagree with or for any individual to improve upon, other than at a normative/values level."
          }
        },
        {
          "title": "AI for Intelligence Explosion Management",
          "description": "Lukas Finnveden is 'specifically concerned about AI potentially causing an intelligence explosion,' and sees epistemics as crucial for managing this risk.",
          "importance": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 24955,
            "endOffset": 25086,
            "quotedText": "I'm specifically concerned about AI potentially causing an intelligence explosion, and us needing better epistemics to handle that."
          }
        },
        {
          "title": "Reasoning from Limited Evidence",
          "description": "Lukas wants AI capabilities applied to 'reasoning well from limited evidence,' especially for risk management. This is a technical and philosophical challenge.",
          "importance": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 24557,
            "endOffset": 24745,
            "quotedText": "ink we could benefit massively from throwing much more cognitive labour at them, and presenting the findings in a way that is hard to disagree with or for any individual to improve upon, o"
          }
        },
        {
          "title": "Alignment Techniques for Hard-to-Evaluate Questions",
          "description": "Lukas emphasizes the need for 'good alignment techniques even on hard-to-evaluate questions.' This is a frontier for epistemic AI.",
          "importance": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 25583,
            "endOffset": 25651,
            "quotedText": "I want good alignment techniques even on hard-to-evaluate questions."
          }
        },
        {
          "title": "Broad Distribution of Epistemic Improvements",
          "description": "Lukas argues that 'AI-driven improvements in general knowledge and decision-making are more broadly distributed among humans' is desirable, as it reduces concentration of power.",
          "importance": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 25659,
            "endOffset": 25948,
            "quotedText": "Seems great to me if AI-driven improvements in general knowledge and decision-making are more broadly distributed among humans. (I think this leads to better decision-making without significantly accelerating the dangers from new tech or misaligned AI. Also reduces concentration of power."
          }
        },
        {
          "title": "Epistemics Degraded by Social Media",
          "description": "Abram Demski claims that 'epistemics has degraded horribly due to social media and the internet,' and that AI could make this worse. This is a cautionary note.",
          "importance": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 26041,
            "endOffset": 26235,
            "quotedText": "There's the general claim that epistemics has degraded horribly due to social media and the internet, at least in a lot of ways, although epistemics has also gotten better in some specific ways."
          }
        },
        {
          "title": "AI for Decision-Making vs. Epistemics",
          "description": "Abram distinguishes between 'AI for decision-making' and 'AI for epistemics,' arguing that the latter is more foundational. This is a subtle but important distinction.",
          "importance": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 26463,
            "endOffset": 26717,
            "quotedText": "The high-level reason why epistemics is important is because epistemics is how we make good decisions. So effectively, this is AI for decision-making. However, if we just set out to make tools for decision-making, I have a sense that this would be worse."
          }
        },
        {
          "title": "Orientation to the Right Problem",
          "description": "Abram notes that 'one super important aspect of decision-making is that you have to orient towards the right problem.' This highlights the role of epistemics in problem formulation.",
          "importance": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 26725,
            "endOffset": 26824,
            "quotedText": "One super important aspect of decision-making is that you have to orient towards the right problem."
          }
        },
        {
          "title": "Common Knowledge and Coordination",
          "description": "Abram argues that 'you have to have common knowledge, not just knowledge,' linking epistemics to coordination. This is a key insight for group epistemics.",
          "importance": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 27163,
            "endOffset": 27442,
            "quotedText": "You have to have common knowledge, not just knowledge. AI for coordination is yet another thing. AI for group epistemics is close to that, though, and again, I have a sense that making \"AI for coordination\" would produce worse effects on the world than \"AI for group epistemics\"."
          }
        },
        {
          "title": "Existential Risk as a Target",
          "description": "Abram identifies 'decisions around existential risk' as 'super super important,' making this a priority area for epistemic tools.",
          "importance": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 27510,
            "endOffset": 27571,
            "quotedText": "Decisions around existential risk seem super super important."
          }
        },
        {
          "title": "Shared Understanding Leads to Better Futures",
          "description": "Abram asserts that 'producing a better shared understanding of the world JUST WOULD result in better things.' This is a strong claim about the power of epistemics.",
          "importance": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 26916,
            "endOffset": 27024,
            "quotedText": "I have a sense that producing a better shared understanding of the world JUST WOULD result in better things."
          }
        },
        {
          "title": "Democratic Decision-Making and Shared Reality",
          "description": "Brendan argues that 'to make decisions in a way that is inclusive and democratic... we must begin with a shared sense of how the world is.' This is a foundational argument for epistemic infrastructure.",
          "importance": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 28517,
            "endOffset": 28669,
            "quotedText": "To make these decisions in a way that is inclusive and democratic, that fosters buy-in and trust, we must begin with a shared sense of how the world is."
          }
        },
        {
          "title": "Epistemic Coherence Across Contexts",
          "description": "Brendan notes that 'maintaining a shared sense of the world becomes more difficult and subtle' as contexts widen. This is a challenge for epistemic tools in a globalized world.",
          "importance": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 28764,
            "endOffset": 28889,
            "quotedText": "As we live together over wider and wider contexts, maintaining a shared sense of the world becomes more difficult and subtle."
          }
        },
        {
          "title": "Cooperation at Larger Scales",
          "description": "Brendan argues that epistemic technologies 'allow new avenues for cooperation at larger scales of human involvement.' This is a key societal benefit.",
          "importance": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 29158,
            "endOffset": 29258,
            "quotedText": "Moreover, such technologies allow new avenues for cooperation at larger scales of human involvement."
          }
        },
        {
          "title": "Nuanced Evaluation of Strategic Skill",
          "description": "Isabel points out that 'the kinds of being right in the past that are most relevant for decision making aren't just about making straightforward predictions,' but involve counterfactual reasoning and strategic skill. This is a subtle and important distinction.",
          "importance": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 29486,
            "endOffset": 29787,
            "quotedText": "The kinds of being right in the past that are most relevant for decision making aren't just about making straightforward predictions about what will happen, but are more nuanced about what kinds of actions one should take to cause those things to happen, and these seem like somewhat different skills."
          }
        },
        {
          "title": "Limits of Forecasting for Strategy",
          "description": "Isabel references Neel Nanda's piece on the limits of 'good research takes' for 'good strategic takes,' and asks whether AI can help with fuzzier, harder-to-evaluate questions.",
          "importance": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 30131,
            "endOffset": 30594,
            "quotedText": "A bit adjacent to Neel Nanda's [recent piece](https://www.lesswrong.com/posts/P5zWiPF5cPJZSkiAK/good-research-takes-are-not-sufficient-for-good-strategic) on how \"good research takes aren't sufficient for good strategic takes\" \\- \"good forecasting takes aren't sufficient for good strategic takes\", but maybe some of this is maybe resolvable on the forecasting tech side \\- can we get better at creating and evaluating results from fuzzier questions? Can AI help?"
          }
        },
        {
          "title": "Trust and Track Record Manipulation",
          "description": "Isabel warns of the risk that actors could 'build up good track records in order to be listened to, but then choosing to burn that track record to get power.' This is a key challenge for trust in epistemic systems.",
          "importance": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 30710,
            "endOffset": 31008,
            "quotedText": "Also, some risk of actors building up good track records in order to be listened to, but then choosing to burn that track record to get power or things like that, something something trust \\- a concern for both human actors and e.g. scheming AIs \\- what things look good until they abruptly aren't?"
          }
        },
        {
          "title": "Engaging with Non-Truth-Seeking Actors",
          "description": "Isabel is 'interested in takes on what to do about and how to engage with actors who have epistemic processes that don't seem to be truth seeking.' This is a practical and philosophical challenge.",
          "importance": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 30710,
            "endOffset": 31202,
            "quotedText": "Also, some risk of actors building up good track records in order to be listened to, but then choosing to burn that track record to get power or things like that, something something trust \\- a concern for both human actors and e.g. scheming AIs \\- what things look good until they abruptly aren't? \n\nOn a different note, also interested in takes on what to do about and how to engage with actors who have epistemic processes that don't seem to be truth seeking in a comprehensible-to-me way."
          }
        },
        {
          "title": "Trustworthiness Beyond Truthfulness",
          "description": "Adam Marblestone distinguishes between 'truthfulness' and 'trustworthiness,' noting that the latter requires provenance and the ability to be trusted by adversarial humans. This is a subtle but crucial point.",
          "importance": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 31269,
            "endOffset": 31674,
            "quotedText": "Trust, is one thing. Our current society seems to be fraught with distrust and motivated reasoning. Can we get a more objectively trustable source of truth? Not only accuracy but with a provenance that gives it an ability to be trusted by humans who are otherwise adversarial. Trust is needed to coordinate. Truthfulness is maybe somewhat necessary but seemingly not sufficient for a source being trusted."
          }
        },
        {
          "title": "Large Knowledge Models as Epistemic Tools",
          "description": "Adam relays Eric Drexler's view that 'development and steering of epistemic tooling, aka Large Knowledge Models, offers the highest leverage.' This is a concrete technical direction.",
          "importance": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 32067,
            "endOffset": 32231,
            "quotedText": "Development and steering of epistemic tooling, aka [Large Knowledge Models](https://aiprospects.substack.com/p/large-knowledge-models), offers the highest leverage."
          }
        },
        {
          "title": "Strategy Assistant AI for Civilizational Navigation",
          "description": "Adam envisions a 'strategy assistant AI' that helps humans coordinate around big technological or geopolitical issues. This is a high-level vision for epistemic AI.",
          "importance": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 33699,
            "endOffset": 33939,
            "quotedText": "So that's a strategy assistant AI – which helps humans at large see ways they could act and coordinate especially around big technological or geopolitical strategy issues I guess – but also a certain embodiment of an AGI oracle type system."
          }
        },
        {
          "title": "Democratic Mechanisms Maximally Utilizing Tech",
          "description": "Adam expresses interest in 'democratic mechanisms that are simply maximally utilizing the tech we have.' This is a call for institutional innovation.",
          "importance": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 34125,
            "endOffset": 34221,
            "quotedText": "I'd like to also see democratic mechanisms that are simply maximally utilizing the tech we have."
          }
        },
        {
          "title": "Medical Epistemics for Individuals",
          "description": "Elizabeth highlights the use case of 'allowing people to be better informed about experimental medical stuff for themselves.' This is a concrete and impactful application.",
          "importance": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 34313,
            "endOffset": 34399,
            "quotedText": "Allowing people to be better informed about experimental medical stuff for themselves."
          }
        },
        {
          "title": "Bargaining and Pareto Optimality",
          "description": "Elizabeth notes that 'salary negotiations feels like the results are often more the consequence of \"human bargaining skill\" and not the pareto optimal result.' This is an area where epistemic tools could improve fairness and efficiency.",
          "importance": 65,
          "isValid": true,
          "highlight": {
            "startOffset": 34498,
            "endOffset": 34656,
            "quotedText": "Bargaining? Even just salary negotiations feels like the results are often more the consequence of \"human bargaining skill\" and not the pareto optimal result."
          }
        },
        {
          "title": "Forecasting Relevant Variables",
          "description": "Saul emphasizes the importance of 'understanding better which things are most relevant to forecast,' which is a meta-epistemic challenge.",
          "importance": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 34800,
            "endOffset": 34899,
            "quotedText": "more useful forecasts, particularly understanding better which things are most relevant to forecast"
          }
        },
        {
          "title": "Group Decisionmaking and Veto Power",
          "description": "Saul notes that 'better group decisionmaking' can help 'unblock things from vetopower type' obstacles. This is a practical application of epistemic tools.",
          "importance": 65,
          "isValid": true,
          "highlight": {
            "startOffset": 34928,
            "endOffset": 34999,
            "quotedText": "better group decisionmaking (e.g. unblocking things from vetopower type"
          }
        },
        {
          "title": "Asymmetric Truth-Tracking",
          "description": "Saul mentions 'societal- and individual-scale asymmetric truth-tracking' as a goal. This is a nuanced and technical challenge.",
          "importance": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 35110,
            "endOffset": 35166,
            "quotedText": "societal- and individual-scale asymmetric truth-tracking"
          }
        },
        {
          "title": "Political Power and Tech Regulation",
          "description": "Saul lists 'which groups of people have political power' and 'who's regulating powerful new tech' as decisions that could go badly. These are high-stakes applications.",
          "importance": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 35264,
            "endOffset": 35387,
            "quotedText": "which groups of people have political power  \n  * who's building powerful new tech  \n  * who's regulating powerful new tech"
          }
        },
        {
          "title": "Better Decisions, Better Outcomes",
          "description": "Saul concludes that 'as a result of better AI for epistemics tech, we get: better decisions across the board; better outcomes from those decisions.' This is the ultimate justification for epistemic tools.",
          "importance": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 35631,
            "endOffset": 35816,
            "quotedText": "better decisions across the board  \n    * better *outcomes* from those decisions; or, like, we can track \"better decisions\" as better-in-expectation outcomes from the decision processes"
          }
        }
      ],
      "thinking": "# Thinking Process\n\n## 1. Overall Assessment\nThe document is a collaborative brainstorming session among leading thinkers in AI safety, epistemics, and governance. It collects a wide range of perspectives on the value, use cases, and societal impact of \"AI for epistemics\"—that is, AI systems designed to improve collective and individual knowledge, reasoning, and decision-making. The document is rich in both concrete and abstract reflections, with contributors highlighting both the promise and the complexity of deploying AI to enhance epistemic processes at scale.\n\n## 2. Key Themes and Patterns\n- **Epistemics as Instrumental**: Many contributors emphasize that epistemics is not an end in itself, but a means to better decisions, coordination, and flourishing. There is a recurring tension between valuing epistemics for its own sake (truth-seeking, understanding) and for its downstream effects (better policy, reduced existential risk).\n- **Raising the Floor and Ceiling**: Several participants distinguish between \"raising the floor\" (improving baseline societal epistemics, reducing misinformation, increasing trust) and \"raising the ceiling\" (enabling superhuman forecasting, resolving expert-level disagreements, advancing science).\n- **Societal and Institutional Impact**: There is a strong focus on how AI for epistemics could improve governance, democratic functioning, resource allocation, and coordination—especially in the context of rapid technological change and global risks.\n- **Trust, Coordination, and Power**: Trustworthiness, provenance, and the ability to coordinate across diverse actors are seen as central challenges. Contributors worry about incentive misalignment, concentration of power, and the risk of epistemic tools being subverted or misused.\n- **Forecasting and Decision Support**: Many see forecasting (quantitative and qualitative) as a core application, but note the limitations of current approaches and the need for tools that support more strategic, counterfactual, and value-laden decisions.\n- **Pluralism and Deliberation**: There is interest in tools that support pluralism, healthy disagreement, and the mapping of viewpoints—moving beyond consensus to richer forms of collective sensemaking.\n- **AI as Epistemic Infrastructure**: The idea of AI as a new layer of epistemic infrastructure recurs, with concerns about public goods, funding, and the risk of epistemic degradation (e.g., deepfakes, persuasive misinformation).\n\n## 3. Expert Perspective\nThe document is notable for its breadth and depth, capturing both optimism and caution. It reflects a sophisticated understanding of the interplay between technology, institutions, and human values. The contributors are aware of the technical, social, and philosophical challenges of building AI systems that genuinely improve epistemic processes, rather than simply accelerating existing dynamics (for better or worse). There is a clear call for interdisciplinary approaches, robust evaluation, and attention to incentive structures. The document serves as a valuable snapshot of current thinking at the intersection of AI, epistemology, and societal governance.\n",
      "summary": "**AI Epistemics** captures a rich, multi-perspective exploration of how AI can enhance collective reasoning, decision-making, and societal trust. Contributors emphasize both the promise and complexity of deploying AI for epistemic improvement, highlighting challenges around trust, coordination, and incentives. The document underscores the need for robust, pluralistic, and value-aligned epistemic infrastructure in an era of rapid technological change."
    }
  ]
}