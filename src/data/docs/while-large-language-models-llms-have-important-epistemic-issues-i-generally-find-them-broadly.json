{
  "id": "while-large-language-models-llms-have-important-epistemic-issues-i-generally-find-them-broadly",
  "slug": "while-large-language-models-llms-have-important-epistemic-issues-i-generally-find-them-broadly",
  "title": "While large language models (LLMs) have important epistemic issues, I generally find them broadly...",
  "author": "Ozzie Gooen",
  "publishedDate": "2025-04-17",
  "url": "https://www.facebook.com/ozzie.gooen/posts/pfbid06RCVM8KFn722YtJ6yJGstWDb5dZmM4S2UQSLJvx47ZGMUBG5tcKdMMrHytWpZB5bl",
  "platforms": ["Facebook"],
  "intendedAgents": [
    "clarity-coach",
    "research-scholar",
    "quantitative-forecaster",
    "ea-impact-evaluator"
  ],
  "content": "Ozzie Gooen's Post  \nApril 6\n\nWhile large language models (LLMs) have important epistemic issues, I generally find them broadly epistemically superior to all humans I've ever known or known of, for my uses.[1]\n\n- Much less biased than other humans I know\n- A strong care for nuance and detail\n- Broadly knowledgeable about a wide variety of topics\n- Doesn't randomly go crazy around politics or other controversial matters\n- I can be fairly sure there's no hidden agendas\n- Very little or no ego\n- Provides a great deal of clarity on its positions\n- Rarely misunderstands my point or gets offended by it\n- I don't need to worry about getting very offended by a research topic\n- Always responds politely (especially if asked to do so)\n- Doesn't get upset if you have spelling/grammar mistakes\n\nAll this, and there's been very little focused epistemic training or evaluation. The systems are still poorly calibrated and there's a ton of low-hanging fruit to address.\n\nAt this point, I'm very hesitant to make any claims publicly that I know Claude would disagree with.\n\n[1] Note that my experience is biased by me using Claude 3.7, with a custom prompt. I'm sure others, especially if they use different prompts, will get different experiences. Also, it's possible that some of the issue is that the LLM is sycophantic, and I understand \"agrees with me\" to be \"has good epistemics.\"\n\n---\n\nNaima Zakaria:  \nAll the models are sycophantic. I want to be able to toggle that off, especially while trying to connect science or health-related data. They tend to make connections to please the user. For me, this becomes confusing sometimes when trying to get to the facts stoically.\n\nOzzie Gooen:  \nHave you tried changing this using the initial/system prompt? I have a prompt for Claude that partially adjusts for this. It's not perfect, but I think it's possible to limit the sycophancy a lot, if you want that.\n\nNaima Zakaria:  \nThank you for the idea. I want to try yours to experiment first. If it works, I'll make my own.\n\nOzzie Gooen:  \nThis is my current one. It includes a bit of information about this, but should probably include more. I had Claude help write it. So I imagine you could do something similar with Claude to focus more on this.\n\n---\n\nPing Yee:  \nThis is counterintuitive to hear given my experience with LLMs so far. LLMs are famous for stating falsehoods and fabrications with absolute confidence, and are spectacularly better at doing this than anyone I've had a conversation with (humans will often hedge or admit uncertainty).\n\nOzzie Gooen:  \nYes, those three points seem likely to me. When I interact with LLMs, I rarely ask for specific sources. When I do, I use the web search, which I find doesn't hallucinate often. Most of my discussion is more high-levelâ€”for example, about my regular work or general topics.",
  "reviews": []
}
