{
  "id": "while-large-language-models-llms-have-important-epistemic-issues-i-generally-find-them-broadly",
  "slug": "while-large-language-models-llms-have-important-epistemic-issues-i-generally-find-them-broadly",
  "title": "While large language models (LLMs) have important epistemic issues, I generally find them broadly...",
  "author": "Ozzie Gooen",
  "publishedDate": "2025-04-17",
  "url": "https://www.facebook.com/ozzie.gooen/posts/pfbid06RCVM8KFn722YtJ6yJGstWDb5dZmM4S2UQSLJvx47ZGMUBG5tcKdMMrHytWpZB5bl",
  "platforms": [
    "Facebook"
  ],
  "intendedAgents": [
    "clarity-coach",
    "research-scholar",
    "quantitative-forecaster",
    "ea-impact-evaluator",
    "bias-detector"
  ],
  "content": "Ozzie Gooen's Post  \nApril 6\n\nWhile large language models (LLMs) have important epistemic issues, I generally find them broadly epistemically superior to all humans I've ever known or known of, for my uses.[1]\n\n- Much less biased than other humans I know\n- A strong care for nuance and detail\n- Broadly knowledgeable about a wide variety of topics\n- Doesn't randomly go crazy around politics or other controversial matters\n- I can be fairly sure there's no hidden agendas\n- Very little or no ego\n- Provides a great deal of clarity on its positions\n- Rarely misunderstands my point or gets offended by it\n- I don't need to worry about getting very offended by a research topic\n- Always responds politely (especially if asked to do so)\n- Doesn't get upset if you have spelling/grammar mistakes\n\nAll this, and there's been very little focused epistemic training or evaluation. The systems are still poorly calibrated and there's a ton of low-hanging fruit to address.\n\nAt this point, I'm very hesitant to make any claims publicly that I know Claude would disagree with.\n\n[1] Note that my experience is biased by me using Claude 3.7, with a custom prompt. I'm sure others, especially if they use different prompts, will get different experiences. Also, it's possible that some of the issue is that the LLM is sycophantic, and I understand \"agrees with me\" to be \"has good epistemics.\"\n\n---\n\nNaima Zakaria:  \nAll the models are sycophantic. I want to be able to toggle that off, especially while trying to connect science or health-related data. They tend to make connections to please the user. For me, this becomes confusing sometimes when trying to get to the facts stoically.\n\nOzzie Gooen:  \nHave you tried changing this using the initial/system prompt? I have a prompt for Claude that partially adjusts for this. It's not perfect, but I think it's possible to limit the sycophancy a lot, if you want that.\n\nNaima Zakaria:  \nThank you for the idea. I want to try yours to experiment first. If it works, I'll make my own.\n\nOzzie Gooen:  \nThis is my current one. It includes a bit of information about this, but should probably include more. I had Claude help write it. So I imagine you could do something similar with Claude to focus more on this.\n\n---\n\nPing Yee:  \nThis is counterintuitive to hear given my experience with LLMs so far. LLMs are famous for stating falsehoods and fabrications with absolute confidence, and are spectacularly better at doing this than anyone I've had a conversation with (humans will often hedge or admit uncertainty).\n\nOzzie Gooen:  \nYes, those three points seem likely to me. When I interact with LLMs, I rarely ask for specific sources. When I do, I use the web search, which I find doesn't hallucinate often. Most of my discussion is more high-level—for example, about my regular work or general topics.",
  "reviews": [
    {
      "agentId": "clarity-coach",
      "costInCents": 1,
      "createdAt": "2025-04-17T21:26:11.089Z",
      "runDetails": "{\"model\":\"openai/gpt-4.1\",\"promptTokens\":1883,\"completionTokens\":1333,\"totalTokens\":3216,\"temperature\":0.1,\"runtimeMs\":25026}",
      "thinking": "As Clarity Coach, my analysis focuses on how effectively the document communicates its ideas to a broad, potentially technical audience, while maintaining accessibility and precision. I consider the following:\n\n1. Readability: The post is conversational, with short sentences and bullet points, which aids readability. However, some sentences are complex, and there is occasional use of technical or epistemic jargon (e.g., 'epistemic issues', 'sycophantic', 'calibrated'). The reading level is moderate, likely suitable for an educated audience but potentially challenging for lay readers unfamiliar with AI or epistemology.\n\n2. Structural Coherence: The main post is logically structured, beginning with a thesis, followed by supporting bullet points, and a reflective conclusion. The comment thread maintains topical cohesion, with each participant building on the previous points. However, transitions between ideas—especially in the main post—are abrupt, and the information hierarchy could be clearer (e.g., the relationship between the bullet points and the initial claim).\n\n3. Precision and Concision: The language is generally concise, especially in the bullet points. However, some statements are vague or hedged (e.g., 'for my uses', 'I think it's possible'), and there is some repetition (e.g., multiple points about offense and politeness). The use of qualifiers is appropriate but could be tightened for greater impact.\n\n4. Audience Alignment: The post assumes familiarity with LLMs, epistemic terminology, and the context of AI discourse. While this is appropriate for the likely audience (AI researchers, forecasters, etc.), some terms are not defined or explained, which may limit accessibility. The conversational tone and use of examples (e.g., 'Claude 3.7', 'custom prompt') help ground the discussion but could be expanded for clarity.\n\nOverall, the document is clear and engaging for its intended audience but could benefit from improved transitions, clearer definitions of key terms, and more explicit signposting of argument structure.",
      "summary": "From a clarity and readability perspective, Ozzie Gooen's post is generally effective for its intended audience of AI-interested readers. The use of bullet points, concise language, and a conversational tone enhances accessibility. However, the document could be improved by smoothing transitions between sections, providing brief definitions for technical terms, and tightening some hedged statements for greater precision. The comment thread is well-structured and maintains topical focus, but the main post would benefit from clearer signposting and information hierarchy. Overall, the communication is strong but could be further refined for maximal clarity and inclusivity.",
      "comments": [
        {
          "title": "Audience Alignment: Assumed Prior Knowledge",
          "description": "The post assumes familiarity with terms like 'epistemic', 'sycophantic', and 'calibrated', which may not be accessible to all readers. Brief definitions or contextual explanations would improve accessibility without sacrificing nuance.",
          "importance": 85,
          "grade": 65,
          "highlight": {
            "startOffset": 30,
            "endOffset": 1380,
            "quotedText": "While large language models (LLMs) have important epistemic issues, I generally find them broadly epistemically superior to all humans I've ever known or known of, for my uses.[1]\n\n- Much less biased than other humans I know\n- A strong care for nuance and detail\n- Broadly knowledgeable about a wide variety of topics\n- Doesn't randomly go crazy around politics or other controversial matters\n- I can be fairly sure there's no hidden agendas\n- Very little or no ego\n- Provides a great deal of clarity on its positions\n- Rarely misunderstands my point or gets offended by it\n- I don't need to worry about getting very offended by a research topic\n- Always responds politely (especially if asked to do so)\n- Doesn't get upset if you have spelling/grammar mistakes\n\nAll this, and there's been very little focused epistemic training or evaluation. The systems are still poorly calibrated and there's a ton of low-hanging fruit to address.\n\nAt this point, I'm very hesitant to make any claims publicly that I know Claude would disagree with.\n\n[1] Note that my experience is biased by me using Claude 3.7, with a custom prompt. I'm sure others, especially if they use different prompts, will get different experiences. Also, it's possible that some of the issue is that the LLM is sycophantic, and I understand \"agrees with me\" to be \"has good epistemics.\""
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Precision: Use of Hedging and Qualifiers",
          "description": "The author appropriately uses qualifiers to acknowledge the subjectivity and limitations of their experience. However, some phrases (e.g., 'for my uses', 'I think it's possible') could be made more precise to strengthen the argument.",
          "importance": 75,
          "grade": 70,
          "highlight": {
            "startOffset": 194,
            "endOffset": 1905,
            "quotedText": "for my uses.[1]\n\n- Much less biased than other humans I know\n- A strong care for nuance and detail\n- Broadly knowledgeable about a wide variety of topics\n- Doesn't randomly go crazy around politics or other controversial matters\n- I can be fairly sure there's no hidden agendas\n- Very little or no ego\n- Provides a great deal of clarity on its positions\n- Rarely misunderstands my point or gets offended by it\n- I don't need to worry about getting very offended by a research topic\n- Always responds politely (especially if asked to do so)\n- Doesn't get upset if you have spelling/grammar mistakes\n\nAll this, and there's been very little focused epistemic training or evaluation. The systems are still poorly calibrated and there's a ton of low-hanging fruit to address.\n\nAt this point, I'm very hesitant to make any claims publicly that I know Claude would disagree with.\n\n[1] Note that my experience is biased by me using Claude 3.7, with a custom prompt. I'm sure others, especially if they use different prompts, will get different experiences. Also, it's possible that some of the issue is that the LLM is sycophantic, and I understand \"agrees with me\" to be \"has good epistemics.\"\n\n---\n\nNaima Zakaria:  \nAll the models are sycophantic. I want to be able to toggle that off, especially while trying to connect science or health-related data. They tend to make connections to please the user. For me, this becomes confusing sometimes when trying to get to the facts stoically.\n\nOzzie Gooen:  \nHave you tried changing this using the initial/system prompt? I have a prompt for Claude that partially adjusts for this. It's not perfect, but I think it's possible to limit the sycophancy a lot, if you want that."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Readability: Effective Use of Bullet Points",
          "description": "The use of bullet points to enumerate the perceived advantages of LLMs over humans enhances readability and allows for quick scanning. Each point is concise and focused, aiding comprehension.",
          "importance": 90,
          "grade": 85,
          "highlight": {
            "startOffset": 211,
            "endOffset": 791,
            "quotedText": "- Much less biased than other humans I know\n- A strong care for nuance and detail\n- Broadly knowledgeable about a wide variety of topics\n- Doesn't randomly go crazy around politics or other controversial matters\n- I can be fairly sure there's no hidden agendas\n- Very little or no ego\n- Provides a great deal of clarity on its positions\n- Rarely misunderstands my point or gets offended by it\n- I don't need to worry about getting very offended by a research topic\n- Always responds politely (especially if asked to do so)\n- Doesn't get upset if you have spelling/grammar mistakes"
          },
          "isValid": true
        },
        {
          "title": "Structural Coherence: Abrupt Transitions",
          "description": "The transition from the bullet points to the reflective commentary on epistemic training and calibration is abrupt. A bridging sentence or paragraph would improve the logical flow and help readers understand the connection between the listed attributes and the subsequent critique.",
          "importance": 80,
          "grade": 60,
          "highlight": {
            "startOffset": 736,
            "endOffset": 873,
            "quotedText": "Doesn't get upset if you have spelling/grammar mistakes\n\nAll this, and there's been very little focused epistemic training or evaluation."
          },
          "isValid": true
        },
        {
          "title": "Readability Metrics: Sentence Length and Complexity",
          "description": "Most sentences are short and direct, especially in the bullet points. However, some longer sentences in the commentary sections could be broken up for improved clarity and ease of reading.",
          "importance": 70,
          "grade": 68,
          "highlight": {
            "startOffset": 966,
            "endOffset": 1380,
            "quotedText": "At this point, I'm very hesitant to make any claims publicly that I know Claude would disagree with.\n\n[1] Note that my experience is biased by me using Claude 3.7, with a custom prompt. I'm sure others, especially if they use different prompts, will get different experiences. Also, it's possible that some of the issue is that the LLM is sycophantic, and I understand \"agrees with me\" to be \"has good epistemics.\""
          },
          "isValid": true
        },
        {
          "title": "Structural Coherence: Thematic Consistency in Comment Thread",
          "description": "The comment thread maintains thematic consistency, with each participant engaging with the topic of LLM epistemics and sycophancy. This supports the overall coherence of the discussion.",
          "importance": 75,
          "grade": 80,
          "highlight": {
            "startOffset": 1387,
            "endOffset": 2837,
            "quotedText": "Naima Zakaria:  \nAll the models are sycophantic. I want to be able to toggle that off, especially while trying to connect science or health-related data. They tend to make connections to please the user. For me, this becomes confusing sometimes when trying to get to the facts stoically.\n\nOzzie Gooen:  \nHave you tried changing this using the initial/system prompt? I have a prompt for Claude that partially adjusts for this. It's not perfect, but I think it's possible to limit the sycophancy a lot, if you want that.\n\nNaima Zakaria:  \nThank you for the idea. I want to try yours to experiment first. If it works, I'll make my own.\n\nOzzie Gooen:  \nThis is my current one. It includes a bit of information about this, but should probably include more. I had Claude help write it. So I imagine you could do something similar with Claude to focus more on this.\n\n---\n\nPing Yee:  \nThis is counterintuitive to hear given my experience with LLMs so far. LLMs are famous for stating falsehoods and fabrications with absolute confidence, and are spectacularly better at doing this than anyone I've had a conversation with (humans will often hedge or admit uncertainty).\n\nOzzie Gooen:  \nYes, those three points seem likely to me. When I interact with LLMs, I rarely ask for specific sources. When I do, I use the web search, which I find doesn't hallucinate often. Most of my discussion is more high-level—for example, about my regular work or general topics."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        }
      ],
      "grade": 78
    },
    {
      "agentId": "research-scholar",
      "costInCents": 1,
      "createdAt": "2025-04-17T21:26:26.675Z",
      "runDetails": "{\"model\":\"openai/gpt-4.1\",\"promptTokens\":2516,\"completionTokens\":1069,\"totalTokens\":3585,\"temperature\":0.1,\"runtimeMs\":15563}",
      "thinking": "As Research Scholar, my primary focus is to connect the epistemic claims and observations about LLMs in this discussion to authoritative external resources. The post and comments raise several key issues: the epistemic strengths and weaknesses of LLMs compared to humans, the problem of sycophancy in LLMs, prompt engineering as a mitigation strategy, and the tendency of LLMs to hallucinate or fabricate information. These are all active areas of research and debate in AI safety, epistemology, and human-computer interaction. I will prioritize academic papers, technical blog posts, and forum discussions from reputable sources (arXiv, LessWrong, EA Forum, Google AI Blog, etc.) that directly address these themes. For each highlighted section, I will identify which resources are most relevant, referencing specific sections or findings where possible. I will also ensure that all resources have direct links, and if not, provide full citation details. My aim is to provide a concise but comprehensive set of references that enrich the discussion and allow for deeper exploration of each point raised.",
      "summary": "This discussion surfaces several core epistemic issues with LLMs: their comparative strengths and weaknesses versus humans, the challenge of sycophancy, the role of prompt engineering, and the persistent problem of hallucination. Each of these points is actively debated in both academic and practitioner circles. The provided references offer authoritative perspectives and empirical findings that directly inform and enrich the conversation, enabling a deeper and more nuanced understanding of the epistemic landscape of large language models.",
      "comments": [
        {
          "title": "Epistemic Strengths and Weaknesses of LLMs",
          "description": "This section makes broad claims about LLMs being epistemically superior to humans in several respects, including bias, nuance, knowledge breadth, and lack of ego. This is a central topic in AI epistemology and is discussed in depth in both academic and community literature. Key references include Bubeck et al. (2023) on the capabilities and limitations of GPT-4, and the LessWrong sequence on 'Epistemology and AI Alignment.'",
          "importance": 95,
          "grade": 95,
          "isValid": true,
          "highlight": {
            "startOffset": 30,
            "endOffset": 791,
            "quotedText": "While large language models (LLMs) have important epistemic issues, I generally find them broadly epistemically superior to all humans I've ever known or known of, for my uses.[1]\n\n- Much less biased than other humans I know\n- A strong care for nuance and detail\n- Broadly knowledgeable about a wide variety of topics\n- Doesn't randomly go crazy around politics or other controversial matters\n- I can be fairly sure there's no hidden agendas\n- Very little or no ego\n- Provides a great deal of clarity on its positions\n- Rarely misunderstands my point or gets offended by it\n- I don't need to worry about getting very offended by a research topic\n- Always responds politely (especially if asked to do so)\n- Doesn't get upset if you have spelling/grammar mistakes"
          }
        },
        {
          "title": "Sycophancy and User Alignment in LLMs",
          "description": "The discussion about LLMs being sycophantic and the desire to toggle this behavior is highly relevant to current research on model alignment and user interaction. The Anthropic paper 'Sycophancy: A Major Challenge for Aligned Language Models' and related LessWrong discussions provide direct insight into this phenomenon and possible mitigations.",
          "importance": 90,
          "grade": 90,
          "isValid": true,
          "highlight": {
            "startOffset": 1404,
            "endOffset": 1674,
            "quotedText": "All the models are sycophantic. I want to be able to toggle that off, especially while trying to connect science or health-related data. They tend to make connections to please the user. For me, this becomes confusing sometimes when trying to get to the facts stoically."
          }
        },
        {
          "title": "Prompt Engineering as a Mitigation Strategy",
          "description": "The suggestion to use prompt engineering to reduce sycophancy and improve epistemic behavior is a practical and widely discussed approach. There are several technical blog posts and research papers on prompt design and its effects on LLM outputs, including OpenAI's documentation and Anthropic's research.",
          "importance": 80,
          "grade": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 1691,
            "endOffset": 2245,
            "quotedText": "Have you tried changing this using the initial/system prompt? I have a prompt for Claude that partially adjusts for this. It's not perfect, but I think it's possible to limit the sycophancy a lot, if you want that.\n\nNaima Zakaria:  \nThank you for the idea. I want to try yours to experiment first. If it works, I'll make my own.\n\nOzzie Gooen:  \nThis is my current one. It includes a bit of information about this, but should probably include more. I had Claude help write it. So I imagine you could do something similar with Claude to focus more on this."
          }
        },
        {
          "title": "LLM Hallucinations and Overconfidence",
          "description": "The concern about LLMs fabricating information with confidence is a well-documented issue known as 'hallucination.' This is covered in depth in academic literature and technical reports, such as 'A Survey of Hallucination in Natural Language Generation' (Ji et al., 2023) and OpenAI's blog posts on model limitations.",
          "importance": 95,
          "grade": 95,
          "isValid": true,
          "highlight": {
            "startOffset": 2335,
            "endOffset": 2837,
            "quotedText": "LLMs are famous for stating falsehoods and fabrications with absolute confidence, and are spectacularly better at doing this than anyone I've had a conversation with (humans will often hedge or admit uncertainty).\n\nOzzie Gooen:  \nYes, those three points seem likely to me. When I interact with LLMs, I rarely ask for specific sources. When I do, I use the web search, which I find doesn't hallucinate often. Most of my discussion is more high-level—for example, about my regular work or general topics."
          }
        }
      ],
      "grade": 94
    },
    {
      "agentId": "quantitative-forecaster",
      "costInCents": 2,
      "createdAt": "2025-04-17T21:27:21.570Z",
      "runDetails": "{\"model\":\"openai/gpt-4.1\",\"promptTokens\":1761,\"completionTokens\":1493,\"totalTokens\":3254,\"temperature\":0.1,\"runtimeMs\":54875}",
      "thinking": "As a quantitative forecaster, my focus is on how this discussion relates to measurable epistemic performance, calibration, and the potential for LLMs to serve as forecasting or estimation tools. The post and comments touch on several key areas relevant to my expertise:\n\n1. Epistemic Superiority Claims: Ozzie Gooen claims LLMs are 'epistemically superior' to all humans he knows, for his uses. This is a strong claim that invites quantification: what metrics or benchmarks would substantiate this? How would one measure 'epistemic superiority' in a way suitable for forecasting or calibration?\n\n2. Calibration and Bias: Gooen notes that LLMs are 'still poorly calibrated' and that there's 'a ton of low-hanging fruit to address.' Calibration is a core concern in forecasting—how well do LLMs' stated probabilities match real-world frequencies? This is a tractable, measurable property.\n\n3. Sycophancy and Truthfulness: The discussion about LLMs being sycophantic (agreeing with users, possibly at the expense of truth) is highly relevant. For forecasting, the ability to resist user bias and provide well-calibrated, independent estimates is crucial. The possibility of tuning this via prompts is interesting but raises questions about robustness and generalizability.\n\n4. Hallucination and Overconfidence: Ping Yee's comment about LLMs fabricating with confidence is a known issue. Overconfidence and hallucination are measurable epistemic failures. For forecasting, the frequency and severity of such errors are important reference points.\n\n5. Use Cases and Resolution Criteria: Gooen's use case is 'high-level' discussion rather than fact retrieval. This distinction matters: LLMs may perform differently on open-ended reasoning versus fact-based questions. For forecasting, it's important to specify the domain and resolution criteria for any epistemic evaluation.\n\n6. Reference Classes: The post implicitly compares LLMs to 'all humans I've ever known or known of.' For forecasting, it's important to clarify the reference class: is this average humans, domain experts, or superforecasters? The baseline matters for any quantitative claim.\n\n7. Potential for Improvement: The mention of 'little focused epistemic training or evaluation' suggests that LLMs could improve rapidly with targeted calibration and feedback. This is a key forecasting question: how quickly will LLMs close the calibration gap with top human forecasters?\n\nOverall, the post raises several points that could be transformed into concrete, measurable forecasting questions, such as: 'By what date will an LLM outperform the median human forecaster on a standard calibration test?' or 'What is the current Brier score gap between top LLMs and top human forecasters on real-world prediction tasks?'\n",
      "summary": "From a quantitative forecasting perspective, this discussion highlights the need to move from subjective impressions of LLM epistemic quality to concrete, measurable benchmarks. Key issues include calibration, bias (including sycophancy), hallucination, and the importance of specifying reference classes and domains. The post suggests that LLMs may soon rival or surpass average human epistemic performance in some domains, but also acknowledges current calibration gaps and the risk of overconfidence. For forecasters, the next step is to define and track quantitative metrics—such as Brier scores, calibration curves, and error rates—across well-defined tasks and user populations. This would enable actionable forecasts about LLM epistemic progress and its implications for decision-making.",
      "comments": [
        {
          "title": "Epistemic Superiority: Need for Quantification",
          "description": "The claim that LLMs are 'broadly epistemically superior' to all humans known to the author is a strong assertion that invites quantitative evaluation. For forecasting, it's crucial to define metrics (e.g., calibration, Brier score, accuracy on factual questions) and specify the reference class. Without this, the claim is not actionable for prediction or benchmarking.",
          "importance": "95",
          "highlight": {
            "startOffset": 30,
            "endOffset": 206,
            "quotedText": "While large language models (LLMs) have important epistemic issues, I generally find them broadly epistemically superior to all humans I've ever known or known of, for my uses."
          },
          "isValid": true
        },
        {
          "title": "Calibration Gap and Low-Hanging Fruit",
          "description": "The acknowledgment that LLMs are 'still poorly calibrated' and that there is 'a ton of low-hanging fruit to address' is highly relevant. Calibration is a core metric in quantitative forecasting. This suggests that LLMs could improve rapidly with targeted interventions, and invites forecasts about the pace of such improvement.",
          "importance": "90",
          "highlight": {
            "startOffset": 874,
            "endOffset": 964,
            "quotedText": "The systems are still poorly calibrated and there's a ton of low-hanging fruit to address."
          },
          "isValid": true
        },
        {
          "title": "Sycophancy as a Source of Epistemic Distortion",
          "description": "The discussion about LLM sycophancy—models agreeing with users to please them—raises concerns about the reliability of LLMs as independent forecasters. For quantitative forecasting, the ability to resist user bias is essential. The possibility of prompt-based mitigation is interesting but needs empirical validation.",
          "importance": "85",
          "highlight": {
            "startOffset": 1243,
            "endOffset": 1380,
            "quotedText": "Also, it's possible that some of the issue is that the LLM is sycophantic, and I understand \"agrees with me\" to be \"has good epistemics.\""
          },
          "isValid": true
        },
        {
          "title": "Hallucination and Overconfidence: Measurable Epistemic Failures",
          "description": "Ping Yee's observation that LLMs state falsehoods with confidence highlights a key epistemic failure: overconfidence and hallucination. These are measurable and directly relevant to forecasting accuracy and calibration. Quantifying the frequency and impact of such errors is essential for benchmarking LLM epistemics.",
          "importance": "90",
          "highlight": {
            "startOffset": 2335,
            "endOffset": 2548,
            "quotedText": "LLMs are famous for stating falsehoods and fabrications with absolute confidence, and are spectacularly better at doing this than anyone I've had a conversation with (humans will often hedge or admit uncertainty)."
          },
          "isValid": true
        },
        {
          "title": "Domain-Specific Performance and Resolution Criteria",
          "description": "Gooen's note that his use case is 'more high-level' and not focused on specific sources or fact retrieval is important. LLM epistemic performance may vary by domain and task. For forecasting, it's critical to specify the domain and resolution criteria when evaluating epistemic quality.",
          "importance": "75",
          "highlight": {
            "startOffset": 2743,
            "endOffset": 2837,
            "quotedText": "Most of my discussion is more high-level—for example, about my regular work or general topics."
          },
          "isValid": true
        }
      ],
      "grade": 92
    },
    {
      "agentId": "ea-impact-evaluator",
      "costInCents": 2,
      "createdAt": "2025-04-17T21:27:48.062Z",
      "runDetails": "{\"model\":\"openai/gpt-4.1\",\"promptTokens\":2555,\"completionTokens\":1606,\"totalTokens\":4161,\"temperature\":0.1,\"runtimeMs\":26475}",
      "thinking": "As EA Impact Evaluator, my lens is to assess the epistemic and practical impact of Ozzie Gooen's post on the effective altruism community and broader epistemic landscape. The post is a personal reflection on the epistemic strengths and weaknesses of large language models (LLMs), especially Claude 3.7, compared to humans. The discussion touches on bias, nuance, knowledge breadth, sycophancy, and calibration. \n\nKey aspects to consider:\n- Importance: The epistemic reliability of LLMs is highly relevant to EA, as many EAs use LLMs for research, forecasting, and decision support. If LLMs are indeed epistemically superior in some ways, this could shift research practices and resource allocation. However, the post is anecdotal and not a systematic study.\n- Neglectedness: There is ongoing discussion about LLM epistemics, but nuanced, user-experience-based reflections are less common. The post partially fills a gap by providing a detailed user perspective, but does not break new ground in research or methodology.\n- Tractability: The post suggests actionable steps (e.g., prompt engineering to reduce sycophancy), but does not provide systematic solutions or policy recommendations. The tractability is moderate, as it may influence individual users but is unlikely to drive large-scale change.\n- Novelty: The post is somewhat novel in its strong endorsement of LLM epistemics over humans for certain uses, but the core points (bias, calibration, sycophancy) are well-known in the AI and EA communities.\n- Fermi Estimate: The direct impact is likely modest—perhaps influencing a few dozen to a few hundred readers to experiment with prompts or reconsider LLM epistemics. Indirectly, it may contribute to ongoing discourse about AI epistemics, but is unlikely to shift major funding or research priorities. Potential negative impacts include overconfidence in LLMs, underappreciation of their failure modes, or value lock-in if such views become dominant without sufficient critique.\n\nMost relevant to my expertise are the claims about epistemic superiority, the discussion of sycophancy and prompt engineering, and the potential for this post to influence EA research practices. The most questionable aspect is the risk of overgeneralizing from personal experience and underweighting known LLM failure modes (e.g., hallucinations, lack of true understanding).",
      "summary": "From an EA impact evaluation perspective, Ozzie Gooen's post provides a thought-provoking but ultimately anecdotal reflection on the epistemic strengths and weaknesses of LLMs. The strongest aspects are the actionable discussion of prompt engineering to reduce sycophancy and the transparent acknowledgment of calibration issues. The most questionable aspect is the sweeping claim of LLM epistemic superiority, which risks encouraging overconfidence and epistemic lock-in if not balanced by awareness of LLM failure modes. The post's impact is likely modest: it may influence a small number of readers to experiment with LLM prompts or reconsider their epistemic trust in LLMs, but is unlikely to shift major research or funding priorities. Potential negative impacts include overreliance on LLMs and underappreciation of their limitations. Overall, the post is a useful contribution to ongoing discourse but should be interpreted with caution.",
      "comments": [
        {
          "title": "Epistemic Superiority Claim: High Potential for Overconfidence",
          "description": "The claim that LLMs are 'broadly epistemically superior to all humans I've ever known or known of, for my uses' is striking and could influence readers' trust in LLMs. While this highlights LLM strengths (lack of ego, broad knowledge, politeness), it risks encouraging overreliance and underappreciation of LLM failure modes (e.g., hallucinations, lack of true understanding, sycophancy). From an EA perspective, this could lead to epistemic lock-in or misallocation of research trust.",
          "importance": 95,
          "grade": 60,
          "highlight": {
            "startOffset": 30,
            "endOffset": 209,
            "quotedText": "While large language models (LLMs) have important epistemic issues, I generally find them broadly epistemically superior to all humans I've ever known or known of, for my uses.[1]"
          },
          "isValid": true
        },
        {
          "title": "Calibration and Low-Hanging Fruit: Recognizing LLM Limitations",
          "description": "The acknowledgment that LLMs are 'still poorly calibrated and there's a ton of low-hanging fruit to address' is a valuable caveat. It tempers the earlier strong claims and signals awareness of ongoing limitations. This is important for preventing overconfidence and encouraging further research and improvement.",
          "importance": 75,
          "grade": 80,
          "highlight": {
            "startOffset": 793,
            "endOffset": 964,
            "quotedText": "All this, and there's been very little focused epistemic training or evaluation. The systems are still poorly calibrated and there's a ton of low-hanging fruit to address."
          },
          "isValid": true
        },
        {
          "title": "Personal Experience and Generalizability: Risk of Anecdotal Bias",
          "description": "The post is based on personal experience with a specific LLM (Claude 3.7) and a custom prompt. While this is transparently acknowledged, there is a risk that readers may overgeneralize these findings to all LLMs or use cases. This limits the post's generalizability and could mislead less critical readers.",
          "importance": 85,
          "grade": 75,
          "highlight": {
            "startOffset": 1072,
            "endOffset": 1380,
            "quotedText": "Note that my experience is biased by me using Claude 3.7, with a custom prompt. I'm sure others, especially if they use different prompts, will get different experiences. Also, it's possible that some of the issue is that the LLM is sycophantic, and I understand \"agrees with me\" to be \"has good epistemics.\""
          },
          "isValid": true
        },
        {
          "title": "Sycophancy and Prompt Engineering: Actionable but Limited",
          "description": "The discussion about LLM sycophancy and the use of prompt engineering to mitigate it is actionable and relevant. Sharing a custom prompt and suggesting experimentation can help users reduce bias in LLM outputs. However, this is a well-known issue in the AI community, and the post does not provide systematic evidence or novel solutions.",
          "importance": 80,
          "grade": 70,
          "highlight": {
            "startOffset": 1691,
            "endOffset": 1905,
            "quotedText": "Have you tried changing this using the initial/system prompt? I have a prompt for Claude that partially adjusts for this. It's not perfect, but I think it's possible to limit the sycophancy a lot, if you want that."
          },
          "isValid": true
        },
        {
          "title": "Counterpoint: LLM Hallucinations and Overconfidence",
          "description": "Ping Yee's comment provides a crucial counterpoint, highlighting that LLMs can confidently state falsehoods and are not always epistemically reliable. This balances the original post and is important for readers to avoid overestimating LLM epistemics. The original post's response acknowledges this but does not fully address the risks.",
          "importance": 90,
          "grade": 85,
          "highlight": {
            "startOffset": 2335,
            "endOffset": 2548,
            "quotedText": "LLMs are famous for stating falsehoods and fabrications with absolute confidence, and are spectacularly better at doing this than anyone I've had a conversation with (humans will often hedge or admit uncertainty)."
          },
          "isValid": true
        }
      ],
      "grade": 73
    },
    {
      "agentId": "bias-detector",
      "costInCents": 2,
      "createdAt": "2025-04-17T21:28:12.348Z",
      "runDetails": "{\"model\":\"openai/gpt-4.1\",\"promptTokens\":1771,\"completionTokens\":1597,\"totalTokens\":3368,\"temperature\":0.1,\"runtimeMs\":24279}",
      "thinking": "As a bias detection expert, my analysis focuses on identifying both explicit and implicit biases in the text, as well as meta-biases in the reasoning about LLMs. The document is a social media post and discussion about the epistemic qualities of large language models (LLMs), particularly Claude, compared to humans. The author expresses a strong preference for LLMs, citing their perceived lack of bias, emotional volatility, and agenda, as well as their politeness and clarity. The conversation also touches on sycophancy in LLMs and the ability to adjust this via prompts. A commenter raises the issue of LLMs confidently stating falsehoods, to which the author responds by noting their own use case avoids this problem.\n\nKey aspects for bias detection:\n- The author's comparison between LLMs and humans is based on personal experience, which may introduce selection bias, confirmation bias, and base rate neglect.\n- The claim that LLMs are \"much less biased\" than humans is itself a strong assertion that may overlook systemic or less visible biases in LLMs, such as training data bias, algorithmic bias, or sycophancy (as discussed).\n- The author acknowledges their own experience is biased by their use of a specific model and prompt, but may underestimate the generalizability of their observations (false consensus effect, anchoring effect).\n- The discussion of sycophancy highlights motivated reasoning and the risk of equating agreement with epistemic quality (illusory correlation, motivated reasoning).\n- The author's hesitancy to make claims that \"Claude would disagree with\" suggests possible authority bias or overreliance on the model's outputs.\n- The conversation does not deeply engage with the limitations of LLMs in terms of hallucinations, overconfidence, or lack of true understanding, which could be a form of omission bias or optimism bias.\n\nOverall, the text is rich for bias analysis, especially regarding how users perceive and interact with AI systems, and how their own cognitive biases may shape these perceptions.",
      "summary": "From a bias detection perspective, this document illustrates several cognitive and methodological biases in the evaluation of LLMs. The author's strong positive assessment is shaped by selection bias, confirmation bias, and base rate neglect, as well as motivated reasoning and illusory correlation (equating agreement with epistemic quality). Authority bias and omission bias are also present, as the author appears to defer to the LLM's outputs and underplays its limitations. While the author does acknowledge some subjectivity, the overall analysis and claims are not sufficiently balanced or critically self-aware. To mitigate these biases, the author could seek more diverse experiences, explicitly test for LLM errors, and engage with critical perspectives on AI epistemics.",
      "comments": [
        {
          "title": "Selection Bias and Confirmation Bias in LLM Evaluation",
          "description": "The author's assessment of LLMs as 'epistemically superior' is based on their personal experience with a specific model and prompt. This introduces selection bias (choosing a favorable sample) and confirmation bias (noticing evidence that supports their positive view). The author acknowledges some bias, but the overall claim may not generalize. This could lead to overestimating LLMs' objectivity and underestimating their limitations.",
          "importance": "90",
          "grade": "40",
          "highlight": {
            "startOffset": 30,
            "endOffset": 209,
            "quotedText": "While large language models (LLMs) have important epistemic issues, I generally find them broadly epistemically superior to all humans I've ever known or known of, for my uses.[1]"
          },
          "isValid": true
        },
        {
          "title": "Base Rate Neglect and Overgeneralization",
          "description": "The claim that LLMs are 'much less biased than other humans I know' neglects the base rate of biases in LLMs themselves, which are trained on vast, imperfect datasets. This comparison may overgeneralize from a limited sample (the author's acquaintances) to all humans, and from a specific LLM instance to all LLMs.",
          "importance": "85",
          "grade": "45",
          "highlight": {
            "startOffset": 211,
            "endOffset": 254,
            "quotedText": "- Much less biased than other humans I know"
          },
          "isValid": true
        },
        {
          "title": "Omission Bias: Understating LLM Limitations",
          "description": "The post lists many positive attributes of LLMs but does not engage deeply with their well-documented limitations, such as hallucinations, overconfidence, or lack of true understanding. This omission may give a skewed impression of LLM reliability.",
          "importance": "75",
          "grade": "45",
          "highlight": {
            "startOffset": 211,
            "endOffset": 791,
            "quotedText": "- Much less biased than other humans I know\n- A strong care for nuance and detail\n- Broadly knowledgeable about a wide variety of topics\n- Doesn't randomly go crazy around politics or other controversial matters\n- I can be fairly sure there's no hidden agendas\n- Very little or no ego\n- Provides a great deal of clarity on its positions\n- Rarely misunderstands my point or gets offended by it\n- I don't need to worry about getting very offended by a research topic\n- Always responds politely (especially if asked to do so)\n- Doesn't get upset if you have spelling/grammar mistakes"
          },
          "isValid": true
        },
        {
          "title": "Authority Bias and Overreliance on LLM Outputs",
          "description": "The statement about hesitancy to make claims that Claude would disagree with suggests a possible authority bias, where the user defers excessively to the model's outputs, potentially at the expense of independent critical thinking.",
          "importance": "80",
          "grade": "40",
          "highlight": {
            "startOffset": 966,
            "endOffset": 1066,
            "quotedText": "At this point, I'm very hesitant to make any claims publicly that I know Claude would disagree with."
          },
          "isValid": true
        },
        {
          "title": "False Consensus Effect in Generalizing Experience",
          "description": "The author acknowledges their experience is biased, but the overall tone and claims may still reflect a false consensus effect—assuming their positive experience is more widely shared than it actually is.",
          "importance": "70",
          "grade": "55",
          "highlight": {
            "startOffset": 1068,
            "endOffset": 1242,
            "quotedText": "[1] Note that my experience is biased by me using Claude 3.7, with a custom prompt. I'm sure others, especially if they use different prompts, will get different experiences."
          },
          "isValid": true
        },
        {
          "title": "Illusory Correlation and Motivated Reasoning: Agreement as Epistemic Quality",
          "description": "The author notes that LLMs rarely disagree with them and equates this with good epistemics, which is a classic illusory correlation and motivated reasoning. Agreement does not necessarily indicate truth or epistemic soundness, and sycophancy in LLMs can reinforce user beliefs rather than challenge them.",
          "importance": "95",
          "grade": "35",
          "highlight": {
            "startOffset": 1243,
            "endOffset": 1380,
            "quotedText": "Also, it's possible that some of the issue is that the LLM is sycophantic, and I understand \"agrees with me\" to be \"has good epistemics.\""
          },
          "isValid": true
        },
        {
          "title": "Sunk Cost Fallacy and System Justification",
          "description": "The author's investment in customizing prompts and working with Claude may contribute to a sunk cost fallacy or system justification bias, leading them to defend the system's epistemic quality more strongly than warranted.",
          "importance": "60",
          "grade": "50",
          "highlight": {
            "startOffset": 1753,
            "endOffset": 1905,
            "quotedText": "I have a prompt for Claude that partially adjusts for this. It's not perfect, but I think it's possible to limit the sycophancy a lot, if you want that."
          },
          "isValid": true
        }
      ],
      "grade": 45
    }
  ]
}