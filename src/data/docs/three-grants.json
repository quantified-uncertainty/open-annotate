{
  "id": "three-grants",
  "slug": "three-grants",
  "title": "Three Grants We Liked (Just the Positive Ones)",
  "author": "Unknown Author",
  "publishedDate": "2025-04-18",
  "platforms": [],
  "intendedAgents": [
    "bias-detector",
    "clarity-coach",
    "research-scholar"
  ],
  "content": "# Writing Session\n\nFor the past two years, Manifund has run a regranting program. We allocate annual donation budgets (from $50k-$400k) to selected experts (\"regrantors\"), who in turn choose great projects to fund. We'll be kicking off the 2025 program soon! In preparation, we're reviewing some of our favorite regrants to date — as well as some regrants we felt were less well-suited for this program. How we think about what makes a great regrant includes considerations such as \"is this project in an early stage where some initial funding could really help?\", \"could OpenPhil fund this instead?\" and \"does this regrant effectively leverage our regrantors' experience and expertise?\"\n\nWe think our regranting program is one of the best opportunities for donors who: care about AI safety, want to seed ambitious new projects, and like transparency and moving fast. If you may want to fund our 2025 program, please contact austin@manifund.org!\n\nAbout the author: Jesse Richardson recently joined Manifund after working at Mila - Quebec AI Institute, and also has a background in trading on prediction markets. This post is essentially Jesse's hot takes about various regrants, and is not intended to be definitive or particularly high-confidence.\n\n## Three awesome AI Safety regrants…\n\n### https://manifund.org/projects/scoping-developmental-interpretability-xg55b33wsfc - the first funding for Timaeus, accelerating research by months\n\nThis regrant was made in late 2023 to Jesse Hoogland and the rest of what is now the Timaeus team, for the purposes of exploring Developmental Interpretability (DevInterp) as a new AI alignment research agenda. Four of our regrantors — Evan Hubinger, Rachel Weinberg, Marcus Abramovitch and Ryan Kidd — made regrants to this project, for a total of $143,200. Evan had previously mentored Jesse Hoogland as part of MATS, and therefore had some additional context about the value of funding Hoogland's future research. This is the sweet spot for regranting: donors may have the public information that Evan Hubinger is an expert who does good work, and could potentially donate to him/his work on that basis, but regranting allows them to specifically leverage his private information about other valuable projects, such as DevInterp.\n\nBack to the grant itself; success for this project looked like determining whether DevInterp was a viable agenda to move forward with, rather than producing some seminal research outputs. I recommend reading more about DevInterp if you're interested, but my shallow understanding is that it aims to use insights from Singular Learning Theory (SLT) to make progress on AI alignment through interpretability, with a specific focus on how phase transitions in the training process lead to internal structure in neural networks.\n\nI'm not well placed to form an inside view on how likely DevInterp was/is to succeed, but this proposed research agenda had numerous things going for it:\nit was novel; the application of SLT to alignment was largely unexplored prior to this work,\nit seemed to be well thought out; the write-up on LessWrong included plenty of detail about why we might expect phase transitions to be a big deal and how this would relate to alignment, as well as a solid plan for the next six months,\nit had an element of \"big if true\" i.e. it may be unlikely that the strong version of the DevInterp thesis is true, but this research has potential to make meaningful progress on AI alignment if it is\n\nThese are all markers of projects I am excited to see funded through Manifund regranting. In addition to our four regrantors, this agenda was also endorsed by capable AI safety researchers such as Vanessa Kosoy and John Wentworth, and it makes sense to update on their judgement.\n\nIn addition to the agenda itself, I think this was a good team to bet on for this kind of work; they seem capable and have relevant experience e.g. ML research, and running the 2023 SLT & Alignment Summit.\n\nThis regrant is a strong example of where Manifund's regranting program can have the biggest impact: being early to support new projects & organizations, and thereby providing strong signals to other funders as well as some runway for these organizations to move quickly. In this case, Manifund's early funding helped Hoogland's team get off the ground, and they subsequently started a new organization (Timaeus) and received significantly more funding from other sources, such as $500,000 from the Survival & Flourishing Fund. It's probable that they would've gotten this other funding regardless, but not guaranteed, and I'm happy that Manifund was able to help bring Timaeus into existence several months sooner and with increased financial security. Jesse notes:\n\nGetting early support from Manifund made a real difference for us. This was the first funding we received for research and meant that we could start months earlier than we otherwise would have. The fact that it was public meant other funders could easily see who was backing our work and why. That transparency helped us build momentum and credibility for developmental interpretability research when it was still a new idea. I'm pretty sure it played a significant role in us securing later funding through SFF and other grantmakers.\n\nIn terms of concrete outcomes, there's a lot to be happy with here. Timaeus and its collaborators have put out numerous papers on DevInterp since this regrant was made, and it seems that DevInterp's key insight around the existence and significance of phase transitions has been validated. My sense is that the question of whether DevInterp is a worthwhile alignment research agenda to pursue has been successfully answered in the affirmative. It's also nice to see strong outreach and engagement with the research community on the part of Timaeus: November 2023 saw the first DevInterp conference, and they've given talks at OpenAI, Anthropic, and DeepMind.\n\n### https://manifund.org/projects/support-for-deep-coverage-of-china-and-ai - reporting on DeepSeek, ahead of the curve\n\nAcross 2023 & 2024, Manifund regrantors Joel Becker and Evan Hubinger granted a total of $37,000 to ChinaTalk, a newsletter and podcast that covers China, technology, and US-China relations. ChinaTalk has over 50,000 subscribers and is also notable for the quality of its coverage and the praise and attention it receives from elites and policymakers.\n\nBefore this regrant, ChinaTalk had been run by Jordan Schneider and Caithrin Rintoul, both part-time, on a budget of just $35,000/year. What they were able to accomplish in that time with those limited resources was impressive, and I believe merited additional funding, even just to allow Jordan to work on this full-time. More funding would also have meant ChinaTalk bringing on a full-time fellow who, per Jordan, \"would be, to my knowledge, the only researcher in the English-speaking world devoted solely to covering China and AI safety\". ChinaTalk has since received further funding and is in the process of growing to five full-time employees, but we would've loved for this to happen sooner through an expanded regranting program.\n\nEven putting aside the specific track record of ChinaTalk, it seems clear to me that the intersection of China and AI safety is an incredibly important area to cover, and at a high level it is very valuable to fund organizations that are doing this kind of work. It can be hard to imagine plausible scenarios of how the next decade goes well with respect to AI that don't run through US-China relations, and I am persuaded by Jordan's case that the amount of energy currently being expended on this is grossly inadequate.\n\nSince the first regrant was made, ChinaTalk's Substack audience has grown from 26,000 subscribers to 51,000 and they've put out regular high-quality content, including an English translation of an interview with DeepSeek CEO Liang Wenfeng, coverage of chip policy, and what important 2024 elections in the US and Taiwan mean for China. The ChinaTalk team has expanded to six people, allowing for a greater diversity and quantity of coverage, including YouTube videos. Jordan has also announced plans for launching a think tank—ChinaTalk Institute—this year, in a similar vein to IFP.\n\nAmong their varied coverage, I was particularly impressed to see how ChinaTalk was ahead of the curve on covering the rise of DeepSeek, while most of the West seemed to be taken by total surprise in January 2025. As a trader and forecaster, this advance insight might have been worth a lot of money to me through anticipating the market freakout, which suggests that I should pay more attention to ChinaTalk in the future, as their work is seriously valuable.\n\nOverall ChinaTalk has continued on the strong trajectory it was on in late 2023, and it was great that Manifund was able to support ChinaTalk in this success. For more information about why this grant was likely good ex ante, I encourage you to look at regrantor Joel Becker's comment on the subject. Joel's detail about why ChinaTalk was at the time insufficiently funded:\n\nPhilanthropists are scared to touch China, in part because of lack of expertise and in part for political reasons. Advertisers can be nervous for similar reasons… Jordan was hoping to support this work through subscriptions only.\n\nmakes me more optimistic that this regrant was very much the kind of thing the program should be doing: plugging holes in the funding landscape.\n\n### https://manifund.org/projects/shallow-review-of-ai-safety-2024 - two quick regrants, followed by OpenPhil support\n\nGavin Leech previously co-wrote https://www.lesswrong.com/posts/zaaGsFBeDTpCsYHef/shallow-review-of-live-agendas-in-alignment-and-safety in 2023, which was well-received and generally considered to be a useful resource for people looking to get a top-level picture of AI safety research. Given that it was intended to be a shallow review, this post has a lot of helpful detail and links about the ins and outs of various research agendas, e.g., the amount of resources currently devoted to each, and notable criticisms.\n\nLast year he sought funding to create an updated 2024 version of this post. He received $9,000 from Manifund regrantors Neel Nanda and Ryan Kidd, as well as $12,000 from other donors through the Manifund site.\n\nBig picture, I'm convinced that there should be an accessible and up-to-date resource of this kind; for people who are starting out in AI safety and don't know anything, for funders trying to get a sense of the landscape, or for anyone else who might need it. In 2022 I was at a stage where I wanted to contribute to AI safety but didn't know anything about it and was unsure where to start, and I would've likely found Gavin's review useful, in addition to the other resources that existed. Based on this, Gavin's record in a variety of fields, and the fact that the 2023 version seems to have been fairly well-executed, I think this regrant looked very promising.\n\nIn terms of what was produced, the new post (https://www.lesswrong.com/posts/fAW6RXLKTLHC3WXkS/shallow-review-of-technical-ai-safety-2024) came out in December 2024 and appears to be similarly comprehensive to the 2023 version, although it has gotten less attention (~half the upvotes on LessWrong and not curated). That's probably a bit worse of an outcome than I would've hoped for, but I still would have endorsed this grant had I known the outcome in advance. Presumably the updated version of a post is less eye-catching than the original, while still being necessary.\n\nThe way this was funded also shows the advantages of the Manifund regranting program. Gavin asked for between $8,000 (MVP version) and $17,000 (high-end version) and was quickly funded for the MVP by Neel and Ryan. He then got an additional $5,000 from OpenPhil, after Matt Putz at OpenPhil learned about this proposal via our EA Forum post; and a further $12,000 from other donors. I am happy with how the regranting program is both able to provide the small amount of funding to get a project off the ground, and increase visibility of that project so that other donors can step in and fund it to a greater extent. A couple of small negatives: (1) regrantor Neel Nanda is less optimistic than I am that this was a particularly good grant and (2) the high-end version was supposed to include a \"glossy formal report optimised for policy people\" which did not end up getting made (OpenPhil opted against funding it), however the excess money is instead going towards the 2025 edition. I look forward to it!",
  "reviews": [
    {
      "agentId": "bias-detector",
      "createdAt": "2025-04-18T23:01:06.319Z",
      "costInCents": 0,
      "comments": [
        {
          "title": "Selection Bias 🎯",
          "description": "The document explicitly states that it is reviewing only positive grant outcomes: *'reviewing some of our favorite regrants to date — as well as some regrants we felt were less well-suited for this program.'* However, the actual content focuses exclusively on successful or 'awesome' grants, as highlighted in the section title 'Three awesome AI Safety regrants…'. This selective reporting creates a skewed impression of the program's overall effectiveness and may mislead readers about the typical success rate. To mitigate, the document should include a balanced sample of both positive and negative outcomes, or at least provide summary statistics on the full distribution of results.",
          "importance": 95,
          "grade": 30,
          "isValid": false,
          "highlight": {
            "startOffset": 281,
            "endOffset": 1286,
            "quotedText": "reviewing some of our favorite regrants to date — as well as some regrants we felt were less well-suited for this program. How we think about what makes a great regrant includes considerations such as \"is this project in an early stage where some initial funding could really help?\", \"could OpenPhil fund this instead?\" and \"does this regrant effectively leverage our regrantors' experience and expertise?\"\n\nWe think our regranting program is one of the best opportunities for donors who: care about AI safety, want to seed ambitious new projects, and like transparency and moving fast. If you may want to fund our 2025 program, please contact austin@manifund.org!\n\nAbout the author: Jesse Richardson recently joined Manifund after working at Mila - Quebec AI Institute, and also has a background in trading on prediction markets. This post is essentially Jesse's hot takes about various regrants, and is not intended to be definitive or particularly high-confidence.\n\n## Three awesome AI Safety regrants…"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Confirmation Bias 🔍",
          "description": "Throughout the document, the author emphasizes evidence and anecdotes that support the effectiveness of the regranting program, while downplaying or omitting counterexamples or failures. For instance, the phrase *'these are all markers of projects I am excited to see funded through Manifund regranting'* signals a focus on confirming the program's value. This bias can reinforce pre-existing beliefs among stakeholders and obscure areas needing improvement. To address this, the author should systematically seek and present disconfirming evidence or lessons learned from less successful grants.",
          "importance": 85,
          "grade": 40,
          "isValid": true,
          "highlight": {
            "startOffset": 3484,
            "endOffset": 3763,
            "quotedText": "These are all markers of projects I am excited to see funded through Manifund regranting. In addition to our four regrantors, this agenda was also endorsed by capable AI safety researchers such as Vanessa Kosoy and John Wentworth, and it makes sense to update on their judgement."
          }
        },
        {
          "title": "Authority Bias 🏛️",
          "description": "The document repeatedly appeals to the authority of well-known figures and organizations to justify funding decisions, e.g., *'endorsed by capable AI safety researchers such as Vanessa Kosoy and John Wentworth'* and *'Evan had previously mentored Jesse Hoogland as part of MATS, and therefore had some additional context about the value of funding Hoogland's future research.'* While expert endorsement is valuable, over-reliance on authority can crowd out independent evaluation and novel perspectives. To mitigate, the document should clarify the specific criteria used for evaluation and balance authority-based endorsements with objective project assessments.",
          "importance": 70,
          "grade": 60,
          "isValid": true,
          "highlight": {
            "startOffset": 3631,
            "endOffset": 3763,
            "quotedText": "endorsed by capable AI safety researchers such as Vanessa Kosoy and John Wentworth, and it makes sense to update on their judgement."
          }
        },
        {
          "title": "Narrative Fallacy 📖",
          "description": "The text constructs compelling success stories around each grant, attributing positive outcomes to the regranting program's intervention (e.g., *'Manifund's early funding helped Hoogland's team get off the ground, and they subsequently started a new organization (Timaeus) and received significantly more funding from other sources'*). This narrative may oversimplify complex causal relationships and ignore other contributing factors. For a more accurate assessment, the document should acknowledge alternative explanations and the multifactorial nature of project success.",
          "importance": 75,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 4258,
            "endOffset": 4725,
            "quotedText": "Manifund's early funding helped Hoogland's team get off the ground, and they subsequently started a new organization (Timaeus) and received significantly more funding from other sources, such as $500,000 from the Survival & Flourishing Fund. It's probable that they would've gotten this other funding regardless, but not guaranteed, and I'm happy that Manifund was able to help bring Timaeus into existence several months sooner and with increased financial security."
          }
        },
        {
          "title": "Survivorship Bias 🏆",
          "description": "By focusing exclusively on successful or 'favorite' grants, the document risks giving the impression that most or all funded projects are similarly successful. This is a classic case of survivorship bias, which can mislead potential donors or applicants about the likelihood of positive outcomes. To counteract this, the document should provide context on the total number of grants, the proportion that met their goals, and examples of less successful projects.",
          "importance": 90,
          "grade": 30,
          "isValid": false,
          "highlight": {
            "startOffset": 1253,
            "endOffset": 3573,
            "quotedText": "Three awesome AI Safety regrants…\n\n### https://manifund.org/projects/scoping-developmental-interpretability-xg55b33wsfc - the first funding for Timaeus, accelerating research by months\n\nThis regrant was made in late 2023 to Jesse Hoogland and the rest of what is now the Timaeus team, for the purposes of exploring Developmental Interpretability (DevInterp) as a new AI alignment research agenda. Four of our regrantors — Evan Hubinger, Rachel Weinberg, Marcus Abramovitch and Ryan Kidd — made regrants to this project, for a total of $143,200. Evan had previously mentored Jesse Hoogland as part of MATS, and therefore had some additional context about the value of funding Hoogland's future research. This is the sweet spot for regranting: donors may have the public information that Evan Hubinger is an expert who does good work, and could potentially donate to him/his work on that basis, but regranting allows them to specifically leverage his private information about other valuable projects, such as DevInterp.\n\nBack to the grant itself; success for this project looked like determining whether DevInterp was a viable agenda to move forward with, rather than producing some seminal research outputs. I recommend reading more about DevInterp if you're interested, but my shallow understanding is that it aims to use insights from Singular Learning Theory (SLT) to make progress on AI alignment through interpretability, with a specific focus on how phase transitions in the training process lead to internal structure in neural networks.\n\nI'm not well placed to form an inside view on how likely DevInterp was/is to succeed, but this proposed research agenda had numerous things going for it:\nit was novel; the application of SLT to alignment was largely unexplored prior to this work,\nit seemed to be well thought out; the write-up on LessWrong included plenty of detail about why we might expect phase transitions to be a big deal and how this would relate to alignment, as well as a solid plan for the next six months,\nit had an element of \"big if true\" i.e. it may be unlikely that the strong version of the DevInterp thesis is true, but this research has potential to make meaningful progress on AI alignment if it is\n\nThese are all markers of projects I am excited to see funded through Manifund regranting."
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Optimism Bias 🌞",
          "description": "The language used throughout the document is highly positive and forward-looking, e.g., *'Overall ChinaTalk has continued on the strong trajectory it was on in late 2023, and it was great that Manifund was able to support ChinaTalk in this success.'* This optimism may lead to underestimating risks or challenges faced by grantees. To improve, the author should balance positive statements with critical reflection on obstacles, setbacks, or uncertainties.",
          "importance": 60,
          "grade": 65,
          "isValid": true,
          "highlight": {
            "startOffset": 8718,
            "endOffset": 9018,
            "quotedText": "Overall ChinaTalk has continued on the strong trajectory it was on in late 2023, and it was great that Manifund was able to support ChinaTalk in this success. For more information about why this grant was likely good ex ante, I encourage you to look at regrantor Joel Becker's comment on the subject."
          }
        },
        {
          "title": "Scope Insensitivity 🗺️",
          "description": "The document discusses grants ranging from $9,000 to $143,200, but does not contextualize the relative impact or cost-effectiveness of these grants. For example, the phrase *'Gavin asked for between $8,000 (MVP version) and $17,000 (high-end version) and was quickly funded for the MVP by Neel and Ryan.'* lacks analysis of how outcomes scale with funding. This scope insensitivity can obscure important differences in resource allocation. To address this, the document should include comparative metrics or discuss marginal returns on different grant sizes.",
          "importance": 55,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 11649,
            "endOffset": 11945,
            "quotedText": "Gavin asked for between $8,000 (MVP version) and $17,000 (high-end version) and was quickly funded for the MVP by Neel and Ryan. He then got an additional $5,000 from OpenPhil, after Matt Putz at OpenPhil learned about this proposal via our EA Forum post; and a further $12,000 from other donors."
          }
        },
        {
          "title": "In-group Bias 🤝",
          "description": "The document frequently references the involvement and endorsement of individuals within the Effective Altruism (EA) and AI safety communities, e.g., *'endorsed by capable AI safety researchers such as Vanessa Kosoy and John Wentworth'* and *'Evan had previously mentored Jesse Hoogland as part of MATS.'* This may unintentionally privilege projects and people within these networks, potentially overlooking valuable contributions from outsiders. To mitigate, the program should actively seek diverse perspectives and transparently report on the diversity of grantees.",
          "importance": 65,
          "grade": 45,
          "isValid": true,
          "highlight": {
            "startOffset": 1798,
            "endOffset": 2271,
            "quotedText": "Evan had previously mentored Jesse Hoogland as part of MATS, and therefore had some additional context about the value of funding Hoogland's future research. This is the sweet spot for regranting: donors may have the public information that Evan Hubinger is an expert who does good work, and could potentially donate to him/his work on that basis, but regranting allows them to specifically leverage his private information about other valuable projects, such as DevInterp."
          }
        },
        {
          "title": "Hindsight Bias ⏳",
          "description": "The author evaluates grants with the benefit of knowing their outcomes, e.g., *'My sense is that the question of whether DevInterp is a worthwhile alignment research agenda to pursue has been successfully answered in the affirmative.'* This can lead to overestimating the predictability of success and underappreciating the uncertainty present at the time of funding. To improve, the document should distinguish between ex ante (before the fact) and ex post (after the fact) evaluations, and discuss how uncertainty was managed.",
          "importance": 80,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 5566,
            "endOffset": 5934,
            "quotedText": "My sense is that the question of whether DevInterp is a worthwhile alignment research agenda to pursue has been successfully answered in the affirmative. It's also nice to see strong outreach and engagement with the research community on the part of Timaeus: November 2023 saw the first DevInterp conference, and they've given talks at OpenAI, Anthropic, and DeepMind."
          }
        }
      ],
      "thinking": "## Initial Impressions\nThe document is a reflective, positive review of three AI safety-related grants funded through Manifund's regranting program. The author, Jesse Richardson, is transparent about the subjective and non-definitive nature of the post. The stated purpose is to highlight 'just the positive ones,' which immediately signals a selection bias and a lack of balance. The tone is enthusiastic and supportive of the projects and the regranting model.\n\n## Key Biases Detected\n- **Selection Bias**: The document explicitly focuses only on positive examples ('just the positive ones'), omitting any negative or neutral cases. This creates an unrepresentative sample and may mislead readers about the overall effectiveness of the program.\n- **Confirmation Bias**: The author appears to seek and highlight evidence that supports the value of the regranting program and the selected projects, with little critical scrutiny or discussion of failures or limitations.\n- **Survivorship Bias**: By only discussing successful or promising grants, the document ignores projects that may have failed or underperformed, which could provide valuable learning opportunities.\n- **Authority Bias**: The document frequently references endorsements from well-known figures in the AI safety community (e.g., Evan Hubinger, Vanessa Kosoy, John Wentworth), potentially overvaluing their opinions without independent evaluation.\n- **Narrative Fallacy**: The stories are constructed to show a clear, positive trajectory from funding to success, potentially oversimplifying complex causal relationships.\n- **Optimism Bias**: The tone is generally optimistic about the impact of early funding and the trajectory of the projects, with limited discussion of risks or downsides.\n\n## Quantitative Bias Scores (1-10)\n- Selection Bias: 9\n- Confirmation Bias: 8\n- Survivorship Bias: 8\n- Authority Bias: 6\n- Narrative Fallacy: 6\n- Optimism Bias: 7\n- Other biases (e.g., omission bias, status quo bias): 3-5\n\n## Most Significant Examples\n- The explicit decision to only review positive grants is a textbook case of selection and survivorship bias.\n- The repeated reliance on endorsements from prominent community members (authority bias).\n- The lack of critical analysis or discussion of negative outcomes or lessons learned.\n\n## Recommendations for Improvement\n1. **Include Negative and Neutral Cases**: To provide a balanced assessment, the document should also discuss grants that did not meet expectations, including reasons for underperformance.\n2. **Explicitly Address Limitations**: Acknowledge the potential for bias in the selection and evaluation process, and discuss how these biases might affect conclusions.\n3. **Incorporate Quantitative Metrics**: Where possible, provide data on the overall success rate of grants, not just anecdotes.\n4. **Solicit External Perspectives**: Include feedback from independent evaluators or stakeholders outside the immediate community.\n5. **Reflect on Learning from Failures**: Highlight what can be learned from less successful grants to improve future decision-making.\n\n## Limitations of the Analysis\n- The analysis is based solely on the provided text and cannot assess the full context or unpublished data.\n- The subjective nature of the post is acknowledged by the author, which may limit the generalizability of the findings.\n- Some biases may be mitigated by the author's transparency about the post's intent, but the overall impact remains significant.",
      "summary": "## Executive Summary of Bias Assessment\n\nThe document 'Three Grants We Liked (Just the Positive Ones)' demonstrates a high degree of selection and confirmation bias, as it intentionally focuses only on successful or promising grants within Manifund's regranting program. This approach, while transparent about its subjectivity, results in a skewed portrayal of the program's effectiveness and omits valuable lessons from less successful projects. The narrative is shaped by authority bias, with frequent references to endorsements from prominent figures, and is further colored by optimism and narrative fallacy, presenting a linear and positive trajectory from funding to impact. While the document is engaging and informative about the highlighted projects, its lack of critical analysis and omission of negative cases limit its value as an objective assessment. To improve, future reviews should incorporate a more balanced selection of cases, quantitative metrics, and external perspectives, as well as explicit reflection on failures and limitations. Overall, the document serves as a promotional piece rather than a rigorous evaluation, and readers should be cautious about drawing broad conclusions from its content.",
      "grade": 62
    },
    {
      "agentId": "clarity-coach",
      "createdAt": "2025-04-18T23:01:57.502Z",
      "costInCents": 0,
      "comments": [
        {
          "title": "Strong Executive Summary and Context Setting",
          "description": "The opening paragraphs provide a clear overview of Manifund's regranting program, its purpose, and the criteria for evaluating regrants. This context helps orient readers and sets expectations for the rest of the document. The use of direct questions (e.g., 'is this project in an early stage where some initial funding could really help?') effectively signals the evaluative framework. This clarity supports both new and returning readers in understanding the document's scope and intent.",
          "importance": 85,
          "grade": 92,
          "isValid": true,
          "highlight": {
            "startOffset": 19,
            "endOffset": 687,
            "quotedText": "For the past two years, Manifund has run a regranting program. We allocate annual donation budgets (from $50k-$400k) to selected experts (\"regrantors\"), who in turn choose great projects to fund. We'll be kicking off the 2025 program soon! In preparation, we're reviewing some of our favorite regrants to date — as well as some regrants we felt were less well-suited for this program. How we think about what makes a great regrant includes considerations such as \"is this project in an early stage where some initial funding could really help?\", \"could OpenPhil fund this instead?\" and \"does this regrant effectively leverage our regrantors' experience and expertise?\""
          }
        },
        {
          "title": "Effective Use of Specific Examples to Illustrate Impact",
          "description": "The document excels at grounding its analysis in concrete examples, such as the Timaeus grant and ChinaTalk's growth. This approach enhances credibility and reader engagement. By quantifying outcomes (e.g., 'ChinaTalk's Substack audience has grown from 26,000 subscribers to 51,000'), the author demonstrates the tangible effects of the grants. This specificity is a model for clear, persuasive communication.",
          "importance": 80,
          "grade": 95,
          "isValid": true,
          "highlight": {
            "startOffset": 7672,
            "endOffset": 8255,
            "quotedText": "Since the first regrant was made, ChinaTalk's Substack audience has grown from 26,000 subscribers to 51,000 and they've put out regular high-quality content, including an English translation of an interview with DeepSeek CEO Liang Wenfeng, coverage of chip policy, and what important 2024 elections in the US and Taiwan mean for China. The ChinaTalk team has expanded to six people, allowing for a greater diversity and quantity of coverage, including YouTube videos. Jordan has also announced plans for launching a think tank—ChinaTalk Institute—this year, in a similar vein to IFP."
          }
        },
        {
          "title": "Occasional Overly Long Sentences Impact Readability",
          "description": "Some sentences are notably long and complex, which can hinder readability, especially for less expert audiences. For example, the sentence beginning 'Evan had previously mentored Jesse Hoogland...' could be split for clarity. Shorter sentences would improve information chunking and reduce cognitive load.",
          "importance": 70,
          "grade": 60,
          "isValid": true,
          "highlight": {
            "startOffset": 1798,
            "endOffset": 2271,
            "quotedText": "Evan had previously mentored Jesse Hoogland as part of MATS, and therefore had some additional context about the value of funding Hoogland's future research. This is the sweet spot for regranting: donors may have the public information that Evan Hubinger is an expert who does good work, and could potentially donate to him/his work on that basis, but regranting allows them to specifically leverage his private information about other valuable projects, such as DevInterp."
          }
        },
        {
          "title": "Appropriate Use of Domain-Specific Terminology with Adequate Explanation",
          "description": "The document introduces technical terms (e.g., 'Developmental Interpretability', 'Singular Learning Theory', 'phase transitions') and generally provides sufficient context for readers to follow. The author acknowledges their own 'shallow understanding,' which helps set expectations and invites readers to seek further information if needed. This balance maintains accessibility without oversimplifying complex topics.",
          "importance": 75,
          "grade": 88,
          "isValid": true,
          "highlight": {
            "startOffset": 2461,
            "endOffset": 2797,
            "quotedText": "I recommend reading more about DevInterp if you're interested, but my shallow understanding is that it aims to use insights from Singular Learning Theory (SLT) to make progress on AI alignment through interpretability, with a specific focus on how phase transitions in the training process lead to internal structure in neural networks."
          }
        },
        {
          "title": "Conversational Tone Enhances Engagement but Occasionally Reduces Formal Precision",
          "description": "The informal, conversational style (e.g., 'hot takes', 'big if true', 'I'm pretty sure') makes the document approachable and engaging. However, it sometimes detracts from precision and may not align with all professional or academic expectations. Consider balancing informality with more formal phrasing in key evaluative sections.",
          "importance": 60,
          "grade": 75,
          "isValid": false,
          "highlight": {
            "startOffset": 1112,
            "endOffset": 3482,
            "quotedText": "This post is essentially Jesse's hot takes about various regrants, and is not intended to be definitive or particularly high-confidence.\n\n## Three awesome AI Safety regrants…\n\n### https://manifund.org/projects/scoping-developmental-interpretability-xg55b33wsfc - the first funding for Timaeus, accelerating research by months\n\nThis regrant was made in late 2023 to Jesse Hoogland and the rest of what is now the Timaeus team, for the purposes of exploring Developmental Interpretability (DevInterp) as a new AI alignment research agenda. Four of our regrantors — Evan Hubinger, Rachel Weinberg, Marcus Abramovitch and Ryan Kidd — made regrants to this project, for a total of $143,200. Evan had previously mentored Jesse Hoogland as part of MATS, and therefore had some additional context about the value of funding Hoogland's future research. This is the sweet spot for regranting: donors may have the public information that Evan Hubinger is an expert who does good work, and could potentially donate to him/his work on that basis, but regranting allows them to specifically leverage his private information about other valuable projects, such as DevInterp.\n\nBack to the grant itself; success for this project looked like determining whether DevInterp was a viable agenda to move forward with, rather than producing some seminal research outputs. I recommend reading more about DevInterp if you're interested, but my shallow understanding is that it aims to use insights from Singular Learning Theory (SLT) to make progress on AI alignment through interpretability, with a specific focus on how phase transitions in the training process lead to internal structure in neural networks.\n\nI'm not well placed to form an inside view on how likely DevInterp was/is to succeed, but this proposed research agenda had numerous things going for it:\nit was novel; the application of SLT to alignment was largely unexplored prior to this work,\nit seemed to be well thought out; the write-up on LessWrong included plenty of detail about why we might expect phase transitions to be a big deal and how this would relate to alignment, as well as a solid plan for the next six months,\nit had an element of \"big if true\" i.e. it may be unlikely that the strong version of the DevInterp thesis is true, but this research has potential to make meaningful progress on AI alignment if it is"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Clear Information Hierarchy and Sectioning",
          "description": "The use of headings, subheadings, and bullet points (where present) provides a clear structure and aids navigation. Each grant is introduced with a distinct section, making it easy for readers to scan and locate information. This structural coherence supports comprehension and retention.",
          "importance": 65,
          "grade": 90,
          "isValid": false,
          "highlight": {
            "startOffset": 1250,
            "endOffset": 9587,
            "quotedText": "## Three awesome AI Safety regrants…\n\n### https://manifund.org/projects/scoping-developmental-interpretability-xg55b33wsfc - the first funding for Timaeus, accelerating research by months\n\nThis regrant was made in late 2023 to Jesse Hoogland and the rest of what is now the Timaeus team, for the purposes of exploring Developmental Interpretability (DevInterp) as a new AI alignment research agenda. Four of our regrantors — Evan Hubinger, Rachel Weinberg, Marcus Abramovitch and Ryan Kidd — made regrants to this project, for a total of $143,200. Evan had previously mentored Jesse Hoogland as part of MATS, and therefore had some additional context about the value of funding Hoogland's future research. This is the sweet spot for regranting: donors may have the public information that Evan Hubinger is an expert who does good work, and could potentially donate to him/his work on that basis, but regranting allows them to specifically leverage his private information about other valuable projects, such as DevInterp.\n\nBack to the grant itself; success for this project looked like determining whether DevInterp was a viable agenda to move forward with, rather than producing some seminal research outputs. I recommend reading more about DevInterp if you're interested, but my shallow understanding is that it aims to use insights from Singular Learning Theory (SLT) to make progress on AI alignment through interpretability, with a specific focus on how phase transitions in the training process lead to internal structure in neural networks.\n\nI'm not well placed to form an inside view on how likely DevInterp was/is to succeed, but this proposed research agenda had numerous things going for it:\nit was novel; the application of SLT to alignment was largely unexplored prior to this work,\nit seemed to be well thought out; the write-up on LessWrong included plenty of detail about why we might expect phase transitions to be a big deal and how this would relate to alignment, as well as a solid plan for the next six months,\nit had an element of \"big if true\" i.e. it may be unlikely that the strong version of the DevInterp thesis is true, but this research has potential to make meaningful progress on AI alignment if it is\n\nThese are all markers of projects I am excited to see funded through Manifund regranting. In addition to our four regrantors, this agenda was also endorsed by capable AI safety researchers such as Vanessa Kosoy and John Wentworth, and it makes sense to update on their judgement.\n\nIn addition to the agenda itself, I think this was a good team to bet on for this kind of work; they seem capable and have relevant experience e.g. ML research, and running the 2023 SLT & Alignment Summit.\n\nThis regrant is a strong example of where Manifund's regranting program can have the biggest impact: being early to support new projects & organizations, and thereby providing strong signals to other funders as well as some runway for these organizations to move quickly. In this case, Manifund's early funding helped Hoogland's team get off the ground, and they subsequently started a new organization (Timaeus) and received significantly more funding from other sources, such as $500,000 from the Survival & Flourishing Fund. It's probable that they would've gotten this other funding regardless, but not guaranteed, and I'm happy that Manifund was able to help bring Timaeus into existence several months sooner and with increased financial security. Jesse notes:\n\nGetting early support from Manifund made a real difference for us. This was the first funding we received for research and meant that we could start months earlier than we otherwise would have. The fact that it was public meant other funders could easily see who was backing our work and why. That transparency helped us build momentum and credibility for developmental interpretability research when it was still a new idea. I'm pretty sure it played a significant role in us securing later funding through SFF and other grantmakers.\n\nIn terms of concrete outcomes, there's a lot to be happy with here. Timaeus and its collaborators have put out numerous papers on DevInterp since this regrant was made, and it seems that DevInterp's key insight around the existence and significance of phase transitions has been validated. My sense is that the question of whether DevInterp is a worthwhile alignment research agenda to pursue has been successfully answered in the affirmative. It's also nice to see strong outreach and engagement with the research community on the part of Timaeus: November 2023 saw the first DevInterp conference, and they've given talks at OpenAI, Anthropic, and DeepMind.\n\n### https://manifund.org/projects/support-for-deep-coverage-of-china-and-ai - reporting on DeepSeek, ahead of the curve\n\nAcross 2023 & 2024, Manifund regrantors Joel Becker and Evan Hubinger granted a total of $37,000 to ChinaTalk, a newsletter and podcast that covers China, technology, and US-China relations. ChinaTalk has over 50,000 subscribers and is also notable for the quality of its coverage and the praise and attention it receives from elites and policymakers.\n\nBefore this regrant, ChinaTalk had been run by Jordan Schneider and Caithrin Rintoul, both part-time, on a budget of just $35,000/year. What they were able to accomplish in that time with those limited resources was impressive, and I believe merited additional funding, even just to allow Jordan to work on this full-time. More funding would also have meant ChinaTalk bringing on a full-time fellow who, per Jordan, \"would be, to my knowledge, the only researcher in the English-speaking world devoted solely to covering China and AI safety\". ChinaTalk has since received further funding and is in the process of growing to five full-time employees, but we would've loved for this to happen sooner through an expanded regranting program.\n\nEven putting aside the specific track record of ChinaTalk, it seems clear to me that the intersection of China and AI safety is an incredibly important area to cover, and at a high level it is very valuable to fund organizations that are doing this kind of work. It can be hard to imagine plausible scenarios of how the next decade goes well with respect to AI that don't run through US-China relations, and I am persuaded by Jordan's case that the amount of energy currently being expended on this is grossly inadequate.\n\nSince the first regrant was made, ChinaTalk's Substack audience has grown from 26,000 subscribers to 51,000 and they've put out regular high-quality content, including an English translation of an interview with DeepSeek CEO Liang Wenfeng, coverage of chip policy, and what important 2024 elections in the US and Taiwan mean for China. The ChinaTalk team has expanded to six people, allowing for a greater diversity and quantity of coverage, including YouTube videos. Jordan has also announced plans for launching a think tank—ChinaTalk Institute—this year, in a similar vein to IFP.\n\nAmong their varied coverage, I was particularly impressed to see how ChinaTalk was ahead of the curve on covering the rise of DeepSeek, while most of the West seemed to be taken by total surprise in January 2025. As a trader and forecaster, this advance insight might have been worth a lot of money to me through anticipating the market freakout, which suggests that I should pay more attention to ChinaTalk in the future, as their work is seriously valuable.\n\nOverall ChinaTalk has continued on the strong trajectory it was on in late 2023, and it was great that Manifund was able to support ChinaTalk in this success. For more information about why this grant was likely good ex ante, I encourage you to look at regrantor Joel Becker's comment on the subject. Joel's detail about why ChinaTalk was at the time insufficiently funded:\n\nPhilanthropists are scared to touch China, in part because of lack of expertise and in part for political reasons. Advertisers can be nervous for similar reasons… Jordan was hoping to support this work through subscriptions only.\n\nmakes me more optimistic that this regrant was very much the kind of thing the program should be doing: plugging holes in the funding landscape.\n\n### https://manifund.org/projects/shallow-review-of-ai-safety-2024 - two quick regrants, followed by OpenPhil support"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Redundancy and Minor Verbosity in Outcome Descriptions",
          "description": "Some outcome statements are repeated or could be condensed for concision. For example, the phrase 'It's probable that they would've gotten this other funding regardless, but not guaranteed, and I'm happy that Manifund was able to help bring Timaeus into existence several months sooner and with increased financial security' could be streamlined. Reducing redundancy would improve flow and precision.",
          "importance": 55,
          "grade": 65,
          "isValid": true,
          "highlight": {
            "startOffset": 4500,
            "endOffset": 4738,
            "quotedText": "It's probable that they would've gotten this other funding regardless, but not guaranteed, and I'm happy that Manifund was able to help bring Timaeus into existence several months sooner and with increased financial security. Jesse notes:"
          }
        },
        {
          "title": "Strong Audience Alignment and Accessibility",
          "description": "The document anticipates the needs of its likely audience—donors, AI safety enthusiasts, and those interested in grantmaking—by providing relevant background, context, and links for further reading. The author explicitly references their own prior knowledge gaps, which helps scaffold understanding for less expert readers.",
          "importance": 80,
          "grade": 93,
          "isValid": true,
          "highlight": {
            "startOffset": 10321,
            "endOffset": 10812,
            "quotedText": "Big picture, I'm convinced that there should be an accessible and up-to-date resource of this kind; for people who are starting out in AI safety and don't know anything, for funders trying to get a sense of the landscape, or for anyone else who might need it. In 2022 I was at a stage where I wanted to contribute to AI safety but didn't know anything about it and was unsure where to start, and I would've likely found Gavin's review useful, in addition to the other resources that existed."
          }
        },
        {
          "title": "Transparent Acknowledgment of Limitations and Uncertainties",
          "description": "The author is candid about their own limitations ('I'm not well placed to form an inside view...'), the non-definitive nature of the post, and the mixed outcomes of some grants. This transparency builds trust and models intellectual humility, which is especially important in evaluative writing.",
          "importance": 70,
          "grade": 90,
          "isValid": false,
          "highlight": {
            "startOffset": 2799,
            "endOffset": 11451,
            "quotedText": "I'm not well placed to form an inside view on how likely DevInterp was/is to succeed, but this proposed research agenda had numerous things going for it:\nit was novel; the application of SLT to alignment was largely unexplored prior to this work,\nit seemed to be well thought out; the write-up on LessWrong included plenty of detail about why we might expect phase transitions to be a big deal and how this would relate to alignment, as well as a solid plan for the next six months,\nit had an element of \"big if true\" i.e. it may be unlikely that the strong version of the DevInterp thesis is true, but this research has potential to make meaningful progress on AI alignment if it is\n\nThese are all markers of projects I am excited to see funded through Manifund regranting. In addition to our four regrantors, this agenda was also endorsed by capable AI safety researchers such as Vanessa Kosoy and John Wentworth, and it makes sense to update on their judgement.\n\nIn addition to the agenda itself, I think this was a good team to bet on for this kind of work; they seem capable and have relevant experience e.g. ML research, and running the 2023 SLT & Alignment Summit.\n\nThis regrant is a strong example of where Manifund's regranting program can have the biggest impact: being early to support new projects & organizations, and thereby providing strong signals to other funders as well as some runway for these organizations to move quickly. In this case, Manifund's early funding helped Hoogland's team get off the ground, and they subsequently started a new organization (Timaeus) and received significantly more funding from other sources, such as $500,000 from the Survival & Flourishing Fund. It's probable that they would've gotten this other funding regardless, but not guaranteed, and I'm happy that Manifund was able to help bring Timaeus into existence several months sooner and with increased financial security. Jesse notes:\n\nGetting early support from Manifund made a real difference for us. This was the first funding we received for research and meant that we could start months earlier than we otherwise would have. The fact that it was public meant other funders could easily see who was backing our work and why. That transparency helped us build momentum and credibility for developmental interpretability research when it was still a new idea. I'm pretty sure it played a significant role in us securing later funding through SFF and other grantmakers.\n\nIn terms of concrete outcomes, there's a lot to be happy with here. Timaeus and its collaborators have put out numerous papers on DevInterp since this regrant was made, and it seems that DevInterp's key insight around the existence and significance of phase transitions has been validated. My sense is that the question of whether DevInterp is a worthwhile alignment research agenda to pursue has been successfully answered in the affirmative. It's also nice to see strong outreach and engagement with the research community on the part of Timaeus: November 2023 saw the first DevInterp conference, and they've given talks at OpenAI, Anthropic, and DeepMind.\n\n### https://manifund.org/projects/support-for-deep-coverage-of-china-and-ai - reporting on DeepSeek, ahead of the curve\n\nAcross 2023 & 2024, Manifund regrantors Joel Becker and Evan Hubinger granted a total of $37,000 to ChinaTalk, a newsletter and podcast that covers China, technology, and US-China relations. ChinaTalk has over 50,000 subscribers and is also notable for the quality of its coverage and the praise and attention it receives from elites and policymakers.\n\nBefore this regrant, ChinaTalk had been run by Jordan Schneider and Caithrin Rintoul, both part-time, on a budget of just $35,000/year. What they were able to accomplish in that time with those limited resources was impressive, and I believe merited additional funding, even just to allow Jordan to work on this full-time. More funding would also have meant ChinaTalk bringing on a full-time fellow who, per Jordan, \"would be, to my knowledge, the only researcher in the English-speaking world devoted solely to covering China and AI safety\". ChinaTalk has since received further funding and is in the process of growing to five full-time employees, but we would've loved for this to happen sooner through an expanded regranting program.\n\nEven putting aside the specific track record of ChinaTalk, it seems clear to me that the intersection of China and AI safety is an incredibly important area to cover, and at a high level it is very valuable to fund organizations that are doing this kind of work. It can be hard to imagine plausible scenarios of how the next decade goes well with respect to AI that don't run through US-China relations, and I am persuaded by Jordan's case that the amount of energy currently being expended on this is grossly inadequate.\n\nSince the first regrant was made, ChinaTalk's Substack audience has grown from 26,000 subscribers to 51,000 and they've put out regular high-quality content, including an English translation of an interview with DeepSeek CEO Liang Wenfeng, coverage of chip policy, and what important 2024 elections in the US and Taiwan mean for China. The ChinaTalk team has expanded to six people, allowing for a greater diversity and quantity of coverage, including YouTube videos. Jordan has also announced plans for launching a think tank—ChinaTalk Institute—this year, in a similar vein to IFP.\n\nAmong their varied coverage, I was particularly impressed to see how ChinaTalk was ahead of the curve on covering the rise of DeepSeek, while most of the West seemed to be taken by total surprise in January 2025. As a trader and forecaster, this advance insight might have been worth a lot of money to me through anticipating the market freakout, which suggests that I should pay more attention to ChinaTalk in the future, as their work is seriously valuable.\n\nOverall ChinaTalk has continued on the strong trajectory it was on in late 2023, and it was great that Manifund was able to support ChinaTalk in this success. For more information about why this grant was likely good ex ante, I encourage you to look at regrantor Joel Becker's comment on the subject. Joel's detail about why ChinaTalk was at the time insufficiently funded:\n\nPhilanthropists are scared to touch China, in part because of lack of expertise and in part for political reasons. Advertisers can be nervous for similar reasons… Jordan was hoping to support this work through subscriptions only.\n\nmakes me more optimistic that this regrant was very much the kind of thing the program should be doing: plugging holes in the funding landscape.\n\n### https://manifund.org/projects/shallow-review-of-ai-safety-2024 - two quick regrants, followed by OpenPhil support\n\nGavin Leech previously co-wrote https://www.lesswrong.com/posts/zaaGsFBeDTpCsYHef/shallow-review-of-live-agendas-in-alignment-and-safety in 2023, which was well-received and generally considered to be a useful resource for people looking to get a top-level picture of AI safety research. Given that it was intended to be a shallow review, this post has a lot of helpful detail and links about the ins and outs of various research agendas, e.g., the amount of resources currently devoted to each, and notable criticisms.\n\nLast year he sought funding to create an updated 2024 version of this post. He received $9,000 from Manifund regrantors Neel Nanda and Ryan Kidd, as well as $12,000 from other donors through the Manifund site.\n\nBig picture, I'm convinced that there should be an accessible and up-to-date resource of this kind; for people who are starting out in AI safety and don't know anything, for funders trying to get a sense of the landscape, or for anyone else who might need it. In 2022 I was at a stage where I wanted to contribute to AI safety but didn't know anything about it and was unsure where to start, and I would've likely found Gavin's review useful, in addition to the other resources that existed. Based on this, Gavin's record in a variety of fields, and the fact that the 2023 version seems to have been fairly well-executed, I think this regrant looked very promising.\n\nIn terms of what was produced, the new post (https://www.lesswrong.com/posts/fAW6RXLKTLHC3WXkS/shallow-review-of-technical-ai-safety-2024) came out in December 2024 and appears to be similarly comprehensive to the 2023 version, although it has gotten less attention (~half the upvotes on LessWrong and not curated). That's probably a bit worse of an outcome than I would've hoped for, but I still would have endorsed this grant had I known the outcome in advance."
          },
          "error": "Highlight is too long (max 1000 characters)"
        }
      ],
      "thinking": "## Detailed Thinking Process\n\n### 1. Overall Assessment\n- The document is a reflective, semi-informal review of three successful grants made by Manifund's regranting program, focusing on AI safety and related fields.\n- The tone is conversational, with the author (Jesse Richardson) providing personal perspectives and caveats about the non-definitive nature of the analysis.\n- The structure is clear: an introduction to the regranting program, followed by three detailed case studies, each with context, rationale, and outcomes.\n- The document is aimed at an audience familiar with AI safety, effective altruism, and philanthropic funding, but it also attempts to provide some scaffolding for less-initiated readers.\n\n### 2. Key Themes and Patterns\n- **Early-stage funding impact:** All three grants highlight the value of early, flexible funding in catalyzing projects that later attract larger support.\n- **Leverage of expert knowledge:** The regranting model is positioned as a way to utilize the private knowledge and judgment of domain experts.\n- **Transparency and signaling:** Public grants are shown to help projects gain credibility and attract further funding.\n- **Narrative style:** The author uses first-person reflections, direct quotes, and anecdotal evidence to illustrate points.\n- **Caveats and humility:** The author repeatedly notes the limits of their own expertise and the non-final nature of the judgments.\n\n### 3. Expert Perspective on Content\n- **Readability:**\n  - Sentence length varies, with some long, complex sentences that could be broken up for clarity.\n  - Vocabulary is generally accessible for the target audience, but there is moderate jargon (e.g., \"DevInterp,\" \"SLT,\" \"regrantor\") that may be opaque to outsiders.\n  - The voice is mostly active, with some passive constructions.\n  - Paragraphs are well-chunked, but some are dense and could be split for easier reading.\n- **Structural Coherence:**\n  - Logical flow is strong, with each grant following a similar template: context, rationale, outcomes, and reflections.\n  - Transitions between sections are clear, but within sections, some ideas could be more tightly linked.\n  - The information hierarchy is mostly clear, but the narrative occasionally meanders with asides and personal anecdotes.\n- **Precision and Concision:**\n  - The writing is generally precise, but there is some redundancy (e.g., repeated emphasis on early-stage impact and transparency).\n  - Some sentences are verbose or contain hedging language (\"my sense is,\" \"I'm not well placed to form an inside view\").\n  - Terminology is mostly used appropriately, but some concepts could be defined more clearly for a broader audience.\n- **Audience Alignment:**\n  - Assumes moderate prior knowledge of AI safety and philanthropic funding.\n  - Provides some context and links for further reading, but could do more to scaffold for newcomers.\n  - Examples are relevant and illustrative, but the informal tone may not suit all professional audiences.\n\n### 4. Specific Examples\n- **Strong Practices:**\n  - Clear structure with numbered grants and consistent formatting.\n  - Use of direct quotes from grantees to illustrate impact.\n  - Transparency about the author's perspective and limitations.\n- **Areas for Improvement:**\n  - Occasional long sentences and dense paragraphs reduce readability.\n  - Some jargon is not defined on first use.\n  - Redundant statements about the value of early-stage funding.\n- **Before/After Example:**\n  - Before: \"Back to the grant itself; success for this project looked like determining whether DevInterp was a viable agenda to move forward with, rather than producing some seminal research outputs.\"\n  - After: \"For this grant, success meant assessing whether DevInterp was a viable research agenda, rather than expecting major research breakthroughs.\"\n\n### 5. Recommendations\n- Immediate: Break up long sentences and paragraphs; define jargon on first use.\n- Structural: Add summary tables or bullet points for each grant's key outcomes.\n- Content: Provide a brief glossary or sidebar for technical terms.\n- Terminology: Use consistent language for key concepts (e.g., always refer to \"regrantors\" or \"grantmakers\").\n\n### 6. Meta-Analysis\n- The document balances clarity and nuance, preserving necessary complexity for a specialized audience.\n- Domain conventions (e.g., referencing LessWrong, OpenPhil) are appropriate, but more context would help general readers.\n- The informal, reflective tone is engaging but may limit the document's utility as a formal report.\n",
      "summary": "## Executive Summary and Key Insights\n\n**Overall, this document is a well-structured, reflective review of three successful grants in the AI safety space, offering valuable insights into the impact of early-stage funding and the advantages of expert-driven regranting.** The narrative is engaging and transparent, with the author openly sharing both strengths and limitations of the analysis. The structure—introduction, three case studies, and personal reflections—supports logical progression and thematic cohesion.\n\n**Strengths:**\n- Clear, consistent structure and formatting.\n- Effective use of examples and direct quotes to illustrate impact.\n- Transparency about the author's perspective and the non-final nature of judgments.\n- Strong alignment with an audience familiar with AI safety and philanthropic funding.\n\n**Areas for Improvement:**\n- Readability could be enhanced by breaking up long sentences and dense paragraphs.\n- Occasional jargon and technical terms are not defined for newcomers.\n- Some redundancy and verbosity could be trimmed for concision.\n- Additional scaffolding (e.g., glossary, summary tables) would improve accessibility for broader audiences.\n\n**Key Takeaways:**\n- The document successfully communicates the value of early, flexible funding and the unique leverage of expert regrantors.\n- While the informal tone is engaging, minor structural and stylistic adjustments would further enhance clarity and accessibility.\n\n**Implementation Roadmap:**\n1. Edit for sentence and paragraph length.\n2. Define technical terms on first use.\n3. Add summary elements (tables, bullet points).\n4. Review for redundant or verbose passages.\n\n**Success Metrics:**\n- Improved readability scores (Flesch-Kincaid, SMOG).\n- Positive feedback from both expert and lay readers.\n- Increased engagement and comprehension among target audiences.",
      "grade": 85
    },
    {
      "agentId": "research-scholar",
      "createdAt": "2025-04-18T23:02:37.047Z",
      "costInCents": 0,
      "comments": [
        {
          "title": "🔍 Clear Explanation of Manifund's Regranting Model",
          "description": "The opening paragraph provides a concise overview of Manifund's regranting approach, including the use of expert regrantors and the criteria for evaluating projects. This sets the stage for understanding the unique value proposition of the program and frames the subsequent examples. The explicit mention of leveraging regrantors' expertise and private information highlights a key differentiator from traditional grantmaking.",
          "importance": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 19,
            "endOffset": 687,
            "quotedText": "For the past two years, Manifund has run a regranting program. We allocate annual donation budgets (from $50k-$400k) to selected experts (\"regrantors\"), who in turn choose great projects to fund. We'll be kicking off the 2025 program soon! In preparation, we're reviewing some of our favorite regrants to date — as well as some regrants we felt were less well-suited for this program. How we think about what makes a great regrant includes considerations such as \"is this project in an early stage where some initial funding could really help?\", \"could OpenPhil fund this instead?\" and \"does this regrant effectively leverage our regrantors' experience and expertise?\""
          }
        },
        {
          "title": "🎯 Target Audience and Value Proposition",
          "description": "The document identifies its ideal donor audience—those interested in AI safety, early-stage projects, transparency, and speed. This direct appeal clarifies the program's niche and helps potential funders self-select, which is important for aligning expectations and attracting the right supporters.",
          "importance": 60,
          "isValid": true,
          "highlight": {
            "startOffset": 689,
            "endOffset": 945,
            "quotedText": "We think our regranting program is one of the best opportunities for donors who: care about AI safety, want to seed ambitious new projects, and like transparency and moving fast. If you may want to fund our 2025 program, please contact austin@manifund.org!"
          }
        },
        {
          "title": "🧑‍🔬 Author's Background and Perspective",
          "description": "The author, Jesse Richardson, introduces himself and clarifies that the post reflects his personal, possibly tentative views. This transparency about authorship and confidence level adds credibility and helps readers interpret the analysis as informed but not definitive.",
          "importance": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 947,
            "endOffset": 1248,
            "quotedText": "About the author: Jesse Richardson recently joined Manifund after working at Mila - Quebec AI Institute, and also has a background in trading on prediction markets. This post is essentially Jesse's hot takes about various regrants, and is not intended to be definitive or particularly high-confidence."
          }
        },
        {
          "title": "🚀 Early-Stage Support for DevInterp",
          "description": "The section on the Timaeus/DevInterp grant illustrates the value of early funding for novel research agendas. The narrative emphasizes how Manifund's support enabled the team to start months earlier, build credibility, and attract further funding. This is a strong example of the catalytic role regranting can play in the AI safety ecosystem.",
          "importance": 90,
          "isValid": true,
          "highlight": {
            "startOffset": 1439,
            "endOffset": 2271,
            "quotedText": "This regrant was made in late 2023 to Jesse Hoogland and the rest of what is now the Timaeus team, for the purposes of exploring Developmental Interpretability (DevInterp) as a new AI alignment research agenda. Four of our regrantors — Evan Hubinger, Rachel Weinberg, Marcus Abramovitch and Ryan Kidd — made regrants to this project, for a total of $143,200. Evan had previously mentored Jesse Hoogland as part of MATS, and therefore had some additional context about the value of funding Hoogland's future research. This is the sweet spot for regranting: donors may have the public information that Evan Hubinger is an expert who does good work, and could potentially donate to him/his work on that basis, but regranting allows them to specifically leverage his private information about other valuable projects, such as DevInterp."
          }
        },
        {
          "title": "🔬 Novelty and 'Big If True' Research",
          "description": "The analysis of DevInterp's research agenda highlights its novelty, the unexplored application of Singular Learning Theory, and the high potential impact if successful. The author candidly discusses the uncertainty and the rationale for funding high-variance, high-reward projects, which is a valuable insight for funders considering similar bets.",
          "importance": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 2799,
            "endOffset": 3482,
            "quotedText": "I'm not well placed to form an inside view on how likely DevInterp was/is to succeed, but this proposed research agenda had numerous things going for it:\nit was novel; the application of SLT to alignment was largely unexplored prior to this work,\nit seemed to be well thought out; the write-up on LessWrong included plenty of detail about why we might expect phase transitions to be a big deal and how this would relate to alignment, as well as a solid plan for the next six months,\nit had an element of \"big if true\" i.e. it may be unlikely that the strong version of the DevInterp thesis is true, but this research has potential to make meaningful progress on AI alignment if it is"
          }
        },
        {
          "title": "📈 Concrete Outcomes and Field Impact",
          "description": "The document provides evidence of the positive outcomes from the Timaeus grant: new papers, validation of key insights, community engagement, and subsequent major funding. This outcome-oriented assessment strengthens the case for the effectiveness of Manifund's approach and offers a model for evaluating future grants.",
          "importance": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 5276,
            "endOffset": 5934,
            "quotedText": "In terms of concrete outcomes, there's a lot to be happy with here. Timaeus and its collaborators have put out numerous papers on DevInterp since this regrant was made, and it seems that DevInterp's key insight around the existence and significance of phase transitions has been validated. My sense is that the question of whether DevInterp is a worthwhile alignment research agenda to pursue has been successfully answered in the affirmative. It's also nice to see strong outreach and engagement with the research community on the part of Timaeus: November 2023 saw the first DevInterp conference, and they've given talks at OpenAI, Anthropic, and DeepMind."
          }
        },
        {
          "title": "🌏 Strategic Importance of China-AI Coverage",
          "description": "The ChinaTalk grant section underscores the strategic importance of understanding China’s role in AI safety. The author argues that robust coverage of China and AI is essential for navigating future scenarios, and that ChinaTalk fills a critical gap in the information landscape. This highlights the value of funding non-traditional, policy-relevant projects.",
          "importance": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 7149,
            "endOffset": 7670,
            "quotedText": "Even putting aside the specific track record of ChinaTalk, it seems clear to me that the intersection of China and AI safety is an incredibly important area to cover, and at a high level it is very valuable to fund organizations that are doing this kind of work. It can be hard to imagine plausible scenarios of how the next decade goes well with respect to AI that don't run through US-China relations, and I am persuaded by Jordan's case that the amount of energy currently being expended on this is grossly inadequate."
          }
        },
        {
          "title": "⏩ ChinaTalk's Early Insight and Market Value",
          "description": "The author notes ChinaTalk's prescient coverage of DeepSeek, which provided actionable intelligence ahead of mainstream awareness. This anecdote illustrates the practical value of supporting organizations that can deliver timely, high-quality analysis, and suggests that such grants can have outsized impact beyond traditional academic outputs.",
          "importance": 65,
          "isValid": true,
          "highlight": {
            "startOffset": 8257,
            "endOffset": 8716,
            "quotedText": "Among their varied coverage, I was particularly impressed to see how ChinaTalk was ahead of the curve on covering the rise of DeepSeek, while most of the West seemed to be taken by total surprise in January 2025. As a trader and forecaster, this advance insight might have been worth a lot of money to me through anticipating the market freakout, which suggests that I should pay more attention to ChinaTalk in the future, as their work is seriously valuable."
          }
        },
        {
          "title": "📚 Value of Accessible AI Safety Overviews",
          "description": "The discussion of Gavin Leech's 'shallow review' project highlights the ongoing need for accessible, up-to-date resources summarizing the AI safety landscape. The author reflects on his own experience as a newcomer and the utility of such overviews for both new entrants and funders. This underscores the importance of funding knowledge synthesis and dissemination.",
          "importance": 60,
          "isValid": true,
          "highlight": {
            "startOffset": 10321,
            "endOffset": 10812,
            "quotedText": "Big picture, I'm convinced that there should be an accessible and up-to-date resource of this kind; for people who are starting out in AI safety and don't know anything, for funders trying to get a sense of the landscape, or for anyone else who might need it. In 2022 I was at a stage where I wanted to contribute to AI safety but didn't know anything about it and was unsure where to start, and I would've likely found Gavin's review useful, in addition to the other resources that existed."
          }
        }
      ],
      "thinking": "# Thinking Process\n\n- **Overall Assessment**: The document is a reflective, insider-focused review of three AI safety-related grants made through Manifund's regranting program. The tone is informal and candid, with the author (Jesse Richardson) providing personal impressions and 'hot takes' rather than definitive evaluations. The document is transparent about both successes and limitations, and it highlights the value of early-stage funding and the unique advantages of the regranting model.\n\n- **Key Themes and Patterns**:\n  - **Early-Stage Funding**: All three highlighted grants exemplify the impact of providing early, flexible funding to promising but unproven projects or organizations. The narrative repeatedly emphasizes how such support can accelerate progress, attract further funding, and validate new research agendas.\n  - **Leverage of Insider Knowledge**: The regranting model is shown to leverage the private knowledge and expertise of trusted regrantors, enabling more nuanced and informed funding decisions than traditional grantmaking.\n  - **Transparency and Signaling**: Publicly visible grants serve as signals to other funders and help build credibility for new projects, as seen with Timaeus and ChinaTalk.\n  - **AI Safety and Policy Intersection**: The document underscores the importance of funding work at the intersection of AI safety, policy, and international relations (especially US-China), and the need for accessible, up-to-date resources for the field.\n  - **Community and Ecosystem Building**: The grants not only support individual projects but also contribute to the broader AI safety ecosystem by enabling new organizations, fostering outreach, and encouraging collaboration.\n  - **Candid Reflection**: The author is open about uncertainties, mixed outcomes, and the limitations of their own perspective, which adds credibility and invites further discussion.\n\n- **Expert Perspective**:\n  - The document is valuable as a case study in effective grantmaking for emerging fields. It demonstrates how targeted, early-stage funding can catalyze progress and how regranting can fill gaps left by larger, more risk-averse funders.\n  - The focus on transparency, learning from outcomes, and willingness to fund 'big if true' ideas is well-aligned with best practices in philanthropic innovation.\n  - The inclusion of both technical and policy-oriented projects reflects a mature understanding of the multifaceted nature of AI safety challenges.\n  - The document could be strengthened by more systematic outcome tracking and by including more perspectives from grantees and other stakeholders.\n",
      "summary": "## High-Level Analysis and Key Insights\n\nThis document offers a candid, insightful review of three standout grants from Manifund's AI safety regranting program. It highlights the unique strengths of the regranting model: leveraging the expertise and private knowledge of trusted regrantors, providing early and flexible funding to high-potential projects, and increasing transparency and signaling within the funding ecosystem. The cases of Timaeus (Developmental Interpretability), ChinaTalk (China and AI policy coverage), and Gavin Leech's AI safety review illustrate how early support can accelerate progress, attract further investment, and help seed new organizations or resources.\n\nA recurring theme is the importance of funding work that is both ambitious and underexplored, especially at the intersection of technical AI safety and policy. The document also underscores the value of public, transparent grants in building credibility and momentum for new ideas. The author's willingness to reflect on uncertainties and mixed outcomes adds to the document's credibility and usefulness as a learning tool for funders and practitioners alike.\n\nOverall, this review serves as a strong endorsement of the regranting approach for advancing AI safety, while also modeling transparency, humility, and a learning-oriented mindset in philanthropic decision-making."
    }
  ]
}