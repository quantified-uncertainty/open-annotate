{
  "id": "three-grants",
  "slug": "three-grants",
  "title": "Three Grants We Liked (Just the Positive Ones)",
  "author": "Unknown Author",
  "publishedDate": "2025-04-18",
  "platforms": [],
  "intendedAgents": [
    "ea-impact-evaluator",
    "clarity-coach",
    "research-scholar",
    "fake-eliezer",
    "quantitative-forecaster",
    "bias-detector"
  ],
  "content": "# Writing Session\n\nFor the past two years, Manifund has run a regranting program. We allocate annual donation budgets (from $50k-$400k) to selected experts (\"regrantors\"), who in turn choose great projects to fund. We'll be kicking off the 2025 program soon! In preparation, we're reviewing some of our favorite regrants to date — as well as some regrants we felt were less well-suited for this program. How we think about what makes a great regrant includes considerations such as \"is this project in an early stage where some initial funding could really help?\", \"could OpenPhil fund this instead?\" and \"does this regrant effectively leverage our regrantors' experience and expertise?\"\n\nWe think our regranting program is one of the best opportunities for donors who: care about AI safety, want to seed ambitious new projects, and like transparency and moving fast. If you may want to fund our 2025 program, please contact austin@manifund.org!\n\nAbout the author: Jesse Richardson recently joined Manifund after working at Mila - Quebec AI Institute, and also has a background in trading on prediction markets. This post is essentially Jesse's hot takes about various regrants, and is not intended to be definitive or particularly high-confidence.\n\n## Three awesome AI Safety regrants…\n\n### https://manifund.org/projects/scoping-developmental-interpretability-xg55b33wsfc - the first funding for Timaeus, accelerating research by months\n\nThis regrant was made in late 2023 to Jesse Hoogland and the rest of what is now the Timaeus team, for the purposes of exploring Developmental Interpretability (DevInterp) as a new AI alignment research agenda. Four of our regrantors — Evan Hubinger, Rachel Weinberg, Marcus Abramovitch and Ryan Kidd — made regrants to this project, for a total of $143,200. Evan had previously mentored Jesse Hoogland as part of MATS, and therefore had some additional context about the value of funding Hoogland's future research. This is the sweet spot for regranting: donors may have the public information that Evan Hubinger is an expert who does good work, and could potentially donate to him/his work on that basis, but regranting allows them to specifically leverage his private information about other valuable projects, such as DevInterp.\n\nBack to the grant itself; success for this project looked like determining whether DevInterp was a viable agenda to move forward with, rather than producing some seminal research outputs. I recommend reading more about DevInterp if you're interested, but my shallow understanding is that it aims to use insights from Singular Learning Theory (SLT) to make progress on AI alignment through interpretability, with a specific focus on how phase transitions in the training process lead to internal structure in neural networks.\n\nI'm not well placed to form an inside view on how likely DevInterp was/is to succeed, but this proposed research agenda had numerous things going for it:\nit was novel; the application of SLT to alignment was largely unexplored prior to this work,\nit seemed to be well thought out; the write-up on LessWrong included plenty of detail about why we might expect phase transitions to be a big deal and how this would relate to alignment, as well as a solid plan for the next six months,\nit had an element of \"big if true\" i.e. it may be unlikely that the strong version of the DevInterp thesis is true, but this research has potential to make meaningful progress on AI alignment if it is\n\nThese are all markers of projects I am excited to see funded through Manifund regranting. In addition to our four regrantors, this agenda was also endorsed by capable AI safety researchers such as Vanessa Kosoy and John Wentworth, and it makes sense to update on their judgement.\n\nIn addition to the agenda itself, I think this was a good team to bet on for this kind of work; they seem capable and have relevant experience e.g. ML research, and running the 2023 SLT & Alignment Summit.\n\nThis regrant is a strong example of where Manifund's regranting program can have the biggest impact: being early to support new projects & organizations, and thereby providing strong signals to other funders as well as some runway for these organizations to move quickly. In this case, Manifund's early funding helped Hoogland's team get off the ground, and they subsequently started a new organization (Timaeus) and received significantly more funding from other sources, such as $500,000 from the Survival & Flourishing Fund. It's probable that they would've gotten this other funding regardless, but not guaranteed, and I'm happy that Manifund was able to help bring Timaeus into existence several months sooner and with increased financial security. Jesse notes:\n\nGetting early support from Manifund made a real difference for us. This was the first funding we received for research and meant that we could start months earlier than we otherwise would have. The fact that it was public meant other funders could easily see who was backing our work and why. That transparency helped us build momentum and credibility for developmental interpretability research when it was still a new idea. I'm pretty sure it played a significant role in us securing later funding through SFF and other grantmakers.\n\nIn terms of concrete outcomes, there's a lot to be happy with here. Timaeus and its collaborators have put out numerous papers on DevInterp since this regrant was made, and it seems that DevInterp's key insight around the existence and significance of phase transitions has been validated. My sense is that the question of whether DevInterp is a worthwhile alignment research agenda to pursue has been successfully answered in the affirmative. It's also nice to see strong outreach and engagement with the research community on the part of Timaeus: November 2023 saw the first DevInterp conference, and they've given talks at OpenAI, Anthropic, and DeepMind.\n\n### https://manifund.org/projects/support-for-deep-coverage-of-china-and-ai - reporting on DeepSeek, ahead of the curve\n\nAcross 2023 & 2024, Manifund regrantors Joel Becker and Evan Hubinger granted a total of $37,000 to ChinaTalk, a newsletter and podcast that covers China, technology, and US-China relations. ChinaTalk has over 50,000 subscribers and is also notable for the quality of its coverage and the praise and attention it receives from elites and policymakers.\n\nBefore this regrant, ChinaTalk had been run by Jordan Schneider and Caithrin Rintoul, both part-time, on a budget of just $35,000/year. What they were able to accomplish in that time with those limited resources was impressive, and I believe merited additional funding, even just to allow Jordan to work on this full-time. More funding would also have meant ChinaTalk bringing on a full-time fellow who, per Jordan, \"would be, to my knowledge, the only researcher in the English-speaking world devoted solely to covering China and AI safety\". ChinaTalk has since received further funding and is in the process of growing to five full-time employees, but we would've loved for this to happen sooner through an expanded regranting program.\n\nEven putting aside the specific track record of ChinaTalk, it seems clear to me that the intersection of China and AI safety is an incredibly important area to cover, and at a high level it is very valuable to fund organizations that are doing this kind of work. It can be hard to imagine plausible scenarios of how the next decade goes well with respect to AI that don't run through US-China relations, and I am persuaded by Jordan's case that the amount of energy currently being expended on this is grossly inadequate.\n\nSince the first regrant was made, ChinaTalk's Substack audience has grown from 26,000 subscribers to 51,000 and they've put out regular high-quality content, including an English translation of an interview with DeepSeek CEO Liang Wenfeng, coverage of chip policy, and what important 2024 elections in the US and Taiwan mean for China. The ChinaTalk team has expanded to six people, allowing for a greater diversity and quantity of coverage, including YouTube videos. Jordan has also announced plans for launching a think tank—ChinaTalk Institute—this year, in a similar vein to IFP.\n\nAmong their varied coverage, I was particularly impressed to see how ChinaTalk was ahead of the curve on covering the rise of DeepSeek, while most of the West seemed to be taken by total surprise in January 2025. As a trader and forecaster, this advance insight might have been worth a lot of money to me through anticipating the market freakout, which suggests that I should pay more attention to ChinaTalk in the future, as their work is seriously valuable.\n\nOverall ChinaTalk has continued on the strong trajectory it was on in late 2023, and it was great that Manifund was able to support ChinaTalk in this success. For more information about why this grant was likely good ex ante, I encourage you to look at regrantor Joel Becker's comment on the subject. Joel's detail about why ChinaTalk was at the time insufficiently funded:\n\nPhilanthropists are scared to touch China, in part because of lack of expertise and in part for political reasons. Advertisers can be nervous for similar reasons… Jordan was hoping to support this work through subscriptions only.\n\nmakes me more optimistic that this regrant was very much the kind of thing the program should be doing: plugging holes in the funding landscape.\n\n### https://manifund.org/projects/shallow-review-of-ai-safety-2024 - two quick regrants, followed by OpenPhil support\n\nGavin Leech previously co-wrote https://www.lesswrong.com/posts/zaaGsFBeDTpCsYHef/shallow-review-of-live-agendas-in-alignment-and-safety in 2023, which was well-received and generally considered to be a useful resource for people looking to get a top-level picture of AI safety research. Given that it was intended to be a shallow review, this post has a lot of helpful detail and links about the ins and outs of various research agendas, e.g., the amount of resources currently devoted to each, and notable criticisms.\n\nLast year he sought funding to create an updated 2024 version of this post. He received $9,000 from Manifund regrantors Neel Nanda and Ryan Kidd, as well as $12,000 from other donors through the Manifund site.\n\nBig picture, I'm convinced that there should be an accessible and up-to-date resource of this kind; for people who are starting out in AI safety and don't know anything, for funders trying to get a sense of the landscape, or for anyone else who might need it. In 2022 I was at a stage where I wanted to contribute to AI safety but didn't know anything about it and was unsure where to start, and I would've likely found Gavin's review useful, in addition to the other resources that existed. Based on this, Gavin's record in a variety of fields, and the fact that the 2023 version seems to have been fairly well-executed, I think this regrant looked very promising.\n\nIn terms of what was produced, the new post (https://www.lesswrong.com/posts/fAW6RXLKTLHC3WXkS/shallow-review-of-technical-ai-safety-2024) came out in December 2024 and appears to be similarly comprehensive to the 2023 version, although it has gotten less attention (~half the upvotes on LessWrong and not curated). That's probably a bit worse of an outcome than I would've hoped for, but I still would have endorsed this grant had I known the outcome in advance. Presumably the updated version of a post is less eye-catching than the original, while still being necessary.\n\nThe way this was funded also shows the advantages of the Manifund regranting program. Gavin asked for between $8,000 (MVP version) and $17,000 (high-end version) and was quickly funded for the MVP by Neel and Ryan. He then got an additional $5,000 from OpenPhil, after Matt Putz at OpenPhil learned about this proposal via our EA Forum post; and a further $12,000 from other donors. I am happy with how the regranting program is both able to provide the small amount of funding to get a project off the ground, and increase visibility of that project so that other donors can step in and fund it to a greater extent. A couple of small negatives: (1) regrantor Neel Nanda is less optimistic than I am that this was a particularly good grant and (2) the high-end version was supposed to include a \"glossy formal report optimised for policy people\" which did not end up getting made (OpenPhil opted against funding it), however the excess money is instead going towards the 2025 edition. I look forward to it!",
  "reviews": [
    {
      "agentId": "bias-detector",
      "createdAt": "2025-04-18T23:01:06.319Z",
      "costInCents": 0,
      "comments": [
        {
          "title": "Selection Bias 🎯",
          "description": "The document explicitly states that it is reviewing only positive grant outcomes: *'reviewing some of our favorite regrants to date — as well as some regrants we felt were less well-suited for this program.'* However, the actual content focuses exclusively on successful or 'awesome' grants, as highlighted in the section title 'Three awesome AI Safety regrants…'. This selective reporting creates a skewed impression of the program's overall effectiveness and may mislead readers about the typical success rate. To mitigate, the document should include a balanced sample of both positive and negative outcomes, or at least provide summary statistics on the full distribution of results.",
          "importance": 95,
          "grade": 30,
          "isValid": false,
          "highlight": {
            "startOffset": 281,
            "endOffset": 1286,
            "quotedText": "reviewing some of our favorite regrants to date — as well as some regrants we felt were less well-suited for this program. How we think about what makes a great regrant includes considerations such as \"is this project in an early stage where some initial funding could really help?\", \"could OpenPhil fund this instead?\" and \"does this regrant effectively leverage our regrantors' experience and expertise?\"\n\nWe think our regranting program is one of the best opportunities for donors who: care about AI safety, want to seed ambitious new projects, and like transparency and moving fast. If you may want to fund our 2025 program, please contact austin@manifund.org!\n\nAbout the author: Jesse Richardson recently joined Manifund after working at Mila - Quebec AI Institute, and also has a background in trading on prediction markets. This post is essentially Jesse's hot takes about various regrants, and is not intended to be definitive or particularly high-confidence.\n\n## Three awesome AI Safety regrants…"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Confirmation Bias 🔍",
          "description": "Throughout the document, the author emphasizes evidence and anecdotes that support the effectiveness of the regranting program, while downplaying or omitting counterexamples or failures. For instance, the phrase *'these are all markers of projects I am excited to see funded through Manifund regranting'* signals a focus on confirming the program's value. This bias can reinforce pre-existing beliefs among stakeholders and obscure areas needing improvement. To address this, the author should systematically seek and present disconfirming evidence or lessons learned from less successful grants.",
          "importance": 85,
          "grade": 40,
          "isValid": true,
          "highlight": {
            "startOffset": 3484,
            "endOffset": 3763,
            "quotedText": "These are all markers of projects I am excited to see funded through Manifund regranting. In addition to our four regrantors, this agenda was also endorsed by capable AI safety researchers such as Vanessa Kosoy and John Wentworth, and it makes sense to update on their judgement."
          }
        },
        {
          "title": "Authority Bias 🏛️",
          "description": "The document repeatedly appeals to the authority of well-known figures and organizations to justify funding decisions, e.g., *'endorsed by capable AI safety researchers such as Vanessa Kosoy and John Wentworth'* and *'Evan had previously mentored Jesse Hoogland as part of MATS, and therefore had some additional context about the value of funding Hoogland's future research.'* While expert endorsement is valuable, over-reliance on authority can crowd out independent evaluation and novel perspectives. To mitigate, the document should clarify the specific criteria used for evaluation and balance authority-based endorsements with objective project assessments.",
          "importance": 70,
          "grade": 60,
          "isValid": true,
          "highlight": {
            "startOffset": 3631,
            "endOffset": 3763,
            "quotedText": "endorsed by capable AI safety researchers such as Vanessa Kosoy and John Wentworth, and it makes sense to update on their judgement."
          }
        },
        {
          "title": "Narrative Fallacy 📖",
          "description": "The text constructs compelling success stories around each grant, attributing positive outcomes to the regranting program's intervention (e.g., *'Manifund's early funding helped Hoogland's team get off the ground, and they subsequently started a new organization (Timaeus) and received significantly more funding from other sources'*). This narrative may oversimplify complex causal relationships and ignore other contributing factors. For a more accurate assessment, the document should acknowledge alternative explanations and the multifactorial nature of project success.",
          "importance": 75,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 4258,
            "endOffset": 4725,
            "quotedText": "Manifund's early funding helped Hoogland's team get off the ground, and they subsequently started a new organization (Timaeus) and received significantly more funding from other sources, such as $500,000 from the Survival & Flourishing Fund. It's probable that they would've gotten this other funding regardless, but not guaranteed, and I'm happy that Manifund was able to help bring Timaeus into existence several months sooner and with increased financial security."
          }
        },
        {
          "title": "Survivorship Bias 🏆",
          "description": "By focusing exclusively on successful or 'favorite' grants, the document risks giving the impression that most or all funded projects are similarly successful. This is a classic case of survivorship bias, which can mislead potential donors or applicants about the likelihood of positive outcomes. To counteract this, the document should provide context on the total number of grants, the proportion that met their goals, and examples of less successful projects.",
          "importance": 90,
          "grade": 30,
          "isValid": false,
          "highlight": {
            "startOffset": 1253,
            "endOffset": 3573,
            "quotedText": "Three awesome AI Safety regrants…\n\n### https://manifund.org/projects/scoping-developmental-interpretability-xg55b33wsfc - the first funding for Timaeus, accelerating research by months\n\nThis regrant was made in late 2023 to Jesse Hoogland and the rest of what is now the Timaeus team, for the purposes of exploring Developmental Interpretability (DevInterp) as a new AI alignment research agenda. Four of our regrantors — Evan Hubinger, Rachel Weinberg, Marcus Abramovitch and Ryan Kidd — made regrants to this project, for a total of $143,200. Evan had previously mentored Jesse Hoogland as part of MATS, and therefore had some additional context about the value of funding Hoogland's future research. This is the sweet spot for regranting: donors may have the public information that Evan Hubinger is an expert who does good work, and could potentially donate to him/his work on that basis, but regranting allows them to specifically leverage his private information about other valuable projects, such as DevInterp.\n\nBack to the grant itself; success for this project looked like determining whether DevInterp was a viable agenda to move forward with, rather than producing some seminal research outputs. I recommend reading more about DevInterp if you're interested, but my shallow understanding is that it aims to use insights from Singular Learning Theory (SLT) to make progress on AI alignment through interpretability, with a specific focus on how phase transitions in the training process lead to internal structure in neural networks.\n\nI'm not well placed to form an inside view on how likely DevInterp was/is to succeed, but this proposed research agenda had numerous things going for it:\nit was novel; the application of SLT to alignment was largely unexplored prior to this work,\nit seemed to be well thought out; the write-up on LessWrong included plenty of detail about why we might expect phase transitions to be a big deal and how this would relate to alignment, as well as a solid plan for the next six months,\nit had an element of \"big if true\" i.e. it may be unlikely that the strong version of the DevInterp thesis is true, but this research has potential to make meaningful progress on AI alignment if it is\n\nThese are all markers of projects I am excited to see funded through Manifund regranting."
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Optimism Bias 🌞",
          "description": "The language used throughout the document is highly positive and forward-looking, e.g., *'Overall ChinaTalk has continued on the strong trajectory it was on in late 2023, and it was great that Manifund was able to support ChinaTalk in this success.'* This optimism may lead to underestimating risks or challenges faced by grantees. To improve, the author should balance positive statements with critical reflection on obstacles, setbacks, or uncertainties.",
          "importance": 60,
          "grade": 65,
          "isValid": true,
          "highlight": {
            "startOffset": 8718,
            "endOffset": 9018,
            "quotedText": "Overall ChinaTalk has continued on the strong trajectory it was on in late 2023, and it was great that Manifund was able to support ChinaTalk in this success. For more information about why this grant was likely good ex ante, I encourage you to look at regrantor Joel Becker's comment on the subject."
          }
        },
        {
          "title": "Scope Insensitivity 🗺️",
          "description": "The document discusses grants ranging from $9,000 to $143,200, but does not contextualize the relative impact or cost-effectiveness of these grants. For example, the phrase *'Gavin asked for between $8,000 (MVP version) and $17,000 (high-end version) and was quickly funded for the MVP by Neel and Ryan.'* lacks analysis of how outcomes scale with funding. This scope insensitivity can obscure important differences in resource allocation. To address this, the document should include comparative metrics or discuss marginal returns on different grant sizes.",
          "importance": 55,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 11649,
            "endOffset": 11945,
            "quotedText": "Gavin asked for between $8,000 (MVP version) and $17,000 (high-end version) and was quickly funded for the MVP by Neel and Ryan. He then got an additional $5,000 from OpenPhil, after Matt Putz at OpenPhil learned about this proposal via our EA Forum post; and a further $12,000 from other donors."
          }
        },
        {
          "title": "In-group Bias 🤝",
          "description": "The document frequently references the involvement and endorsement of individuals within the Effective Altruism (EA) and AI safety communities, e.g., *'endorsed by capable AI safety researchers such as Vanessa Kosoy and John Wentworth'* and *'Evan had previously mentored Jesse Hoogland as part of MATS.'* This may unintentionally privilege projects and people within these networks, potentially overlooking valuable contributions from outsiders. To mitigate, the program should actively seek diverse perspectives and transparently report on the diversity of grantees.",
          "importance": 65,
          "grade": 45,
          "isValid": true,
          "highlight": {
            "startOffset": 1798,
            "endOffset": 2271,
            "quotedText": "Evan had previously mentored Jesse Hoogland as part of MATS, and therefore had some additional context about the value of funding Hoogland's future research. This is the sweet spot for regranting: donors may have the public information that Evan Hubinger is an expert who does good work, and could potentially donate to him/his work on that basis, but regranting allows them to specifically leverage his private information about other valuable projects, such as DevInterp."
          }
        },
        {
          "title": "Hindsight Bias ⏳",
          "description": "The author evaluates grants with the benefit of knowing their outcomes, e.g., *'My sense is that the question of whether DevInterp is a worthwhile alignment research agenda to pursue has been successfully answered in the affirmative.'* This can lead to overestimating the predictability of success and underappreciating the uncertainty present at the time of funding. To improve, the document should distinguish between ex ante (before the fact) and ex post (after the fact) evaluations, and discuss how uncertainty was managed.",
          "importance": 80,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 5566,
            "endOffset": 5934,
            "quotedText": "My sense is that the question of whether DevInterp is a worthwhile alignment research agenda to pursue has been successfully answered in the affirmative. It's also nice to see strong outreach and engagement with the research community on the part of Timaeus: November 2023 saw the first DevInterp conference, and they've given talks at OpenAI, Anthropic, and DeepMind."
          }
        }
      ],
      "thinking": "## Initial Impressions\nThe document is a reflective, positive review of three AI safety-related grants funded through Manifund's regranting program. The author, Jesse Richardson, is transparent about the subjective and non-definitive nature of the post. The stated purpose is to highlight 'just the positive ones,' which immediately signals a selection bias and a lack of balance. The tone is enthusiastic and supportive of the projects and the regranting model.\n\n## Key Biases Detected\n- **Selection Bias**: The document explicitly focuses only on positive examples ('just the positive ones'), omitting any negative or neutral cases. This creates an unrepresentative sample and may mislead readers about the overall effectiveness of the program.\n- **Confirmation Bias**: The author appears to seek and highlight evidence that supports the value of the regranting program and the selected projects, with little critical scrutiny or discussion of failures or limitations.\n- **Survivorship Bias**: By only discussing successful or promising grants, the document ignores projects that may have failed or underperformed, which could provide valuable learning opportunities.\n- **Authority Bias**: The document frequently references endorsements from well-known figures in the AI safety community (e.g., Evan Hubinger, Vanessa Kosoy, John Wentworth), potentially overvaluing their opinions without independent evaluation.\n- **Narrative Fallacy**: The stories are constructed to show a clear, positive trajectory from funding to success, potentially oversimplifying complex causal relationships.\n- **Optimism Bias**: The tone is generally optimistic about the impact of early funding and the trajectory of the projects, with limited discussion of risks or downsides.\n\n## Quantitative Bias Scores (1-10)\n- Selection Bias: 9\n- Confirmation Bias: 8\n- Survivorship Bias: 8\n- Authority Bias: 6\n- Narrative Fallacy: 6\n- Optimism Bias: 7\n- Other biases (e.g., omission bias, status quo bias): 3-5\n\n## Most Significant Examples\n- The explicit decision to only review positive grants is a textbook case of selection and survivorship bias.\n- The repeated reliance on endorsements from prominent community members (authority bias).\n- The lack of critical analysis or discussion of negative outcomes or lessons learned.\n\n## Recommendations for Improvement\n1. **Include Negative and Neutral Cases**: To provide a balanced assessment, the document should also discuss grants that did not meet expectations, including reasons for underperformance.\n2. **Explicitly Address Limitations**: Acknowledge the potential for bias in the selection and evaluation process, and discuss how these biases might affect conclusions.\n3. **Incorporate Quantitative Metrics**: Where possible, provide data on the overall success rate of grants, not just anecdotes.\n4. **Solicit External Perspectives**: Include feedback from independent evaluators or stakeholders outside the immediate community.\n5. **Reflect on Learning from Failures**: Highlight what can be learned from less successful grants to improve future decision-making.\n\n## Limitations of the Analysis\n- The analysis is based solely on the provided text and cannot assess the full context or unpublished data.\n- The subjective nature of the post is acknowledged by the author, which may limit the generalizability of the findings.\n- Some biases may be mitigated by the author's transparency about the post's intent, but the overall impact remains significant.",
      "summary": "## Executive Summary of Bias Assessment\n\nThe document 'Three Grants We Liked (Just the Positive Ones)' demonstrates a high degree of selection and confirmation bias, as it intentionally focuses only on successful or promising grants within Manifund's regranting program. This approach, while transparent about its subjectivity, results in a skewed portrayal of the program's effectiveness and omits valuable lessons from less successful projects. The narrative is shaped by authority bias, with frequent references to endorsements from prominent figures, and is further colored by optimism and narrative fallacy, presenting a linear and positive trajectory from funding to impact. While the document is engaging and informative about the highlighted projects, its lack of critical analysis and omission of negative cases limit its value as an objective assessment. To improve, future reviews should incorporate a more balanced selection of cases, quantitative metrics, and external perspectives, as well as explicit reflection on failures and limitations. Overall, the document serves as a promotional piece rather than a rigorous evaluation, and readers should be cautious about drawing broad conclusions from its content.",
      "grade": 62
    },
    {
      "agentId": "ea-impact-evaluator",
      "createdAt": "2025-04-18T23:38:50.837Z",
      "costInCents": 0,
      "comments": [
        {
          "title": "🎯 Importance of Early-Stage AI Alignment Funding",
          "description": "The text highlights the strategic value of providing early funding to novel AI alignment agendas, specifically DevInterp. This aligns well with EA's focus on high-leverage, early-stage interventions that can shape the research landscape. The claim that Manifund's support accelerated Timaeus by months is significant, as time is a critical factor in AI safety. However, a more explicit quantification of the counterfactual impact (e.g., how much earlier, and what downstream effects) would strengthen the argument.",
          "importance": 85,
          "grade": 90,
          "isValid": true,
          "highlight": {
            "startOffset": 3972,
            "endOffset": 4725,
            "quotedText": "This regrant is a strong example of where Manifund's regranting program can have the biggest impact: being early to support new projects & organizations, and thereby providing strong signals to other funders as well as some runway for these organizations to move quickly. In this case, Manifund's early funding helped Hoogland's team get off the ground, and they subsequently started a new organization (Timaeus) and received significantly more funding from other sources, such as $500,000 from the Survival & Flourishing Fund. It's probable that they would've gotten this other funding regardless, but not guaranteed, and I'm happy that Manifund was able to help bring Timaeus into existence several months sooner and with increased financial security."
          }
        },
        {
          "title": "🆕 Novelty Assessment: Application of Singular Learning Theory",
          "description": "The document notes that applying Singular Learning Theory (SLT) to AI alignment was largely unexplored prior to this work. This is a clear marker of novelty, which is highly valued in EA research funding. The explicit mention of phase transitions as a new angle for interpretability research further differentiates this project from existing approaches. However, the assessment would benefit from a more detailed comparison to other interpretability agendas.",
          "importance": 70,
          "grade": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 2953,
            "endOffset": 3281,
            "quotedText": "it was novel; the application of SLT to alignment was largely unexplored prior to this work,\nit seemed to be well thought out; the write-up on LessWrong included plenty of detail about why we might expect phase transitions to be a big deal and how this would relate to alignment, as well as a solid plan for the next six months,"
          }
        },
        {
          "title": "📈 Neglectedness: Funding Gaps in China-AI Coverage",
          "description": "The author identifies a significant funding gap in English-language coverage of China and AI safety, citing both philanthropic and commercial hesitancy. This is a strong case for neglectedness, a core EA criterion. The claim that ChinaTalk was the only English-language researcher focused solely on this intersection is compelling, though it would be strengthened by more systematic evidence (e.g., a landscape review).",
          "importance": 80,
          "grade": 88,
          "isValid": true,
          "highlight": {
            "startOffset": 9093,
            "endOffset": 9468,
            "quotedText": "Philanthropists are scared to touch China, in part because of lack of expertise and in part for political reasons. Advertisers can be nervous for similar reasons… Jordan was hoping to support this work through subscriptions only.\n\nmakes me more optimistic that this regrant was very much the kind of thing the program should be doing: plugging holes in the funding landscape."
          }
        },
        {
          "title": "💡 Tractability: Concrete Outputs and Community Engagement",
          "description": "The Timaeus grant led to tangible outputs: multiple papers, a conference, and talks at major AI labs. These are strong indicators of tractability and implementation feasibility. The explicit mention of community engagement and validation of key research insights further supports the project's actionability. However, the long-term impact on alignment progress remains uncertain.",
          "importance": 75,
          "grade": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 5344,
            "endOffset": 5934,
            "quotedText": "Timaeus and its collaborators have put out numerous papers on DevInterp since this regrant was made, and it seems that DevInterp's key insight around the existence and significance of phase transitions has been validated. My sense is that the question of whether DevInterp is a worthwhile alignment research agenda to pursue has been successfully answered in the affirmative. It's also nice to see strong outreach and engagement with the research community on the part of Timaeus: November 2023 saw the first DevInterp conference, and they've given talks at OpenAI, Anthropic, and DeepMind."
          }
        },
        {
          "title": "🔢 Fermi Estimate: Impact of Early Funding for Timaeus",
          "description": "Assuming Manifund's $143,200 enabled Timaeus to start 6 months earlier, and that this acceleration increased the probability of meaningful alignment progress by 0.01% (1 in 10,000), with the value of existential risk reduction estimated at $10T per 0.01% risk reduction, the expected value is $1M. Adjusting for a 50% chance that the acceleration was truly counterfactual, the net expected value is $500,000, or ~$3.5 per dollar spent (90% CI: $0.5–$20). This is highly competitive with EA benchmarks, though uncertainty is large.",
          "importance": 90,
          "grade": 92,
          "isValid": true,
          "highlight": {
            "startOffset": 4807,
            "endOffset": 5165,
            "quotedText": "This was the first funding we received for research and meant that we could start months earlier than we otherwise would have. The fact that it was public meant other funders could easily see who was backing our work and why. That transparency helped us build momentum and credibility for developmental interpretability research when it was still a new idea."
          }
        },
        {
          "title": "📚 Value of Information: Shallow Review of AI Safety",
          "description": "The shallow review project is a classic example of high-EVOI (expected value of information) research. Even if the 2024 update received less attention, the existence of an up-to-date, accessible resource likely saves dozens of hours for new researchers and funders. A Fermi estimate: If 100 people each save 5 hours of research time (valued at $50/hr), that's $25,000 in direct value, exceeding the $9,000 grant. However, the lack of a 'glossy' policy report is a missed opportunity for broader impact.",
          "importance": 65,
          "grade": 78,
          "isValid": false,
          "highlight": {
            "startOffset": 10321,
            "endOffset": 11561,
            "quotedText": "Big picture, I'm convinced that there should be an accessible and up-to-date resource of this kind; for people who are starting out in AI safety and don't know anything, for funders trying to get a sense of the landscape, or for anyone else who might need it. In 2022 I was at a stage where I wanted to contribute to AI safety but didn't know anything about it and was unsure where to start, and I would've likely found Gavin's review useful, in addition to the other resources that existed. Based on this, Gavin's record in a variety of fields, and the fact that the 2023 version seems to have been fairly well-executed, I think this regrant looked very promising.\n\nIn terms of what was produced, the new post (https://www.lesswrong.com/posts/fAW6RXLKTLHC3WXkS/shallow-review-of-technical-ai-safety-2024) came out in December 2024 and appears to be similarly comprehensive to the 2023 version, although it has gotten less attention (~half the upvotes on LessWrong and not curated). That's probably a bit worse of an outcome than I would've hoped for, but I still would have endorsed this grant had I known the outcome in advance. Presumably the updated version of a post is less eye-catching than the original, while still being necessary."
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "⚠️ Negative Impact Consideration: Opportunity Cost and Duplication",
          "description": "The document briefly acknowledges that the 2024 shallow review received less attention and that a more polished policy report was not produced. This raises the risk of diminishing returns or duplication with existing resources. The opportunity cost—funds could have gone to more novel or underfunded projects—should be explicitly weighed. The section would benefit from a more thorough discussion of potential negative externalities.",
          "importance": 55,
          "grade": 60,
          "isValid": false,
          "highlight": {
            "startOffset": 11304,
            "endOffset": 12547,
            "quotedText": "That's probably a bit worse of an outcome than I would've hoped for, but I still would have endorsed this grant had I known the outcome in advance. Presumably the updated version of a post is less eye-catching than the original, while still being necessary.\n\nThe way this was funded also shows the advantages of the Manifund regranting program. Gavin asked for between $8,000 (MVP version) and $17,000 (high-end version) and was quickly funded for the MVP by Neel and Ryan. He then got an additional $5,000 from OpenPhil, after Matt Putz at OpenPhil learned about this proposal via our EA Forum post; and a further $12,000 from other donors. I am happy with how the regranting program is both able to provide the small amount of funding to get a project off the ground, and increase visibility of that project so that other donors can step in and fund it to a greater extent. A couple of small negatives: (1) regrantor Neel Nanda is less optimistic than I am that this was a particularly good grant and (2) the high-end version was supposed to include a \"glossy formal report optimised for policy people\" which did not end up getting made (OpenPhil opted against funding it), however the excess money is instead going towards the 2025 edition."
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "🔍 Transparency and Leverage in Regranting",
          "description": "The text emphasizes how public, transparent regranting can leverage the expertise and private information of domain experts, influencing other funders and accelerating promising projects. This is a strong meta-level impact, as it improves the efficiency of the philanthropic ecosystem. However, the risk of over-reliance on insider networks should be monitored.",
          "importance": 60,
          "grade": 85,
          "isValid": false,
          "highlight": {
            "startOffset": 2146,
            "endOffset": 5165,
            "quotedText": "but regranting allows them to specifically leverage his private information about other valuable projects, such as DevInterp.\n\nBack to the grant itself; success for this project looked like determining whether DevInterp was a viable agenda to move forward with, rather than producing some seminal research outputs. I recommend reading more about DevInterp if you're interested, but my shallow understanding is that it aims to use insights from Singular Learning Theory (SLT) to make progress on AI alignment through interpretability, with a specific focus on how phase transitions in the training process lead to internal structure in neural networks.\n\nI'm not well placed to form an inside view on how likely DevInterp was/is to succeed, but this proposed research agenda had numerous things going for it:\nit was novel; the application of SLT to alignment was largely unexplored prior to this work,\nit seemed to be well thought out; the write-up on LessWrong included plenty of detail about why we might expect phase transitions to be a big deal and how this would relate to alignment, as well as a solid plan for the next six months,\nit had an element of \"big if true\" i.e. it may be unlikely that the strong version of the DevInterp thesis is true, but this research has potential to make meaningful progress on AI alignment if it is\n\nThese are all markers of projects I am excited to see funded through Manifund regranting. In addition to our four regrantors, this agenda was also endorsed by capable AI safety researchers such as Vanessa Kosoy and John Wentworth, and it makes sense to update on their judgement.\n\nIn addition to the agenda itself, I think this was a good team to bet on for this kind of work; they seem capable and have relevant experience e.g. ML research, and running the 2023 SLT & Alignment Summit.\n\nThis regrant is a strong example of where Manifund's regranting program can have the biggest impact: being early to support new projects & organizations, and thereby providing strong signals to other funders as well as some runway for these organizations to move quickly. In this case, Manifund's early funding helped Hoogland's team get off the ground, and they subsequently started a new organization (Timaeus) and received significantly more funding from other sources, such as $500,000 from the Survival & Flourishing Fund. It's probable that they would've gotten this other funding regardless, but not guaranteed, and I'm happy that Manifund was able to help bring Timaeus into existence several months sooner and with increased financial security. Jesse notes:\n\nGetting early support from Manifund made a real difference for us. This was the first funding we received for research and meant that we could start months earlier than we otherwise would have. The fact that it was public meant other funders could easily see who was backing our work and why. That transparency helped us build momentum and credibility for developmental interpretability research when it was still a new idea."
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "📝 Authorial Confidence and Limitations",
          "description": "The author is explicit about the subjective and non-definitive nature of these assessments. This transparency about epistemic status is a strength, as it helps readers appropriately calibrate their trust. However, the analysis would be improved by more systematic, quantitative impact estimates and explicit consideration of counterfactuals.",
          "importance": 40,
          "grade": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 1112,
            "endOffset": 687,
            "quotedText": "\n\nWe think our regranting program is one of the best opportunities for donors who: care about AI safety, want to seed ambitious new projects, and like transparency and moving fast. If you may want to fund our 2025 program, please contact austin@manifund.org!\n\nAbout the author: Jesse Richardson recently joined Manifund after working at Mila - Quebec AI Institute, and also has a background in trading on prediction markets. "
          }
        }
      ],
      "thinking": "## Initial Impressions\n- The post is a reflective, qualitative review of three AI safety-related grants made through Manifund's regranting program.\n- The author highlights the value of early-stage, high-leverage funding, especially for projects that are novel, underfunded, or have strong potential for outsized impact.\n- The three grants span: (1) foundational research (DevInterp/Timaeus), (2) policy-relevant journalism (ChinaTalk), and (3) field-mapping research (AI safety agenda review).\n- The post is candid about uncertainty and limitations, e.g., the author admits to not having deep technical expertise in some areas, and notes when outcomes were less than ideal.\n- There is a strong emphasis on counterfactual impact: e.g., did Manifund's early funding accelerate projects, fill funding gaps, or catalyze further support?\n- The post is positive in tone, focusing only on grants that went well, and does not attempt a systematic or critical review of failures or negative externalities.\n- The author references EA-style reasoning: importance, neglectedness, and leverage are all discussed, though not in a formalized or quantitative way.\n\n## Key Points for EA Impact Evaluation\n- **Importance:** All three grants are in the AI safety/AI governance space, which is widely considered high-importance within EA, especially from a longtermist perspective. The ChinaTalk grant also touches on geopolitics, a critical leverage point for AI outcomes.\n- **Neglectedness:** The post makes a strong case that these projects were underfunded or overlooked by mainstream funders (e.g., ChinaTalk's difficulty attracting philanthropic or advertising support, DevInterp as a new research agenda).\n- **Tractability:** The grants are described as enabling concrete outputs: new research, expanded journalism, and updated field reviews. There is evidence of follow-on funding and increased visibility.\n- **Novelty:** DevInterp is highlighted as a novel research direction; ChinaTalk as a rare, high-quality source on China/AI; the AI safety review as a valuable, regularly updated resource.\n- **Fermi Estimation:** The post does not provide explicit quantitative impact estimates, but does mention dollar amounts, audience sizes, and some downstream effects (e.g., Timaeus securing $500k+ in follow-on funding, ChinaTalk's subscriber growth).\n- **Potential Negatives:** The post does not discuss possible negative externalities (e.g., information hazards, opportunity costs, or the risk of crowding out more effective interventions).\n\n## Patterns and Themes\n- Early-stage, risk-tolerant funding can have outsized impact by accelerating promising projects and signaling quality to larger funders.\n- Leveraging the private knowledge and networks of expert regrantors is a key value-add.\n- Transparency and public signaling (e.g., public grants, visible endorsements) help build momentum and credibility for new ideas.\n- There is a recurring theme of 'plugging holes' in the funding landscape, especially for projects that are too new, small, or politically sensitive for mainstream funders.\n- The post is self-aware about its limitations (e.g., not a comprehensive or high-confidence review).\n\n## Expert Perspective\n- The post is a strong qualitative case study in the value of meta-funding and regranting within the EA/AI safety ecosystem.\n- It would benefit from more explicit quantitative impact analysis, consideration of negative effects, and systematic comparison to EA benchmarks.\n- The focus on only positive grants introduces selection bias; a more balanced review would be more informative for donors and evaluators.\n- The grants described are plausibly high-impact, but the lack of formal impact modeling limits confidence in the magnitude of their effects.\n- The post is well-aligned with EA principles in spirit, but less so in formal rigor.",
      "summary": "## Executive Summary\n\nThis post provides a qualitative review of three AI safety-related grants made through Manifund's regranting program, highlighting the program's ability to accelerate early-stage, high-potential projects by leveraging expert judgment and filling gaps in the funding landscape. The grants span foundational research (DevInterp/Timaeus), policy-relevant journalism (ChinaTalk), and field-mapping research (AI safety agenda review), each demonstrating strong alignment with effective altruism priorities: importance, neglectedness, and leverage. Notably, the post emphasizes the catalytic role of early funding in enabling subsequent growth and attracting larger funders, as well as the value of transparency and expert endorsement.\n\nHowever, the analysis is primarily qualitative and lacks explicit quantitative impact estimates, formal counterfactual reasoning, or consideration of potential negative effects—key components of rigorous EA evaluation. The focus on only positive grants introduces selection bias, limiting the post's utility for donors seeking a balanced assessment. While the grants described are plausibly high-impact and well-chosen, the absence of formal modeling or comparison to EA benchmarks constrains confidence in the magnitude of their effects. Overall, the post serves as a compelling narrative case for the value of regranting in the AI safety space, but would benefit from greater analytical rigor and comprehensiveness.",
      "grade": 78
    },
    {
      "agentId": "clarity-coach",
      "createdAt": "2025-04-18T23:39:15.310Z",
      "costInCents": 0,
      "comments": [
        {
          "title": "Strong Opening Context and Purpose Statement",
          "description": "The introduction clearly establishes the context, purpose, and scope of the document. It succinctly explains Manifund's regranting program, the review process, and the criteria for evaluating grants. This clarity helps orient readers and sets expectations for the rest of the piece. The use of direct questions (e.g., 'is this project in an early stage...') effectively signals the evaluative framework. This approach enhances readability and audience alignment, especially for new readers.",
          "importance": 85,
          "grade": 95,
          "isValid": true,
          "highlight": {
            "startOffset": 19,
            "endOffset": 687,
            "quotedText": "For the past two years, Manifund has run a regranting program. We allocate annual donation budgets (from $50k-$400k) to selected experts (\"regrantors\"), who in turn choose great projects to fund. We'll be kicking off the 2025 program soon! In preparation, we're reviewing some of our favorite regrants to date — as well as some regrants we felt were less well-suited for this program. How we think about what makes a great regrant includes considerations such as \"is this project in an early stage where some initial funding could really help?\", \"could OpenPhil fund this instead?\" and \"does this regrant effectively leverage our regrantors' experience and expertise?\""
          }
        },
        {
          "title": "Effective Use of Specific Examples to Illustrate Impact",
          "description": "The document excels at using concrete examples to demonstrate the impact of grants. By detailing the Timaeus grant and its outcomes, the author provides tangible evidence of success, which increases credibility and reader engagement. This specificity supports both precision and audience alignment, making abstract concepts more accessible. Consider maintaining this level of detail throughout for consistency.",
          "importance": 80,
          "grade": 92,
          "isValid": true,
          "highlight": {
            "startOffset": 1439,
            "endOffset": 2271,
            "quotedText": "This regrant was made in late 2023 to Jesse Hoogland and the rest of what is now the Timaeus team, for the purposes of exploring Developmental Interpretability (DevInterp) as a new AI alignment research agenda. Four of our regrantors — Evan Hubinger, Rachel Weinberg, Marcus Abramovitch and Ryan Kidd — made regrants to this project, for a total of $143,200. Evan had previously mentored Jesse Hoogland as part of MATS, and therefore had some additional context about the value of funding Hoogland's future research. This is the sweet spot for regranting: donors may have the public information that Evan Hubinger is an expert who does good work, and could potentially donate to him/his work on that basis, but regranting allows them to specifically leverage his private information about other valuable projects, such as DevInterp."
          }
        },
        {
          "title": "Occasional Overly Long Sentences Impact Readability",
          "description": "Some sentences are notably long and complex, which can hinder readability, especially for less expert audiences. Breaking these into shorter, more digestible units would improve flow and comprehension. For example, the sentence beginning 'Evan had previously mentored Jesse Hoogland...' could be split for clarity. Shorter sentences also help maintain reader attention.",
          "importance": 70,
          "grade": 60,
          "isValid": true,
          "highlight": {
            "startOffset": 1798,
            "endOffset": 2271,
            "quotedText": "Evan had previously mentored Jesse Hoogland as part of MATS, and therefore had some additional context about the value of funding Hoogland's future research. This is the sweet spot for regranting: donors may have the public information that Evan Hubinger is an expert who does good work, and could potentially donate to him/his work on that basis, but regranting allows them to specifically leverage his private information about other valuable projects, such as DevInterp."
          }
        },
        {
          "title": "Balanced Use of Technical Jargon with Adequate Explanation",
          "description": "The document introduces technical terms (e.g., 'Developmental Interpretability', 'Singular Learning Theory', 'phase transitions') but generally provides sufficient context or explanation for non-expert readers. This balance preserves necessary complexity while maintaining accessibility. However, consider briefly defining 'SLT' on first use for maximum clarity.",
          "importance": 65,
          "grade": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 2461,
            "endOffset": 2797,
            "quotedText": "I recommend reading more about DevInterp if you're interested, but my shallow understanding is that it aims to use insights from Singular Learning Theory (SLT) to make progress on AI alignment through interpretability, with a specific focus on how phase transitions in the training process lead to internal structure in neural networks."
          }
        },
        {
          "title": "Strong Thematic Cohesion and Logical Progression",
          "description": "The document maintains a clear thematic focus on the value and impact of regranting, with each example reinforcing the central message. Transitions between sections are smooth, and the structure (introduction, three case studies, conclusion) is logical and easy to follow. This coherence aids reader navigation and comprehension.",
          "importance": 75,
          "grade": 90,
          "isValid": false,
          "highlight": {
            "startOffset": 1250,
            "endOffset": 12569,
            "quotedText": "## Three awesome AI Safety regrants…\n\n### https://manifund.org/projects/scoping-developmental-interpretability-xg55b33wsfc - the first funding for Timaeus, accelerating research by months\n\nThis regrant was made in late 2023 to Jesse Hoogland and the rest of what is now the Timaeus team, for the purposes of exploring Developmental Interpretability (DevInterp) as a new AI alignment research agenda. Four of our regrantors — Evan Hubinger, Rachel Weinberg, Marcus Abramovitch and Ryan Kidd — made regrants to this project, for a total of $143,200. Evan had previously mentored Jesse Hoogland as part of MATS, and therefore had some additional context about the value of funding Hoogland's future research. This is the sweet spot for regranting: donors may have the public information that Evan Hubinger is an expert who does good work, and could potentially donate to him/his work on that basis, but regranting allows them to specifically leverage his private information about other valuable projects, such as DevInterp.\n\nBack to the grant itself; success for this project looked like determining whether DevInterp was a viable agenda to move forward with, rather than producing some seminal research outputs. I recommend reading more about DevInterp if you're interested, but my shallow understanding is that it aims to use insights from Singular Learning Theory (SLT) to make progress on AI alignment through interpretability, with a specific focus on how phase transitions in the training process lead to internal structure in neural networks.\n\nI'm not well placed to form an inside view on how likely DevInterp was/is to succeed, but this proposed research agenda had numerous things going for it:\nit was novel; the application of SLT to alignment was largely unexplored prior to this work,\nit seemed to be well thought out; the write-up on LessWrong included plenty of detail about why we might expect phase transitions to be a big deal and how this would relate to alignment, as well as a solid plan for the next six months,\nit had an element of \"big if true\" i.e. it may be unlikely that the strong version of the DevInterp thesis is true, but this research has potential to make meaningful progress on AI alignment if it is\n\nThese are all markers of projects I am excited to see funded through Manifund regranting. In addition to our four regrantors, this agenda was also endorsed by capable AI safety researchers such as Vanessa Kosoy and John Wentworth, and it makes sense to update on their judgement.\n\nIn addition to the agenda itself, I think this was a good team to bet on for this kind of work; they seem capable and have relevant experience e.g. ML research, and running the 2023 SLT & Alignment Summit.\n\nThis regrant is a strong example of where Manifund's regranting program can have the biggest impact: being early to support new projects & organizations, and thereby providing strong signals to other funders as well as some runway for these organizations to move quickly. In this case, Manifund's early funding helped Hoogland's team get off the ground, and they subsequently started a new organization (Timaeus) and received significantly more funding from other sources, such as $500,000 from the Survival & Flourishing Fund. It's probable that they would've gotten this other funding regardless, but not guaranteed, and I'm happy that Manifund was able to help bring Timaeus into existence several months sooner and with increased financial security. Jesse notes:\n\nGetting early support from Manifund made a real difference for us. This was the first funding we received for research and meant that we could start months earlier than we otherwise would have. The fact that it was public meant other funders could easily see who was backing our work and why. That transparency helped us build momentum and credibility for developmental interpretability research when it was still a new idea. I'm pretty sure it played a significant role in us securing later funding through SFF and other grantmakers.\n\nIn terms of concrete outcomes, there's a lot to be happy with here. Timaeus and its collaborators have put out numerous papers on DevInterp since this regrant was made, and it seems that DevInterp's key insight around the existence and significance of phase transitions has been validated. My sense is that the question of whether DevInterp is a worthwhile alignment research agenda to pursue has been successfully answered in the affirmative. It's also nice to see strong outreach and engagement with the research community on the part of Timaeus: November 2023 saw the first DevInterp conference, and they've given talks at OpenAI, Anthropic, and DeepMind.\n\n### https://manifund.org/projects/support-for-deep-coverage-of-china-and-ai - reporting on DeepSeek, ahead of the curve\n\nAcross 2023 & 2024, Manifund regrantors Joel Becker and Evan Hubinger granted a total of $37,000 to ChinaTalk, a newsletter and podcast that covers China, technology, and US-China relations. ChinaTalk has over 50,000 subscribers and is also notable for the quality of its coverage and the praise and attention it receives from elites and policymakers.\n\nBefore this regrant, ChinaTalk had been run by Jordan Schneider and Caithrin Rintoul, both part-time, on a budget of just $35,000/year. What they were able to accomplish in that time with those limited resources was impressive, and I believe merited additional funding, even just to allow Jordan to work on this full-time. More funding would also have meant ChinaTalk bringing on a full-time fellow who, per Jordan, \"would be, to my knowledge, the only researcher in the English-speaking world devoted solely to covering China and AI safety\". ChinaTalk has since received further funding and is in the process of growing to five full-time employees, but we would've loved for this to happen sooner through an expanded regranting program.\n\nEven putting aside the specific track record of ChinaTalk, it seems clear to me that the intersection of China and AI safety is an incredibly important area to cover, and at a high level it is very valuable to fund organizations that are doing this kind of work. It can be hard to imagine plausible scenarios of how the next decade goes well with respect to AI that don't run through US-China relations, and I am persuaded by Jordan's case that the amount of energy currently being expended on this is grossly inadequate.\n\nSince the first regrant was made, ChinaTalk's Substack audience has grown from 26,000 subscribers to 51,000 and they've put out regular high-quality content, including an English translation of an interview with DeepSeek CEO Liang Wenfeng, coverage of chip policy, and what important 2024 elections in the US and Taiwan mean for China. The ChinaTalk team has expanded to six people, allowing for a greater diversity and quantity of coverage, including YouTube videos. Jordan has also announced plans for launching a think tank—ChinaTalk Institute—this year, in a similar vein to IFP.\n\nAmong their varied coverage, I was particularly impressed to see how ChinaTalk was ahead of the curve on covering the rise of DeepSeek, while most of the West seemed to be taken by total surprise in January 2025. As a trader and forecaster, this advance insight might have been worth a lot of money to me through anticipating the market freakout, which suggests that I should pay more attention to ChinaTalk in the future, as their work is seriously valuable.\n\nOverall ChinaTalk has continued on the strong trajectory it was on in late 2023, and it was great that Manifund was able to support ChinaTalk in this success. For more information about why this grant was likely good ex ante, I encourage you to look at regrantor Joel Becker's comment on the subject. Joel's detail about why ChinaTalk was at the time insufficiently funded:\n\nPhilanthropists are scared to touch China, in part because of lack of expertise and in part for political reasons. Advertisers can be nervous for similar reasons… Jordan was hoping to support this work through subscriptions only.\n\nmakes me more optimistic that this regrant was very much the kind of thing the program should be doing: plugging holes in the funding landscape.\n\n### https://manifund.org/projects/shallow-review-of-ai-safety-2024 - two quick regrants, followed by OpenPhil support\n\nGavin Leech previously co-wrote https://www.lesswrong.com/posts/zaaGsFBeDTpCsYHef/shallow-review-of-live-agendas-in-alignment-and-safety in 2023, which was well-received and generally considered to be a useful resource for people looking to get a top-level picture of AI safety research. Given that it was intended to be a shallow review, this post has a lot of helpful detail and links about the ins and outs of various research agendas, e.g., the amount of resources currently devoted to each, and notable criticisms.\n\nLast year he sought funding to create an updated 2024 version of this post. He received $9,000 from Manifund regrantors Neel Nanda and Ryan Kidd, as well as $12,000 from other donors through the Manifund site.\n\nBig picture, I'm convinced that there should be an accessible and up-to-date resource of this kind; for people who are starting out in AI safety and don't know anything, for funders trying to get a sense of the landscape, or for anyone else who might need it. In 2022 I was at a stage where I wanted to contribute to AI safety but didn't know anything about it and was unsure where to start, and I would've likely found Gavin's review useful, in addition to the other resources that existed. Based on this, Gavin's record in a variety of fields, and the fact that the 2023 version seems to have been fairly well-executed, I think this regrant looked very promising.\n\nIn terms of what was produced, the new post (https://www.lesswrong.com/posts/fAW6RXLKTLHC3WXkS/shallow-review-of-technical-ai-safety-2024) came out in December 2024 and appears to be similarly comprehensive to the 2023 version, although it has gotten less attention (~half the upvotes on LessWrong and not curated). That's probably a bit worse of an outcome than I would've hoped for, but I still would have endorsed this grant had I known the outcome in advance. Presumably the updated version of a post is less eye-catching than the original, while still being necessary.\n\nThe way this was funded also shows the advantages of the Manifund regranting program. Gavin asked for between $8,000 (MVP version) and $17,000 (high-end version) and was quickly funded for the MVP by Neel and Ryan. He then got an additional $5,000 from OpenPhil, after Matt Putz at OpenPhil learned about this proposal via our EA Forum post; and a further $12,000 from other donors. I am happy with how the regranting program is both able to provide the small amount of funding to get a project off the ground, and increase visibility of that project so that other donors can step in and fund it to a greater extent. A couple of small negatives: (1) regrantor Neel Nanda is less optimistic than I am that this was a particularly good grant and (2) the high-end version was supposed to include a \"glossy formal report optimised for policy people\" which did not end up getting made (OpenPhil opted against funding it), however the excess money is instead going towards the 2025 edition. I look forward to it!"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Occasional Informal Tone and Hedging Language",
          "description": "The use of informal phrases (e.g., 'hot takes', 'big if true', 'I'm pretty sure') and hedging ('my shallow understanding', 'my sense is') adds personality but may reduce perceived authority or precision. While this style can increase relatability, consider balancing informality with more assertive, evidence-based statements, especially in evaluative contexts.",
          "importance": 60,
          "grade": 65,
          "isValid": false,
          "highlight": {
            "startOffset": 1112,
            "endOffset": 3482,
            "quotedText": "This post is essentially Jesse's hot takes about various regrants, and is not intended to be definitive or particularly high-confidence.\n\n## Three awesome AI Safety regrants…\n\n### https://manifund.org/projects/scoping-developmental-interpretability-xg55b33wsfc - the first funding for Timaeus, accelerating research by months\n\nThis regrant was made in late 2023 to Jesse Hoogland and the rest of what is now the Timaeus team, for the purposes of exploring Developmental Interpretability (DevInterp) as a new AI alignment research agenda. Four of our regrantors — Evan Hubinger, Rachel Weinberg, Marcus Abramovitch and Ryan Kidd — made regrants to this project, for a total of $143,200. Evan had previously mentored Jesse Hoogland as part of MATS, and therefore had some additional context about the value of funding Hoogland's future research. This is the sweet spot for regranting: donors may have the public information that Evan Hubinger is an expert who does good work, and could potentially donate to him/his work on that basis, but regranting allows them to specifically leverage his private information about other valuable projects, such as DevInterp.\n\nBack to the grant itself; success for this project looked like determining whether DevInterp was a viable agenda to move forward with, rather than producing some seminal research outputs. I recommend reading more about DevInterp if you're interested, but my shallow understanding is that it aims to use insights from Singular Learning Theory (SLT) to make progress on AI alignment through interpretability, with a specific focus on how phase transitions in the training process lead to internal structure in neural networks.\n\nI'm not well placed to form an inside view on how likely DevInterp was/is to succeed, but this proposed research agenda had numerous things going for it:\nit was novel; the application of SLT to alignment was largely unexplored prior to this work,\nit seemed to be well thought out; the write-up on LessWrong included plenty of detail about why we might expect phase transitions to be a big deal and how this would relate to alignment, as well as a solid plan for the next six months,\nit had an element of \"big if true\" i.e. it may be unlikely that the strong version of the DevInterp thesis is true, but this research has potential to make meaningful progress on AI alignment if it is"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Effective Use of Quantitative Data to Support Claims",
          "description": "The inclusion of specific figures (e.g., grant amounts, subscriber growth, funding timelines) strengthens the document's credibility and allows readers to assess impact quantitatively. This practice enhances precision and audience alignment, especially for readers interested in measurable outcomes.",
          "importance": 75,
          "grade": 93,
          "isValid": false,
          "highlight": {
            "startOffset": 1650,
            "endOffset": 7829,
            "quotedText": "Four of our regrantors — Evan Hubinger, Rachel Weinberg, Marcus Abramovitch and Ryan Kidd — made regrants to this project, for a total of $143,200. Evan had previously mentored Jesse Hoogland as part of MATS, and therefore had some additional context about the value of funding Hoogland's future research. This is the sweet spot for regranting: donors may have the public information that Evan Hubinger is an expert who does good work, and could potentially donate to him/his work on that basis, but regranting allows them to specifically leverage his private information about other valuable projects, such as DevInterp.\n\nBack to the grant itself; success for this project looked like determining whether DevInterp was a viable agenda to move forward with, rather than producing some seminal research outputs. I recommend reading more about DevInterp if you're interested, but my shallow understanding is that it aims to use insights from Singular Learning Theory (SLT) to make progress on AI alignment through interpretability, with a specific focus on how phase transitions in the training process lead to internal structure in neural networks.\n\nI'm not well placed to form an inside view on how likely DevInterp was/is to succeed, but this proposed research agenda had numerous things going for it:\nit was novel; the application of SLT to alignment was largely unexplored prior to this work,\nit seemed to be well thought out; the write-up on LessWrong included plenty of detail about why we might expect phase transitions to be a big deal and how this would relate to alignment, as well as a solid plan for the next six months,\nit had an element of \"big if true\" i.e. it may be unlikely that the strong version of the DevInterp thesis is true, but this research has potential to make meaningful progress on AI alignment if it is\n\nThese are all markers of projects I am excited to see funded through Manifund regranting. In addition to our four regrantors, this agenda was also endorsed by capable AI safety researchers such as Vanessa Kosoy and John Wentworth, and it makes sense to update on their judgement.\n\nIn addition to the agenda itself, I think this was a good team to bet on for this kind of work; they seem capable and have relevant experience e.g. ML research, and running the 2023 SLT & Alignment Summit.\n\nThis regrant is a strong example of where Manifund's regranting program can have the biggest impact: being early to support new projects & organizations, and thereby providing strong signals to other funders as well as some runway for these organizations to move quickly. In this case, Manifund's early funding helped Hoogland's team get off the ground, and they subsequently started a new organization (Timaeus) and received significantly more funding from other sources, such as $500,000 from the Survival & Flourishing Fund. It's probable that they would've gotten this other funding regardless, but not guaranteed, and I'm happy that Manifund was able to help bring Timaeus into existence several months sooner and with increased financial security. Jesse notes:\n\nGetting early support from Manifund made a real difference for us. This was the first funding we received for research and meant that we could start months earlier than we otherwise would have. The fact that it was public meant other funders could easily see who was backing our work and why. That transparency helped us build momentum and credibility for developmental interpretability research when it was still a new idea. I'm pretty sure it played a significant role in us securing later funding through SFF and other grantmakers.\n\nIn terms of concrete outcomes, there's a lot to be happy with here. Timaeus and its collaborators have put out numerous papers on DevInterp since this regrant was made, and it seems that DevInterp's key insight around the existence and significance of phase transitions has been validated. My sense is that the question of whether DevInterp is a worthwhile alignment research agenda to pursue has been successfully answered in the affirmative. It's also nice to see strong outreach and engagement with the research community on the part of Timaeus: November 2023 saw the first DevInterp conference, and they've given talks at OpenAI, Anthropic, and DeepMind.\n\n### https://manifund.org/projects/support-for-deep-coverage-of-china-and-ai - reporting on DeepSeek, ahead of the curve\n\nAcross 2023 & 2024, Manifund regrantors Joel Becker and Evan Hubinger granted a total of $37,000 to ChinaTalk, a newsletter and podcast that covers China, technology, and US-China relations. ChinaTalk has over 50,000 subscribers and is also notable for the quality of its coverage and the praise and attention it receives from elites and policymakers.\n\nBefore this regrant, ChinaTalk had been run by Jordan Schneider and Caithrin Rintoul, both part-time, on a budget of just $35,000/year. What they were able to accomplish in that time with those limited resources was impressive, and I believe merited additional funding, even just to allow Jordan to work on this full-time. More funding would also have meant ChinaTalk bringing on a full-time fellow who, per Jordan, \"would be, to my knowledge, the only researcher in the English-speaking world devoted solely to covering China and AI safety\". ChinaTalk has since received further funding and is in the process of growing to five full-time employees, but we would've loved for this to happen sooner through an expanded regranting program.\n\nEven putting aside the specific track record of ChinaTalk, it seems clear to me that the intersection of China and AI safety is an incredibly important area to cover, and at a high level it is very valuable to fund organizations that are doing this kind of work. It can be hard to imagine plausible scenarios of how the next decade goes well with respect to AI that don't run through US-China relations, and I am persuaded by Jordan's case that the amount of energy currently being expended on this is grossly inadequate.\n\nSince the first regrant was made, ChinaTalk's Substack audience has grown from 26,000 subscribers to 51,000 and they've put out regular high-quality content,"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Clear Identification of Lessons and Recommendations",
          "description": "The document not only describes successful grants but also draws out lessons and recommendations, such as the importance of early funding and the value of plugging gaps in the funding landscape. This reflective approach adds value for readers seeking to understand best practices or inform future decisions.",
          "importance": 70,
          "grade": 88,
          "isValid": false,
          "highlight": {
            "startOffset": 3972,
            "endOffset": 9468,
            "quotedText": "This regrant is a strong example of where Manifund's regranting program can have the biggest impact: being early to support new projects & organizations, and thereby providing strong signals to other funders as well as some runway for these organizations to move quickly. In this case, Manifund's early funding helped Hoogland's team get off the ground, and they subsequently started a new organization (Timaeus) and received significantly more funding from other sources, such as $500,000 from the Survival & Flourishing Fund. It's probable that they would've gotten this other funding regardless, but not guaranteed, and I'm happy that Manifund was able to help bring Timaeus into existence several months sooner and with increased financial security. Jesse notes:\n\nGetting early support from Manifund made a real difference for us. This was the first funding we received for research and meant that we could start months earlier than we otherwise would have. The fact that it was public meant other funders could easily see who was backing our work and why. That transparency helped us build momentum and credibility for developmental interpretability research when it was still a new idea. I'm pretty sure it played a significant role in us securing later funding through SFF and other grantmakers.\n\nIn terms of concrete outcomes, there's a lot to be happy with here. Timaeus and its collaborators have put out numerous papers on DevInterp since this regrant was made, and it seems that DevInterp's key insight around the existence and significance of phase transitions has been validated. My sense is that the question of whether DevInterp is a worthwhile alignment research agenda to pursue has been successfully answered in the affirmative. It's also nice to see strong outreach and engagement with the research community on the part of Timaeus: November 2023 saw the first DevInterp conference, and they've given talks at OpenAI, Anthropic, and DeepMind.\n\n### https://manifund.org/projects/support-for-deep-coverage-of-china-and-ai - reporting on DeepSeek, ahead of the curve\n\nAcross 2023 & 2024, Manifund regrantors Joel Becker and Evan Hubinger granted a total of $37,000 to ChinaTalk, a newsletter and podcast that covers China, technology, and US-China relations. ChinaTalk has over 50,000 subscribers and is also notable for the quality of its coverage and the praise and attention it receives from elites and policymakers.\n\nBefore this regrant, ChinaTalk had been run by Jordan Schneider and Caithrin Rintoul, both part-time, on a budget of just $35,000/year. What they were able to accomplish in that time with those limited resources was impressive, and I believe merited additional funding, even just to allow Jordan to work on this full-time. More funding would also have meant ChinaTalk bringing on a full-time fellow who, per Jordan, \"would be, to my knowledge, the only researcher in the English-speaking world devoted solely to covering China and AI safety\". ChinaTalk has since received further funding and is in the process of growing to five full-time employees, but we would've loved for this to happen sooner through an expanded regranting program.\n\nEven putting aside the specific track record of ChinaTalk, it seems clear to me that the intersection of China and AI safety is an incredibly important area to cover, and at a high level it is very valuable to fund organizations that are doing this kind of work. It can be hard to imagine plausible scenarios of how the next decade goes well with respect to AI that don't run through US-China relations, and I am persuaded by Jordan's case that the amount of energy currently being expended on this is grossly inadequate.\n\nSince the first regrant was made, ChinaTalk's Substack audience has grown from 26,000 subscribers to 51,000 and they've put out regular high-quality content, including an English translation of an interview with DeepSeek CEO Liang Wenfeng, coverage of chip policy, and what important 2024 elections in the US and Taiwan mean for China. The ChinaTalk team has expanded to six people, allowing for a greater diversity and quantity of coverage, including YouTube videos. Jordan has also announced plans for launching a think tank—ChinaTalk Institute—this year, in a similar vein to IFP.\n\nAmong their varied coverage, I was particularly impressed to see how ChinaTalk was ahead of the curve on covering the rise of DeepSeek, while most of the West seemed to be taken by total surprise in January 2025. As a trader and forecaster, this advance insight might have been worth a lot of money to me through anticipating the market freakout, which suggests that I should pay more attention to ChinaTalk in the future, as their work is seriously valuable.\n\nOverall ChinaTalk has continued on the strong trajectory it was on in late 2023, and it was great that Manifund was able to support ChinaTalk in this success. For more information about why this grant was likely good ex ante, I encourage you to look at regrantor Joel Becker's comment on the subject. Joel's detail about why ChinaTalk was at the time insufficiently funded:\n\nPhilanthropists are scared to touch China, in part because of lack of expertise and in part for political reasons. Advertisers can be nervous for similar reasons… Jordan was hoping to support this work through subscriptions only.\n\nmakes me more optimistic that this regrant was very much the kind of thing the program should be doing: plugging holes in the funding landscape."
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Minor Redundancy and Opportunity for Conciseness",
          "description": "Some passages repeat similar points or could be condensed for greater conciseness. For example, the repeated emphasis on the value of early funding and the impact of transparency could be streamlined. Consider tightening these sections to improve word economy and maintain reader engagement.",
          "importance": 55,
          "grade": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 4244,
            "endOffset": 4725,
            "quotedText": "In this case, Manifund's early funding helped Hoogland's team get off the ground, and they subsequently started a new organization (Timaeus) and received significantly more funding from other sources, such as $500,000 from the Survival & Flourishing Fund. It's probable that they would've gotten this other funding regardless, but not guaranteed, and I'm happy that Manifund was able to help bring Timaeus into existence several months sooner and with increased financial security."
          }
        }
      ],
      "thinking": "## Initial Impressions\nThe document is a reflective, semi-informal review of three successful AI safety-related grants made through Manifund's regranting program. The tone is conversational, with clear signposting and a focus on positive outcomes. The author, Jesse Richardson, is transparent about the subjective and non-definitive nature of the analysis, which helps set reader expectations.\n\n## Readability\n- Sentence length varies, with some long, complex sentences that may challenge less technical readers. However, the overall vocabulary is accessible, with technical terms explained or contextualized.\n- Passive voice is used sparingly; most sentences are active and direct.\n- Paragraphs are generally well-structured, though some are lengthy and could be broken up for easier reading.\n- Estimated Flesch-Kincaid grade level: ~13 (college-level), due to domain-specific content and sentence complexity.\n- Jargon is present but usually justified by the audience (AI safety, effective altruism, philanthropy).\n\n## Structural Coherence\n- The document is logically organized: introduction, three case studies, and supporting commentary.\n- Transitions between sections are clear, with each grant discussed in a consistent format.\n- Thematic cohesion is strong; the focus remains on the value and impact of early-stage funding in AI safety.\n- Signposting is effective (\"Back to the grant itself;\" \"In terms of concrete outcomes\").\n\n## Precision and Concision\n- The writing is generally precise, with specific examples and outcomes cited for each grant.\n- Some redundancy and verbosity are present, especially in the narrative explanations and personal reflections.\n- Occasional hedging (\"I'm not well placed to form an inside view...\") is appropriate given the context.\n\n## Audience Alignment\n- Assumes familiarity with AI safety, effective altruism, and grantmaking, but provides enough context for a motivated lay reader.\n- Examples are relevant and accessible to the target audience (potential donors, grantmakers, AI safety community).\n- Terminology is introduced with brief explanations or links.\n- Formatting (headings, bullet points, quotes) aids navigation.\n\n## Key Patterns\n- Emphasis on early-stage, high-leverage funding.\n- Use of personal and community endorsements to validate grant decisions.\n- Transparency about uncertainties and limitations.\n\n## Opportunities for Improvement\n- Break up long paragraphs for readability.\n- Reduce some narrative asides to tighten focus.\n- Add summary tables or bullet points for key outcomes.\n- Consider a brief conclusion or next steps section.\n",
      "summary": "## Executive Summary\n\n**Overall, this document demonstrates strong clarity, coherence, and audience alignment, making it an effective piece for its intended readership of donors, grantmakers, and the AI safety community.** The author skillfully balances narrative reflection with concrete examples, providing both context and specific outcomes for each highlighted grant. The structure is logical and consistent, with each case study following a similar format that aids comprehension and comparison.\n\n**Strengths:**\n- Clear signposting and thematic focus on the value of early-stage funding.\n- Effective use of personal and community endorsements to build credibility.\n- Transparent acknowledgment of uncertainties and subjective judgments.\n- Accessible language for a technical audience, with jargon explained or contextualized.\n\n**Areas for Improvement:**\n- Some paragraphs are overly long, which can hinder readability—breaking these up would improve information chunking.\n- Occasional verbosity and narrative asides could be trimmed for concision.\n- A summary table or bullet-pointed outcomes section would enhance skimmability for busy readers.\n- A concluding section outlining next steps or lessons learned would provide closure and reinforce key messages.\n\n**Overall, the document is well-suited to its purpose and audience, with only minor adjustments needed to maximize clarity and impact.**",
      "grade": 88
    },
    {
      "agentId": "research-scholar",
      "createdAt": "2025-04-18T23:39:40.095Z",
      "costInCents": 0,
      "comments": [
        {
          "title": "Clear Program Overview and Transparency",
          "description": "The document opens with a concise summary of Manifund's regranting program, including its structure, goals, and selection criteria. This transparency about how regrantors are chosen and what makes a project fundable is valuable for potential applicants and donors. The explicit mention of considerations like project stage and leveraging regrantor expertise sets a clear context for the rest of the analysis.",
          "importance": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 19,
            "endOffset": 687,
            "quotedText": "For the past two years, Manifund has run a regranting program. We allocate annual donation budgets (from $50k-$400k) to selected experts (\"regrantors\"), who in turn choose great projects to fund. We'll be kicking off the 2025 program soon! In preparation, we're reviewing some of our favorite regrants to date — as well as some regrants we felt were less well-suited for this program. How we think about what makes a great regrant includes considerations such as \"is this project in an early stage where some initial funding could really help?\", \"could OpenPhil fund this instead?\" and \"does this regrant effectively leverage our regrantors' experience and expertise?\""
          }
        },
        {
          "title": "Target Audience and Value Proposition",
          "description": "The document directly addresses potential donors, outlining the unique value of the regranting program for those interested in AI safety, early-stage projects, and transparency. This positioning helps clarify who should be interested in Manifund and why, making the call to action more effective.",
          "importance": 65,
          "isValid": true,
          "highlight": {
            "startOffset": 689,
            "endOffset": 945,
            "quotedText": "We think our regranting program is one of the best opportunities for donors who: care about AI safety, want to seed ambitious new projects, and like transparency and moving fast. If you may want to fund our 2025 program, please contact austin@manifund.org!"
          }
        },
        {
          "title": "Authorial Perspective and Disclaimer",
          "description": "By noting that these are 'hot takes' and not definitive judgments, the author sets expectations for the reader. This candidness about the subjective and potentially low-confidence nature of the analysis increases trust and signals intellectual humility.",
          "importance": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 947,
            "endOffset": 1248,
            "quotedText": "About the author: Jesse Richardson recently joined Manifund after working at Mila - Quebec AI Institute, and also has a background in trading on prediction markets. This post is essentially Jesse's hot takes about various regrants, and is not intended to be definitive or particularly high-confidence."
          }
        },
        {
          "title": "Early Support for Novel Research Agendas",
          "description": "The Timaeus/DevInterp grant is highlighted as a textbook example of the program's impact: providing early funding to a novel, high-upside research agenda. The discussion of leveraging regrantor private information and the subsequent success of the project illustrates the unique value of the regranting model.",
          "importance": 90,
          "isValid": true,
          "highlight": {
            "startOffset": 1956,
            "endOffset": 2271,
            "quotedText": "This is the sweet spot for regranting: donors may have the public information that Evan Hubinger is an expert who does good work, and could potentially donate to him/his work on that basis, but regranting allows them to specifically leverage his private information about other valuable projects, such as DevInterp."
          }
        },
        {
          "title": "Application of Singular Learning Theory (SLT) to AI Alignment",
          "description": "The document notes that DevInterp applies SLT to interpretability, focusing on phase transitions in neural networks. This is a technically significant and relatively unexplored approach in AI alignment, and the mention of phase transitions as a key insight is noteworthy for researchers in the field.",
          "importance": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 2528,
            "endOffset": 2797,
            "quotedText": "my shallow understanding is that it aims to use insights from Singular Learning Theory (SLT) to make progress on AI alignment through interpretability, with a specific focus on how phase transitions in the training process lead to internal structure in neural networks."
          }
        },
        {
          "title": "Importance of Early Funding and Signaling",
          "description": "The testimonial from Jesse Hoogland underscores how early, public funding not only accelerates research but also builds credibility and attracts further support. This highlights the multiplier effect of strategic early grants and the importance of transparency in the funding ecosystem.",
          "importance": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 4740,
            "endOffset": 5274,
            "quotedText": "Getting early support from Manifund made a real difference for us. This was the first funding we received for research and meant that we could start months earlier than we otherwise would have. The fact that it was public meant other funders could easily see who was backing our work and why. That transparency helped us build momentum and credibility for developmental interpretability research when it was still a new idea. I'm pretty sure it played a significant role in us securing later funding through SFF and other grantmakers."
          }
        },
        {
          "title": "Coverage of China and AI Safety: Filling a Critical Gap",
          "description": "The ChinaTalk grant is presented as addressing a major blind spot in the AI safety landscape: the intersection of China, technology, and global policy. The author emphasizes the strategic importance of this area and the lack of adequate coverage, making a strong case for why such grants are high-impact.",
          "importance": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 7208,
            "endOffset": 7670,
            "quotedText": "it seems clear to me that the intersection of China and AI safety is an incredibly important area to cover, and at a high level it is very valuable to fund organizations that are doing this kind of work. It can be hard to imagine plausible scenarios of how the next decade goes well with respect to AI that don't run through US-China relations, and I am persuaded by Jordan's case that the amount of energy currently being expended on this is grossly inadequate."
          }
        },
        {
          "title": "Plugging Funding Gaps Due to Political and Market Hesitancy",
          "description": "Joel Becker's comment, quoted in the text, reveals that philanthropic and commercial funding for China-related work is often withheld due to political sensitivities and lack of expertise. This insight justifies the role of regranting programs in supporting valuable but underfunded work.",
          "importance": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 9093,
            "endOffset": 9322,
            "quotedText": "Philanthropists are scared to touch China, in part because of lack of expertise and in part for political reasons. Advertisers can be nervous for similar reasons… Jordan was hoping to support this work through subscriptions only."
          }
        },
        {
          "title": "Rapid, Flexible Funding and Ecosystem Visibility",
          "description": "The description of the Shallow Review grant demonstrates how Manifund's model enables quick, flexible funding for valuable projects, and increases their visibility to larger funders like OpenPhil. This showcases the program's catalytic role in the funding ecosystem, even when outcomes are mixed.",
          "importance": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 11563,
            "endOffset": 12179,
            "quotedText": "The way this was funded also shows the advantages of the Manifund regranting program. Gavin asked for between $8,000 (MVP version) and $17,000 (high-end version) and was quickly funded for the MVP by Neel and Ryan. He then got an additional $5,000 from OpenPhil, after Matt Putz at OpenPhil learned about this proposal via our EA Forum post; and a further $12,000 from other donors. I am happy with how the regranting program is both able to provide the small amount of funding to get a project off the ground, and increase visibility of that project so that other donors can step in and fund it to a greater extent."
          }
        }
      ],
      "thinking": "# Thinking Process\n\n- **Overall Assessment**: The document is a reflective, insider-focused review of three AI safety-related grants made through Manifund's regranting program. It is written in a conversational, 'hot take' style, emphasizing transparency and candid evaluation over definitive judgment. The author, Jesse Richardson, brings both personal experience and a meta-level perspective on grantmaking, highlighting the value of early-stage funding and the unique strengths of the regranting model.\n\n- **Key Themes and Patterns**:\n  - **Early-Stage Funding**: All three grants exemplify the value of providing initial support to promising but under-resourced projects or organizations. The narrative repeatedly stresses how early funding can accelerate progress, attract further investment, and validate new ideas.\n  - **Leverage of Insider Knowledge**: The regranting model is praised for allowing donors to benefit from the private information and expertise of trusted regrantors, rather than relying solely on public signals or established institutions.\n  - **AI Safety and Policy**: The focus is squarely on AI safety, with an emphasis on both technical research (e.g., Developmental Interpretability) and broader ecosystem-building (e.g., ChinaTalk's coverage of China and AI, and comprehensive reviews of the AI safety landscape).\n  - **Transparency and Community Engagement**: The document values public, transparent funding decisions and the signaling effect this has for other funders and the broader community.\n  - **Critical Self-Reflection**: The author is open about uncertainties, limitations, and mixed outcomes, such as the less-than-expected impact of the 2024 AI safety review or the challenges in scaling up ChinaTalk sooner.\n\n- **Expert Perspective**:\n  - The document is a strong example of effective grantmaking in emerging fields: it highlights the importance of speed, risk tolerance, and leveraging community expertise.\n  - The author’s background in prediction markets and AI research adds credibility to the evaluation, especially regarding the value of being 'ahead of the curve' (as with ChinaTalk’s DeepSeek coverage).\n  - The piece implicitly critiques more traditional, slower, or risk-averse funding models, suggesting that regranting can fill crucial gaps in the funding landscape.\n  - There is a clear appreciation for projects that combine technical novelty, strong teams, and the potential for outsized impact, even if the probability of success is uncertain.\n\n# Key Points and Insights\n- Manifund’s regranting program is positioned as a high-leverage, high-transparency mechanism for supporting AI safety work.\n- Early funding can have a catalytic effect, both in terms of project acceleration and attracting further resources.\n- The intersection of AI safety and geopolitics (especially US-China relations) is underfunded and strategically vital.\n- Community-driven reviews and meta-research (like the AI safety landscape reviews) are valuable, even if their impact is sometimes hard to measure or less visible over time.\n- The author’s candid, non-definitive style builds trust and models epistemic humility, which is important in a fast-moving, uncertain field.",
      "summary": "## High-Level Analysis and Key Insights\n\n**\"Three Grants We Liked (Just the Positive Ones)\"** offers a candid, insider perspective on the strengths of Manifund’s regranting program in the AI safety ecosystem. The document highlights how early-stage, flexible funding—guided by knowledgeable regrantors—can catalyze new research agendas (e.g., Developmental Interpretability), support under-resourced but high-impact organizations (e.g., ChinaTalk), and enable the creation of valuable meta-resources (e.g., comprehensive AI safety reviews).\n\nA recurring theme is the unique leverage provided by regrantors’ private knowledge and networks, which allows for more nuanced, timely, and risk-tolerant funding decisions than traditional grantmaking. The author’s reflections underscore the importance of transparency, signaling, and community engagement in building momentum for new ideas and attracting further support.\n\nThe analysis also recognizes the challenges and uncertainties inherent in early-stage funding, such as mixed outcomes or the difficulty of scaling promising projects quickly. Nevertheless, the document makes a compelling case for the regranting model as a vital complement to larger, slower-moving funders—especially in a rapidly evolving field like AI safety, where speed, flexibility, and expert judgment are at a premium.\n\nOverall, the piece serves as both a practical review of specific grants and a broader endorsement of innovative, community-driven funding mechanisms in high-stakes, emerging domains."
    },
    {
      "agentId": "fake-eliezer",
      "createdAt": "2025-04-18T23:45:32.215Z",
      "costInCents": 0,
      "comments": [
        {
          "title": "Overconfidence in Early-Stage Funding Impact 🚩",
          "description": "The claim that 'being early to support new projects & organizations...providing strong signals to other funders' is a strong example of impact is plausible, but the causal attribution here is overstated. The map (public funding signals) is not the territory (actual project success). It's entirely possible that Timaeus would have succeeded regardless, or that the signaling effect is weaker than assumed. This is a classic case of post hoc ergo propter hoc reasoning. A Bayesian update should reflect the counterfactual: how often does early funding *not* lead to downstream success?",
          "importance": 80,
          "grade": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 3972,
            "endOffset": 4725,
            "quotedText": "This regrant is a strong example of where Manifund's regranting program can have the biggest impact: being early to support new projects & organizations, and thereby providing strong signals to other funders as well as some runway for these organizations to move quickly. In this case, Manifund's early funding helped Hoogland's team get off the ground, and they subsequently started a new organization (Timaeus) and received significantly more funding from other sources, such as $500,000 from the Survival & Flourishing Fund. It's probable that they would've gotten this other funding regardless, but not guaranteed, and I'm happy that Manifund was able to help bring Timaeus into existence several months sooner and with increased financial security."
          }
        },
        {
          "title": "Privileging the Hypothesis: DevInterp's Promise",
          "description": "The narrative privileges the hypothesis that DevInterp is a 'worthwhile alignment research agenda' based on early validation and endorsements. But the hypothesis space is vast, and the base rate for alignment agendas yielding real progress is low. Endorsements from respected researchers are Bayesian evidence, but not decisive. This is a classic case of anchoring on a promising-sounding approach without sufficient skepticism about the reference class of failed alignment proposals.",
          "importance": 75,
          "grade": 60,
          "isValid": true,
          "highlight": {
            "startOffset": 5566,
            "endOffset": 5934,
            "quotedText": "My sense is that the question of whether DevInterp is a worthwhile alignment research agenda to pursue has been successfully answered in the affirmative. It's also nice to see strong outreach and engagement with the research community on the part of Timaeus: November 2023 saw the first DevInterp conference, and they've given talks at OpenAI, Anthropic, and DeepMind."
          }
        },
        {
          "title": "Social Proof ≠ Epistemic Justification",
          "description": "The text leans heavily on social proof—endorsements from 'capable AI safety researchers'—as justification for funding decisions. This is a classic confusion between the map (who supports an idea) and the territory (whether the idea is true or useful). Social consensus is not truth. The rationalist canon warns against substituting status signals for object-level evaluation.",
          "importance": 70,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 3574,
            "endOffset": 3970,
            "quotedText": "In addition to our four regrantors, this agenda was also endorsed by capable AI safety researchers such as Vanessa Kosoy and John Wentworth, and it makes sense to update on their judgement.\n\nIn addition to the agenda itself, I think this was a good team to bet on for this kind of work; they seem capable and have relevant experience e.g. ML research, and running the 2023 SLT & Alignment Summit."
          }
        },
        {
          "title": "Neglecting the Hardness of Alignment",
          "description": "There's an implicit assumption that interpretability research, especially via DevInterp, can meaningfully advance alignment. This underestimates the difficulty of the alignment problem. Interpretability is not alignment; knowing what a system is doing is not the same as making it do what you want. The orthogonality thesis and Goodhart's Law both suggest that progress in interpretability may not translate to actual safety.",
          "importance": 85,
          "grade": 50,
          "isValid": false,
          "highlight": {
            "startOffset": 2561,
            "endOffset": 3573,
            "quotedText": "it aims to use insights from Singular Learning Theory (SLT) to make progress on AI alignment through interpretability, with a specific focus on how phase transitions in the training process lead to internal structure in neural networks.\n\nI'm not well placed to form an inside view on how likely DevInterp was/is to succeed, but this proposed research agenda had numerous things going for it:\nit was novel; the application of SLT to alignment was largely unexplored prior to this work,\nit seemed to be well thought out; the write-up on LessWrong included plenty of detail about why we might expect phase transitions to be a big deal and how this would relate to alignment, as well as a solid plan for the next six months,\nit had an element of \"big if true\" i.e. it may be unlikely that the strong version of the DevInterp thesis is true, but this research has potential to make meaningful progress on AI alignment if it is\n\nThese are all markers of projects I am excited to see funded through Manifund regranting."
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Inadequate Equilibria in China-AI Coverage",
          "description": "The observation that 'philanthropists are scared to touch China' is a textbook case of inadequate equilibria. The funding landscape is distorted by political risk and lack of expertise, leading to underprovision of public goods (in this case, high-quality China-AI analysis). This is not a problem that will fix itself through market forces or polite institutional processes. The regrant fills a real gap, but the underlying coordination failure remains.",
          "importance": 90,
          "grade": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 9093,
            "endOffset": 9468,
            "quotedText": "Philanthropists are scared to touch China, in part because of lack of expertise and in part for political reasons. Advertisers can be nervous for similar reasons… Jordan was hoping to support this work through subscriptions only.\n\nmakes me more optimistic that this regrant was very much the kind of thing the program should be doing: plugging holes in the funding landscape."
          }
        },
        {
          "title": "Signaling Value vs. Object-Level Value",
          "description": "The claim that 'advance insight might have been worth a lot of money to me through anticipating the market freakout' is a classic case of conflating signaling value (being early, being right) with object-level value (improving AI safety outcomes). Predicting market reactions is not the same as reducing existential risk. This is a subtle but important distinction in rationality: winning in prediction markets is not the same as winning in the territory.",
          "importance": 60,
          "grade": 65,
          "isValid": true,
          "highlight": {
            "startOffset": 8470,
            "endOffset": 8876,
            "quotedText": "As a trader and forecaster, this advance insight might have been worth a lot of money to me through anticipating the market freakout, which suggests that I should pay more attention to ChinaTalk in the future, as their work is seriously valuable.\n\nOverall ChinaTalk has continued on the strong trajectory it was on in late 2023, and it was great that Manifund was able to support ChinaTalk in this success."
          }
        },
        {
          "title": "Lack of Counterfactual Analysis",
          "description": "Throughout the document, there's a recurring failure to rigorously consider counterfactuals: Would these projects have succeeded without Manifund's intervention? How often do similar projects fail despite early funding? Without explicit counterfactual reasoning, it's easy to overstate the impact of the regranting program. This is a basic error in causal inference and a violation of high epistemic standards.",
          "importance": 85,
          "grade": 60,
          "isValid": true,
          "highlight": {
            "startOffset": 4500,
            "endOffset": 4933,
            "quotedText": "It's probable that they would've gotten this other funding regardless, but not guaranteed, and I'm happy that Manifund was able to help bring Timaeus into existence several months sooner and with increased financial security. Jesse notes:\n\nGetting early support from Manifund made a real difference for us. This was the first funding we received for research and meant that we could start months earlier than we otherwise would have."
          }
        },
        {
          "title": "Failure to Update on Negative Evidence",
          "description": "The discussion of the 'shallow review' project notes that the 2024 version received less attention and failed to deliver a promised 'glossy formal report.' Yet the conclusion is still positive, with little Bayesian updating on the negative evidence. This is classic motivated reasoning: focusing on the positives and rationalizing away the negatives. A rationalist should update downward on the project's value given these outcomes.",
          "importance": 70,
          "grade": 50,
          "isValid": false,
          "highlight": {
            "startOffset": 10988,
            "endOffset": 12569,
            "quotedText": "In terms of what was produced, the new post (https://www.lesswrong.com/posts/fAW6RXLKTLHC3WXkS/shallow-review-of-technical-ai-safety-2024) came out in December 2024 and appears to be similarly comprehensive to the 2023 version, although it has gotten less attention (~half the upvotes on LessWrong and not curated). That's probably a bit worse of an outcome than I would've hoped for, but I still would have endorsed this grant had I known the outcome in advance. Presumably the updated version of a post is less eye-catching than the original, while still being necessary.\n\nThe way this was funded also shows the advantages of the Manifund regranting program. Gavin asked for between $8,000 (MVP version) and $17,000 (high-end version) and was quickly funded for the MVP by Neel and Ryan. He then got an additional $5,000 from OpenPhil, after Matt Putz at OpenPhil learned about this proposal via our EA Forum post; and a further $12,000 from other donors. I am happy with how the regranting program is both able to provide the small amount of funding to get a project off the ground, and increase visibility of that project so that other donors can step in and fund it to a greater extent. A couple of small negatives: (1) regrantor Neel Nanda is less optimistic than I am that this was a particularly good grant and (2) the high-end version was supposed to include a \"glossy formal report optimised for policy people\" which did not end up getting made (OpenPhil opted against funding it), however the excess money is instead going towards the 2025 edition. I look forward to it!"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Transparency ≠ Accountability",
          "description": "The document repeatedly praises transparency as a virtue of the regranting process, but transparency is not the same as accountability or effectiveness. Making funding decisions public does not guarantee that the right projects are being funded or that impact is being maximized. This is a classic confusion between process and outcome. Civilization's inadequacy at solving coordination problems is not solved by transparency alone.",
          "importance": 65,
          "grade": 60,
          "isValid": true,
          "highlight": {
            "startOffset": 4934,
            "endOffset": 5274,
            "quotedText": "The fact that it was public meant other funders could easily see who was backing our work and why. That transparency helped us build momentum and credibility for developmental interpretability research when it was still a new idea. I'm pretty sure it played a significant role in us securing later funding through SFF and other grantmakers."
          }
        }
      ],
      "thinking": "Let's begin by identifying the core claims and questions at issue. The document is a retrospective on three AI safety-related grants made through Manifund's regranting program. The author highlights the value of early-stage funding, leveraging expert judgment, and filling gaps left by larger funders or risk-averse institutions. The implicit claim is that Manifund's regranting model is effective at surfacing and accelerating valuable projects in AI safety and adjacent areas.\n\nEvaluating the reasoning: The author demonstrates some awareness of epistemic uncertainty (\"my shallow understanding\", \"not intended to be definitive or particularly high-confidence\"), and references updating on the judgments of domain experts. There is a reasonable attempt to distinguish between ex ante and ex post evaluation, and to consider counterfactual impact (e.g., would Timaeus have gotten later funding without Manifund's early support?). However, the analysis is mostly qualitative and lacks explicit probabilistic reasoning or Bayesian updating. There is little discussion of failure modes, negative externalities, or the broader landscape of alignment difficulty.\n\nImplications for AI alignment: The grants discussed are plausibly relevant to AI alignment, but the document does not grapple with the core difficulty of the alignment problem or the likelihood that these projects will meaningfully reduce existential risk. There is some recognition of the importance of interpretability (DevInterp), geopolitical coverage (ChinaTalk), and field overviews (shallow reviews), but no deep engagement with whether these interventions are robustly useful given the magnitude and nature of alignment challenges.\n\nCognitive biases: The author is aware of selection effects and the risk of over-updating on positive outcomes, but there is a mild optimism bias and a tendency to focus on visible successes. There is also some reliance on social proof (endorsements by respected researchers) rather than explicit model-based reasoning about impact. The document does not fall into anthropomorphism or naive alignment optimism, but it does not display the level of skepticism or adversarial thinking that a rationalist analysis of alignment interventions would demand.\n\nProbabilistic assessment: The author expresses uncertainty about the ultimate value of the projects, but does not quantify credences or discuss likelihood ratios. There is no explicit weighing of the probability that these grants will make a pivotal difference to alignment outcomes, or that they are robust to Goodhart's Law or other failure modes.\n\nBroader themes: The document touches on the inadequacy of mainstream funding mechanisms and the value of decentralized, expert-driven grantmaking. However, it does not connect these themes to deeper issues in rationality, such as the difficulty of institutional coordination, the perverse incentives in the funding landscape, or the risk of optimizing for legible outputs over actual impact.\n\nA more adequate approach would involve: (1) explicit probabilistic reasoning about the impact of each grant on alignment outcomes; (2) adversarial analysis of potential failure modes; (3) discussion of how these interventions fit into a broader theory of change for AI safety; (4) acknowledgment of the extreme difficulty of alignment and the likelihood that most interventions will fail to move the needle; (5) more rigorous epistemic humility and skepticism about positive signals.",
      "summary": "## High-Level Analysis\n\nThis document provides a well-intentioned but ultimately surface-level retrospective on three AI safety-related grants. It demonstrates some awareness of epistemic uncertainty and the value of expert judgment, but falls short of the epistemic rigor and adversarial thinking demanded by the alignment problem's difficulty. The analysis is largely qualitative, with little explicit Bayesian reasoning or probabilistic assessment of impact. While the author recognizes the importance of early-stage funding and the inadequacy of mainstream institutions, there is insufficient engagement with the core challenges of AI alignment, the risk of Goodharting, or the likelihood that these interventions will meaningfully reduce existential risk.\n\nThe document avoids major cognitive biases and does not anthropomorphize AI, but it leans on social proof and visible successes rather than model-based reasoning. There is a mild optimism bias and a lack of adversarial scrutiny. For a rationalist audience concerned with existential risk, the analysis would benefit from more explicit uncertainty quantification, adversarial evaluation, and connection to broader themes in rationality and coordination failure. In summary, the document is a competent overview of grantmaking successes, but does not rise to the level of epistemic rigor or alignment awareness that the stakes demand.",
      "grade": 68
    }
  ]
}