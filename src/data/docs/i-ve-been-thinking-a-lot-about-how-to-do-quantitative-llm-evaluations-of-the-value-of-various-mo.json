{
  "id": "i-ve-been-thinking-a-lot-about-how-to-do-quantitative-llm-evaluations-of-the-value-of-various-mo",
  "slug": "i-ve-been-thinking-a-lot-about-how-to-do-quantitative-llm-evaluations-of-the-value-of-various-mo",
  "title": "I've been thinking a lot about how to do quantitative LLM evaluations of the value of various (mo...",
  "author": "Ozzie Gooen",
  "publishedDate": "2025-04-17",
  "url": "https://www.facebook.com/ozzie.gooen/posts/pfbid02w7b8Y4jQ19sf1ioAKHPqaNhiJePF2WadKKXn4hncZjLgu1R4kp5LyVQVRHGSwcTXl",
  "intendedAgents": [
    "clarity-coach"
  ],
  "platforms": [
    "Facebook"
  ],
  "content": "I've been thinking a lot about how to do quantitative LLM evaluations of the value of various (mostly Effective Altruism, or EA) projects.\n\nThe idea is to have language models give their best guesses at the value of different projects or outputs. While these evaluations would likely be mediocre at first, they could help us understand how promising this area is and where we might want to go with it.\n\nOne initial idea is to estimate the value in terms of dollars from a certain EA funder, expressed as a probability distribution. However, this approach quickly becomes complicated, as it combines several key uncertainties into a single value. This is probably too difficult for early experiments.\n\nA more elegant approach might be to use \"relative value functions.\" This is theoretically appealing, as it helps separate some of the key uncertainties, but it would require more technical infrastructure and investment.\n\nAnother option is to ask for a simple rank order: \"Just order these projects in terms of expected value.\" While scoring rank orders is somewhat inelegant, it is definitely feasible.\n\nHere's an experiment I’m imagining:\n\n1. We create a list of interesting EA outputs—such as blog posts, research articles, interventions, etc.—resulting in a list of perhaps 20 to 100 items. This list would be made public.\n2. People are invited to compete by ranking these items. Each submission would include an ordering of all the elements and an optional document defending their choices.\n3. All submissions are then evaluated by an LLM system using a detailed, predefined prompt. The system takes in all the provided orderings and defenses, and then outputs its own ordering.\n4. Each submission is scored based on how closely it matches the LLM’s ordering.\n5. The winner receives a cash prize. Ideally, all submissions would be made public.\n\nThis is similar to a previous competition we ran: https://forum.effectivealtruism.org/posts/Zc5jki9nXihueDcKj/usd300-fermi-model-competition\n\nQuestions:\n\n1. How would you choose which projects or items to analyze?\n   - One option is to start with a mix of well-regarded posts on the EA Forum, perhaps focusing on a limited domain (such as existential risk) but covering a range of different levels of popularity or \"karma.\"\n\n2. Wouldn't the LLM do a poor job? Why not use humans?\n   - Having human judges would significantly increase costs, potentially doubling the project’s expense. Additionally, it’s valuable to learn how to use LLMs for evaluating these competitions, as this has more long-term potential.\n\n3. The resulting lists might be poor quality.\n   - I think the results would still be interesting for several reasons. I’d expect them to be better than what many individuals could produce. More importantly, it’s crucial to start somewhere rather than waiting for a perfect solution, which may never materialize.",
  "reviews": [
    {
      "agentId": "clarity-coach",
      "createdAt": "2025-04-18T17:55:12.149Z",
      "costInCents": 0,
      "thinking": "## Initial Impressions\n- The document is a reflective, exploratory post about using LLMs to quantitatively evaluate the value of Effective Altruism (EA) projects.\n- The tone is informal and speculative, with the author openly considering different approaches and their trade-offs.\n- The structure is mostly linear, moving from the problem statement to possible solutions, then to a proposed experiment, and finally to anticipated questions and objections.\n\n## Readability\n- Sentences are generally short to medium in length, with some variation.\n- Vocabulary is accessible, with minimal jargon (\"relative value functions\" is the most technical term, but it's briefly explained).\n- Passive voice is rare; the author uses active constructions.\n- Paragraphs are short and information is chunked logically.\n- Reading level is moderate—likely around grade 10-12, accessible to a general audience with some familiarity with EA concepts.\n\n## Structural Coherence\n- The document flows logically: problem → approaches → experiment → Q&A.\n- Transitions are mostly implicit, but the use of numbered lists and questions helps guide the reader.\n- Thematic consistency is strong; the focus remains on LLM evaluation of EA projects.\n- The structure is somewhat informal, but appropriate for a blog or social media post.\n\n## Precision and Concision\n- The writing is concise, with little redundancy.\n- Some sentences could be tightened for clarity (e.g., \"This is probably too difficult for early experiments\" could specify why).\n- The author hedges appropriately, acknowledging uncertainty without excessive qualifiers.\n- Terminology is mostly precise, though \"relative value functions\" could use more explanation for non-experts.\n\n## Audience Alignment\n- Assumes some familiarity with EA and LLMs, but not excessive prior knowledge.\n- Provides context for the experiment and references a previous competition for grounding.\n- Examples are relevant and accessible to the intended audience.\n- Formatting (lists, questions) aids readability for online consumption.\n\n## Key Strengths\n- Clear articulation of the problem and possible solutions.\n- Logical progression and use of examples.\n- Concise, accessible language.\n\n## Areas for Improvement\n- Could provide more explicit definitions for technical terms.\n- Some transitions between sections could be smoother.\n- More signposting (e.g., section headers) would aid navigation.\n\n## Before/After Example\n- Before: \"This is probably too difficult for early experiments.\"\n- After: \"Because this approach combines several uncertainties into a single value, it may be too complex for initial trials.\"\n\n## Overall Assessment\n- The document is clear, coherent, and well-aligned with its intended audience, though minor improvements could enhance clarity and accessibility.",
      "summary": "## Executive Summary\n\nThis document offers a thoughtful exploration of how large language models (LLMs) might be used to quantitatively evaluate the value of Effective Altruism (EA) projects. The author, Ozzie Gooen, presents a range of possible approaches, weighs their feasibility, and proposes a concrete experimental framework. The writing is accessible, logically structured, and well-aligned with an audience familiar with EA and AI evaluation methods.\n\n### Key Strengths\n- **Readability:** The prose is clear and concise, with short paragraphs and varied sentence lengths. Technical jargon is minimal and, where present, is generally contextualized.\n- **Structural Coherence:** The document follows a logical progression, moving from problem identification to solution brainstorming, then to a detailed experimental proposal, and finally to a Q&A addressing anticipated concerns. The use of lists and questions enhances navigability.\n- **Precision and Concision:** The author avoids unnecessary verbosity and hedging, focusing on the core issues and potential solutions. Language is specific, and the rationale for each approach is clearly articulated.\n- **Audience Alignment:** The content assumes some prior knowledge of EA and LLMs but remains accessible to a broader audience interested in quantitative evaluation and AI applications. References to previous competitions and practical considerations (e.g., cost of human judges) ground the discussion.\n\n### Areas for Improvement\n- **Technical Term Clarification:** Terms like \"relative value functions\" could be more explicitly defined for readers less familiar with the domain.\n- **Transitions and Signposting:** While the structure is logical, more explicit transitions or section headers would further enhance readability and help guide the reader through the argument.\n- **Depth of Examples:** Providing a concrete example of how an LLM might evaluate a specific project could make the proposal more tangible.\n\n### Overall Impression\nThe document is a strong example of clear, reflective writing on a complex topic. With minor adjustments to terminology and structure, it could serve as an effective blueprint for further discussion and experimentation in LLM-based evaluation of EA projects.",
      "grade": 87,
      "comments": [
        {
          "title": "Clear Introduction and Framing of Purpose",
          "description": "The opening paragraph succinctly introduces the topic and context, specifying both the subject (quantitative LLM evaluations) and the domain (Effective Altruism projects). This clarity helps orient the reader and sets expectations for the rest of the document. The use of parentheses to clarify 'EA' is especially helpful for readers unfamiliar with the acronym. This is a strong example of effective audience alignment and purposeful framing.",
          "importance": 80,
          "grade": 95,
          "isValid": true,
          "highlight": {
            "startOffset": 0,
            "endOffset": 138,
            "quotedText": "I've been thinking a lot about how to do quantitative LLM evaluations of the value of various (mostly Effective Altruism, or EA) projects."
          }
        },
        {
          "title": "Dense Technical Jargon Without Sufficient Scaffolding",
          "description": "The passage introduces several technical terms and concepts—such as 'probability distribution,' 'relative value functions,' and 'key uncertainties'—in rapid succession. While these terms are appropriate for a technically literate audience, the lack of brief definitions or contextual explanations may hinder accessibility for less specialized readers. Consider adding concise clarifications or examples to improve comprehension without oversimplifying.",
          "importance": 70,
          "grade": 60,
          "isValid": true,
          "highlight": {
            "startOffset": 403,
            "endOffset": 920,
            "quotedText": "One initial idea is to estimate the value in terms of dollars from a certain EA funder, expressed as a probability distribution. However, this approach quickly becomes complicated, as it combines several key uncertainties into a single value. This is probably too difficult for early experiments.\n\nA more elegant approach might be to use \"relative value functions.\" This is theoretically appealing, as it helps separate some of the key uncertainties, but it would require more technical infrastructure and investment."
          }
        },
        {
          "title": "Effective Use of Stepwise Structure for Experimental Design",
          "description": "The experiment is described using a clear, numbered list, which enhances readability and logical flow. Each step is concise and follows a natural progression, making the process easy to follow. This structure is exemplary for conveying procedural information and supports both skimming and in-depth reading.",
          "importance": 85,
          "grade": 98,
          "isValid": true,
          "highlight": {
            "startOffset": 1105,
            "endOffset": 1848,
            "quotedText": "Here's an experiment I’m imagining:\n\n1. We create a list of interesting EA outputs—such as blog posts, research articles, interventions, etc.—resulting in a list of perhaps 20 to 100 items. This list would be made public.\n2. People are invited to compete by ranking these items. Each submission would include an ordering of all the elements and an optional document defending their choices.\n3. All submissions are then evaluated by an LLM system using a detailed, predefined prompt. The system takes in all the provided orderings and defenses, and then outputs its own ordering.\n4. Each submission is scored based on how closely it matches the LLM’s ordering.\n5. The winner receives a cash prize. Ideally, all submissions would be made public."
          }
        },
        {
          "title": "Occasional Informal Language and Hedging",
          "description": "Phrases such as 'this is probably too difficult,' 'might be,' and 'I think the results would still be interesting' introduce hedging and informal tone. While this can make the text more approachable, excessive use may reduce perceived authority or precision. Consider balancing informality with more assertive, evidence-based statements where appropriate.",
          "importance": 50,
          "grade": 65,
          "isValid": false,
          "highlight": {
            "startOffset": 646,
            "endOffset": 2680,
            "quotedText": "This is probably too difficult for early experiments.\n\nA more elegant approach might be to use \"relative value functions.\" This is theoretically appealing, as it helps separate some of the key uncertainties, but it would require more technical infrastructure and investment.\n\nAnother option is to ask for a simple rank order: \"Just order these projects in terms of expected value.\" While scoring rank orders is somewhat inelegant, it is definitely feasible.\n\nHere's an experiment I’m imagining:\n\n1. We create a list of interesting EA outputs—such as blog posts, research articles, interventions, etc.—resulting in a list of perhaps 20 to 100 items. This list would be made public.\n2. People are invited to compete by ranking these items. Each submission would include an ordering of all the elements and an optional document defending their choices.\n3. All submissions are then evaluated by an LLM system using a detailed, predefined prompt. The system takes in all the provided orderings and defenses, and then outputs its own ordering.\n4. Each submission is scored based on how closely it matches the LLM’s ordering.\n5. The winner receives a cash prize. Ideally, all submissions would be made public.\n\nThis is similar to a previous competition we ran: https://forum.effectivealtruism.org/posts/Zc5jki9nXihueDcKj/usd300-fermi-model-competition\n\nQuestions:\n\n1. How would you choose which projects or items to analyze?\n   - One option is to start with a mix of well-regarded posts on the EA Forum, perhaps focusing on a limited domain (such as existential risk) but covering a range of different levels of popularity or \"karma.\"\n\n2. Wouldn't the LLM do a poor job? Why not use humans?\n   - Having human judges would significantly increase costs, potentially doubling the project’s expense. Additionally, it’s valuable to learn how to use LLMs for evaluating these competitions, as this has more long-term potential.\n\n3. The resulting lists might be poor quality.\n   - I think the results would still be interesting for several reasons."
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Strong Use of Anticipatory Structure via Q&A Format",
          "description": "The 'Questions:' section anticipates likely reader concerns and addresses them directly. This approach improves structural coherence and audience alignment by guiding the reader through potential objections and clarifying rationale. The use of bullet points and sub-questions further enhances navigability.",
          "importance": 75,
          "grade": 92,
          "isValid": true,
          "highlight": {
            "startOffset": 1992,
            "endOffset": 2874,
            "quotedText": "Questions:\n\n1. How would you choose which projects or items to analyze?\n   - One option is to start with a mix of well-regarded posts on the EA Forum, perhaps focusing on a limited domain (such as existential risk) but covering a range of different levels of popularity or \"karma.\"\n\n2. Wouldn't the LLM do a poor job? Why not use humans?\n   - Having human judges would significantly increase costs, potentially doubling the project’s expense. Additionally, it’s valuable to learn how to use LLMs for evaluating these competitions, as this has more long-term potential.\n\n3. The resulting lists might be poor quality.\n   - I think the results would still be interesting for several reasons. I’d expect them to be better than what many individuals could produce. More importantly, it’s crucial to start somewhere rather than waiting for a perfect solution, which may never materialize."
          }
        },
        {
          "title": "Limited Explanation of LLM Evaluation Criteria",
          "description": "While the document outlines the process for using LLMs to evaluate submissions, it does not specify the criteria or standards the LLM will use. This omission may leave readers uncertain about the objectivity or reliability of the evaluation. Adding a brief description of the LLM's evaluation rubric or decision-making process would enhance transparency and precision.",
          "importance": 65,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 1499,
            "endOffset": 1683,
            "quotedText": "All submissions are then evaluated by an LLM system using a detailed, predefined prompt. The system takes in all the provided orderings and defenses, and then outputs its own ordering."
          }
        }
      ]
    }
  ]
}