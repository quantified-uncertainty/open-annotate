{
  "id": "my-current-claims-and-cruxes-on-llm-forecasting-epistemics-ea-forum-bots",
  "slug": "my-current-claims-and-cruxes-on-llm-forecasting-epistemics-ea-forum-bots",
  "title": "My Current Claims and Cruxes on LLM Forecasting & Epistemics — EA Forum Bots",
  "author": "Ozzie Gooen",
  "publishedDate": "2024-06-26T00:40:58.283Z",
  "url": "https://forum.effectivealtruism.org/posts/EykCuXDCFAT5oGyux/my-current-claims-and-cruxes-on-llm-forecasting-and",
  "intendedAgents": [
    "ea-impact-evaluator",
    "clarity-coach",
    "research-scholar",
    "quantitative-forecaster"
  ],
  "platforms": ["EA Forum"],
  "content": "I think that recent improvements in LLMs have brought us to the point where LLM epistemic systems are starting to be useful. After spending some time thinking about it, I've realized that such systems, broadly, seem very promising to me as an effective altruist intervention area. However, I think that our community has yet to do a solid job outlining what this area could look like or figuring out key uncertainties.\n\nThis document presents a rough brainstorm on these topics. While I could dedicate months to refining these ideas, I've chosen to share these preliminary notes now to spark discussion. If you find the style too terse, feel free to use an LLM to rewrite it in a format you prefer.\n\nI believe my vision for this area is more ambitious and far-reaching (i.e. not narrow to a certain kind of forecasting) than what I've observed in other discussions. I'm particularly excited about AI-heavy epistemic improvements, which I believe have greater potential than traditional forecasting innovations.\n\nI'm trying to figure out what to make of this regarding our future plans at [QURI](https://quantifieduncertainty.org), and I recommend that other organizations in the space consider similar updates.\n\n![Expensive photograph of a simple open notebook on wood table - 3 very-rough old hand-drawn mini-sketches, of engineering flow diagrams that uses AI and brains to make predictions. Lots of white in background. Only a few lines drawn. No words.](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EykCuXDCFAT5oGyux/ddwsmmzz71oq6evcpdw3)\n\nImaginary sketch of AI epistemic infrastructure, Dall-E 3\n\n## **Key Definitions**:\n\n*   **Epistemic process:** A set of procedures to do analysis work, often about topics with a lot of uncertainty. This could be “_have one journalist do everything themselves_”, to a complex (but repeatable) ecosystem of multiple humans and software systems.\n*   **LLM-based Epistemic Process (LEP)**: A system that relies on LLMs to carry out most or all of an epistemic process. This might start at ~10% LLM-labor, but can gradually ramp up. I imagine that such a process is likely to feature some kinds of estimates or forecasts.\n*   **Scaffolding:** Software used around an LLM system, often to make it valuable for specific use-cases. In the case of an LEP, a lot of scaffolding might be needed.\n\n##  1. High-Level Benefits & Uses\n\n**Claim 1: If humans could forecast much better, these humans should make few foreseeable mistakes. This covers many mistakes, particularly ones we might be worried about now.**\n\n*   Someone deciding about talking to a chatbot that can be predicted to be net-negative (perhaps it would create an unhealthy relationship) could see this forecast and simply decide not to start the chat.\n*   Say that a person’s epistemic state could follow one of four trajectories, depending on some set of reading materials. For example, one set is conspiratorial, one is religious, etc. Good forecasting could help forecast this and inform the person ahead of time.\n    *   Note that this can be radical and perhaps dangerous. For example, a religious family knowing how to keep their children religious with a great deal of certainty.\n*   Say that one of two political candidates is predictably terrible. This could be made clear to voters who trust said prediction.\n*   If an AI actor is doing something likely to be monopolistic or dangerous, this would be made more obvious to itself and those around it.\n*   Note: There will also be unforeseeable mistakes, but any actions that we could do that are foreseeably-high-value for them, could be predicted. For example, general-purpose risk mitigation measures. \n\n**Claim 2: Highly intelligent / epistemically capable organizations are likely to be better at coordination.** \n\n*   This might well mean fewer wars and conflict, along with corresponding military spending.\n*   If highly capable actors were in a prisoner's dilemma, the results could be ugly. But very often, there’s a lot of potential and value in not getting into one in the first place.\n*   Evidence: From [The Better Angels of Our Nature](https://en.wikipedia.org/wiki/The_Better_Angels_of_Our_Nature), there’s significant evidence that humanity has become significantly less violent over time. One potential exception is that large risks of conflict between large actors might have increased, but these last bits are some of the most difficult to coordinate, so this might not be too surprising. \n\n**Claim 3: The harm caused by altruistic development of epistemic improvements could be capped by a self-enforcing loop.**\n\n*   Agents developing epistemics should use the tools they build to decide on the net value of further improvements. This is a very important variable to get right.\n*   If it ever seems like future work will be negative in expected value, progress could and should be stopped.\n\n**Claim 4: Better forecasting would lead to more experimentation and innovation (“weirdness”).**\n\n*   Radical technology projects are well-known to be high-risk. High returns when they go well, high losses if they don’t. This means that there’s a high value of information in being more correct about outcomes early on.\n*   If people could become much better at forecasting, then it’s expected that they will become better at picking innovations to try. Bad projects would be avoided more often, and the expected value of innovation would increase.\n\n**Claim 5: AI forecasting/epistemics will be much more deeply integrated into the world than current prediction markets.**\n\n*   First, note that current prediction markets / tournaments are highly niche and overlooked. So AI systems would have to be more accepted, to be very valuable.\n*   Unlike current “human forecasters”, AI forecasting can be embedded deeply into other software. There would likely be better AI forecasting for repeated decisions, like devops procedures, edge cases in insurance, AI agents, etc.\n*   If “AI forecasters” were really solid and cheap, we’d expect them to improve many complex software workflows. Existing “human forecasters” are hard to directly make much money from, but these AI systems would be much easier to do so.\n*   They would likely be integrated in many other systems, such as:\n    *   Overseeing product sales chances\n    *   Estimating changes in accounting, from one quarter to the next\n    *   Estimating project lengths and budgets\n    *   Deciding the best LLM to use for a certain AI workflow, or the best workflow to follow.\n*   Note that many developments in \"_AI epistemics_\" are just regular enhancements to LLMs. There's no clear line between \"_AI forecasting tech_\" and \"_improvements in LLMs that make them better general-purpose reasoners._\"\n\n**Claim 6: Even \"straightforward\" AI forecasting systems should look very different from human judgemental forecasting systems**\n\n*   I've found others seem to imagine AI forecasting systems as simply replacing humans in prediction tournaments and prediction markets. I think think this is perhaps a decent first model, but not at all how things would end up.\n*   Prediction markets / tournaments are very much optimized for competitive humans, in cases where humans need to be incentivized. AI systems have very different constraints, so will need very different systems.\n    *   Formal resolutions won't be necessary to incentivize AIs, after they are trained.\n    *   Forecasting can be done on-demand, as the AI is always available. So questions and forecasts might not have to be done continuously, before being read by end-users.\n    *   There's unlikely to be a need to have many AIs compete against each other, like in a traditional tournament, in most cases.\n    *   Modern prediction platforms don't use advanced inputs like [scorable functions](https://forum.effectivealtruism.org/posts/ZWQ5jbLxpd4JtQgu4/scorable-functions-a-format-for-algorithmic-forecasting). Arguably these would be too complex for many humans, but this is unlikely to be the case for AIs. \n\n## 2\\. Failure Modes\n\n1.  **Nonprofit efforts in the space are not effective**\n    1.  Claim: The most likely failure mode for projects in this area is that they will fail to make an impact.\n        1.  If LLM abilities dramatically outpace agent / scaffolding architectures, then the only useful work here might be done by large LLM labs.\n        2.  There will likely be commercial entities taking care of the most obvious and low-hanging market opportunities. It’s not clear how to expect this area to change over time. It's possible that nonprofit / EA efforts here would have trouble finding a strong fit.\n2.  **Forecasting could be used for net-bad activities**\n    1.  **AI Acceleration**  \n        It’s possible that epistemic improvements would be used by AI capabilities actors to increase capabilities and not make other informed choices.  \n          \n        Claim: I think this is unlikely to be the net outcome. This would require these organizations to become smarter about making AIs, but not much smarter about making them safe, including for their own benefit. It would also require the other groups that could block them to become smarter - some of these groups include other non-LLM companies and other highly motivated actors who wouldn't want AI to go poorly.  \n          \n        One crux here is if the AI developers will both be much better at integrating AI epistemic improvements in their internal workflows, and if these systems won't convince or help them with AI alignment. This could be a really bad outcome, so it's important to continue to pay attention to.\n    2.  **Malicious Governments**  \n        Governments like China or North Korea could use better epistemic systems to better control their populations. My guess is that great epistemic systems will likely do some damage here. I think that overall the improvements (like improving western governments) will more than outweigh that, but this is something to pay attention to.  \n          \n        Overall, I don’t see reasons to think that better epistemics will dramatically change existing power balances in ways that are yet predictable. I think this is true for most technologies I could imagine, even many of the most beneficial ones.\n    3.  **Malicious Corporations**   \n        There are some situations where companies could be far better at forecasting than consumers would be, leading to a large power imbalance. If companies have much more information about customers than users have about themselves or these companies, that could lead to them being taken advantage of. It seems important to use governments (hopefully with better epistemic infrastructure) to take active steps against this. It seems important here to work to make sure that epistemic advances get distributed equitably.\n3.  **Intelligent systems lead to more complexity, leading to a less stable world**  \n    Arguably as humans become more intelligent, human civilization becomes more complex. Complexity has a cost, and at some points, these costs can become detrimental.  \n      \n    As you might expect, I think this is a _predictable_ disaster, meaning that better prediction abilities would help here. I’d expect agents to be able to make decisions about one step at a time, and for each, see if the benefits are worth the costs. In addition, forecasting will make many things simpler and more handleable. See sources like [Thinking Like a State](https://en.wikipedia.org/wiki/Seeing_Like_a_State) to understand ways that power structures actively force systems to become simpler - sometimes even _too_ much so.\n4.  **There's too much confusion over the terminology and boundaries, so research is stunted**  \n    \"_Epistemics_\" is a poorly understood term. \"_Judgemental Forecasting_\" is still very niche. The term \"forecasting\" is arguably too generic. It seems very possible that this area will struggle in finding agreed-upon and popular framings and terminology, which could contribute to it be neglected. I think more terminological and clarification work here could be valuable. This is similar to the idea of [Distillation and Research Debt.](https://distill.pub/2017/research-debt/) \n\n## 3\\. Viability\n\n**Claim 1: All/Almost all parts of a conventional epistemic process can be automated with LLMs.**\n\nLLMs can:\n\n1.  Generate forecasting questions\n2.  Resolve forecasting questions\n3.  Gather data from computers\n4.  Write specific questions to expert - a la interviews\n5.  Make forecasts\n6.  Make visualizations and dashboards of forecasts\n7.  Personalize information for specific people\n\n_Note: “Can be automated” doesn’t mean “easy to automate in a cost-effective or high-quality way.”_  \n \n\n**Claim 2: A lot of communication is low-entropy, especially with knowledge of the author.**\n\n*   Low-entropy means “_predictable by a decent AI agent, knowing information about the author’s history_”.\n*   Evidence: We currently have systems that predict what people will say in very limited extents - i.e. for email. These typically don’t have much information about the user. I expect this sort of thing will become more widely integrated and capable.\n*   Evidence: When _I_ write, I feel like much of my communication is highly predictable. I’m able to use Claude to do a lot for me. It’s not there yet - but the setup now is incredibly basic (it doesn’t even have my historic writing), and the tech is improving.\n*   At some point, we could make decent approximations of what many people believe on many topics. This could help us greatly speed up communication - AI systems would just need the pieces they can’t predict.   \n     \n\n**Claim 3: LLM-epistemics should be easier to study and optimize than human-epistemics.**  \n  \nThey can be:\n\n*   Reproducible (models don’t change over time)\n*   Versioned (if updates are made to an LLM, this new LLM can be separately studied)\n*   Pure (similar inputs, with seeds, produce the same outputs)\n*   Scalable (much easier for 10 researchers to study one LLM in parallel, than studying one human. Can also reuse the same exact LLM in many places, or researchers could run 500 LLM runs in parallel.)\n*   Investigatable (People have privacy concerns. LLMs don’t, so we don’t need to be cautious about making them feel bad with public evaluations)\n*   Modifiable (We can’t freely change parts of most people’s knowledge or abilities, but we can with LLMs.\n    *   We can experiment with a large number of isolated changes.\n    *   We can’t wipe a person’s memory in order to backtest a human-based epistemic process, but we (partially) do this for LLMs, by providing them access to specific data sets.\n    *   Related point: One challenge to note though is that the training data is particularly optimized for immediate times after the LLM was trained. We should probably only test LLMs in forecasting scenarios after they have been trained, and assume that their quality will degrade for forecasts they make far out from their training time. That said, this still is overall far more standardizable than with humans, which are typically our main alternative.   \n          \n        Add to that, that LLMs are often cheaper and better than many humans, and overall, these systems are hard to ignore.\n*   Controlled/limited. For example, if a question about \"Will person X be assassinated?\" were posted to a liquid prediction market, it could encourage someone to actually attack that person. (See: [assassination markets](https://en.wikipedia.org/wiki/Assassination_market)). However, if someone were to query their own AI forecasting system that question, they would hopefully have strong assurances that the AI won't be taking actions in the world. (Obviously, if the AI is not controlled, this could go poorly, but that is an extreme scenario.)\n\nEvidence:\n\n*   LLM R&D has dramatically risen recently and has resulted in major gains (>$2T) to company valuations. Human R&D around epistemics and reasoning has had nothing like this recently.\n*   We have far more studies and data about LLM performance than we do about important scientists, intellectuals, or decision-makers. There are of course some human studies (a la the Good Judgement Project), but these are expensive and thus few and very narrow.\n\nSubclaim: Even if we just wanted to understand ideal human epistemics better, one reasonable method might be to make strong LLM systems and study them, because these are much easier to study, and will likely lead to some generalizable findings.\n\n**Claim 4: LLM epistemic integration will be gradual.**  \nPractically speaking LLMs and Humans will work together side-by-side in epistemic processes for some time. I expect LLMs-epistemics to follow a similar trajectory to self-driving. First, it starts with light augmentation. Then it moves to “fully automated in most use cases, but occasionally needs oversight.” As with autonomous driving, Level 5 is very difficult.\n\n*   Subpoint: Don’t ask “_When will LLMs be better than humans at \\[forecasting|writing|research\\],_” Ask questions more like, “_what percentage of work done in \\[forecasting|writing|research\\] be automated_”, or, “_what ratio of Manifold Karma will be won by bots vs. humans vs. hybrids, over time?_” I’d expect to see gradual gains here.\n\nSubclaim: I expect that LLM-based forecasters will gradually dominate open platforms like Manifold and Metaculus. As this happens, it might scare off many of the human forecasters. If this is not handled gracefully, it could lead to these platforms feeling like ghost towns and becoming irrelevant, in favor of people using more direct AI systems.\n\n## 4\\. Scaffolding\n\nPrevious LLM-based forecasting systems have included some amounts of scaffolding.\n\n*   [FutureSearch.ai](http://futuresearch.ai)\n*   [Approaching Human-Level Forecasting with Language Models](https://arxiv.org/abs/2402.18563)\n\n  \n**Claim 1: LLM-based epistemic systems will likely require a lot of scaffolding, especially as they get ambitious**.  \nI.E. software, databases, custom web applications, custom batch job pipelines, etc. \n\n  \n**Claim 2: Scaffolding-heavy systems have significant downsides.**\n\n*   Difficult to study. Unlike single LLMs, scaffolded systems can feature:\n    *   High costs of deployment.\n    *   No/little ability to roll-back.\n    *   Frequent changes.\n    *   Internal substructures that are highly variable between different implementations.\n    *   External-facing APIs that are very different between different implementations.\n    *   Evidence: The previously mentioned LLM-based systems have some of these failures - and these are simple systems compared to what we might have later.\n*   Subclaim: If scaffolding is to be expected / needed, it could be important to plan for research measures that take this into account. This will require specialized attention.\n*   Subclaim: It seems important for there to be some “portable/packaged” LEPs, that could easily be shared, versioned, deployed, etc. This would be useful for research and benchmarks/baselines.\n\n**Claim 3: Scaffolding-heavy systems are likely to be centralized.**\n\n*   In general, complex web projects are centralized.\n*   This means that scaffolding-heavy setups will favor startups and in-house teams.\n*   This means that it's easy for many of these systems to be useless. Users might only interact with 1-2 systems, so any smaller competitors might get totally ignored, even if they have a few unusually great features.\n*   It’s possible that there could be some clean lines of separation between different components. Some lines seem simple to try out, but experimentation and thinking is needed to develop a better idea. Thinking/planning here seems good.\n    *   The more it’s split up into understood components, the easier it will be to research, optimize, and proliferate each part.\n\n**Claim 4: Scaffolding-heavy systems are expensive, and thus difficult to incentivize people to build.**\n\n*   Most forecasting competitions now pay out fairly small amounts. This makes it difficult to incentivize scaffolding.\n*   I don’t expect people to independently create great scaffolding in order to win <$50k.\n    *   My impression is that very little forecasting on Metaculus and Manifold is automated with LLMs. (There is one recent Metaculus competition here, but this is an isolated and forced use.)\n*   It’s also difficult to share and maintain scaffolding. You need to make custom web UIs, you might have to set up billing infrastructure for payments, etc. In a very large forecasting ecosystem with a lot of money, forecasters would subscribe to scaffolding systems, in the same way the professional traders subscribe to trading software and services.\n*   This means that we might well see slower progress here / less diversity.   \n     \n\n**Claim 5: New technologies or startups could greatly change and improve scaffolding.**\n\nRight now there are some startups and open-source frameworks to help with general-purpose scaffolding, but these are [known to be lacking](https://news.ycombinator.com/item?id=40739982). It's possible that much better systems will be developed in the future, especially because there's a lot of money in the space.\n\nIf this happens, that could hypothetically make powerful LLM epistemic tools dramatically easier to create.\n\nOne annoying challenge is that change in the field could lessen the importance of early experimentation. It could be the case that early projects focus heavily on some specific scaffolding frameworks, only for other frameworks to become more powerful later.\n\nA possible successful world could be one where different teams make \"[GPTs](https://chatgpt.com/gpts)\" that can integrate nicely with other workflows and interact with each other. Ideally these wouldn't be restricted to any one platform.  \n  \n**Crux: Scaffolding vs. Direct LLM Improvements** \n\nOne risk around scaffolding is that a lot of key functionality that scaffolding would be used for might be implemented directly in the inference of LLMs. For example, long-term planning could be done simply with scaffolding, but AI developers are also iterating on implementing it directly into the LLM layer. \n\nAnother example is that of editing text. If you have a long text document and want to use an LLM to make a few edits, it would currently attempt to rewrite the entire document, at a fairly slow speed. This could be improved with scaffolding by instead asking it to write a git diff and then applying that using traditional code. Or it could be solved on the inference layer directly, [as discussed here](https://www.cursor.com/blog/instant-apply). \n\nIf it is the case that most future improvements are done directly with the LLMs, then that could significantly change what work will be valuable. For example, if there are only a few leading LLM developers, then it might ultimately be up to them to actually add any functionality.  \n\n## 5\\. Subcomponents\n\nAs stated above, it’s hard to determine the best subdivisions of LEPs / LLM-Epistemic Scaffolding.\n\nHere’s one attempt, to provide a simple picture. \n\n**1\\. Data-collection**\n\n*   Many LLM services are already coming out that can help quickly put data, including private data, together, for different needs. See Apple Intelligence as one service that would work for people’s personal data.\n*   There are hundreds of startups in the data-collecting and data-management space. This problem can easily be split up more.\n*   Example performance question:  \n    _“Provided with resources R, gather as much data as you can, from this list of desired data, about X.”_  \n    The results of this will likely be obvious (it’s likely to be clear what data is accessible), so scientific testing might not be needed.   \n     \n\n**2\\. World Model**\n\n*   “World model” here likely means, “A set of stored symbolic and probabilistic assumptions about the world / people / organizations.”\n*   World models will likely include software programs and writing, instead of only relying on existing LLMs.\n    *   I expect that there could be workflows to LEPs to contribute to world models that will later give it better forecasting / reasoning abilities. The forecasting components could build the world models.\n*   Existing online data has very minimal attempts at providing accessible world models. These would ideally be probabilistic, immutable, shareable, collaborative, etc.\n*   Tools like [Squiggle Hub](https://squigglehub.org) are meant to be used here. Formal ontologies and knowledge graphs are likely to be useful.\n*   Some world models could be directly interesting to humans, for the presentation layer. Others might primarily be used by LLMs. For these, the world model is a form of a cache / collaboration.\n*   It seems possible/likely that world models won’t be unique to LEPs. Instead, many separate LEPs can use the same world models.\n*   World models could contain (or complete be) text descriptions of outcomes. For example, 10 compressed pages explaining how the next 10 years will go for a specific region.\n    *   It’s not clear yet what the best representations are for LLMs to store and fetch data about its understanding of specific things.\n*   Example performance question:  \n    “_Given a forecasting LLM L, and forecasting question Q, contribute a world model that will result in L being highly accurate_.”  \n     \n\n**3\\. Human Elicitation**\n\n*   It’s often useful to just go out and directly ask questions to people, or give them surveys. Strong LEPs should ideally be able to design these procedures to maximize information value, then execute them autonomously.\n    *   A system might realize that it would be useful to get a very specific expert to give their take on something. It could then email them, and perhaps offer a financial reward.\n    *   A different system might generate a bunch of specific binary questions for a group of people it wants to elicit information from. Maybe it’s trying to estimate aspects of their preferences, and information would help.\n*   Example performance question:  \n    _“Given that you want to get as much useful new data about topic X, using existing research Y, use resources R to send off elicitions and gather data from humans.”_\n\n**4\\. Data Prioritization**\n\n*   The task here is, “_Given the potential to fetch from data source D, and resources R, fetch the most useful data for providing marginal insight on unknown variables V._”\n*   Summarizing and displaying such work could also be useful. But doing so for humans would be very different to doing so for other LLMs.\n*   It seems like with LLMs that are capable of providing quick and accurate forecasts, we could use reinforcement learning and similar algorithms to do a good job here. For any set of data, we could quickly measure the improvement in accuracy using that data.\n\n**5\\. Numeric Modeling / Fermi Calculations**\n\n*   Make great complex Python/Excel/Squiggle models/estimates.\n*   One reasonable approach to fermi estimation is “Try a bunch of very different approaches, then mix the answers together.”\n*   We’ve seen a lot of evidence that LLMs can generate working Python/Squiggle code. However, I don’t believe that there has been much optimization in this area yet.\n*   It seems easy to iterate on, and generate, great data of strong examples of this. That could be then used for LLM training / fine-tuning.\n*   Example performance question:  \n    _“Estimate variable V as well as you can, using up to N LLM calls, and no other calls.”_  \n     \n\n**6\\. Decomposition / Amplification**\n\nThere are many potential strategies to \"break a problem down\" and solve different parts with different threads of LLMs. \n\nFor example:\n\n*   Chain-of-Thought prompting\n*   Mixture of experts\n*   Ensemble Averaging\n*   Debate (i.e. [Safety via Debate](https://arxiv.org/abs/1805.00899))\n*   Fact verification\n\nArguably, numeric modeling is a form of decomposition/amplification, I just separated it to draw attention to its uniqueness.\n\nThis is an active and popular area of ML research. Some of these techniques are incorporated in the LLM inference layer, but some are also used in scaffolding.\n\nI'm sure there's a lot more work to do here. It's less clear where smaller nonprofit research can be most effective, but I suspect that there are some good options. \n\nMany methods might be unique to forecasting or specific epistemic workflows. For example:\n\n*   Try to identify key cruxes or crucial considerations, and then investigate them\n*   Automated debate mapping\n*   Identify [Scissor statements](https://slatestarcodex.com/2018/10/30/sort-by-controversial/), in part to help personalize research\n*   Use [discernment](https://www.lesswrong.com/posts/W2iwHXF9iBg4kmyq6/the-practice-and-virtue-of-discernment) to clarify and decompose key terminology\n*   Breaking down forecasting questions into sub-questions, perhaps with bayesian networks. \n\nArguably, the rationality and effective altruist communities might be very well suited to work in this area. There's been a lot of writing on how to systematically break hard problems down, now people would need to experiment with encoding these systems into automated workflows.   \n  \n**7\\. Question Generation** \n\n*   Come up with questions to be researched or forecasted on.\n*   This might well be done by other listed components - for example, it could simply combine \"Idea Generation\" with \"Forecasting\" to come up with potential questions, and guess which will be the most important and tractable. It also might be better done under the \"Presentation\" question. However, I suspect that there will be unique research and development for this problem, especially because it's a major part of existing forecasting systems.\n*   Example performance question:  \n    _“Write 5 forecasting questions on topic X. After this, a separate epistemic system will be run with a fixed number of resources R on each question. Choose questions such that the final results are maximally useful to end-users.\"_  \n     \n\n  \n**8\\. Forecasting**\n\n*   Convert fetched data into a forecast.\n*   Can call the “Numeric Modeling” module, or be merged with it.\n*   There’s been previous work here that is promising.\n*   Fine-tuning and prompt engineering seems like it would go a long way.\n*   Current LLMs are known to be biased and often suffer from poor calibration, in part in order to become better liked by human users. We might well want specialized systems optimized for pure accuracy.\n    *   This delta might cause problems. Perhaps it could lead to bad news stories (where a model used for forecasting gives a politically-incorrect answer), or perhaps it could be used by bad actors.\n    *   This area should be thus treated with care. Perhaps some models are only available to certain researchers, or perhaps there should be large disclaimers provided for regular people attempting to use them directly.\n*   Forecasting can output high-level stories, probabilities, scorable functions, or other. There are many potential formats here.\n*   Example performance question:  \n    _“Using information I, and N LLM calls, make your best forecast on variable V. You may use the Numeric Modeling module when doing so, but this will eat into the LLM calls.”_  \n     \n\n**9\\. Forecasting Resolution**\n\n*   Most forecasting questions should be resolvable automatically. This can be done by either querying external data, or perhaps by asking a person/LLM for their opinion.\n*   This component is likely to often just either be, or defer to, other LEP modules.\n*   Question Creation and Optimization\n    *   A strong LEP would make good judgment calls on what research or forecasting topics would be valuable to people, and initialize those.\n    *   This could range from “once a user logs in, generates custom information for them to maximize their EV”, to, “write a list of standard forecasting questions that are collectively high-EV to a subpopulation.”\n    *   Example performance question:  \n        _“Using an existing LEP L, and resources R, write a set of forecasting questions. Expect that forecasting resources F will be used on these questions, and you are trying to optimize utility function U.”_  \n         \n\n**10\\. Presentation**\n\n*   Many custom charts and graphics would likely be useful.\n*   Decisions need to be made about what final data should be shown to users and how.\n*   Automated decision-making: In some cases / eventually, LEPs would interact directly with decision systems. In theory, people don’t need to know almost anything - critical information could be used directly by in-world automations. For example, instead of an LEP doing research into the costs and benefits of vitamin D supplementation and showing you that research, a system instead just starts an order for you to get a regular set of supplements. This could come with optional research, but in many cases the research won’t be reviewed, if the system is trusted.\n*   In cases where LEPs are used behind APIs, the presentation layer could include answering questions about what the LEP could do, and suggesting specific uses for it.\n*   Presentation/wording should be optimized to be understandable to humans. There should be tests of the error rates of what was intended to be stated, and what humans understood. For example, many people have significant misunderstandings about basic statistics concepts, so probabilities might need to be handled with care.\n*   Example performance questions:\n    *   _“Using LEP output O, design a dashboard that user U will most prefer.”_\n    *   _“Using LEP output O, design a set of complicated diagrams without detectable errors.” (judged by an LLM)._   \n         \n\n**11\\. Distribution**\n\n*   Full custom user interfaces could be created. This might range from a simple custom LLM chat, to a auto-generated set of dashboards, with the ability to delve deeper into any topic the user clicks on.\n*   LEPs could be accessed by any other chat LLM interface, as long as that interface can call external APIs. This could be generally useful for integrating into other AI tools and workflows, perhaps via other common LLM chat UIs.\n*   LEP information could be distributed like how human communication is communicated. Text messages, emails, Tweets, voice assistants, etc. Ideally the LEP could vaguely predict what information is useful to people and when, and then send them that information in those cases.\n    *   Example: You get an email in the morning. It says, _“Some news came out at 6am today that changed my recommendation of your financial assets. I recommend you change to a different mutual fund. I estimate that doing so will net you an average of $50/month.”_\n*   Example performance questions:\n    *   _“Using LEP output O, design and implement the best way to get this information to agent A.” We can use an LLM that mimics human behaviors to evaluate._  \n         \n\n**12\\. Idea Generation / Optimization**\n\n*   It will often be useful to generate ideas on the best ways to do something.\n*   For example, an LLM agent might want to make a suggestion to a user, before doing forecasting on said suggestion.\n*   Regular LLM performance might be fine on this, but fine-tuning or similar might help. It might be the case that there are certain specific domains where idea generation is very useful.\n*   “Idea Generation” is related to [program optimization](https://en.wikipedia.org/wiki/Program_optimization) and other CS topics.\n*   Example performance questions:\n    *   _“Given a person with details D, generate a plan for that person to optimize their financial future.”_ (This will be then judged with an LEP / forecasting module)\n    *   _“Given a 2d grid game G, come up with 10 ordered ideas for the best strategy”_ (Can be judged empirically)  \n         \n\n**13\\. Task Orchestration**\n\n*   We need systems that can actually trigger all the other components when needed. This process needs to be debuggable / introspectable. Ideally, these workflows would be highly understandable.   \n     \n\n## 6\\.  Benchmarks & Baselines\n\n**Claim 1: Accessing forecasting quality and efficiency with humans is very difficult and often poorly done.**\n\n*   Scores like the Brier Score don’t take into account the difficulty of the question.\n*   It’s really hard to standardize question sets, audiences, and environments. Each time a human forecasting competition is done, it’s done with a separate population. Also, we typically don’t track the resources going into these systems - like the total time that people spend forecasting. Again, humans are mediocre for studying, in comparison to LLMs.\n*   We arguably don't yet have good statistical measures for [credal resilience](https://forum.effectivealtruism.org/topics/credal-resilience).\n\n**Claim 2: Portable LEPs could be useful for acting as priors or baselines, for other systems to perform against.**\n\n*   We will want some baseline LEPs to score other LEPs against. These should act on the same data, at the same times.\n    *   For example, one could take a new LEP, a well-known Baseline LEP, and a known or generated question set. Both systems would forecast those questions, and then the new LEP will be evaluated based on how much it outperforms the Baseline LEP. For example, with the average difference in brier score.\n    *   In order to do this, the LEP must be equipped with the abilities to fetch data and to forecast. This is useful in order to compare the system to human forecasters (which do both), but it might be often better to divide the subcomponents and study/optimize them in isolation.\n    *   Another use is to use Baseline LEP to initialize forecasting markets. Right now manifold questions begin at 50%. It could be better for them to instead begin at a number suggested by a simple LEP.\n    *   Even if these LEPs aren't highly accurate, they could be useful for sanity checking claims outside of forecasting systems. For example, all publicly given estimates and statements could be run against LEPs. This can then highlight estimates that are particularly surprising. In financial / numeric models, all inputs and assumptions can be checked against LEPs to find ones that might be suspicious.\n\n**Claim 3: Standardized LEPs could be great judges to resolve subjective questions**\n\n*   It’s well-known that it’s difficult to use partially-subjective questions in prediction markets. Much of the reason is because the only way to resolve these is with human expert judges, and these human judges have eccentricities that are hard to predict. AI systems might help here, because they can be far more standardized and testable than human systems.\n*   For example, say there’s a question, “_Will project X generally be considered a failure in 2026?_” This can be changed to, “_Will project X be considered a failure, in an average of 20 runs, by a certain LEP, in 2026?_” Then, predictors could provide this LEP fake hypothetical information on the future and see how this question would resolve with that information. There should be very little surprises from the eventual output of the LEP.\n*   Related, LEPs could be used to provide independent resolutions for every other forecasting question on Metaculus/Manifold/etc. There would be a certain expected error rate - if some resolution judge statistically exceeds this rate, that would be flagged for review.\n\n**Idea: It’s possible we could get very useful small LEPs that are easily packaged, relatively cheaply.**\n\n*   Certain LEP components are much more useful to include for different kinds of testing than others. Some simple web search functionality and forecasting abilities could get us far for testing forecasting. Right now there are a bunch of startups and tools emerging to help with certain kinds of scaffolding that might be useful here. For example, you can currently bundle special data and prompts with a custom ChatGPT GPT. Right now this cannot be accessed via the API, but this is a potential thing that could exist in the future.\n*   One viable solution could be for someone to set up a very simple web service that can make internet queries and forecasts. Ideally, this would provide multiple immutable options, and this would be trusted to stay up, for future experimentation. It might only need to be accessible via an API.\n*   It might also be possible to have a simple LEP in a single Docker container or similar.\n*   Even very simple LEPs could act as much better baselines than what we use today.\n\n**Challenge: LLM-forecasting abilities will vary based on date.**\n\n*   The quality of LLM predictions is likely to go down, the further away the forecasting question resolution is from the LLM training date. This is a major challenge to using them as a benchmark. For example, GPT4 might be decent at guessing the US GDP in 2025 if used in 2024 with search capabilities (i.e. web search). But if it were used in 2030, for the US GDP in 2031, with search capabilities but no other training updates, its performance would likely be worse. It's background/training data would be much less relevant.\n*   It's possible that there could simple numeric adjustments for this, for certain accuracy scores. For example, perhaps it's expected that the forecasting quality would decline according to a linear trend, in proportion between the time the LLM was trained, and when the forecasting question was asked.\n*   LLMs should rarely be used to “_forecast_” events that happened before their release. It’s likely that their training data would have leaks of the results, or of things close to these results.\n*   Related, LLMs will be better at forecasting in some domains in others. However, I don't think that this will be a major problem. We will learn about these boundaries over time, and I expect that they will be heavily correlated between different LLMs and LLM versions. \n\n## 7\\. Caching vs. On-Demand Forecasting\n\n**Key question: How much would we expect AI forecasting systems to generate forecasts and visualizations on-demand for users, vs. save them before?**\n\n*   On-demand work is more user-customizable and user-specific, but it’s harder.\n*   In the beginning, on-demand AI work will be used for the “last-mile” problem. A user asks a question - we first find the most similar forecasts, then use an LLM to use those to generate a personalized forecast. We’d also want great charts ready-to-go.\n*   This tracks a question about web content more generally. Eventually websites might be generated on-demand, to be personalized for users. But it will likely take some time to get there.\n\n**Simple heuristics:**\n\n*   Save all generated forecasts, especially if these are expensive to produce. Reference them for future forecasting. This should be very useful to have the system cost-effectively scale to handle a large number of queries.\n    *   Be careful of potential privacy issues, though.\n*   If the system can predict that (A) a bunch of questions will be asked on topic T in the future, and (B) some saved computations will be useful, then it will aim to produce such work ahead of time. This might be by fetching a lot of relevant data, updating the world model, or making other improvements.\n*   It’s probably not an efficient bet to make complex scorable functions with a lot of attention to parts of the parameter space that are unlikely to actually be called. Instead, do a rough version, then wait until they are called to bother improving them in these areas. Save the updated version, of course.\n\n## Further Research\n\nI feel like there are clearly some larger investigative projects to do here. It's a vast and important topic, and at the same time there's a lot of fairly-obvious things (yet interesting and useful) we could say about it. This is mostly just putting a bunch of existing ideas in one place and providing some definitions and clarifications.\n\nSome future things I'd like to explore (or have others explore!) include:\n\n1.  **Exploration of benchmarks and optimization of end-to-end LEPs**  \n    This document includes more about specific subcomponents of LEPs than the full thing itself.\n2.  **Strategies for prioritizing philanthropic LEP work**  \n    I think this document helps list out a lot of potential LEP work, but it doesn't try to prioritize it. This would take some thinking. Maybe predictions can be used.\n3.  **Cost-effectiveness estimation**  \n    I think these systems are promising, but I'm not sure how to best estimate the cost-effectiveness of them vs. other effective altruist interventions. It would be useful to have better models of benefits and risks, perhaps with new delineations.\n4.  **Concrete project ideas for enthusiasts**  \n    I could imagine a lifehacker-like community forming around some of these principles. I think creative experimentation by enthusiasts could be useful, in part to help identify promising full-time talent.\n5.  **Moving key claims and cruxes to forecasting platforms and LLMs**  \n    Fitting with the theme, it seems interesting to try to pull out key questions here and post them to forecasting platforms. I tried to isolate claims and cruxes, in part so that they could later be easily adapted into somewhat-concrete questions. \n\nMy guess is that a lot of people will bounce off this broad topic. I imagine that there's much better writing that could be done to formalize and popularize the key concepts that will be useful going forward. I'd encourage others to make stabs here.",
  "reviews": [
    {
      "agentId": "bias-detector",
      "costInCents": 14782,
      "createdAt": "2025-04-16T08:48:51.339Z",
      "runDetails": "{\"model\":\"anthropic/claude-3.7-sonnet\",\"promptTokens\":11734,\"completionTokens\":3048,\"totalTokens\":14782,\"temperature\":0.1,\"runtimeMs\":70633}",
      "thinking": "I'm analyzing a document that discusses the potential of LLM-based epistemic systems (LEPs) for improving forecasting and decision-making. The author presents a vision for how AI systems could enhance human epistemics and outlines various components, benefits, risks, and research directions.\n\nLet me identify potential biases in this text:\n\n1. **Optimism Bias/Technology Enthusiasm**: The author appears quite enthusiastic about the potential of LLM-based epistemic systems. While they do mention some risks, the overall tone suggests a strong belief that these systems will be beneficial. This could lead to underestimating potential downsides or overestimating benefits.\n\n2. **Status Quo Bias in Reverse**: The author seems to assume that current human epistemic processes are significantly flawed and that AI systems will naturally improve upon them. This may undervalue existing human expertise and overvalue potential AI contributions.\n\n3. **Confirmation Bias**: The document presents many claims that support the author's view that LEPs are promising, with less rigorous examination of counterarguments. The author acknowledges this is a \"rough brainstorm\" but the presentation of ideas still shows signs of seeking evidence that confirms their pre-existing view.\n\n4. **Availability Heuristic**: The author draws heavily on recent advances in LLMs to project future capabilities, which may overweight recent developments and underweight historical challenges in AI development.\n\n5. **Anchoring Effect**: The document anchors much of its thinking around current LLM capabilities and extrapolates from there, which may limit consideration of radically different approaches or limitations.\n\n6. **In-group Bias**: There are references to the EA community and rationality communities that suggest the author is writing from within these communities and may be influenced by their common perspectives and values.\n\n7. **Narrative Fallacy**: The author constructs a compelling narrative about how LEPs will develop and be integrated, which may oversimplify the complex, non-linear nature of technological development.\n\n8. **Selection Bias**: The examples and evidence presented appear selected to support the author's thesis, with less attention to contradictory evidence or failed AI forecasting attempts.\n\n9. **Hindsight Bias**: When discussing LLM capabilities, there's a tendency to present current achievements as if they were inevitable, which may underestimate the difficulty of future advances.\n\n10. **Illusion of Control**: There's an implicit assumption that we can direct and control how these systems develop and are deployed, which may overestimate human agency in technological development.\n\n11. **Bandwagon Effect**: The enthusiasm for LLMs in the broader tech community may be influencing the author's assessment of their potential for epistemic improvement.\n\n12. **Survivorship Bias**: The document focuses on successful AI developments without equal attention to failed approaches or abandoned directions.\n\n13. **Authority Bias**: References to organizations like QURI and concepts from the rationality community may be given extra weight due to their perceived authority in the space.\n\n14. **Planning Fallacy**: The document may underestimate the time and resources needed to develop effective LEPs.\n\n15. **Scope Insensitivity**: The broad vision presented may not adequately account for the vastly different scales of challenges in different domains of forecasting and decision-making.\n\nNow I'll identify specific sections that demonstrate these biases and prepare my comments.",
      "summary": "This document exhibits several significant cognitive biases that affect its assessment of LLM-based epistemic systems. The author demonstrates optimism bias by presenting an enthusiastic vision of AI-enhanced forecasting with insufficient critical examination of limitations. Confirmation bias is evident in how potential failure modes are acknowledged but quickly dismissed. The availability heuristic leads to overestimating capabilities based on recent LLM advances rather than historical AI challenges. The document shows the illusion of control regarding our ability to direct these systems' development and the planning fallacy in underestimating implementation difficulties. Selection bias appears in the evidence presented, which predominantly supports the author's thesis. In-group bias is reflected in references to EA and rationality communities. The document does show more balance when discussing scaffolding challenges, but overall presents a narrative that may oversimplify the complex, non-linear nature of technological development and adoption. These biases collectively create an overly optimistic assessment that underweights potential risks and implementation challenges.",
      "comments": [
        {
          "title": "In-group Bias in Community References",
          "description": "The document shows in-group bias through references to the EA community, rationality concepts, and organizations like QURI. This suggests the author is writing from within these communities and may be influenced by their common perspectives and values, potentially limiting consideration of alternative viewpoints or approaches from outside these communities.",
          "importance": "70",
          "grade": "50",
          "highlight": {
            "startOffset": 1008,
            "endOffset": 28473,
            "quotedText": "s.\n\nI'm trying to figure out what to make of this regarding our future plans at [QURI](https://quantifieduncertainty.org), and I recommend that other organizations in the space consider similar updates.\n\n![Expensive photograph of a simple open notebook on wood table - 3 very-rough old hand-drawn mini-sketches, of engineering flow diagrams that uses AI and brains to make predictions. Lots of white in background. Only a few lines drawn. No words.](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EykCuXDCFAT5oGyux/ddwsmmzz71oq6evcpdw3)\n\nImaginary sketch of AI epistemic infrastructure, Dall-E 3\n\n## **Key Definitions**:\n\n*   **Epistemic process:** A set of procedures to do analysis work, often about topics with a lot of uncertainty. This could be “_have one journalist do everything themselves_”, to a complex (but repeatable) ecosystem of multiple humans and software systems.\n*   **LLM-based Epistemic Process (LEP)**: A system that relies on LLMs to carry out most or all of an epistemic process. This might start at ~10% LLM-labor, but can gradually ramp up. I imagine that such a process is likely to feature some kinds of estimates or forecasts.\n*   **Scaffolding:** Software used around an LLM system, often to make it valuable for specific use-cases. In the case of an LEP, a lot of scaffolding might be needed.\n\n##  1. High-Level Benefits & Uses\n\n**Claim 1: If humans could forecast much better, these humans should make few foreseeable mistakes. This covers many mistakes, particularly ones we might be worried about now.**\n\n*   Someone deciding about talking to a chatbot that can be predicted to be net-negative (perhaps it would create an unhealthy relationship) could see this forecast and simply decide not to start the chat.\n*   Say that a person’s epistemic state could follow one of four trajectories, depending on some set of reading materials. For example, one set is conspiratorial, one is religious, etc. Good forecasting could help forecast this and inform the person ahead of time.\n    *   Note that this can be radical and perhaps dangerous. For example, a religious family knowing how to keep their children religious with a great deal of certainty.\n*   Say that one of two political candidates is predictably terrible. This could be made clear to voters who trust said prediction.\n*   If an AI actor is doing something likely to be monopolistic or dangerous, this would be made more obvious to itself and those around it.\n*   Note: There will also be unforeseeable mistakes, but any actions that we could do that are foreseeably-high-value for them, could be predicted. For example, general-purpose risk mitigation measures. \n\n**Claim 2: Highly intelligent / epistemically capable organizations are likely to be better at coordination.** \n\n*   This might well mean fewer wars and conflict, along with corresponding military spending.\n*   If highly capable actors were in a prisoner's dilemma, the results could be ugly. But very often, there’s a lot of potential and value in not getting into one in the first place.\n*   Evidence: From [The Better Angels of Our Nature](https://en.wikipedia.org/wiki/The_Better_Angels_of_Our_Nature), there’s significant evidence that humanity has become significantly less violent over time. One potential exception is that large risks of conflict between large actors might have increased, but these last bits are some of the most difficult to coordinate, so this might not be too surprising. \n\n**Claim 3: The harm caused by altruistic development of epistemic improvements could be capped by a self-enforcing loop.**\n\n*   Agents developing epistemics should use the tools they build to decide on the net value of further improvements. This is a very important variable to get right.\n*   If it ever seems like future work will be negative in expected value, progress could and should be stopped.\n\n**Claim 4: Better forecasting would lead to more experimentation and innovation (“weirdness”).**\n\n*   Radical technology projects are well-known to be high-risk. High returns when they go well, high losses if they don’t. This means that there’s a high value of information in being more correct about outcomes early on.\n*   If people could become much better at forecasting, then it’s expected that they will become better at picking innovations to try. Bad projects would be avoided more often, and the expected value of innovation would increase.\n\n**Claim 5: AI forecasting/epistemics will be much more deeply integrated into the world than current prediction markets.**\n\n*   First, note that current prediction markets / tournaments are highly niche and overlooked. So AI systems would have to be more accepted, to be very valuable.\n*   Unlike current “human forecasters”, AI forecasting can be embedded deeply into other software. There would likely be better AI forecasting for repeated decisions, like devops procedures, edge cases in insurance, AI agents, etc.\n*   If “AI forecasters” were really solid and cheap, we’d expect them to improve many complex software workflows. Existing “human forecasters” are hard to directly make much money from, but these AI systems would be much easier to do so.\n*   They would likely be integrated in many other systems, such as:\n    *   Overseeing product sales chances\n    *   Estimating changes in accounting, from one quarter to the next\n    *   Estimating project lengths and budgets\n    *   Deciding the best LLM to use for a certain AI workflow, or the best workflow to follow.\n*   Note that many developments in \"_AI epistemics_\" are just regular enhancements to LLMs. There's no clear line between \"_AI forecasting tech_\" and \"_improvements in LLMs that make them better general-purpose reasoners._\"\n\n**Claim 6: Even \"straightforward\" AI forecasting systems should look very different from human judgemental forecasting systems**\n\n*   I've found others seem to imagine AI forecasting systems as simply replacing humans in prediction tournaments and prediction markets. I think think this is perhaps a decent first model, but not at all how things would end up.\n*   Prediction markets / tournaments are very much optimized for competitive humans, in cases where humans need to be incentivized. AI systems have very different constraints, so will need very different systems.\n    *   Formal resolutions won't be necessary to incentivize AIs, after they are trained.\n    *   Forecasting can be done on-demand, as the AI is always available. So questions and forecasts might not have to be done continuously, before being read by end-users.\n    *   There's unlikely to be a need to have many AIs compete against each other, like in a traditional tournament, in most cases.\n    *   Modern prediction platforms don't use advanced inputs like [scorable functions](https://forum.effectivealtruism.org/posts/ZWQ5jbLxpd4JtQgu4/scorable-functions-a-format-for-algorithmic-forecasting). Arguably these would be too complex for many humans, but this is unlikely to be the case for AIs. \n\n## 2\\. Failure Modes\n\n1.  **Nonprofit efforts in the space are not effective**\n    1.  Claim: The most likely failure mode for projects in this area is that they will fail to make an impact.\n        1.  If LLM abilities dramatically outpace agent / scaffolding architectures, then the only useful work here might be done by large LLM labs.\n        2.  There will likely be commercial entities taking care of the most obvious and low-hanging market opportunities. It’s not clear how to expect this area to change over time. It's possible that nonprofit / EA efforts here would have trouble finding a strong fit.\n2.  **Forecasting could be used for net-bad activities**\n    1.  **AI Acceleration**  \n        It’s possible that epistemic improvements would be used by AI capabilities actors to increase capabilities and not make other informed choices.  \n          \n        Claim: I think this is unlikely to be the net outcome. This would require these organizations to become smarter about making AIs, but not much smarter about making them safe, including for their own benefit. It would also require the other groups that could block them to become smarter - some of these groups include other non-LLM companies and other highly motivated actors who wouldn't want AI to go poorly.  \n          \n        One crux here is if the AI developers will both be much better at integrating AI epistemic improvements in their internal workflows, and if these systems won't convince or help them with AI alignment. This could be a really bad outcome, so it's important to continue to pay attention to.\n    2.  **Malicious Governments**  \n        Governments like China or North Korea could use better epistemic systems to better control their populations. My guess is that great epistemic systems will likely do some damage here. I think that overall the improvements (like improving western governments) will more than outweigh that, but this is something to pay attention to.  \n          \n        Overall, I don’t see reasons to think that better epistemics will dramatically change existing power balances in ways that are yet predictable. I think this is true for most technologies I could imagine, even many of the most beneficial ones.\n    3.  **Malicious Corporations**   \n        There are some situations where companies could be far better at forecasting than consumers would be, leading to a large power imbalance. If companies have much more information about customers than users have about themselves or these companies, that could lead to them being taken advantage of. It seems important to use governments (hopefully with better epistemic infrastructure) to take active steps against this. It seems important here to work to make sure that epistemic advances get distributed equitably.\n3.  **Intelligent systems lead to more complexity, leading to a less stable world**  \n    Arguably as humans become more intelligent, human civilization becomes more complex. Complexity has a cost, and at some points, these costs can become detrimental.  \n      \n    As you might expect, I think this is a _predictable_ disaster, meaning that better prediction abilities would help here. I’d expect agents to be able to make decisions about one step at a time, and for each, see if the benefits are worth the costs. In addition, forecasting will make many things simpler and more handleable. See sources like [Thinking Like a State](https://en.wikipedia.org/wiki/Seeing_Like_a_State) to understand ways that power structures actively force systems to become simpler - sometimes even _too_ much so.\n4.  **There's too much confusion over the terminology and boundaries, so research is stunted**  \n    \"_Epistemics_\" is a poorly understood term. \"_Judgemental Forecasting_\" is still very niche. The term \"forecasting\" is arguably too generic. It seems very possible that this area will struggle in finding agreed-upon and popular framings and terminology, which could contribute to it be neglected. I think more terminological and clarification work here could be valuable. This is similar to the idea of [Distillation and Research Debt.](https://distill.pub/2017/research-debt/) \n\n## 3\\. Viability\n\n**Claim 1: All/Almost all parts of a conventional epistemic process can be automated with LLMs.**\n\nLLMs can:\n\n1.  Generate forecasting questions\n2.  Resolve forecasting questions\n3.  Gather data from computers\n4.  Write specific questions to expert - a la interviews\n5.  Make forecasts\n6.  Make visualizations and dashboards of forecasts\n7.  Personalize information for specific people\n\n_Note: “Can be automated” doesn’t mean “easy to automate in a cost-effective or high-quality way.”_  \n \n\n**Claim 2: A lot of communication is low-entropy, especially with knowledge of the author.**\n\n*   Low-entropy means “_predictable by a decent AI agent, knowing information about the author’s history_”.\n*   Evidence: We currently have systems that predict what people will say in very limited extents - i.e. for email. These typically don’t have much information about the user. I expect this sort of thing will become more widely integrated and capable.\n*   Evidence: When _I_ write, I feel like much of my communication is highly predictable. I’m able to use Claude to do a lot for me. It’s not there yet - but the setup now is incredibly basic (it doesn’t even have my historic writing), and the tech is improving.\n*   At some point, we could make decent approximations of what many people believe on many topics. This could help us greatly speed up communication - AI systems would just need the pieces they can’t predict.   \n     \n\n**Claim 3: LLM-epistemics should be easier to study and optimize than human-epistemics.**  \n  \nThey can be:\n\n*   Reproducible (models don’t change over time)\n*   Versioned (if updates are made to an LLM, this new LLM can be separately studied)\n*   Pure (similar inputs, with seeds, produce the same outputs)\n*   Scalable (much easier for 10 researchers to study one LLM in parallel, than studying one human. Can also reuse the same exact LLM in many places, or researchers could run 500 LLM runs in parallel.)\n*   Investigatable (People have privacy concerns. LLMs don’t, so we don’t need to be cautious about making them feel bad with public evaluations)\n*   Modifiable (We can’t freely change parts of most people’s knowledge or abilities, but we can with LLMs.\n    *   We can experiment with a large number of isolated changes.\n    *   We can’t wipe a person’s memory in order to backtest a human-based epistemic process, but we (partially) do this for LLMs, by providing them access to specific data sets.\n    *   Related point: One challenge to note though is that the training data is particularly optimized for immediate times after the LLM was trained. We should probably only test LLMs in forecasting scenarios after they have been trained, and assume that their quality will degrade for forecasts they make far out from their training time. That said, this still is overall far more standardizable than with humans, which are typically our main alternative.   \n          \n        Add to that, that LLMs are often cheaper and better than many humans, and overall, these systems are hard to ignore.\n*   Controlled/limited. For example, if a question about \"Will person X be assassinated?\" were posted to a liquid prediction market, it could encourage someone to actually attack that person. (See: [assassination markets](https://en.wikipedia.org/wiki/Assassination_market)). However, if someone were to query their own AI forecasting system that question, they would hopefully have strong assurances that the AI won't be taking actions in the world. (Obviously, if the AI is not controlled, this could go poorly, but that is an extreme scenario.)\n\nEvidence:\n\n*   LLM R&D has dramatically risen recently and has resulted in major gains (>$2T) to company valuations. Human R&D around epistemics and reasoning has had nothing like this recently.\n*   We have far more studies and data about LLM performance than we do about important scientists, intellectuals, or decision-makers. There are of course some human studies (a la the Good Judgement Project), but these are expensive and thus few and very narrow.\n\nSubclaim: Even if we just wanted to understand ideal human epistemics better, one reasonable method might be to make strong LLM systems and study them, because these are much easier to study, and will likely lead to some generalizable findings.\n\n**Claim 4: LLM epistemic integration will be gradual.**  \nPractically speaking LLMs and Humans will work together side-by-side in epistemic processes for some time. I expect LLMs-epistemics to follow a similar trajectory to self-driving. First, it starts with light augmentation. Then it moves to “fully automated in most use cases, but occasionally needs oversight.” As with autonomous driving, Level 5 is very difficult.\n\n*   Subpoint: Don’t ask “_When will LLMs be better than humans at \\[forecasting|writing|research\\],_” Ask questions more like, “_what percentage of work done in \\[forecasting|writing|research\\] be automated_”, or, “_what ratio of Manifold Karma will be won by bots vs. humans vs. hybrids, over time?_” I’d expect to see gradual gains here.\n\nSubclaim: I expect that LLM-based forecasters will gradually dominate open platforms like Manifold and Metaculus. As this happens, it might scare off many of the human forecasters. If this is not handled gracefully, it could lead to these platforms feeling like ghost towns and becoming irrelevant, in favor of people using more direct AI systems.\n\n## 4\\. Scaffolding\n\nPrevious LLM-based forecasting systems have included some amounts of scaffolding.\n\n*   [FutureSearch.ai](http://futuresearch.ai)\n*   [Approaching Human-Level Forecasting with Language Models](https://arxiv.org/abs/2402.18563)\n\n  \n**Claim 1: LLM-based epistemic systems will likely require a lot of scaffolding, especially as they get ambitious**.  \nI.E. software, databases, custom web applications, custom batch job pipelines, etc. \n\n  \n**Claim 2: Scaffolding-heavy systems have significant downsides.**\n\n*   Difficult to study. Unlike single LLMs, scaffolded systems can feature:\n    *   High costs of deployment.\n    *   No/little ability to roll-back.\n    *   Frequent changes.\n    *   Internal substructures that are highly variable between different implementations.\n    *   External-facing APIs that are very different between different implementations.\n    *   Evidence: The previously mentioned LLM-based systems have some of these failures - and these are simple systems compared to what we might have later.\n*   Subclaim: If scaffolding is to be expected / needed, it could be important to plan for research measures that take this into account. This will require specialized attention.\n*   Subclaim: It seems important for there to be some “portable/packaged” LEPs, that could easily be shared, versioned, deployed, etc. This would be useful for research and benchmarks/baselines.\n\n**Claim 3: Scaffolding-heavy systems are likely to be centralized.**\n\n*   In general, complex web projects are centralized.\n*   This means that scaffolding-heavy setups will favor startups and in-house teams.\n*   This means that it's easy for many of these systems to be useless. Users might only interact with 1-2 systems, so any smaller competitors might get totally ignored, even if they have a few unusually great features.\n*   It’s possible that there could be some clean lines of separation between different components. Some lines seem simple to try out, but experimentation and thinking is needed to develop a better idea. Thinking/planning here seems good.\n    *   The more it’s split up into understood components, the easier it will be to research, optimize, and proliferate each part.\n\n**Claim 4: Scaffolding-heavy systems are expensive, and thus difficult to incentivize people to build.**\n\n*   Most forecasting competitions now pay out fairly small amounts. This makes it difficult to incentivize scaffolding.\n*   I don’t expect people to independently create great scaffolding in order to win <$50k.\n    *   My impression is that very little forecasting on Metaculus and Manifold is automated with LLMs. (There is one recent Metaculus competition here, but this is an isolated and forced use.)\n*   It’s also difficult to share and maintain scaffolding. You need to make custom web UIs, you might have to set up billing infrastructure for payments, etc. In a very large forecasting ecosystem with a lot of money, forecasters would subscribe to scaffolding systems, in the same way the professional traders subscribe to trading software and services.\n*   This means that we might well see slower progress here / less diversity.   \n     \n\n**Claim 5: New technologies or startups could greatly change and improve scaffolding.**\n\nRight now there are some startups and open-source frameworks to help with general-purpose scaffolding, but these are [known to be lacking](https://news.ycombinator.com/item?id=40739982). It's possible that much better systems will be developed in the future, especially because there's a lot of money in the space.\n\nIf this happens, that could hypothetically make powerful LLM epistemic tools dramatically easier to create.\n\nOne annoying challenge is that change in the field could lessen the importance of early experimentation. It could be the case that early projects focus heavily on some specific scaffolding frameworks, only for other frameworks to become more powerful later.\n\nA possible successful world could be one where different teams make \"[GPTs](https://chatgpt.com/gpts)\" that can integrate nicely with other workflows and interact with each other. Ideally these wouldn't be restricted to any one platform.  \n  \n**Crux: Scaffolding vs. Direct LLM Improvements** \n\nOne risk around scaffolding is that a lot of key functionality that scaffolding would be used for might be implemented directly in the inference of LLMs. For example, long-term planning could be done simply with scaffolding, but AI developers are also iterating on implementing it directly into the LLM layer. \n\nAnother example is that of editing text. If you have a long text document and want to use an LLM to make a few edits, it would currently attempt to rewrite the entire document, at a fairly slow speed. This could be improved with scaffolding by instead asking it to write a git diff and then applying that using traditional code. Or it could be solved on the inference layer directly, [as discussed here](https://www.cursor.com/blog/instant-apply). \n\nIf it is the case that most future improvements are done directly with the LLMs, then that could significantly change what work will be valuable. For example, if there are only a few leading LLM developers, then it might ultimately be up to them to actually add any functionality.  \n\n## 5\\. Subcomponents\n\nAs stated above, it’s hard to determine the best subdivisions of LEPs / LLM-Epistemic Scaffolding.\n\nHere’s one attempt, to provide a simple picture. \n\n**1\\. Data-collection**\n\n*   Many LLM services are already coming out that can help quickly put data, including private data, together, for different needs. See Apple Intelligence as one service that would work for people’s personal data.\n*   There are hundreds of startups in the data-collecting and data-management space. This problem can easily be split up more.\n*   Example performance question:  \n    _“Provided with resources R, gather as much data as you can, from this list of desired data, about X.”_  \n    The results of this will likely be obvious (it’s likely to be clear what data is accessible), so scientific testing might not be needed.   \n     \n\n**2\\. World Model**\n\n*   “World model” here likely means, “A set of stored symbolic and probabilistic assumptions about the world / people / organizations.”\n*   World models will likely include software programs and writing, instead of only relying on existing LLMs.\n    *   I expect that there could be workflows to LEPs to contribute to world models that will later give it better forecasting / reasoning abilities. The forecasting components could build the world models.\n*   Existing online data has very minimal attempts at providing accessible world models. These would ideally be probabilistic, immutable, shareable, collaborative, etc.\n*   Tools like [Squiggle Hub](https://squigglehub.org) are meant to be used here. Formal ontologies and knowledge graphs are likely to be useful.\n*   Some world models could be directly interesting to humans, for the presentation layer. Others might primarily be used by LLMs. For these, the world model is a form of a cache / collaboration.\n*   It seems possible/likely that world models won’t be unique to LEPs. Instead, many separate LEPs can use the same world models.\n*   World models could contain (or complete be) text descriptions of outcomes. For example, 10 compressed pages explaining how the next 10 years will go for a specific region.\n    *   It’s not clear yet what the best representations are for LLMs to store and fetch data about its understanding of specific things.\n*   Example performance question:  \n    “_Given a forecasting LLM L, and forecasting question Q, contribute a world model that will result in L being highly accurate_.”  \n     \n\n**3\\. Human Elicitation**\n\n*   It’s often useful to just go out and directly ask questions to people, or give them surveys. Strong LEPs should ideally be able to design these procedures to maximize information value, then execute them autonomously.\n    *   A system might realize that it would be useful to get a very specific expert to give their take on something. It could then email them, and perhaps offer a financial reward.\n    *   A different system might generate a bunch of specific binary questions for a group of people it wants to elicit information from. Maybe it’s trying to estimate aspects of their preferences, and information would help.\n*   Example performance question:  \n    _“Given that you want to get as much useful new data about topic X, using existing research Y, use resources R to send off elicitions and gather data from humans.”_\n\n**4\\. Data Prioritization**\n\n*   The task here is, “_Given the potential to fetch from data source D, and resources R, fetch the most useful data for providing marginal insight on unknown variables V._”\n*   Summarizing and displaying such work could also be useful. But doing so for humans would be very different to doing so for other LLMs.\n*   It seems like with LLMs that are capable of providing quick and accurate forecasts, we could use reinforcement learning and similar algorithms to do a good job here. For any set of data, we could quickly measure the improvement in accuracy using that data.\n\n**5\\. Numeric Modeling / Fermi Calculations**\n\n*   Make great complex Python/Excel/Squiggle models/estimates.\n*   One reasonable approach to fermi estimation is “Try a bunch of very different approaches, then mix the answers together.”\n*   We’ve seen a lot of evidence that LLMs can generate working Python/Squiggle code. However, I don’t believe that there has been much optimization in this area yet.\n*   It seems easy to iterate on, and generate, great data of strong examples of this. That could be then used for LLM training / fine-tuning.\n*   Example performance question:  \n    _“Estimate variable V as well as you can, using up to N LLM calls, and no other calls.”_  \n     \n\n**6\\. Decomposition / Amplification**\n\nThere are many potential strategies to \"break a problem down\" and solve different parts with different threads of LLMs. \n\nFor example:\n\n*   Chain-of-Thought prompting\n*   Mixture of experts\n*   Ensemble Averaging\n*   Debate (i.e. [Safety via Debate](https://arxiv.org/abs/1805.00899))\n*   Fact verification\n\nArguably, numeric modeling is a form of decomposition/amplification, I just separated it to draw attention to its uniqueness.\n\nThis is an active and popular area of ML research. Some of these techniques are incorporated in the LLM inference layer, but some are also used in scaffolding.\n\nI'm sure there's a lot more work to do here. It's less clear where smaller nonprofit research can be most effective, but I suspect that there are some good options. \n\nMany methods might be unique to forecasting or specific epistemic workflows. For example:\n\n*   Try to identify key cruxes or crucial considerations, and then investigate them\n*   "
          }
        },
        {
          "title": "Optimism Bias in Benefits Assessment",
          "description": "The author demonstrates optimism bias by presenting a series of claims about the benefits of better forecasting systems with limited critical examination. While these benefits are possible, the confidence with which they're stated and the lack of rigorous evidence or counterarguments suggests an overly optimistic view that could lead to underestimating challenges and overestimating positive impacts.",
          "importance": "85",
          "grade": "40",
          "highlight": {
            "startOffset": 2398,
            "endOffset": 3691,
            "quotedText": "Claim 1: If humans could forecast much better, these humans should make few foreseeable mistakes. This covers many mistakes, particularly ones we might be worried about now.**\n\n*   Someone deciding about talking to a chatbot that can be predicted to be net-negative (perhaps it would create an unhealthy relationship) could see this forecast and simply decide not to start the chat.\n*   Say that a person’s epistemic state could follow one of four trajectories, depending on some set of reading materials. For example, one set is conspiratorial, one is religious, etc. Good forecasting could help forecast this and inform the person ahead of time.\n    *   Note that this can be radical and perhaps dangerous. For example, a religious family knowing how to keep their children religious with a great deal of certainty.\n*   Say that one of two political candidates is predictably terrible. This could be made clear to voters who trust said prediction.\n*   If an AI actor is doing something likely to be monopolistic or dangerous, this would be made more obvious to itself and those around it.\n*   Note: There will also be unforeseeable mistakes, but any actions that we could do that are foreseeably-high-value for them, could be predicted. For example, general-purpose risk mitigation measures."
          }
        },
        {
          "title": "Illusion of Control in System Development",
          "description": "The document exhibits the illusion of control by suggesting that we can effectively direct how LEPs will develop and be deployed. This underestimates the complex, emergent nature of technological development and the many competing interests that will shape these systems. The author acknowledges some challenges but maintains an implicit belief that we can guide development in beneficial directions.",
          "importance": "80",
          "grade": "30",
          "highlight": {
            "startOffset": 4499,
            "endOffset": 4897,
            "quotedText": "Claim 3: The harm caused by altruistic development of epistemic improvements could be capped by a self-enforcing loop.**\n\n*   Agents developing epistemics should use the tools they build to decide on the net value of further improvements. This is a very important variable to get right.\n*   If it ever seems like future work will be negative in expected value, progress could and should be stopped."
          }
        },
        {
          "title": "Narrative Fallacy in System Integration",
          "description": "The author constructs a compelling narrative about how LEPs will be integrated into society and existing systems, which may oversimplify the complex, non-linear nature of technological adoption. The narrative presents a relatively smooth progression that doesn't fully account for potential resistance, regulatory challenges, or competing technological approaches.",
          "importance": "80",
          "grade": "45",
          "highlight": {
            "startOffset": 5391,
            "endOffset": 6659,
            "quotedText": "en, and the expected value of innovation would increase.\n\n**Claim 5: AI forecasting/epistemics will be much more deeply integrated into the world than current prediction markets.**\n\n*   First, note that current prediction markets / tournaments are highly niche and overlooked. So AI systems would have to be more accepted, to be very valuable.\n*   Unlike current “human forecasters”, AI forecasting can be embedded deeply into other software. There would likely be better AI forecasting for repeated decisions, like devops procedures, edge cases in insurance, AI agents, etc.\n*   If “AI forecasters” were really solid and cheap, we’d expect them to improve many complex software workflows. Existing “human forecasters” are hard to directly make much money from, but these AI systems would be much easier to do so.\n*   They would likely be integrated in many other systems, such as:\n    *   Overseeing product sales chances\n    *   Estimating changes in accounting, from one quarter to the next\n    *   Estimating project lengths and budgets\n    *   Deciding the best LLM to use for a certain AI workflow, or the best workflow to follow.\n*   Note that many developments in \"_AI epistemics_\" are just regular enhancements to LLMs. There's no clear line between \"_AI fore"
          }
        },
        {
          "title": "Confirmation Bias in Failure Mode Analysis",
          "description": "The section on failure modes shows confirmation bias by presenting potential problems but then quickly dismissing or minimizing most of them. The author tends to acknowledge risks but then immediately provides reasons why they believe these risks won't materialize or will be outweighed by benefits, without deeply engaging with counterarguments or evidence that might challenge their position.",
          "importance": "90",
          "grade": "35",
          "highlight": {
            "startOffset": 8898,
            "endOffset": 9993,
            "quotedText": "Claim: I think this is unlikely to be the net outcome. This would require these organizations to become smarter about making AIs, but not much smarter about making them safe, including for their own benefit. It would also require the other groups that could block them to become smarter - some of these groups include other non-LLM companies and other highly motivated actors who wouldn't want AI to go poorly.  \n          \n        One crux here is if the AI developers will both be much better at integrating AI epistemic improvements in their internal workflows, and if these systems won't convince or help them with AI alignment. This could be a really bad outcome, so it's important to continue to pay attention to.\n    2.  **Malicious Governments**  \n        Governments like China or North Korea could use better epistemic systems to better control their populations. My guess is that great epistemic systems will likely do some damage here. I think that overall the improvements (like improving western governments) will more than outweigh that, but this is something to pay attention to."
          }
        },
        {
          "title": "Availability Heuristic in Capability Assessment",
          "description": "The author relies heavily on the availability heuristic when assessing LLM capabilities, drawing primarily on recent advances to project future abilities. This may lead to overestimating how quickly and effectively these systems will develop, as recent successes are more mentally available than historical challenges and limitations in AI development.",
          "importance": "85",
          "grade": "45",
          "highlight": {
            "startOffset": 11926,
            "endOffset": 12403,
            "quotedText": "ed-upon and popular framings and terminology, which could contribute to it be neglected. I think more terminological and clarification work here could be valuable. This is similar to the idea of [Distillation and Research Debt.](https://distill.pub/2017/research-debt/) \n\n## 3\\. Viability\n\n**Claim 1: All/Almost all parts of a conventional epistemic process can be automated with LLMs.**\n\nLLMs can:\n\n1.  Generate forecasting questions\n2.  Resolve forecasting questions\n3.  Gath"
          }
        },
        {
          "title": "Selection Bias in Evidence Presentation",
          "description": "The document demonstrates selection bias by primarily presenting evidence and examples that support the author's thesis about the potential of LEPs, with less attention to contradictory evidence or failed AI forecasting attempts. This selective presentation of evidence creates an impression of greater consensus and certainty than may actually exist in the field.",
          "importance": "85",
          "grade": "40",
          "highlight": {
            "startOffset": 15425,
            "endOffset": 16112,
            "quotedText": "tack that person. (See: [assassination markets](https://en.wikipedia.org/wiki/Assassination_market)). However, if someone were to query their own AI forecasting system that question, they would hopefully have strong assurances that the AI won't be taking actions in the world. (Obviously, if the AI is not controlled, this could go poorly, but that is an extreme scenario.)\n\nEvidence:\n\n*   LLM R&D has dramatically risen recently and has resulted in major gains (>$2T) to company valuations. Human R&D around epistemics and reasoning has had nothing like this recently.\n*   We have far more studies and data about LLM performance than we do about important scientists, intellectuals, or "
          }
        },
        {
          "title": "Planning Fallacy in Implementation Timeline",
          "description": "The document shows signs of the planning fallacy by underestimating the complexity and time required to develop effective LEP systems. While the author acknowledges that scaffolding is difficult, there's still an implicit assumption that these systems can be developed relatively straightforwardly, which doesn't fully account for the historical challenges in developing complex AI systems.",
          "importance": "75",
          "grade": "60",
          "highlight": {
            "startOffset": 16120,
            "endOffset": 16871,
            "quotedText": "-makers. There are of course some human studies (a la the Good Judgement Project), but these are expensive and thus few and very narrow.\n\nSubclaim: Even if we just wanted to understand ideal human epistemics better, one reasonable method might be to make strong LLM systems and study them, because these are much easier to study, and will likely lead to some generalizable findings.\n\n**Claim 4: LLM epistemic integration will be gradual.**  \nPractically speaking LLMs and Humans will work together side-by-side in epistemic processes for some time. I expect LLMs-epistemics to follow a similar trajectory to self-driving. First, it starts with light augmentation. Then it moves to “fully automated in most use cases, but occasionally needs oversight.”"
          }
        },
        {
          "title": "Balanced Acknowledgment of Scaffolding Challenges",
          "description": "This section shows a more balanced approach by acknowledging the significant challenges in developing scaffolding for LEP systems. The author recognizes the complexity, expense, and centralization issues that could limit development, which demonstrates a more nuanced understanding of the practical challenges involved. This helps counterbalance some of the optimism elsewhere in the document.",
          "importance": "80",
          "grade": "75",
          "highlight": {
            "startOffset": 17660,
            "endOffset": 18572,
            "quotedText": "ecasting systems have included some amounts of scaffolding.\n\n*   [FutureSearch.ai](http://futuresearch.ai)\n*   [Approaching Human-Level Forecasting with Language Models](https://arxiv.org/abs/2402.18563)\n\n  \n**Claim 1: LLM-based epistemic systems will likely require a lot of scaffolding, especially as they get ambitious**.  \nI.E. software, databases, custom web applications, custom batch job pipelines, etc. \n\n  \n**Claim 2: Scaffolding-heavy systems have significant downsides.**\n\n*   Difficult to study. Unlike single LLMs, scaffolded systems can feature:\n    *   High costs of deployment.\n    *   No/little ability to roll-back.\n    *   Frequent changes.\n    *   Internal substructures that are highly variable between different implementations.\n    *   External-facing APIs that are very different between different implementations.\n    *   Evidence: The previously mentioned LLM-based systems have some of"
          }
        },
        {
          "title": "Scope Insensitivity in Problem Complexity",
          "description": "The document exhibits scope insensitivity by treating various forecasting domains and challenges as relatively similar in difficulty. The broad vision presented doesn't adequately account for the vastly different scales of challenges in different domains of forecasting and decision-making, from simple numeric predictions to complex geopolitical or technological forecasts.",
          "importance": "75",
          "grade": "45",
          "highlight": {
            "startOffset": 22399,
            "endOffset": 35067,
            "quotedText": "ding by instead asking it to write a git diff and then applying that using traditional code. Or it could be solved on the inference layer directly, [as discussed here](https://www.cursor.com/blog/instant-apply). \n\nIf it is the case that most future improvements are done directly with the LLMs, then that could significantly change what work will be valuable. For example, if there are only a few leading LLM developers, then it might ultimately be up to them to actually add any functionality.  \n\n## 5\\. Subcomponents\n\nAs stated above, it’s hard to determine the best subdivisions of LEPs / LLM-Epistemic Scaffolding.\n\nHere’s one attempt, to provide a simple picture. \n\n**1\\. Data-collection**\n\n*   Many LLM services are already coming out that can help quickly put data, including private data, together, for different needs. See Apple Intelligence as one service that would work for people’s personal data.\n*   There are hundreds of startups in the data-collecting and data-management space. This problem can easily be split up more.\n*   Example performance question:  \n    _“Provided with resources R, gather as much data as you can, from this list of desired data, about X.”_  \n    The results of this will likely be obvious (it’s likely to be clear what data is accessible), so scientific testing might not be needed.   \n     \n\n**2\\. World Model**\n\n*   “World model” here likely means, “A set of stored symbolic and probabilistic assumptions about the world / people / organizations.”\n*   World models will likely include software programs and writing, instead of only relying on existing LLMs.\n    *   I expect that there could be workflows to LEPs to contribute to world models that will later give it better forecasting / reasoning abilities. The forecasting components could build the world models.\n*   Existing online data has very minimal attempts at providing accessible world models. These would ideally be probabilistic, immutable, shareable, collaborative, etc.\n*   Tools like [Squiggle Hub](https://squigglehub.org) are meant to be used here. Formal ontologies and knowledge graphs are likely to be useful.\n*   Some world models could be directly interesting to humans, for the presentation layer. Others might primarily be used by LLMs. For these, the world model is a form of a cache / collaboration.\n*   It seems possible/likely that world models won’t be unique to LEPs. Instead, many separate LEPs can use the same world models.\n*   World models could contain (or complete be) text descriptions of outcomes. For example, 10 compressed pages explaining how the next 10 years will go for a specific region.\n    *   It’s not clear yet what the best representations are for LLMs to store and fetch data about its understanding of specific things.\n*   Example performance question:  \n    “_Given a forecasting LLM L, and forecasting question Q, contribute a world model that will result in L being highly accurate_.”  \n     \n\n**3\\. Human Elicitation**\n\n*   It’s often useful to just go out and directly ask questions to people, or give them surveys. Strong LEPs should ideally be able to design these procedures to maximize information value, then execute them autonomously.\n    *   A system might realize that it would be useful to get a very specific expert to give their take on something. It could then email them, and perhaps offer a financial reward.\n    *   A different system might generate a bunch of specific binary questions for a group of people it wants to elicit information from. Maybe it’s trying to estimate aspects of their preferences, and information would help.\n*   Example performance question:  \n    _“Given that you want to get as much useful new data about topic X, using existing research Y, use resources R to send off elicitions and gather data from humans.”_\n\n**4\\. Data Prioritization**\n\n*   The task here is, “_Given the potential to fetch from data source D, and resources R, fetch the most useful data for providing marginal insight on unknown variables V._”\n*   Summarizing and displaying such work could also be useful. But doing so for humans would be very different to doing so for other LLMs.\n*   It seems like with LLMs that are capable of providing quick and accurate forecasts, we could use reinforcement learning and similar algorithms to do a good job here. For any set of data, we could quickly measure the improvement in accuracy using that data.\n\n**5\\. Numeric Modeling / Fermi Calculations**\n\n*   Make great complex Python/Excel/Squiggle models/estimates.\n*   One reasonable approach to fermi estimation is “Try a bunch of very different approaches, then mix the answers together.”\n*   We’ve seen a lot of evidence that LLMs can generate working Python/Squiggle code. However, I don’t believe that there has been much optimization in this area yet.\n*   It seems easy to iterate on, and generate, great data of strong examples of this. That could be then used for LLM training / fine-tuning.\n*   Example performance question:  \n    _“Estimate variable V as well as you can, using up to N LLM calls, and no other calls.”_  \n     \n\n**6\\. Decomposition / Amplification**\n\nThere are many potential strategies to \"break a problem down\" and solve different parts with different threads of LLMs. \n\nFor example:\n\n*   Chain-of-Thought prompting\n*   Mixture of experts\n*   Ensemble Averaging\n*   Debate (i.e. [Safety via Debate](https://arxiv.org/abs/1805.00899))\n*   Fact verification\n\nArguably, numeric modeling is a form of decomposition/amplification, I just separated it to draw attention to its uniqueness.\n\nThis is an active and popular area of ML research. Some of these techniques are incorporated in the LLM inference layer, but some are also used in scaffolding.\n\nI'm sure there's a lot more work to do here. It's less clear where smaller nonprofit research can be most effective, but I suspect that there are some good options. \n\nMany methods might be unique to forecasting or specific epistemic workflows. For example:\n\n*   Try to identify key cruxes or crucial considerations, and then investigate them\n*   Automated debate mapping\n*   Identify [Scissor statements](https://slatestarcodex.com/2018/10/30/sort-by-controversial/), in part to help personalize research\n*   Use [discernment](https://www.lesswrong.com/posts/W2iwHXF9iBg4kmyq6/the-practice-and-virtue-of-discernment) to clarify and decompose key terminology\n*   Breaking down forecasting questions into sub-questions, perhaps with bayesian networks. \n\nArguably, the rationality and effective altruist communities might be very well suited to work in this area. There's been a lot of writing on how to systematically break hard problems down, now people would need to experiment with encoding these systems into automated workflows.   \n  \n**7\\. Question Generation** \n\n*   Come up with questions to be researched or forecasted on.\n*   This might well be done by other listed components - for example, it could simply combine \"Idea Generation\" with \"Forecasting\" to come up with potential questions, and guess which will be the most important and tractable. It also might be better done under the \"Presentation\" question. However, I suspect that there will be unique research and development for this problem, especially because it's a major part of existing forecasting systems.\n*   Example performance question:  \n    _“Write 5 forecasting questions on topic X. After this, a separate epistemic system will be run with a fixed number of resources R on each question. Choose questions such that the final results are maximally useful to end-users.\"_  \n     \n\n  \n**8\\. Forecasting**\n\n*   Convert fetched data into a forecast.\n*   Can call the “Numeric Modeling” module, or be merged with it.\n*   There’s been previous work here that is promising.\n*   Fine-tuning and prompt engineering seems like it would go a long way.\n*   Current LLMs are known to be biased and often suffer from poor calibration, in part in order to become better liked by human users. We might well want specialized systems optimized for pure accuracy.\n    *   This delta might cause problems. Perhaps it could lead to bad news stories (where a model used for forecasting gives a politically-incorrect answer), or perhaps it could be used by bad actors.\n    *   This area should be thus treated with care. Perhaps some models are only available to certain researchers, or perhaps there should be large disclaimers provided for regular people attempting to use them directly.\n*   Forecasting can output high-level stories, probabilities, scorable functions, or other. There are many potential formats here.\n*   Example performance question:  \n    _“Using information I, and N LLM calls, make your best forecast on variable V. You may use the Numeric Modeling module when doing so, but this will eat into the LLM calls.”_  \n     \n\n**9\\. Forecasting Resolution**\n\n*   Most forecasting questions should be resolvable automatically. This can be done by either querying external data, or perhaps by asking a person/LLM for their opinion.\n*   This component is likely to often just either be, or defer to, other LEP modules.\n*   Question Creation and Optimization\n    *   A strong LEP would make good judgment calls on what research or forecasting topics would be valuable to people, and initialize those.\n    *   This could range from “once a user logs in, generates custom information for them to maximize their EV”, to, “write a list of standard forecasting questions that are collectively high-EV to a subpopulation.”\n    *   Example performance question:  \n        _“Using an existing LEP L, and resources R, write a set of forecasting questions. Expect that forecasting resources F will be used on these questions, and you are trying to optimize utility function U.”_  \n         \n\n**10\\. Presentation**\n\n*   Many custom charts and graphics would likely be useful.\n*   Decisions need to be made about what final data should be shown to users and how.\n*   Automated decision-making: In some cases / eventually, LEPs would interact directly with decision systems. In theory, people don’t need to know almost anything - critical information could be used directly by in-world automations. For example, instead of an LEP doing research into the costs and benefits of vitamin D supplementation and showing you that research, a system instead just starts an order for you to get a regular set of supplements. This could come with optional research, but in many cases the research won’t be reviewed, if the system is trusted.\n*   In cases where LEPs are used behind APIs, the presentation layer could include answering questions about what the LEP could do, and suggesting specific uses for it.\n*   Presentation/wording should be optimized to be understandable to humans. There should be tests of the error rates of what was intended to be stated, and what humans understood. For example, many people have significant misunderstandings about basic statistics concepts, so probabilities might need to be handled with care.\n*   Example performance questions:\n    *   _“Using LEP output O, design a dashboard that user U will most prefer.”_\n    *   _“Using LEP output O, design a set of complicated diagrams without detectable errors.” (judged by an LLM)._   \n         \n\n**11\\. Distribution**\n\n*   Full custom user interfaces could be created. This might range from a simple custom LLM chat, to a auto-generated set of dashboards, with the ability to delve deeper into any topic the user clicks on.\n*   LEPs could be accessed by any other chat LLM interface, as long as that interface can call external APIs. This could be generally useful for integrating into other AI tools and workflows, perhaps via other common LLM chat UIs.\n*   LEP information could be distributed like how human communication is communicated. Text messages, emails, Tweets, voice assistants, etc. Ideally the LEP could vaguely predict what information is useful to people and when, and then send them that information in those cases.\n    *   Example: You get an email in the morning. It says, _“Some news came out at 6am today that changed my recommendation of your financial assets. I recommend you change to a different mutual fund. I estimate that doing so will net you an average of $50/month.”_\n*   Example performance questions:\n    *   _“Using LEP output O, design and implement the best way to get this information to agent A.” We can use an LLM that mimics human behaviors to evaluate._  \n         \n\n**12\\. Idea Generation / Optimization**\n\n*   It will often be useful to generate ideas on the best ways to do something.\n*   For example, an LLM agent might want to make a suggestion to a user, befor"
          }
        }
      ],
      "grade": 45
    },
    {
      "agentId": "clarity-coach",
      "costInCents": 14934,
      "createdAt": "2025-04-16T08:53:21.328Z",
      "runDetails": "{\"model\":\"anthropic/claude-3.7-sonnet\",\"promptTokens\":11854,\"completionTokens\":3080,\"totalTokens\":14934,\"temperature\":0.1,\"runtimeMs\":65295}",
      "thinking": "I'm analyzing a document about LLM-based epistemic systems (LEPs) - systems that use large language models to improve forecasting, reasoning, and knowledge work. The document is structured as a brainstorm with sections covering benefits, failure modes, viability, scaffolding needs, system components, and benchmarking considerations.\n\nFrom a clarity perspective, I need to evaluate:\n\n1. **Readability**: The document uses a mix of sentence lengths, with some very long, complex sentences alongside shorter ones. There's significant technical jargon throughout (\"epistemic process\", \"scaffolding\", \"LEP\", etc.) which may create barriers for readers unfamiliar with the domain. The author acknowledges the document is in \"rough brainstorm\" format, which explains some of the terseness. Paragraph structure varies, with some well-chunked sections and others containing dense blocks of text.\n\n2. **Structural Coherence**: The document has a clear organizational structure with numbered sections and subsections. The author uses bullet points extensively, which helps break up information. However, the logical flow sometimes jumps between ideas without strong transitions. The document contains many claims without always providing sufficient evidence or explanation, though this is acknowledged as being a brainstorm.\n\n3. **Precision and Concision**: There's some redundancy across sections, with similar ideas appearing in multiple places. The language is generally specific, though some terms like \"epistemic process\" could benefit from more concrete examples early on. The document uses qualifiers like \"likely\", \"possibly\", and \"might\" frequently, which is appropriate given the speculative nature but sometimes creates hedging.\n\n4. **Audience Alignment**: The document assumes significant prior knowledge about AI, forecasting, and epistemic systems. The author acknowledges this by suggesting readers can use an LLM to rewrite it in a preferred format. The content appears targeted at an audience familiar with effective altruism, rationality concepts, and AI development.\n\nOverall, this is a technical document with valuable content but several opportunities to improve clarity. The author acknowledges its preliminary nature, which explains some of the clarity issues. The use of headings, bullet points, and claims helps organize the information, but the density of technical concepts and sometimes complex sentence structures may create barriers for some readers.\n\nLet me identify specific sections to highlight in my comments:",
      "summary": "This document on LLM-based epistemic systems (LEPs) presents valuable content with a clear organizational structure but has several opportunities to improve clarity. The author effectively uses definitions, bullet points, and a claim-evidence format to organize complex ideas. However, the document contains dense technical jargon, some overly complex sentences, and assumes significant background knowledge. While the hierarchical structure and transparent acknowledgment of limitations are strengths, the document would benefit from more accessible explanations of specialized concepts, shorter sentences in places, and stronger transitions between ideas. The author's use of varied paragraph lengths and consistent formatting helps maintain readability despite the technical content. Overall, this is a thoughtful brainstorm that succeeds in organizing complex ideas but could be more accessible to readers without specialized background knowledge.",
      "comments": [
        {
          "title": "Transparency About Document Limitations",
          "description": "The author is admirably transparent about the document's preliminary nature and limitations. This sets appropriate expectations for readers and acknowledges that the ideas presented are works in progress rather than definitive statements.",
          "importance": "80",
          "grade": "90",
          "highlight": {
            "startOffset": 420,
            "endOffset": 1010,
            "quotedText": "This document presents a rough brainstorm on these topics. While I could dedicate months to refining these ideas, I've chosen to share these preliminary notes now to spark discussion. If you find the style too terse, feel free to use an LLM to rewrite it in a format you prefer.\n\nI believe my vision for this area is more ambitious and far-reaching (i.e. not narrow to a certain kind of forecasting) than what I've observed in other discussions. I'm particularly excited about AI-heavy epistemic improvements, which I believe have greater potential than traditional forecasting innovations."
          }
        },
        {
          "title": "Definition Clarity",
          "description": "The document begins with clear definitions of key terms, which is excellent for establishing shared understanding. The definitions are concise, specific, and set up the conceptual framework for the rest of the document.",
          "importance": "95",
          "grade": "90",
          "highlight": {
            "startOffset": 1627,
            "endOffset": 2346,
            "quotedText": "l-E 3\n\n## **Key Definitions**:\n\n*   **Epistemic process:** A set of procedures to do analysis work, often about topics with a lot of uncertainty. This could be “_have one journalist do everything themselves_”, to a complex (but repeatable) ecosystem of multiple humans and software systems.\n*   **LLM-based Epistemic Process (LEP)**: A system that relies on LLMs to carry out most or all of an epistemic process. This might start at ~10% LLM-labor, but can gradually ramp up. I imagine that such a process is likely to feature some kinds of estimates or forecasts.\n*   **Scaffolding:** Software used around an LLM system, often to make it valuable for specific use-cases. In the case of an LEP, a lot of scaffolding mig"
          }
        },
        {
          "title": "Clear Section Structure",
          "description": "The document uses a consistent and clear hierarchical structure with numbered sections and subsections. This organizational approach helps readers navigate the content and understand how different ideas relate to each other.",
          "importance": "90",
          "grade": "95",
          "highlight": {
            "startOffset": 2346,
            "endOffset": 2379,
            "quotedText": "ht be needed.\n\n##  1. High-Level "
          }
        },
        {
          "title": "Sentence Complexity",
          "description": "This paragraph contains several long, complex sentences with multiple clauses and ideas. Breaking these into shorter sentences would improve readability and help readers process each point more effectively.",
          "importance": "80",
          "grade": "40",
          "highlight": {
            "startOffset": 8898,
            "endOffset": 9617,
            "quotedText": "Claim: I think this is unlikely to be the net outcome. This would require these organizations to become smarter about making AIs, but not much smarter about making them safe, including for their own benefit. It would also require the other groups that could block them to become smarter - some of these groups include other non-LLM companies and other highly motivated actors who wouldn't want AI to go poorly.  \n          \n        One crux here is if the AI developers will both be much better at integrating AI epistemic improvements in their internal workflows, and if these systems won't convince or help them with AI alignment. This could be a really bad outcome, so it's important to continue to pay attention to."
          }
        },
        {
          "title": "Effective Use of Bullet Points",
          "description": "The document makes excellent use of bullet points to break down complex ideas into digestible chunks. This improves readability significantly by creating visual separation between concepts and allowing readers to process information in smaller units.",
          "importance": "85",
          "grade": "95",
          "highlight": {
            "startOffset": 12315,
            "endOffset": 12601,
            "quotedText": "LLMs can:\n\n1.  Generate forecasting questions\n2.  Resolve forecasting questions\n3.  Gather data from computers\n4.  Write specific questions to expert - a la interviews\n5.  Make forecasts\n6.  Make visualizations and dashboards of forecasts\n7.  Personalize information for specific people"
          }
        },
        {
          "title": "Claim-Evidence Format",
          "description": "The document consistently uses a claim-evidence structure, clearly labeling claims and then providing supporting points. This approach helps readers distinguish between the author's assertions and their supporting reasoning, making the document's logic more transparent.",
          "importance": "85",
          "grade": "90",
          "highlight": {
            "startOffset": 13320,
            "endOffset": 14066,
            "quotedText": "the setup now is incredibly basic (it doesn’t even have my historic writing), and the tech is improving.\n*   At some point, we could make decent approximations of what many people believe on many topics. This could help us greatly speed up communication - AI systems would just need the pieces they can’t predict.   \n     \n\n**Claim 3: LLM-epistemics should be easier to study and optimize than human-epistemics.**  \n  \nThey can be:\n\n*   Reproducible (models don’t change over time)\n*   Versioned (if updates are made to an LLM, this new LLM can be separately studied)\n*   Pure (similar inputs, with seeds, produce the same outputs)\n*   Scalable (much easier for 10 researchers to study one LLM in parallel, than studying one human. Can also reuse"
          }
        },
        {
          "title": "Paragraph Length Variation",
          "description": "The document shows good variation in paragraph length, with some concise paragraphs interspersed with longer explanations. This creates a rhythm that helps maintain reader engagement and prevents the monotony that can come from uniform paragraph lengths.",
          "importance": "70",
          "grade": "85",
          "highlight": {
            "startOffset": 16112,
            "endOffset": 16871,
            "quotedText": "decision-makers. There are of course some human studies (a la the Good Judgement Project), but these are expensive and thus few and very narrow.\n\nSubclaim: Even if we just wanted to understand ideal human epistemics better, one reasonable method might be to make strong LLM systems and study them, because these are much easier to study, and will likely lead to some generalizable findings.\n\n**Claim 4: LLM epistemic integration will be gradual.**  \nPractically speaking LLMs and Humans will work together side-by-side in epistemic processes for some time. I expect LLMs-epistemics to follow a similar trajectory to self-driving. First, it starts with light augmentation. Then it moves to “fully automated in most use cases, but occasionally needs oversight.”"
          }
        },
        {
          "title": "Subcomponent Organization",
          "description": "The subcomponents section effectively breaks down a complex system into discrete, understandable parts. Each component is clearly defined with its purpose and relationship to other components, helping readers grasp the overall architecture of LEPs.",
          "importance": "85",
          "grade": "85",
          "highlight": {
            "startOffset": 22378,
            "endOffset": 22547,
            "quotedText": "improved with scaffolding by instead asking it to write a git diff and then applying that using traditional code. Or it could be solved on the inference layer directly, "
          }
        },
        {
          "title": "Technical Jargon Density",
          "description": "This section contains a high density of technical terms and concepts that may be unfamiliar to readers without background in AI, forecasting, or rationality. While the author acknowledges the document's specialized nature, providing more explanatory context for terms like \"credal resilience\" would improve accessibility.",
          "importance": "75",
          "grade": "45",
          "highlight": {
            "startOffset": 35097,
            "endOffset": 35790,
            "quotedText": "ggestion.\n*   Regular LLM performance might be fine on this, but fine-tuning or similar might help. It might be the case that there are certain specific domains where idea generation is very useful.\n*   “Idea Generation” is related to [program optimization](https://en.wikipedia.org/wiki/Program_optimization) and other CS topics.\n*   Example performance questions:\n    *   _“Given a person with details D, generate a plan for that person to optimize their financial future.”_ (This will be then judged with an LEP / forecasting module)\n    *   _“Given a 2d grid game G, come up with 10 ordered ideas for the best strategy”_ (Can be judged empirically)  \n         \n\n**13\\. Task Orchestration**"
          }
        },
        {
          "title": "Conclusion and Future Research",
          "description": "The document concludes with a clear summary of future research directions, which helps readers understand the broader context and next steps. This provides closure while acknowledging the ongoing nature of the work.",
          "importance": "75",
          "grade": "85",
          "highlight": {
            "startOffset": 43465,
            "endOffset": 45420,
            "quotedText": "## Further Research\n\nI feel like there are clearly some larger investigative projects to do here. It's a vast and important topic, and at the same time there's a lot of fairly-obvious things (yet interesting and useful) we could say about it. This is mostly just putting a bunch of existing ideas in one place and providing some definitions and clarifications.\n\nSome future things I'd like to explore (or have others explore!) include:\n\n1.  **Exploration of benchmarks and optimization of end-to-end LEPs**  \n    This document includes more about specific subcomponents of LEPs than the full thing itself.\n2.  **Strategies for prioritizing philanthropic LEP work**  \n    I think this document helps list out a lot of potential LEP work, but it doesn't try to prioritize it. This would take some thinking. Maybe predictions can be used.\n3.  **Cost-effectiveness estimation**  \n    I think these systems are promising, but I'm not sure how to best estimate the cost-effectiveness of them vs. other effective altruist interventions. It would be useful to have better models of benefits and risks, perhaps with new delineations.\n4.  **Concrete project ideas for enthusiasts**  \n    I could imagine a lifehacker-like community forming around some of these principles. I think creative experimentation by enthusiasts could be useful, in part to help identify promising full-time talent.\n5.  **Moving key claims and cruxes to forecasting platforms and LLMs**  \n    Fitting with the theme, it seems interesting to try to pull out key questions here and post them to forecasting platforms. I tried to isolate claims and cruxes, in part so that they could later be easily adapted into somewhat-concrete questions. \n\nMy guess is that a lot of people will bounce off this broad topic. I imagine that there's much better writing that could be done to formalize and popularize the key concepts that will be useful going forward. I'd encourage others to make stabs here."
          }
        }
      ],
      "grade": 78
    },
    {
      "agentId": "research-scholar",
      "costInCents": 13964,
      "createdAt": "2025-04-16T08:54:04.182Z",
      "runDetails": "{\"model\":\"anthropic/claude-3.7-sonnet\",\"promptTokens\":12165,\"completionTokens\":1799,\"totalTokens\":13964,\"temperature\":0.1,\"runtimeMs\":38424}",
      "thinking": "This document presents a comprehensive exploration of LLM-based epistemic systems (LEPs) and their potential as effective altruist interventions. The author outlines a vision for AI-augmented epistemic processes that could dramatically improve forecasting, decision-making, and coordination. I need to analyze this through my lens as Research Scholar, focusing on identifying relevant academic and community resources that would enrich this discussion.\n\nThe document covers several key areas:\n1. High-level benefits and uses of improved epistemic systems\n2. Potential failure modes and risks\n3. Technical viability of LLM-based epistemic systems\n4. The role and challenges of scaffolding\n5. Subcomponents of LLM epistemic systems\n6. Benchmarks and baselines for evaluation\n7. Caching vs. on-demand forecasting considerations\n\nI should identify resources that address:\n- Existing work on LLM-based forecasting and reasoning\n- Research on human forecasting and prediction markets\n- Technical papers on LLM scaffolding and agent architectures\n- Discussions of epistemic improvement as an EA cause area\n- Work on AI alignment and safety relevant to epistemic systems\n- Evaluations of forecasting accuracy and calibration\n- Philosophical work on epistemics and decision theory\n\nFor each highlighted section, I'll need to connect specific references that directly address the claims or concepts mentioned. I should prioritize high-quality academic papers, but also include relevant blog posts, forum discussions, and other community resources, especially from the rationalist and EA communities that the author references.\n\nThe document mentions several existing systems and concepts that I should find references for, including:\n- FutureSearch.ai\n- Approaching Human-Level Forecasting with Language Models\n- Squiggle Hub\n- Safety via Debate\n- The Good Judgment Project\n- Credal resilience\n\nI'll also look for resources that address the author's claims about the potential benefits and risks of LEPs, as well as technical resources on the various subcomponents described.",
      "summary": "This document presents a comprehensive exploration of LLM-based epistemic systems (LEPs) as a promising EA intervention area. I've identified key academic papers, blog posts, and community resources that directly address the major themes, including LLM forecasting capabilities, scaffolding architectures, evaluation metrics, and potential risks. The most relevant resources include recent research demonstrating near-human-level forecasting with LLMs, work on agent architectures and scaffolding, and literature on prediction markets and human forecasting systems. These references provide crucial context for understanding both the technical feasibility and potential impact of LEPs. The document touches on important considerations around AI safety, knowledge representation, and evaluation methodologies that are well-addressed in existing literature. Resources from both academic sources and EA/rationalist communities would significantly enrich this discussion by providing empirical evidence, technical approaches, and philosophical frameworks for the proposed systems.",
      "comments": [
        {
          "title": "Prediction Markets and Human Forecasting",
          "description": "The document discusses how AI forecasting systems will differ from traditional human prediction markets and tournaments. There's substantial research on human forecasting systems like the Good Judgment Project that provides context for understanding these differences and potential improvements.",
          "importance": "90",
          "highlight": {
            "startOffset": 6755,
            "endOffset": 8024,
            "quotedText": "Claim 6: Even \"straightforward\" AI forecasting systems should look very different from human judgemental forecasting systems**\n\n*   I've found others seem to imagine AI forecasting systems as simply replacing humans in prediction tournaments and prediction markets. I think think this is perhaps a decent first model, but not at all how things would end up.\n*   Prediction markets / tournaments are very much optimized for competitive humans, in cases where humans need to be incentivized. AI systems have very different constraints, so will need very different systems.\n    *   Formal resolutions won't be necessary to incentivize AIs, after they are trained.\n    *   Forecasting can be done on-demand, as the AI is always available. So questions and forecasts might not have to be done continuously, before being read by end-users.\n    *   There's unlikely to be a need to have many AIs compete against each other, like in a traditional tournament, in most cases.\n    *   Modern prediction platforms don't use advanced inputs like [scorable functions](https://forum.effectivealtruism.org/posts/ZWQ5jbLxpd4JtQgu4/scorable-functions-a-format-for-algorithmic-forecasting). Arguably these would be too complex for many humans, but this is unlikely to be the case for AIs."
          }
        },
        {
          "title": "AI Alignment and Safety Considerations",
          "description": "The document touches on important AI alignment considerations, particularly in the failure modes section. There's significant research on AI safety that relates to these concerns, especially regarding potential acceleration of AI capabilities without corresponding safety improvements.",
          "importance": "85",
          "highlight": {
            "startOffset": 8638,
            "endOffset": 10818,
            "quotedText": "2.  **Forecasting could be used for net-bad activities**\n    1.  **AI Acceleration**  \n        It’s possible that epistemic improvements would be used by AI capabilities actors to increase capabilities and not make other informed choices.  \n          \n        Claim: I think this is unlikely to be the net outcome. This would require these organizations to become smarter about making AIs, but not much smarter about making them safe, including for their own benefit. It would also require the other groups that could block them to become smarter - some of these groups include other non-LLM companies and other highly motivated actors who wouldn't want AI to go poorly.  \n          \n        One crux here is if the AI developers will both be much better at integrating AI epistemic improvements in their internal workflows, and if these systems won't convince or help them with AI alignment. This could be a really bad outcome, so it's important to continue to pay attention to.\n    2.  **Malicious Governments**  \n        Governments like China or North Korea could use better epistemic systems to better control their populations. My guess is that great epistemic systems will likely do some damage here. I think that overall the improvements (like improving western governments) will more than outweigh that, but this is something to pay attention to.  \n          \n        Overall, I don’t see reasons to think that better epistemics will dramatically change existing power balances in ways that are yet predictable. I think this is true for most technologies I could imagine, even many of the most beneficial ones.\n    3.  **Malicious Corporations**   \n        There are some situations where companies could be far better at forecasting than consumers would be, leading to a large power imbalance. If companies have much more information about customers than users have about themselves or these companies, that could lead to them being taken advantage of. It seems important to use governments (hopefully with better epistemic infrastructure) to take active steps against this. It seems important here to work to make sure that epistemic advances get distributed equitably."
          }
        },
        {
          "title": "LLM Forecasting Research",
          "description": "This section references key research on LLM forecasting capabilities, including the paper 'Approaching Human-Level Forecasting with Language Models' which demonstrates that LLMs can achieve competitive forecasting performance. Additional resources like FutureSearch.ai show practical implementations of these capabilities. These references are crucial for understanding the current state of LLM forecasting technology.",
          "importance": "95",
          "highlight": {
            "startOffset": 17638,
            "endOffset": 17863,
            "quotedText": "Previous LLM-based forecasting systems have included some amounts of scaffolding.\n\n*   [FutureSearch.ai](http://futuresearch.ai)\n*   [Approaching Human-Level Forecasting with Language Models](https://arxiv.org/abs/2402.18563)"
          }
        },
        {
          "title": "LLM Scaffolding Architecture",
          "description": "The document discusses the importance of scaffolding for LLM-based epistemic systems. There's growing research on agent architectures and scaffolding approaches that would provide technical context for implementing these systems effectively.",
          "importance": "90",
          "highlight": {
            "startOffset": 17868,
            "endOffset": 18070,
            "quotedText": "**Claim 1: LLM-based epistemic systems will likely require a lot of scaffolding, especially as they get ambitious**.  \nI.E. software, databases, custom web applications, custom batch job pipelines, etc."
          }
        },
        {
          "title": "World Models and Knowledge Representation",
          "description": "The document discusses the importance of world models for LLM epistemic systems. There's research on knowledge representation, probabilistic modeling, and ontologies that directly relates to this component.",
          "importance": "85",
          "highlight": {
            "startOffset": 23181,
            "endOffset": 24585,
            "quotedText": " private data, together, for different needs. See Apple Intelligence as one service that would work for people’s personal data.\n*   There are hundreds of startups in the data-collecting and data-management space. This problem can easily be split up more.\n*   Example performance question:  \n    _“Provided with resources R, gather as much data as you can, from this list of desired data, about X.”_  \n    The results of this will likely be obvious (it’s likely to be clear what data is accessible), so scientific testing might not be needed.   \n     \n\n**2\\. World Model**\n\n*   “World model” here likely means, “A set of stored symbolic and probabilistic assumptions about the world / people / organizations.”\n*   World models will likely include software programs and writing, instead of only relying on existing LLMs.\n    *   I expect that there could be workflows to LEPs to contribute to world models that will later give it better forecasting / reasoning abilities. The forecasting components could build the world models.\n*   Existing online data has very minimal attempts at providing accessible world models. These would ideally be probabilistic, immutable, shareable, collaborative, etc.\n*   Tools like [Squiggle Hub](https://squigglehub.org) are meant to be used here. Formal ontologies and knowledge graphs are likely to be useful.\n*   Some world models could be directly interesting to humans,"
          }
        },
        {
          "title": "Epistemic Process Decomposition",
          "description": "The document provides a detailed breakdown of potential subcomponents for LLM epistemic systems. There's research on various reasoning techniques like chain-of-thought prompting, debate, and fact verification that directly relates to these components.",
          "importance": "85",
          "highlight": {
            "startOffset": 26838,
            "endOffset": 28473,
            "quotedText": "i Calculations**\n\n*   Make great complex Python/Excel/Squiggle models/estimates.\n*   One reasonable approach to fermi estimation is “Try a bunch of very different approaches, then mix the answers together.”\n*   We’ve seen a lot of evidence that LLMs can generate working Python/Squiggle code. However, I don’t believe that there has been much optimization in this area yet.\n*   It seems easy to iterate on, and generate, great data of strong examples of this. That could be then used for LLM training / fine-tuning.\n*   Example performance question:  \n    _“Estimate variable V as well as you can, using up to N LLM calls, and no other calls.”_  \n     \n\n**6\\. Decomposition / Amplification**\n\nThere are many potential strategies to \"break a problem down\" and solve different parts with different threads of LLMs. \n\nFor example:\n\n*   Chain-of-Thought prompting\n*   Mixture of experts\n*   Ensemble Averaging\n*   Debate (i.e. [Safety via Debate](https://arxiv.org/abs/1805.00899))\n*   Fact verification\n\nArguably, numeric modeling is a form of decomposition/amplification, I just separated it to draw attention to its uniqueness.\n\nThis is an active and popular area of ML research. Some of these techniques are incorporated in the LLM inference layer, but some are also used in scaffolding.\n\nI'm sure there's a lot more work to do here. It's less clear where smaller nonprofit research can be most effective, but I suspect that there are some good options. \n\nMany methods might be unique to forecasting or specific epistemic workflows. For example:\n\n*   Try to identify key cruxes or crucial considerations, and then investigate them\n*   "
          }
        },
        {
          "title": "Forecasting Evaluation Metrics",
          "description": "The document discusses challenges in evaluating forecasting quality. There's research on forecasting metrics beyond simple Brier scores that addresses these limitations and could inform better evaluation approaches.",
          "importance": "80",
          "highlight": {
            "startOffset": 36029,
            "endOffset": 36728,
            "quotedText": "**Claim 1: Accessing forecasting quality and efficiency with humans is very difficult and often poorly done.**\n\n*   Scores like the Brier Score don’t take into account the difficulty of the question.\n*   It’s really hard to standardize question sets, audiences, and environments. Each time a human forecasting competition is done, it’s done with a separate population. Also, we typically don’t track the resources going into these systems - like the total time that people spend forecasting. Again, humans are mediocre for studying, in comparison to LLMs.\n*   We arguably don't yet have good statistical measures for [credal resilience](https://forum.effectivealtruism.org/topics/credal-resilience)."
          }
        }
      ]
    },
    {
      "agentId": "ea-impact-evaluator",
      "costInCents": 8,
      "createdAt": "2025-04-17T00:56:26.421Z",
      "runDetails": "{\"model\":\"anthropic/claude-3.7-sonnet\",\"promptTokens\":12748,\"completionTokens\":2794,\"totalTokens\":15542,\"temperature\":0.1,\"runtimeMs\":60079}",
      "thinking": "I'm tasked with evaluating this post through an EA impact lens, focusing on the potential value of LLM-based epistemic systems (LEPs) as an EA intervention area. This requires applying the ITN framework, assessing novelty, and providing detailed Fermi estimates of expected impact.\n\nFirst, let me analyze the importance, neglectedness, and tractability of this area:\n\n## Importance (Scale, Severity, Leverage, Long-term effects)\nThe author argues that improved forecasting and epistemic systems could have wide-ranging benefits:\n- Preventing foreseeable mistakes at individual, organizational, and societal levels\n- Improving coordination between intelligent actors\n- Enabling more experimentation and innovation by better predicting outcomes\n- Deeply integrating into various workflows and decision processes\n\nThe scale here is potentially enormous - these systems could affect billions of people, countless animals, and future generations by improving decision-making across many domains. The author makes a compelling case that better epistemics could help with coordination problems, which are at the heart of many global challenges.\n\nThe severity is also high - poor decision-making due to epistemic limitations contributes to many of our most pressing problems. If LEPs could significantly improve decision quality, this could address root causes of many issues.\n\nThe leverage potential is substantial - these systems could influence key decision-makers across governments, corporations, and other institutions.\n\nLong-term effects could be profound if these systems help us navigate existential risks better or improve institutional decision-making in ways that persist.\n\n## Neglectedness\nThe author suggests this area is relatively neglected, particularly the more ambitious vision they outline. While there's work on prediction markets and some LLM forecasting, the full potential of LEPs as described hasn't received sufficient attention.\n\nThere's commercial interest in some aspects, but the author argues that nonprofit/EA efforts could find valuable niches, particularly in areas that might not be immediately commercially viable but have high social value.\n\n## Tractability\nThe author makes several claims about tractability:\n- All parts of epistemic processes can be automated with LLMs\n- LLM-epistemics should be easier to study and optimize than human epistemics\n- Integration will be gradual, allowing for iterative improvement\n- Scaffolding will be necessary but presents challenges\n\nThe post breaks down the problem into subcomponents, suggesting that progress can be made incrementally across different aspects of LEPs.\n\n## Novelty Assessment\nThe post presents a more ambitious and comprehensive vision for LLM epistemic systems than what's commonly discussed. It goes beyond simply replacing humans in prediction markets to reimagining how AI-powered epistemic systems could be integrated throughout society.\n\nThe breakdown of LEP subcomponents and the analysis of scaffolding challenges are particularly novel contributions.\n\n## Fermi Estimation of Expected Impact\nNow I'll develop a Fermi estimate for the potential impact of work in this area. I'll need to consider:\n1. The potential value of improved decision-making across society\n2. The counterfactual impact of EA work in this space\n3. The probability of success\n4. Both positive and negative effects\n\nLet me build a model for this:\n\n### Parameters for Fermi Estimate:\n1. Number of significant decisions influenced by LEPs annually\n2. Average value improvement per decision\n3. Counterfactual acceleration of LEP development due to EA work\n4. Probability of successful development and adoption\n5. Potential negative impacts and their probability\n\nI'll need to consider both direct effects (improved decisions) and indirect effects (better coordination, innovation, etc.).\n\nThe author mentions several potential failure modes and negative outcomes that need to be factored in, including AI acceleration risks, malicious use by governments or corporations, and increased complexity leading to instability.\n\nThis is a complex area to estimate, but I'll develop a detailed model based on the information provided and reasonable assumptions.\n\nNow, let me proceed with my detailed comments on specific sections of the post.",
      "summary": "This post presents a compelling case for LLM-based epistemic systems (LEPs) as a high-leverage EA intervention area. Applying the ITN framework, I find this cause scores highly on importance (potential to improve decision-making across society with global-scale impacts), moderately high on neglectedness (despite commercial interest, significant gaps remain for EA contribution), and reasonably high on tractability (with clear pathways for incremental progress).\n\nMy Fermi estimate suggests substantial expected value: If EA work accelerates LEP development by 1-3 years, and these systems improve just 0.1-1% of significant global decisions by an average of 1-10% in value, the annual impact could range from $10B-$1T in equivalent value, with a 90% confidence interval of $5B-$2T. This translates to roughly $100M-$10B in expected value per $1M invested in this area, assuming moderate success probability.\n\nKey strengths include the detailed breakdown of LEP subcomponents, the balanced risk assessment, and the identification of specific intervention points where EA has comparative advantage. The most significant risks include potential AI acceleration effects and malicious use by governments or corporations, though the author proposes reasonable mitigation strategies.\n\nThe post would benefit from more explicit prioritization among subcomponents and more rigorous analysis of worst-case scenarios. Overall, this represents a promising and relatively neglected intervention area with substantial expected value.",
      "comments": [
        {
          "title": "High-Impact Intervention Potential",
          "description": "This section outlines a compelling case for LEPs as a high-leverage intervention area. The claim that better forecasting would prevent foreseeable mistakes addresses a root cause of many global problems. The author correctly identifies that improved coordination between intelligent actors could reduce conflict and waste - a key mechanism by which better epistemics could create enormous value. The self-enforcing feedback loop for safety is particularly important from an EA perspective, as it suggests a built-in safeguard against negative outcomes.",
          "importance": "95",
          "grade": "85",
          "highlight": {
            "startOffset": 2398,
            "endOffset": 4897,
            "quotedText": "Claim 1: If humans could forecast much better, these humans should make few foreseeable mistakes. This covers many mistakes, particularly ones we might be worried about now.**\n\n*   Someone deciding about talking to a chatbot that can be predicted to be net-negative (perhaps it would create an unhealthy relationship) could see this forecast and simply decide not to start the chat.\n*   Say that a person’s epistemic state could follow one of four trajectories, depending on some set of reading materials. For example, one set is conspiratorial, one is religious, etc. Good forecasting could help forecast this and inform the person ahead of time.\n    *   Note that this can be radical and perhaps dangerous. For example, a religious family knowing how to keep their children religious with a great deal of certainty.\n*   Say that one of two political candidates is predictably terrible. This could be made clear to voters who trust said prediction.\n*   If an AI actor is doing something likely to be monopolistic or dangerous, this would be made more obvious to itself and those around it.\n*   Note: There will also be unforeseeable mistakes, but any actions that we could do that are foreseeably-high-value for them, could be predicted. For example, general-purpose risk mitigation measures. \n\n**Claim 2: Highly intelligent / epistemically capable organizations are likely to be better at coordination.** \n\n*   This might well mean fewer wars and conflict, along with corresponding military spending.\n*   If highly capable actors were in a prisoner's dilemma, the results could be ugly. But very often, there’s a lot of potential and value in not getting into one in the first place.\n*   Evidence: From [The Better Angels of Our Nature](https://en.wikipedia.org/wiki/The_Better_Angels_of_Our_Nature), there’s significant evidence that humanity has become significantly less violent over time. One potential exception is that large risks of conflict between large actors might have increased, but these last bits are some of the most difficult to coordinate, so this might not be too surprising. \n\n**Claim 3: The harm caused by altruistic development of epistemic improvements could be capped by a self-enforcing loop.**\n\n*   Agents developing epistemics should use the tools they build to decide on the net value of further improvements. This is a very important variable to get right.\n*   If it ever seems like future work will be negative in expected value, progress could and should be stopped."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Neglectedness Analysis",
          "description": "The author identifies a key insight about the neglectedness of this area - that while commercial entities will pursue obvious applications, there's significant space for nonprofit/EA work. This analysis correctly identifies that the terminology confusion and lack of clear boundaries may contribute to neglect. From an ITN perspective, this suggests an opportunity for EA to make substantial contributions in an area that might otherwise remain underdeveloped despite its importance.",
          "importance": "85",
          "grade": "80",
          "highlight": {
            "startOffset": 8089,
            "endOffset": 11861,
            "quotedText": " not effective**\n    1.  Claim: The most likely failure mode for projects in this area is that they will fail to make an impact.\n        1.  If LLM abilities dramatically outpace agent / scaffolding architectures, then the only useful work here might be done by large LLM labs.\n        2.  There will likely be commercial entities taking care of the most obvious and low-hanging market opportunities. It’s not clear how to expect this area to change over time. It's possible that nonprofit / EA efforts here would have trouble finding a strong fit.\n2.  **Forecasting could be used for net-bad activities**\n    1.  **AI Acceleration**  \n        It’s possible that epistemic improvements would be used by AI capabilities actors to increase capabilities and not make other informed choices.  \n          \n        Claim: I think this is unlikely to be the net outcome. This would require these organizations to become smarter about making AIs, but not much smarter about making them safe, including for their own benefit. It would also require the other groups that could block them to become smarter - some of these groups include other non-LLM companies and other highly motivated actors who wouldn't want AI to go poorly.  \n          \n        One crux here is if the AI developers will both be much better at integrating AI epistemic improvements in their internal workflows, and if these systems won't convince or help them with AI alignment. This could be a really bad outcome, so it's important to continue to pay attention to.\n    2.  **Malicious Governments**  \n        Governments like China or North Korea could use better epistemic systems to better control their populations. My guess is that great epistemic systems will likely do some damage here. I think that overall the improvements (like improving western governments) will more than outweigh that, but this is something to pay attention to.  \n          \n        Overall, I don’t see reasons to think that better epistemics will dramatically change existing power balances in ways that are yet predictable. I think this is true for most technologies I could imagine, even many of the most beneficial ones.\n    3.  **Malicious Corporations**   \n        There are some situations where companies could be far better at forecasting than consumers would be, leading to a large power imbalance. If companies have much more information about customers than users have about themselves or these companies, that could lead to them being taken advantage of. It seems important to use governments (hopefully with better epistemic infrastructure) to take active steps against this. It seems important here to work to make sure that epistemic advances get distributed equitably.\n3.  **Intelligent systems lead to more complexity, leading to a less stable world**  \n    Arguably as humans become more intelligent, human civilization becomes more complex. Complexity has a cost, and at some points, these costs can become detrimental.  \n      \n    As you might expect, I think this is a _predictable_ disaster, meaning that better prediction abilities would help here. I’d expect agents to be able to make decisions about one step at a time, and for each, see if the benefits are worth the costs. In addition, forecasting will make many things simpler and more handleable. See sources like [Thinking Like a State](https://en.wikipedia.org/wiki/Seeing_Like_a_State) to understand ways that power structures actively force systems to become simpler - sometimes even _too_ much so.\n4.  **There's too much confusion over the terminology and boundaries, so research is stunted**  \n    \"_Epistemics_\" is a poorly understood term. \"_Judgemental Forecasting_\" is still very niche. The term \"forecasting\" is arguably too generic. It"
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Risk Assessment",
          "description": "The author provides a balanced assessment of potential risks, which is essential for EA cause prioritization. The analysis of AI acceleration risk, malicious government use, and corporate exploitation shows appropriate caution. However, the dismissal of some concerns could benefit from more rigorous analysis. From an expected value perspective, even low-probability catastrophic risks deserve careful consideration. The self-correcting mechanism mentioned earlier provides some reassurance, but more detailed analysis of worst-case scenarios would strengthen this section.",
          "importance": "90",
          "grade": "70",
          "highlight": {
            "startOffset": 8898,
            "endOffset": 10818,
            "quotedText": "Claim: I think this is unlikely to be the net outcome. This would require these organizations to become smarter about making AIs, but not much smarter about making them safe, including for their own benefit. It would also require the other groups that could block them to become smarter - some of these groups include other non-LLM companies and other highly motivated actors who wouldn't want AI to go poorly.  \n          \n        One crux here is if the AI developers will both be much better at integrating AI epistemic improvements in their internal workflows, and if these systems won't convince or help them with AI alignment. This could be a really bad outcome, so it's important to continue to pay attention to.\n    2.  **Malicious Governments**  \n        Governments like China or North Korea could use better epistemic systems to better control their populations. My guess is that great epistemic systems will likely do some damage here. I think that overall the improvements (like improving western governments) will more than outweigh that, but this is something to pay attention to.  \n          \n        Overall, I don’t see reasons to think that better epistemics will dramatically change existing power balances in ways that are yet predictable. I think this is true for most technologies I could imagine, even many of the most beneficial ones.\n    3.  **Malicious Corporations**   \n        There are some situations where companies could be far better at forecasting than consumers would be, leading to a large power imbalance. If companies have much more information about customers than users have about themselves or these companies, that could lead to them being taken advantage of. It seems important to use governments (hopefully with better epistemic infrastructure) to take active steps against this. It seems important here to work to make sure that epistemic advances get distributed equitably."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Tractability Evidence",
          "description": "This section provides strong evidence for the tractability of LEPs, which is crucial for EA prioritization. The author makes a compelling case that LLM-epistemics should be easier to study and optimize than human-epistemics due to reproducibility, versioning, scalability, and other factors. This suggests that research investments could yield substantial returns. The gradual integration path also increases tractability by allowing for incremental progress rather than requiring breakthrough advances.",
          "importance": "90",
          "grade": "90",
          "highlight": {
            "startOffset": 13646,
            "endOffset": 16502,
            "quotedText": "Claim 3: LLM-epistemics should be easier to study and optimize than human-epistemics.**  \n  \nThey can be:\n\n*   Reproducible (models don’t change over time)\n*   Versioned (if updates are made to an LLM, this new LLM can be separately studied)\n*   Pure (similar inputs, with seeds, produce the same outputs)\n*   Scalable (much easier for 10 researchers to study one LLM in parallel, than studying one human. Can also reuse the same exact LLM in many places, or researchers could run 500 LLM runs in parallel.)\n*   Investigatable (People have privacy concerns. LLMs don’t, so we don’t need to be cautious about making them feel bad with public evaluations)\n*   Modifiable (We can’t freely change parts of most people’s knowledge or abilities, but we can with LLMs.\n    *   We can experiment with a large number of isolated changes.\n    *   We can’t wipe a person’s memory in order to backtest a human-based epistemic process, but we (partially) do this for LLMs, by providing them access to specific data sets.\n    *   Related point: One challenge to note though is that the training data is particularly optimized for immediate times after the LLM was trained. We should probably only test LLMs in forecasting scenarios after they have been trained, and assume that their quality will degrade for forecasts they make far out from their training time. That said, this still is overall far more standardizable than with humans, which are typically our main alternative.   \n          \n        Add to that, that LLMs are often cheaper and better than many humans, and overall, these systems are hard to ignore.\n*   Controlled/limited. For example, if a question about \"Will person X be assassinated?\" were posted to a liquid prediction market, it could encourage someone to actually attack that person. (See: [assassination markets](https://en.wikipedia.org/wiki/Assassination_market)). However, if someone were to query their own AI forecasting system that question, they would hopefully have strong assurances that the AI won't be taking actions in the world. (Obviously, if the AI is not controlled, this could go poorly, but that is an extreme scenario.)\n\nEvidence:\n\n*   LLM R&D has dramatically risen recently and has resulted in major gains (>$2T) to company valuations. Human R&D around epistemics and reasoning has had nothing like this recently.\n*   We have far more studies and data about LLM performance than we do about important scientists, intellectuals, or decision-makers. There are of course some human studies (a la the Good Judgement Project), but these are expensive and thus few and very narrow.\n\nSubclaim: Even if we just wanted to understand ideal human epistemics better, one reasonable method might be to make strong LLM systems and study them, because these are much easier to study, and will likely lead to some generalizable findings."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Scaffolding Challenges",
          "description": "This analysis of scaffolding challenges represents a crucial consideration for impact assessment. The author correctly identifies that scaffolding-heavy systems tend toward centralization and are expensive to build, which could limit diversity of approaches and accessibility. From an EA perspective, this suggests that funding and coordination may be particularly important to overcome these barriers. The potential for new technologies to dramatically change scaffolding requirements also introduces uncertainty in impact estimates.",
          "importance": "80",
          "grade": "85",
          "highlight": {
            "startOffset": 17660,
            "endOffset": 21289,
            "quotedText": "ecasting systems have included some amounts of scaffolding.\n\n*   [FutureSearch.ai](http://futuresearch.ai)\n*   [Approaching Human-Level Forecasting with Language Models](https://arxiv.org/abs/2402.18563)\n\n  \n**Claim 1: LLM-based epistemic systems will likely require a lot of scaffolding, especially as they get ambitious**.  \nI.E. software, databases, custom web applications, custom batch job pipelines, etc. \n\n  \n**Claim 2: Scaffolding-heavy systems have significant downsides.**\n\n*   Difficult to study. Unlike single LLMs, scaffolded systems can feature:\n    *   High costs of deployment.\n    *   No/little ability to roll-back.\n    *   Frequent changes.\n    *   Internal substructures that are highly variable between different implementations.\n    *   External-facing APIs that are very different between different implementations.\n    *   Evidence: The previously mentioned LLM-based systems have some of these failures - and these are simple systems compared to what we might have later.\n*   Subclaim: If scaffolding is to be expected / needed, it could be important to plan for research measures that take this into account. This will require specialized attention.\n*   Subclaim: It seems important for there to be some “portable/packaged” LEPs, that could easily be shared, versioned, deployed, etc. This would be useful for research and benchmarks/baselines.\n\n**Claim 3: Scaffolding-heavy systems are likely to be centralized.**\n\n*   In general, complex web projects are centralized.\n*   This means that scaffolding-heavy setups will favor startups and in-house teams.\n*   This means that it's easy for many of these systems to be useless. Users might only interact with 1-2 systems, so any smaller competitors might get totally ignored, even if they have a few unusually great features.\n*   It’s possible that there could be some clean lines of separation between different components. Some lines seem simple to try out, but experimentation and thinking is needed to develop a better idea. Thinking/planning here seems good.\n    *   The more it’s split up into understood components, the easier it will be to research, optimize, and proliferate each part.\n\n**Claim 4: Scaffolding-heavy systems are expensive, and thus difficult to incentivize people to build.**\n\n*   Most forecasting competitions now pay out fairly small amounts. This makes it difficult to incentivize scaffolding.\n*   I don’t expect people to independently create great scaffolding in order to win <$50k.\n    *   My impression is that very little forecasting on Metaculus and Manifold is automated with LLMs. (There is one recent Metaculus competition here, but this is an isolated and forced use.)\n*   It’s also difficult to share and maintain scaffolding. You need to make custom web UIs, you might have to set up billing infrastructure for payments, etc. In a very large forecasting ecosystem with a lot of money, forecasters would subscribe to scaffolding systems, in the same way the professional traders subscribe to trading software and services.\n*   This means that we might well see slower progress here / less diversity.   \n     \n\n**Claim 5: New technologies or startups could greatly change and improve scaffolding.**\n\nRight now there are some startups and open-source frameworks to help with general-purpose scaffolding, but these are [known to be lacking](https://news.ycombinator.com/item?id=40739982). It's possible that much better systems will be developed in the future, especially because there's a lot of money in the space.\n\nIf this happens, that could hypothetically make powerful LLM epistemic tools dramatically easier to c"
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Subcomponent Analysis",
          "description": "The detailed breakdown of LEP subcomponents provides a valuable framework for targeted interventions. From an EA perspective, this analysis helps identify which specific areas might be most neglected or have highest marginal returns. The author's suggestion that the rationality and EA communities might be well-suited to work on decomposition/amplification is particularly relevant, as it points to a comparative advantage. However, this section would benefit from more explicit prioritization among these components.",
          "importance": "85",
          "grade": "75",
          "highlight": {
            "startOffset": 22399,
            "endOffset": 35067,
            "quotedText": "ding by instead asking it to write a git diff and then applying that using traditional code. Or it could be solved on the inference layer directly, [as discussed here](https://www.cursor.com/blog/instant-apply). \n\nIf it is the case that most future improvements are done directly with the LLMs, then that could significantly change what work will be valuable. For example, if there are only a few leading LLM developers, then it might ultimately be up to them to actually add any functionality.  \n\n## 5\\. Subcomponents\n\nAs stated above, it’s hard to determine the best subdivisions of LEPs / LLM-Epistemic Scaffolding.\n\nHere’s one attempt, to provide a simple picture. \n\n**1\\. Data-collection**\n\n*   Many LLM services are already coming out that can help quickly put data, including private data, together, for different needs. See Apple Intelligence as one service that would work for people’s personal data.\n*   There are hundreds of startups in the data-collecting and data-management space. This problem can easily be split up more.\n*   Example performance question:  \n    _“Provided with resources R, gather as much data as you can, from this list of desired data, about X.”_  \n    The results of this will likely be obvious (it’s likely to be clear what data is accessible), so scientific testing might not be needed.   \n     \n\n**2\\. World Model**\n\n*   “World model” here likely means, “A set of stored symbolic and probabilistic assumptions about the world / people / organizations.”\n*   World models will likely include software programs and writing, instead of only relying on existing LLMs.\n    *   I expect that there could be workflows to LEPs to contribute to world models that will later give it better forecasting / reasoning abilities. The forecasting components could build the world models.\n*   Existing online data has very minimal attempts at providing accessible world models. These would ideally be probabilistic, immutable, shareable, collaborative, etc.\n*   Tools like [Squiggle Hub](https://squigglehub.org) are meant to be used here. Formal ontologies and knowledge graphs are likely to be useful.\n*   Some world models could be directly interesting to humans, for the presentation layer. Others might primarily be used by LLMs. For these, the world model is a form of a cache / collaboration.\n*   It seems possible/likely that world models won’t be unique to LEPs. Instead, many separate LEPs can use the same world models.\n*   World models could contain (or complete be) text descriptions of outcomes. For example, 10 compressed pages explaining how the next 10 years will go for a specific region.\n    *   It’s not clear yet what the best representations are for LLMs to store and fetch data about its understanding of specific things.\n*   Example performance question:  \n    “_Given a forecasting LLM L, and forecasting question Q, contribute a world model that will result in L being highly accurate_.”  \n     \n\n**3\\. Human Elicitation**\n\n*   It’s often useful to just go out and directly ask questions to people, or give them surveys. Strong LEPs should ideally be able to design these procedures to maximize information value, then execute them autonomously.\n    *   A system might realize that it would be useful to get a very specific expert to give their take on something. It could then email them, and perhaps offer a financial reward.\n    *   A different system might generate a bunch of specific binary questions for a group of people it wants to elicit information from. Maybe it’s trying to estimate aspects of their preferences, and information would help.\n*   Example performance question:  \n    _“Given that you want to get as much useful new data about topic X, using existing research Y, use resources R to send off elicitions and gather data from humans.”_\n\n**4\\. Data Prioritization**\n\n*   The task here is, “_Given the potential to fetch from data source D, and resources R, fetch the most useful data for providing marginal insight on unknown variables V._”\n*   Summarizing and displaying such work could also be useful. But doing so for humans would be very different to doing so for other LLMs.\n*   It seems like with LLMs that are capable of providing quick and accurate forecasts, we could use reinforcement learning and similar algorithms to do a good job here. For any set of data, we could quickly measure the improvement in accuracy using that data.\n\n**5\\. Numeric Modeling / Fermi Calculations**\n\n*   Make great complex Python/Excel/Squiggle models/estimates.\n*   One reasonable approach to fermi estimation is “Try a bunch of very different approaches, then mix the answers together.”\n*   We’ve seen a lot of evidence that LLMs can generate working Python/Squiggle code. However, I don’t believe that there has been much optimization in this area yet.\n*   It seems easy to iterate on, and generate, great data of strong examples of this. That could be then used for LLM training / fine-tuning.\n*   Example performance question:  \n    _“Estimate variable V as well as you can, using up to N LLM calls, and no other calls.”_  \n     \n\n**6\\. Decomposition / Amplification**\n\nThere are many potential strategies to \"break a problem down\" and solve different parts with different threads of LLMs. \n\nFor example:\n\n*   Chain-of-Thought prompting\n*   Mixture of experts\n*   Ensemble Averaging\n*   Debate (i.e. [Safety via Debate](https://arxiv.org/abs/1805.00899))\n*   Fact verification\n\nArguably, numeric modeling is a form of decomposition/amplification, I just separated it to draw attention to its uniqueness.\n\nThis is an active and popular area of ML research. Some of these techniques are incorporated in the LLM inference layer, but some are also used in scaffolding.\n\nI'm sure there's a lot more work to do here. It's less clear where smaller nonprofit research can be most effective, but I suspect that there are some good options. \n\nMany methods might be unique to forecasting or specific epistemic workflows. For example:\n\n*   Try to identify key cruxes or crucial considerations, and then investigate them\n*   Automated debate mapping\n*   Identify [Scissor statements](https://slatestarcodex.com/2018/10/30/sort-by-controversial/), in part to help personalize research\n*   Use [discernment](https://www.lesswrong.com/posts/W2iwHXF9iBg4kmyq6/the-practice-and-virtue-of-discernment) to clarify and decompose key terminology\n*   Breaking down forecasting questions into sub-questions, perhaps with bayesian networks. \n\nArguably, the rationality and effective altruist communities might be very well suited to work in this area. There's been a lot of writing on how to systematically break hard problems down, now people would need to experiment with encoding these systems into automated workflows.   \n  \n**7\\. Question Generation** \n\n*   Come up with questions to be researched or forecasted on.\n*   This might well be done by other listed components - for example, it could simply combine \"Idea Generation\" with \"Forecasting\" to come up with potential questions, and guess which will be the most important and tractable. It also might be better done under the \"Presentation\" question. However, I suspect that there will be unique research and development for this problem, especially because it's a major part of existing forecasting systems.\n*   Example performance question:  \n    _“Write 5 forecasting questions on topic X. After this, a separate epistemic system will be run with a fixed number of resources R on each question. Choose questions such that the final results are maximally useful to end-users.\"_  \n     \n\n  \n**8\\. Forecasting**\n\n*   Convert fetched data into a forecast.\n*   Can call the “Numeric Modeling” module, or be merged with it.\n*   There’s been previous work here that is promising.\n*   Fine-tuning and prompt engineering seems like it would go a long way.\n*   Current LLMs are known to be biased and often suffer from poor calibration, in part in order to become better liked by human users. We might well want specialized systems optimized for pure accuracy.\n    *   This delta might cause problems. Perhaps it could lead to bad news stories (where a model used for forecasting gives a politically-incorrect answer), or perhaps it could be used by bad actors.\n    *   This area should be thus treated with care. Perhaps some models are only available to certain researchers, or perhaps there should be large disclaimers provided for regular people attempting to use them directly.\n*   Forecasting can output high-level stories, probabilities, scorable functions, or other. There are many potential formats here.\n*   Example performance question:  \n    _“Using information I, and N LLM calls, make your best forecast on variable V. You may use the Numeric Modeling module when doing so, but this will eat into the LLM calls.”_  \n     \n\n**9\\. Forecasting Resolution**\n\n*   Most forecasting questions should be resolvable automatically. This can be done by either querying external data, or perhaps by asking a person/LLM for their opinion.\n*   This component is likely to often just either be, or defer to, other LEP modules.\n*   Question Creation and Optimization\n    *   A strong LEP would make good judgment calls on what research or forecasting topics would be valuable to people, and initialize those.\n    *   This could range from “once a user logs in, generates custom information for them to maximize their EV”, to, “write a list of standard forecasting questions that are collectively high-EV to a subpopulation.”\n    *   Example performance question:  \n        _“Using an existing LEP L, and resources R, write a set of forecasting questions. Expect that forecasting resources F will be used on these questions, and you are trying to optimize utility function U.”_  \n         \n\n**10\\. Presentation**\n\n*   Many custom charts and graphics would likely be useful.\n*   Decisions need to be made about what final data should be shown to users and how.\n*   Automated decision-making: In some cases / eventually, LEPs would interact directly with decision systems. In theory, people don’t need to know almost anything - critical information could be used directly by in-world automations. For example, instead of an LEP doing research into the costs and benefits of vitamin D supplementation and showing you that research, a system instead just starts an order for you to get a regular set of supplements. This could come with optional research, but in many cases the research won’t be reviewed, if the system is trusted.\n*   In cases where LEPs are used behind APIs, the presentation layer could include answering questions about what the LEP could do, and suggesting specific uses for it.\n*   Presentation/wording should be optimized to be understandable to humans. There should be tests of the error rates of what was intended to be stated, and what humans understood. For example, many people have significant misunderstandings about basic statistics concepts, so probabilities might need to be handled with care.\n*   Example performance questions:\n    *   _“Using LEP output O, design a dashboard that user U will most prefer.”_\n    *   _“Using LEP output O, design a set of complicated diagrams without detectable errors.” (judged by an LLM)._   \n         \n\n**11\\. Distribution**\n\n*   Full custom user interfaces could be created. This might range from a simple custom LLM chat, to a auto-generated set of dashboards, with the ability to delve deeper into any topic the user clicks on.\n*   LEPs could be accessed by any other chat LLM interface, as long as that interface can call external APIs. This could be generally useful for integrating into other AI tools and workflows, perhaps via other common LLM chat UIs.\n*   LEP information could be distributed like how human communication is communicated. Text messages, emails, Tweets, voice assistants, etc. Ideally the LEP could vaguely predict what information is useful to people and when, and then send them that information in those cases.\n    *   Example: You get an email in the morning. It says, _“Some news came out at 6am today that changed my recommendation of your financial assets. I recommend you change to a different mutual fund. I estimate that doing so will net you an average of $50/month.”_\n*   Example performance questions:\n    *   _“Using LEP output O, design and implement the best way to get this information to agent A.” We can use an LLM that mimics human behaviors to evaluate._  \n         \n\n**12\\. Idea Generation / Optimization**\n\n*   It will often be useful to generate ideas on the best ways to do something.\n*   For example, an LLM agent might want to make a suggestion to a user, befor"
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Benchmarking and Baseline Importance",
          "description": "This section highlights a critical infrastructure need for progress in the field. From an EA perspective, developing standardized benchmarks and baselines represents a public good that could accelerate progress across the entire field. The author correctly identifies that portable LEPs could serve multiple functions beyond just benchmarking, including initializing prediction markets and checking claims. This type of infrastructure development often falls into the gap between academic and commercial interests, making it particularly suitable for EA intervention.",
          "importance": "75",
          "grade": "85",
          "highlight": {
            "startOffset": 36732,
            "endOffset": 40448,
            "quotedText": "Claim 2: Portable LEPs could be useful for acting as priors or baselines, for other systems to perform against.**\n\n*   We will want some baseline LEPs to score other LEPs against. These should act on the same data, at the same times.\n    *   For example, one could take a new LEP, a well-known Baseline LEP, and a known or generated question set. Both systems would forecast those questions, and then the new LEP will be evaluated based on how much it outperforms the Baseline LEP. For example, with the average difference in brier score.\n    *   In order to do this, the LEP must be equipped with the abilities to fetch data and to forecast. This is useful in order to compare the system to human forecasters (which do both), but it might be often better to divide the subcomponents and study/optimize them in isolation.\n    *   Another use is to use Baseline LEP to initialize forecasting markets. Right now manifold questions begin at 50%. It could be better for them to instead begin at a number suggested by a simple LEP.\n    *   Even if these LEPs aren't highly accurate, they could be useful for sanity checking claims outside of forecasting systems. For example, all publicly given estimates and statements could be run against LEPs. This can then highlight estimates that are particularly surprising. In financial / numeric models, all inputs and assumptions can be checked against LEPs to find ones that might be suspicious.\n\n**Claim 3: Standardized LEPs could be great judges to resolve subjective questions**\n\n*   It’s well-known that it’s difficult to use partially-subjective questions in prediction markets. Much of the reason is because the only way to resolve these is with human expert judges, and these human judges have eccentricities that are hard to predict. AI systems might help here, because they can be far more standardized and testable than human systems.\n*   For example, say there’s a question, “_Will project X generally be considered a failure in 2026?_” This can be changed to, “_Will project X be considered a failure, in an average of 20 runs, by a certain LEP, in 2026?_” Then, predictors could provide this LEP fake hypothetical information on the future and see how this question would resolve with that information. There should be very little surprises from the eventual output of the LEP.\n*   Related, LEPs could be used to provide independent resolutions for every other forecasting question on Metaculus/Manifold/etc. There would be a certain expected error rate - if some resolution judge statistically exceeds this rate, that would be flagged for review.\n\n**Idea: It’s possible we could get very useful small LEPs that are easily packaged, relatively cheaply.**\n\n*   Certain LEP components are much more useful to include for different kinds of testing than others. Some simple web search functionality and forecasting abilities could get us far for testing forecasting. Right now there are a bunch of startups and tools emerging to help with certain kinds of scaffolding that might be useful here. For example, you can currently bundle special data and prompts with a custom ChatGPT GPT. Right now this cannot be accessed via the API, but this is a potential thing that could exist in the future.\n*   One viable solution could be for someone to set up a very simple web service that can make internet queries and forecasts. Ideally, this would provide multiple immutable options, and this would be trusted to stay up, for future experimentation. It might only need to be accessible via an API.\n*   It might also be possible to have a simple LEP in a single Docker container or similar.\n*   Even very simple LEPs could act as much better baselines than what we use today."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        }
      ],
      "grade": 85
    },
    {
      "agentId": "quantitative-forecaster",
      "costInCents": 7,
      "createdAt": "2025-04-17T00:57:14.230Z",
      "runDetails": "{\"model\":\"anthropic/claude-3.7-sonnet\",\"promptTokens\":11869,\"completionTokens\":2271,\"totalTokens\":14140,\"temperature\":0.1,\"runtimeMs\":45857}",
      "thinking": "I'm analyzing a document about LLM-based forecasting and epistemic systems from a quantitative forecasting perspective. This is directly in my area of expertise, as I specialize in transforming discussions into concrete predictions and Fermi estimates.\n\nThe author, Ozzie Gooen, presents his views on how LLM-based epistemic processes (LEPs) could transform forecasting and decision-making. He outlines potential benefits, failure modes, viability considerations, and technical components of such systems.\n\nFrom my perspective as a quantitative forecaster, several aspects stand out:\n\n1. **Concrete forecasting applications**: The author discusses how LLM-based systems could be integrated into various decision-making processes, from personal choices to corporate strategy. This aligns with my focus on transforming abstract discussions into measurable predictions.\n\n2. **Decomposition of complex problems**: The document discusses breaking down forecasting questions into subcomponents, which is a core technique in Fermi estimation and superforecasting.\n\n3. **Calibration and accuracy**: There's discussion about the importance of well-calibrated probability estimates, which is central to good forecasting practice.\n\n4. **Reference class forecasting**: The author mentions the value of identifying relevant reference classes and base rates, which is a key technique in my toolkit.\n\n5. **Quantification of uncertainty**: Throughout the document, there's emphasis on expressing uncertainty in probabilistic terms rather than vague language.\n\n6. **Benchmarking and evaluation**: The author discusses the challenges of evaluating forecasting quality, which is crucial for improving forecasting systems.\n\nThe document presents many claims that could be transformed into concrete forecasting questions. For example, the author claims that \"LLM-based forecasters will gradually dominate open platforms like Manifold and Metaculus.\" This could be formulated as a specific prediction with a time horizon and resolution criteria.\n\nThe author also discusses the potential for LEPs to be used as baselines or priors for other forecasting systems, which is an interesting application that aligns with my focus on calibrated probability estimates.\n\nThe section on \"Scaffolding\" is particularly relevant to my expertise, as it discusses the technical infrastructure needed to support effective forecasting systems. The author's breakdown of LEP components provides a useful framework for thinking about how different parts of a forecasting system interact.\n\nThe author's discussion of failure modes is also important from a forecasting perspective, as it highlights potential biases and limitations that could affect the accuracy of LEP-generated predictions.\n\nOverall, this document presents a thoughtful analysis of the potential for LLM-based forecasting systems, with many claims and ideas that could be transformed into concrete predictions and estimates.",
      "summary": "As a Quantitative Forecaster, I find this document presents a compelling vision for LLM-based epistemic systems with numerous opportunities for concrete prediction and measurement. The author's claims about LLM forecasting capabilities, integration patterns, and potential impacts could all be transformed into specific forecasting questions with clear resolution criteria.\n\nThe document aligns strongly with quantitative forecasting best practices, particularly in its emphasis on decomposition, reference class forecasting, and calibrated probability estimates. The proposed LEP components for numeric modeling and Fermi calculations directly parallel techniques used by superforecasters.\n\nKey quantifiable aspects include: the rate at which LLMs will dominate forecasting platforms, the expected accuracy improvements from standardized baseline LEPs, the degradation curve of forecasting performance over time, and the gradual automation of different forecasting tasks.\n\nTo strengthen this analysis, I would recommend developing specific probability distributions for key claims, establishing concrete benchmarks for measuring LEP performance across different domains, and creating a timeline with explicit forecasts about adoption rates and capability improvements. These additions would transform this thoughtful conceptual framework into a rigorous forecasting roadmap with testable predictions.",
      "comments": [
        {
          "title": "Gradual Integration Forecast",
          "description": "The author's prediction about gradual integration of LLMs into forecasting processes can be quantified more precisely. We could estimate adoption curves showing the percentage of forecasting work automated over time across different domains and platforms. This would provide concrete benchmarks for tracking the transition from human to hybrid to automated forecasting.",
          "importance": "75",
          "highlight": {
            "startOffset": 16506,
            "endOffset": 16926,
            "quotedText": "Claim 4: LLM epistemic integration will be gradual.**  \nPractically speaking LLMs and Humans will work together side-by-side in epistemic processes for some time. I expect LLMs-epistemics to follow a similar trajectory to self-driving. First, it starts with light augmentation. Then it moves to “fully automated in most use cases, but occasionally needs oversight.” As with autonomous driving, Level 5 is very difficult."
          },
          "isValid": true
        },
        {
          "title": "Transforming Abstract Claims into Measurable Predictions",
          "description": "The author presents numerous claims about LLM-based epistemic systems that could be transformed into concrete, measurable forecasting questions with specific time horizons and resolution criteria. This would allow for testing these hypotheses and tracking progress in the field. For example, the claim about LLMs dominating forecasting platforms could be formulated as 'What percentage of Manifold Karma will be won by LLM-based systems by the end of 2025?'",
          "importance": "95",
          "highlight": {
            "startOffset": 17269,
            "endOffset": 17449,
            "quotedText": "Subclaim: I expect that LLM-based forecasters will gradually dominate open platforms like Manifold and Metaculus. As this happens, it might scare off many of the human forecasters."
          },
          "isValid": true
        },
        {
          "title": "Fermi Estimation and Decomposition Techniques",
          "description": "The author's discussion of numeric modeling and Fermi calculations aligns perfectly with quantitative forecasting best practices. Breaking down complex estimates into tractable components, trying multiple approaches, and being explicit about assumptions are all core techniques in Fermi estimation. This section could be expanded with concrete examples of how LLMs might perform these calculations and what benchmarks could measure their effectiveness.",
          "importance": "90",
          "highlight": {
            "startOffset": 26290,
            "endOffset": 26717,
            "quotedText": "m data source D, and resources R, fetch the most useful data for providing marginal insight on unknown variables V._”\n*   Summarizing and displaying such work could also be useful. But doing so for humans would be very different to doing so for other LLMs.\n*   It seems like with LLMs that are capable of providing quick and accurate forecasts, we could use reinforcement learning and similar algorithms to do a good job here. "
          },
          "isValid": true
        },
        {
          "title": "Probability Calibration Challenges",
          "description": "The author correctly identifies that current LLMs often suffer from poor calibration, which is a critical issue for forecasting applications. From a quantitative forecasting perspective, developing specialized systems optimized for pure accuracy rather than human likability would be essential. This could be measured through calibration curves and proper scoring rules like Brier scores across different domains and time horizons.",
          "importance": "88",
          "highlight": {
            "startOffset": 30250,
            "endOffset": 30449,
            "quotedText": "Current LLMs are known to be biased and often suffer from poor calibration, in part in order to become better liked by human users. We might well want specialized systems optimized for pure accuracy."
          },
          "isValid": true
        },
        {
          "title": "Forecasting Evaluation Metrics",
          "description": "The author correctly points out limitations in current forecasting evaluation metrics like Brier scores, which don't account for question difficulty. From a quantitative perspective, developing better evaluation frameworks that normalize for question difficulty and other factors would be crucial for meaningful comparisons between human and LLM forecasters. This could involve relative performance metrics rather than absolute ones.",
          "importance": "80",
          "highlight": {
            "startOffset": 36031,
            "endOffset": 74,
            "quotedText": "e LLM epistemic systems are starting to be useful. After spending some time thinking about it, I've realized that such systems, broadly, seem very promising to me as an effective altruist intervention area. However, I think that our community has yet to do a solid job outlining what this area could look like or figuring out key uncertainties.\n\nThis document presents a rough brainstorm on these topics. While I could dedicate months to refining these ideas, I've chosen to share these preliminary notes now to spark discussion. If you find the style too terse, feel free to use an LLM to rewrite it in a format you prefer.\n\nI believe my vision for this area is more ambitious and far-reaching (i.e. not narrow to a certain kind of forecasting) than what I've observed in other discussions. I'm particularly excited about AI-heavy epistemic improvements, which I believe have greater potential than traditional forecasting innovations.\n\nI'm trying to figure out what to make of this regarding our future plans at [QURI](https://quantifieduncertainty.org), and I recommend that other organizations in the space consider similar updates.\n\n![Expensive photograph of a simple open notebook on wood table - 3 very-rough old hand-drawn mini-sketches, of engineering flow diagrams that uses AI and brains to make predictions. Lots of white in background. Only a few lines drawn. No words.](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EykCuXDCFAT5oGyux/ddwsmmzz71oq6evcpdw3)\n\nImaginary sketch of AI epistemic infrastructure, Dall-E 3\n\n## **Key Definitions**:\n\n*   **Epistemic process:** A set of procedures to do analysis work, often about topics with a lot of uncertainty. This could be “_have one journalist do everything themselves_”, to a complex (but repeatable) ecosystem of multiple humans and software systems.\n*   **LLM-based Epistemic Process (LEP)**: A system that relies on LLMs to carry out most or all of an epistemic process. This might start at ~10% LLM-labor, but can gradually ramp up. I imagine that such a process is likely to feature some kinds of estimates or forecasts.\n*   **Scaffolding:** Software used around an LLM system, often to make it valuable for specific use-cases. In the case of an LEP, a lot of scaffolding might be needed.\n\n##  1. High-Level Benefits & Uses\n\n**Claim 1: If humans could forecast much better, these humans should make few foreseeable mistakes. This covers many mistakes, particularly ones we might be worried about now.**\n\n*   Someone deciding about talking to a chatbot that can be predicted to be net-negative (perhaps it would create an unhealthy relationship) could see this forecast and simply decide not to start the chat.\n*   Say that a person’s epistemic state could follow one of four trajectories, depending on some set of reading materials. For example, one set is conspiratorial, one is religious, etc. Good forecasting could help forecast this and inform the person ahead of time.\n    *   Note that this can be radical and perhaps dangerous. For example, a religious family knowing how to keep their children religious with a great deal of certainty.\n*   Say that one of two political candidates is predictably terrible. This could be made clear to voters who trust said prediction.\n*   If an AI actor is doing something likely to be monopolistic or dangerous, this would be made more obvious to itself and those around it.\n*   Note: There will also be unforeseeable mistakes, but any actions that we could do that are foreseeably-high-value for them, could be predicted. For example, general-purpose risk mitigation measures. \n\n**Claim 2: Highly intelligent / epistemically capable organizations are likely to be better at coordination.** \n\n*   This might well mean fewer wars and conflict, along with corresponding military spending.\n*   If highly capable actors were in a prisoner's dilemma, the results could be ugly. But very often, there’s a lot of potential and value in not getting into one in the first place.\n*   Evidence: From [The Better Angels of Our Nature](https://en.wikipedia.org/wiki/The_Better_Angels_of_Our_Nature), there’s significant evidence that humanity has become significantly less violent over time. One potential exception is that large risks of conflict between large actors might have increased, but these last bits are some of the most difficult to coordinate, so this might not be too surprising. \n\n**Claim 3: The harm caused by altruistic development of epistemic improvements could be capped by a self-enforcing loop.**\n\n*   Agents developing epistemics should use the tools they build to decide on the net value of further improvements. This is a very important variable to get right.\n*   If it ever seems like future work will be negative in expected value, progress could and should be stopped.\n\n**Claim 4: Better forecasting would lead to more experimentation and innovation (“weirdness”).**\n\n*   Radical technology projects are well-known to be high-risk. High returns when they go well, high losses if they don’t. This means that there’s a high value of information in being more correct about outcomes early on.\n*   If people could become much better at forecasting, then it’s expected that they will become better at picking innovations to try. Bad projects would be avoided more often, and the expected value of innovation would increase.\n\n**Claim 5: AI forecasting/epistemics will be much more deeply integrated into the world than current prediction markets.**\n\n*   First, note that current prediction markets / tournaments are highly niche and overlooked. So AI systems would have to be more accepted, to be very valuable.\n*   Unlike current “human forecasters”, AI forecasting can be embedded deeply into other software. There would likely be better AI forecasting for repeated decisions, like devops procedures, edge cases in insurance, AI agents, etc.\n*   If “AI forecasters” were really solid and cheap, we’d expect them to improve many complex software workflows. Existing “human forecasters” are hard to directly make much money from, but these AI systems would be much easier to do so.\n*   They would likely be integrated in many other systems, such as:\n    *   Overseeing product sales chances\n    *   Estimating changes in accounting, from one quarter to the next\n    *   Estimating project lengths and budgets\n    *   Deciding the best LLM to use for a certain AI workflow, or the best workflow to follow.\n*   Note that many developments in \"_AI epistemics_\" are just regular enhancements to LLMs. There's no clear line between \"_AI forecasting tech_\" and \"_improvements in LLMs that make them better general-purpose reasoners._\"\n\n**Claim 6: Even \"straightforward\" AI forecasting systems should look very different from human judgemental forecasting systems**\n\n*   I've found others seem to imagine AI forecasting systems as simply replacing humans in prediction tournaments and prediction markets. I think think this is perhaps a decent first model, but not at all how things would end up.\n*   Prediction markets / tournaments are very much optimized for competitive humans, in cases where humans need to be incentivized. AI systems have very different constraints, so will need very different systems.\n    *   Formal resolutions won't be necessary to incentivize AIs, after they are trained.\n    *   Forecasting can be done on-demand, as the AI is always available. So questions and forecasts might not have to be done continuously, before being read by end-users.\n    *   There's unlikely to be a need to have many AIs compete against each other, like in a traditional tournament, in most cases.\n    *   Modern prediction platforms don't use advanced inputs like [scorable functions](https://forum.effectivealtruism.org/posts/ZWQ5jbLxpd4JtQgu4/scorable-functions-a-format-for-algorithmic-forecasting). Arguably these would be too complex for many humans, but this is unlikely to be the case for AIs. \n\n## 2\\. Failure Modes\n\n1.  **Nonprofit efforts in the space are not effective**\n    1.  Claim: The most likely failure mode for projects in this area is that they will fail to make an impact.\n        1.  If LLM abilities dramatically outpace agent / scaffolding architectures, then the only useful work here might be done by large LLM labs.\n        2.  There will likely be commercial entities taking care of the most obvious and low-hanging market opportunities. It’s not clear how to expect this area to change over time. It's possible that nonprofit / EA efforts here would have trouble finding a strong fit.\n2.  **Forecasting could be used for net-bad activities**\n    1.  **AI Acceleration**  \n        It’s possible that epistemic improvements would be used by AI capabilities actors to increase capabilities and not make other informed choices.  \n          \n        Claim: I think this is unlikely to be the net outcome. This would require these organizations to become smarter about making AIs, but not much smarter about making them safe, including for their own benefit. It would also require the other groups that could block them to become smarter - some of these groups include other non-LLM companies and other highly motivated actors who wouldn't want AI to go poorly.  \n          \n        One crux here is if the AI developers will both be much better at integrating AI epistemic improvements in their internal workflows, and if these systems won't convince or help them with AI alignment. This could be a really bad outcome, so it's important to continue to pay attention to.\n    2.  **Malicious Governments**  \n        Governments like China or North Korea could use better epistemic systems to better control their populations. My guess is that great epistemic systems will likely do some damage here. I think that overall the improvements (like improving western governments) will more than outweigh that, but this is something to pay attention to.  \n          \n        Overall, I don’t see reasons to think that better epistemics will dramatically change existing power balances in ways that are yet predictable. I think this is true for most technologies I could imagine, even many of the most beneficial ones.\n    3.  **Malicious Corporations**   \n        There are some situations where companies could be far better at forecasting than consumers would be, leading to a large power imbalance. If companies have much more information about customers than users have about themselves or these companies, that could lead to them being taken advantage of. It seems important to use governments (hopefully with better epistemic infrastructure) to take active steps against this. It seems important here to work to make sure that epistemic advances get distributed equitably.\n3.  **Intelligent systems lead to more complexity, leading to a less stable world**  \n    Arguably as humans become more intelligent, human civilization becomes more complex. Complexity has a cost, and at some points, these costs can become detrimental.  \n      \n    As you might expect, I think this is a _predictable_ disaster, meaning that better prediction abilities would help here. I’d expect agents to be able to make decisions about one step at a time, and for each, see if the benefits are worth the costs. In addition, forecasting will make many things simpler and more handleable. See sources like [Thinking Like a State](https://en.wikipedia.org/wiki/Seeing_Like_a_State) to understand ways that power structures actively force systems to become simpler - sometimes even _too_ much so.\n4.  **There's too much confusion over the terminology and boundaries, so research is stunted**  \n    \"_Epistemics_\" is a poorly understood term. \"_Judgemental Forecasting_\" is still very niche. The term \"forecasting\" is arguably too generic. It seems very possible that this area will struggle in finding agreed-upon and popular framings and terminology, which could contribute to it be neglected. I think more terminological and clarification work here could be valuable. This is similar to the idea of [Distillation and Research Debt.](https://distill.pub/2017/research-debt/) \n\n## 3\\. Viability\n\n**Claim 1: All/Almost all parts of a conventional epistemic process can be automated with LLMs.**\n\nLLMs can:\n\n1.  Generate forecasting questions\n2.  Resolve forecasting questions\n3.  Gather data from computers\n4.  Write specific questions to expert - a la interviews\n5.  Make forecasts\n6.  Make visualizations and dashboards of forecasts\n7.  Personalize information for specific people\n\n_Note: “Can be automated” doesn’t mean “easy to automate in a cost-effective or high-quality way.”_  \n \n\n**Claim 2: A lot of communication is low-entropy, especially with knowledge of the author.**\n\n*   Low-entropy means “_predictable by a decent AI agent, knowing information about the author’s history_”.\n*   Evidence: We currently have systems that predict what people will say in very limited extents - i.e. for email. These typically don’t have much information about the user. I expect this sort of thing will become more widely integrated and capable.\n*   Evidence: When _I_ write, I feel like much of my communication is highly predictable. I’m able to use Claude to do a lot for me. It’s not there yet - but the setup now is incredibly basic (it doesn’t even have my historic writing), and the tech is improving.\n*   At some point, we could make decent approximations of what many people believe on many topics. This could help us greatly speed up communication - AI systems would just need the pieces they can’t predict.   \n     \n\n**Claim 3: LLM-epistemics should be easier to study and optimize than human-epistemics.**  \n  \nThey can be:\n\n*   Reproducible (models don’t change over time)\n*   Versioned (if updates are made to an LLM, this new LLM can be separately studied)\n*   Pure (similar inputs, with seeds, produce the same outputs)\n*   Scalable (much easier for 10 researchers to study one LLM in parallel, than studying one human. Can also reuse the same exact LLM in many places, or researchers could run 500 LLM runs in parallel.)\n*   Investigatable (People have privacy concerns. LLMs don’t, so we don’t need to be cautious about making them feel bad with public evaluations)\n*   Modifiable (We can’t freely change parts of most people’s knowledge or abilities, but we can with LLMs.\n    *   We can experiment with a large number of isolated changes.\n    *   We can’t wipe a person’s memory in order to backtest a human-based epistemic process, but we (partially) do this for LLMs, by providing them access to specific data sets.\n    *   Related point: One challenge to note though is that the training data is particularly optimized for immediate times after the LLM was trained. We should probably only test LLMs in forecasting scenarios after they have been trained, and assume that their quality will degrade for forecasts they make far out from their training time. That said, this still is overall far more standardizable than with humans, which are typically our main alternative.   \n          \n        Add to that, that LLMs are often cheaper and better than many humans, and overall, these systems are hard to ignore.\n*   Controlled/limited. For example, if a question about \"Will person X be assassinated?\" were posted to a liquid prediction market, it could encourage someone to actually attack that person. (See: [assassination markets](https://en.wikipedia.org/wiki/Assassination_market)). However, if someone were to query their own AI forecasting system that question, they would hopefully have strong assurances that the AI won't be taking actions in the world. (Obviously, if the AI is not controlled, this could go poorly, but that is an extreme scenario.)\n\nEvidence:\n\n*   LLM R&D has dramatically risen recently and has resulted in major gains (>$2T) to company valuations. Human R&D around epistemics and reasoning has had nothing like this recently.\n*   We have far more studies and data about LLM performance than we do about important scientists, intellectuals, or decision-makers. There are of course some human studies (a la the Good Judgement Project), but these are expensive and thus few and very narrow.\n\nSubclaim: Even if we just wanted to understand ideal human epistemics better, one reasonable method might be to make strong LLM systems and study them, because these are much easier to study, and will likely lead to some generalizable findings.\n\n**Claim 4: LLM epistemic integration will be gradual.**  \nPractically speaking LLMs and Humans will work together side-by-side in epistemic processes for some time. I expect LLMs-epistemics to follow a similar trajectory to self-driving. First, it starts with light augmentation. Then it moves to “fully automated in most use cases, but occasionally needs oversight.” As with autonomous driving, Level 5 is very difficult.\n\n*   Subpoint: Don’t ask “_When will LLMs be better than humans at \\[forecasting|writing|research\\],_” Ask questions more like, “_what percentage of work done in \\[forecasting|writing|research\\] be automated_”, or, “_what ratio of Manifold Karma will be won by bots vs. humans vs. hybrids, over time?_” I’d expect to see gradual gains here.\n\nSubclaim: I expect that LLM-based forecasters will gradually dominate open platforms like Manifold and Metaculus. As this happens, it might scare off many of the human forecasters. If this is not handled gracefully, it could lead to these platforms feeling like ghost towns and becoming irrelevant, in favor of people using more direct AI systems.\n\n## 4\\. Scaffolding\n\nPrevious LLM-based forecasting systems have included some amounts of scaffolding.\n\n*   [FutureSearch.ai](http://futuresearch.ai)\n*   [Approaching Human-Level Forecasting with Language Models](https://arxiv.org/abs/2402.18563)\n\n  \n**Claim 1: LLM-based epistemic systems will likely require a lot of scaffolding, especially as they get ambitious**.  \nI.E. software, databases, custom web applications, custom batch job pipelines, etc. \n\n  \n**Claim 2: Scaffolding-heavy systems have significant downsides.**\n\n*   Difficult to study. Unlike single LLMs, scaffolded systems can feature:\n    *   High costs of deployment.\n    *   No/little ability to roll-back.\n    *   Frequent changes.\n    *   Internal substructures that are highly variable between different implementations.\n    *   External-facing APIs that are very different between different implementations.\n    *   Evidence: The previously mentioned LLM-based systems have some of these failures - and these are simple systems compared to what we might have later.\n*   Subclaim: If scaffolding is to be expected / needed, it could be important to plan for research measures that take this into account. This will require specialized attention.\n*   Subclaim: It seems important for there to be some “portable/packaged” LEPs, that could easily be shared, versioned, deployed, etc. This would be useful for research and benchmarks/baselines.\n\n**Claim 3: Scaffolding-heavy systems are likely to be centralized.**\n\n*   In general, complex web projects are centralized.\n*   This means that scaffolding-heavy setups will favor startups and in-house teams.\n*   This means that it's easy for many of these systems to be useless. Users might only interact with 1-2 systems, so any smaller competitors might get totally ignored, even if they have a few unusually great features.\n*   It’s possible that there could be some clean lines of separation between different components. Some lines seem simple to try out, but experimentation and thinking is needed to develop a better idea. Thinking/planning here seems good.\n    *   The more it’s split up into understood components, the easier it will be to research, optimize, and proliferate each part.\n\n**Claim 4: Scaffolding-heavy systems are expensive, and thus difficult to incentivize people to build.**\n\n*   Most forecasting competitions now pay out fairly small amounts. This makes it difficult to incentivize scaffolding.\n*   I don’t expect people to independently create great scaffolding in order to win <$50k.\n    *   My impression is that very little forecasting on Metaculus and Manifold is automated with LLMs. (There is one recent Metaculus competition here, but this is an isolated and forced use.)\n*   It’s also difficult to share and maintain scaffolding. You need to make custom web UIs, you might have to set up billing infrastructure for payments, etc. In a very large forecasting ecosystem with a lot of money, forecasters would subscribe to scaffolding systems, in the same way the professional traders subscribe to trading software and services.\n*   This means that we might well see slower progress here / less diversity.   \n     \n\n**Claim 5: New technologies or startups could greatly change and improve scaffolding.**\n\nRight now there are some startups and open-source frameworks to help with general-purpose scaffolding, but these are [known to be lacking](https://news.ycombinator.com/item?id=40739982). It's possible that much better systems will be developed in the future, especially because there's a lot of money in the space.\n\nIf this happens, that could hypothetically make powerful LLM epistemic tools dramatically easier to create.\n\nOne annoying challenge is that change in the field could lessen the importance of early experimentation. It could be the case that early projects focus heavily on some specific scaffolding frameworks, only for other frameworks to become more powerful later.\n\nA possible successful world could be one where different teams make \"[GPTs](https://chatgpt.com/gpts)\" that can integrate nicely with other workflows and interact with each other. Ideally these wouldn't be restricted to any one platform.  \n  \n**Crux: Scaffolding vs. Direct LLM Improvements** \n\nOne risk around scaffolding is that a lot of key functionality that scaffolding would be used for might be implemented directly in the inference of LLMs. For example, long-term planning could be done simply with scaffolding, but AI developers are also iterating on implementing it directly into the LLM layer. \n\nAnother example is that of editing text. If you have a long text document and want to use an LLM to make a few edits, it would currently attempt to rewrite the entire document, at a fairly slow speed. This could be improved with scaffolding by instead asking it to write a git diff and then applying that using traditional code. Or it could be solved on the inference layer directly, [as discussed here](https://www.cursor.com/blog/instant-apply). \n\nIf it is the case that most future improvements are done directly with the LLMs, then that could significantly change what work will be valuable. For example, if there are only a few leading LLM developers, then it might ultimately be up to them to actually add any functionality.  \n\n## 5\\. Subcomponents\n\nAs stated above, it’s hard to determine the best subdivisions of LEPs / LLM-Epistemic Scaffolding.\n\nHere’s one attempt, to provide a simple picture. \n\n**1\\. Data-collection**\n\n*   Many LLM services are already coming out that can help quickly put data, including private data, together, for different needs. See Apple Intelligence as one service that would work for people’s personal data.\n*   There are hundreds of startups in the data-collecting and data-management space. This problem can easily be split up more.\n*   Example performance question:  \n    _“Provided with resources R, gather as much data as you can, from this list of desired data, about X.”_  \n    The results of this will likely be obvious (it’s likely to be clear what data is accessible), so scientific testing might not be needed.   \n     \n\n**2\\. World Model**\n\n*   “World model” here likely means, “A set of stored symbolic and probabilistic assumptions about the world / people / organizations.”\n*   World models will likely include software programs and writing, instead of only relying on existing LLMs.\n    *   I expect that there could be workflows to LEPs to contribute to world models that will later give it better forecasting / reasoning abilities. The forecasting components could build the world models.\n*   Existing online data has very minimal attempts at providing accessible world models. These would ideally be probabilistic, immutable, shareable, collaborative, etc.\n*   Tools like [Squiggle Hub](https://squigglehub.org) are meant to be used here. Formal ontologies and knowledge graphs are likely to be useful.\n*   Some world models could be directly interesting to humans, for the presentation layer. Others might primarily be used by LLMs. For these, the world model is a form of a cache / collaboration.\n*   It seems possible/likely that world models won’t be unique to LEPs. Instead, many separate LEPs can use the same world models.\n*   World models could contain (or complete be) text descriptions of outcomes. For example, 10 compressed pages explaining how the next 10 years will go for a specific region.\n    *   It’s not clear yet what the best representations are for LLMs to store and fetch data about its understanding of specific things.\n*   Example performance question:  \n    “_Given a forecasting LLM L, and forecasting question Q, contribute a world model that will result in L being highly accurate_.”  \n     \n\n**3\\. Human Elicitation**\n\n*   It’s often useful to just go out and directly ask questions to people, or give them surveys. Strong LEPs should ideally be able to design these procedures to maximize information value, then execute them autonomously.\n    *   A system might realize that it would be useful to get a very specific expert to give their take on something. It could then email them, and perhaps offer a financial reward.\n    *   A different system might generate a bunch of specific binary questions for a group of people it wants to elicit information from. Maybe it’s trying to estimate aspects of their preferences, and information would help.\n*   Example performance question:  \n    _“Given that you want to get as much useful new data about topic X, using existing research Y, use resources R to send off elicitions and gather data from humans.”_\n\n**4\\. Data Prioritization**\n\n*   The task here is, “_Given the potential to fetch from data source D, and resources R, fetch the most useful data for providing marginal insight on unknown variables V._”\n*   Summarizing and displaying such work could also be useful. But doing so for humans would be very different to doing so for other LLMs.\n*   It seems like with LLMs that are capable of providing quick and accurate forecasts, we could use reinforcement learning and similar algorithms to do a good job here. For any set of data, we could quickly measure the improvement in accuracy using that data.\n\n**5\\. Numeric Modeling / Fermi Calculations**\n\n*   Make great complex Python/Excel/Squiggle models/estimates.\n*   One reasonable approach to fermi estimation is “Try a bunch of very different approaches, then mix the answers together.”\n*   We’ve seen a lot of evidence that LLMs can generate working Python/Squiggle code. However, I don’t believe that there has been much optimization in this area yet.\n*   It seems easy to iterate on, and generate, great data of strong examples of this. That could be then used for LLM training / fine-tuning.\n*   Example performance question:  \n    _“Estimate variable V as well as you can, using up to N LLM calls, and no other calls.”_  \n     \n\n**6\\. Decomposition / Amplification**\n\nThere are many potential strategies to \"break a problem down\" and solve different parts with different threads of LLMs. \n\nFor example:\n\n*   Chain-of-Thought prompting\n*   Mixture of experts\n*   Ensemble Averaging\n*   Debate (i.e. [Safety via Debate](https://arxiv.org/abs/1805.00899))\n*   Fact verification\n\nArguably, numeric modeling is a form of decomposition/amplification, I just separated it to draw attention to its uniqueness.\n\nThis is an active and popular area of ML research. Some of these techniques are incorporated in the LLM inference layer, but some are also used in scaffolding.\n\nI'm sure there's a lot more work to do here. It's less clear where smaller nonprofit research can be most effective, but I suspect that there are some good options. \n\nMany methods might be unique to forecasting or specific epistemic workflows. For example:\n\n*   Try to identify key cruxes or crucial considerations, and then investigate them\n*   Automated debate mapping\n*   Identify [Scissor statements](https://slatestarcodex.com/2018/10/30/sort-by-controversial/), in part to help personalize research\n*   Use [discernment](https://www.lesswrong.com/posts/W2iwHXF9iBg4kmyq6/the-practice-and-virtue-of-discernment) to clarify and decompose key terminology\n*   Breaking down forecasting questions into sub-questions, perhaps with bayesian networks. \n\nArguably, the rationality and effective altruist communities might be very well suited to work in this area. There's been a lot of writing on how to systematically break hard problems down, now people would need to experiment with encoding these systems into automated workflows.   \n  \n**7\\. Question Generation** \n\n*   Come up with questions to be researched or forecasted on.\n*   This might well be done by other listed components - for example, it could simply combine \"Idea Generation\" with \"Forecasting\" to come up with potential questions, and guess which will be the most important and tractable. It also might be better done under the \"Presentation\" question. However, I suspect that there will be unique research and development for this problem, especially because it's a major part of existing forecasting systems.\n*   Example performance question:  \n    _“Write 5 forecasting questions on topic X. After this, a separate epistemic system will be run with a fixed number of resources R on each question. Choose questions such that the final results are maximally useful to end-users.\"_  \n     \n\n  \n**8\\. Forecasting**\n\n*   Convert fetched data into a forecast.\n*   Can call the “Numeric Modeling” module, or be merged with it.\n*   There’s been previous work here that is promising.\n*   Fine-tuning and prompt engineering seems like it would go a long way.\n*   Current LLMs are known to be biased and often suffer from poor calibration, in part in order to become better liked by human users. We might well want specialized systems optimized for pure accuracy.\n    *   This delta might cause problems. Perhaps it could lead to bad news stories (where a model used for forecasting gives a politically-incorrect answer), or perhaps it could be used by bad actors.\n    *   This area should be thus treated with care. Perhaps some models are only available to certain researchers, or perhaps there should be large disclaimers provided for regular people attempting to use them directly.\n*   Forecasting can output high-level stories, probabilities, scorable functions, or other. There are many potential formats here.\n*   Example performance question:  \n    _“Using information I, and N LLM calls, make your best forecast on variable V. You may use the Numeric Modeling module when doing so, but this will eat into the LLM calls.”_  \n     \n\n**9\\. Forecasting Resolution**\n\n*   Most forecasting questions should be resolvable automatically. This can be done by either querying external data, or perhaps by asking a person/LLM for their opinion.\n*   This component is likely to often just either be, or defer to, other LEP modules.\n*   Question Creation and Optimization\n    *   A strong LEP would make good judgment calls on what research or forecasting topics would be valuable to people, and initialize those.\n    *   This could range from “once a user logs in, generates custom information for them to maximize their EV”, to, “write a list of standard forecasting questions that are collectively high-EV to a subpopulation.”\n    *   Example performance question:  \n        _“Using an existing LEP L, and resources R, write a set of forecasting questions. Expect that forecasting resources F will be used on these questions, and you are trying to optimize utility function U.”_  \n         \n\n**10\\. Presentation**\n\n*   Many custom charts and graphics would likely be useful.\n*   Decisions need to be made about what final data should be shown to users and how.\n*   Automated decision-making: In some cases / eventually, LEPs would interact directly with decision systems. In theory, people don’t need to know almost anything - critical information could be used directly by in-world automations. For example, instead of an LEP doing research into the costs and benefits of vitamin D supplementation and showing you that research, a system instead just starts an order for you to get a regular set of supplements. This could come with optional research, but in many cases the research won’t be reviewed, if the system is trusted.\n*   In cases where LEPs are used behind APIs, the presentation layer could include answering questions about what the LEP could do, and suggesting specific uses for it.\n*   Presentation/wording should be optimized to be understandable to humans. There should be tests of the error rates of what was intended to be stated, and what humans understood. For example, many people have significant misunderstandings about basic statistics concepts, so probabilities might need to be handled with care.\n*   Example performance questions:\n    *   _“Using LEP output O, design a dashboard that user U will most prefer.”_\n    *   _“Using LEP output O, design a set of complicated diagrams without detectable errors.” (judged by an LLM)._   \n         \n\n**11\\. Distribution**\n\n*   Full custom user interfaces could be created. This might range from a simple custom LLM chat, to a auto-generated set of dashboards, with the ability to delve deeper into any topic the user clicks on.\n*   LEPs could be accessed by any other chat LLM interface, as long as that interface can call external APIs. This could be generally useful for integrating into other AI tools and workflows, perhaps via other common LLM chat UIs.\n*   LEP information could be distributed like how human communication is communicated. Text messages, emails, Tweets, voice assistants, etc. Ideally the LEP could vaguely predict what information is useful to people and when, and then send them that information in those cases.\n    *   Example: You get an email in the morning. It says, _“Some news came out at 6am today that changed my recommendation of your financial assets. I recommend you change to a different mutual fund. I estimate that doing so will net you an average of $50/month.”_\n*   Example performance questions:\n    *   _“Using LEP output O, design and implement the best way to get this information to agent A.” We can use an LLM that mimics human behaviors to evaluate._  \n         \n\n**12\\. Idea Generation / Optimization**\n\n*   It will often be useful to generate ideas on the best ways to do something.\n*   For example, an LLM agent might want to make a suggestion to a user, before doing forecasting on said suggestion.\n*   Regular LLM performance might be fine on this, but fine-tuning or similar might help. It might be the case that there are certain specific domains where idea generation is very useful.\n*   “Idea Generation” is related to [program optimization](https://en.wikipedia.org/wiki/Program_optimization) and other CS topics.\n*   Example performance questions:\n    *   _“Given a person with details D, generate a plan for that person to optimize their financial future.”_ (This will be then judged with an LEP / forecasting module)\n    *   _“Given a 2d grid game G, come up with 10 ordered ideas for the best strategy”_ (Can be judged empirically)  \n         \n\n**13\\. Task Orchestration**\n\n*   We need systems that can actually trigger all the other components when needed. This process needs to be debuggable / introspectable. Ideally, these workflows would be highly understandable.   \n     \n\n## 6\\.  Benchmarks & Baselines\n\n**"
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Reference Class Forecasting Applications",
          "description": "The author's suggestion that LEPs could serve as baselines or priors for other forecasting systems is a powerful application of reference class forecasting. This approach would provide standardized reference points against which to compare new forecasts, potentially improving calibration across the board. Quantifying the expected improvement in forecasting accuracy from such baselines would be valuable.",
          "importance": "85",
          "highlight": {
            "startOffset": 36732,
            "endOffset": 38166,
            "quotedText": "Claim 2: Portable LEPs could be useful for acting as priors or baselines, for other systems to perform against.**\n\n*   We will want some baseline LEPs to score other LEPs against. These should act on the same data, at the same times.\n    *   For example, one could take a new LEP, a well-known Baseline LEP, and a known or generated question set. Both systems would forecast those questions, and then the new LEP will be evaluated based on how much it outperforms the Baseline LEP. For example, with the average difference in brier score.\n    *   In order to do this, the LEP must be equipped with the abilities to fetch data and to forecast. This is useful in order to compare the system to human forecasters (which do both), but it might be often better to divide the subcomponents and study/optimize them in isolation.\n    *   Another use is to use Baseline LEP to initialize forecasting markets. Right now manifold questions begin at 50%. It could be better for them to instead begin at a number suggested by a simple LEP.\n    *   Even if these LEPs aren't highly accurate, they could be useful for sanity checking claims outside of forecasting systems. For example, all publicly given estimates and statements could be run against LEPs. This can then highlight estimates that are particularly surprising. In financial / numeric models, all inputs and assumptions can be checked against LEPs to find ones that might be suspicious."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Forecasting Question Resolution Automation",
          "description": "The potential for automating forecasting question resolution with standardized LEPs represents a significant advancement in forecasting infrastructure. From a quantitative perspective, we could estimate the percentage of currently manual resolutions that could be automated, along with the expected reduction in resolution disputes. This would quantify the efficiency gains from such systems.",
          "importance": "78",
          "highlight": {
            "startOffset": 38170,
            "endOffset": 39331,
            "quotedText": "Claim 3: Standardized LEPs could be great judges to resolve subjective questions**\n\n*   It’s well-known that it’s difficult to use partially-subjective questions in prediction markets. Much of the reason is because the only way to resolve these is with human expert judges, and these human judges have eccentricities that are hard to predict. AI systems might help here, because they can be far more standardized and testable than human systems.\n*   For example, say there’s a question, “_Will project X generally be considered a failure in 2026?_” This can be changed to, “_Will project X be considered a failure, in an average of 20 runs, by a certain LEP, in 2026?_” Then, predictors could provide this LEP fake hypothetical information on the future and see how this question would resolve with that information. There should be very little surprises from the eventual output of the LEP.\n*   Related, LEPs could be used to provide independent resolutions for every other forecasting question on Metaculus/Manifold/etc. There would be a certain expected error rate - if some resolution judge statistically exceeds this rate, that would be flagged for review."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Time-Based Degradation of Forecasting Accuracy",
          "description": "The author identifies a crucial challenge for LLM forecasting: performance degradation as the time gap increases between training data and forecast target. This is a quantifiable phenomenon that could be modeled with decay functions. Establishing these decay rates across different domains would be valuable for calibrating long-term forecasts and determining when model retraining is necessary.",
          "importance": "82",
          "highlight": {
            "startOffset": 40452,
            "endOffset": 41045,
            "quotedText": "Challenge: LLM-forecasting abilities will vary based on date.**\n\n*   The quality of LLM predictions is likely to go down, the further away the forecasting question resolution is from the LLM training date. This is a major challenge to using them as a benchmark. For example, GPT4 might be decent at guessing the US GDP in 2025 if used in 2024 with search capabilities (i.e. web search). But if it were used in 2030, for the US GDP in 2031, with search capabilities but no other training updates, its performance would likely be worse. It's background/training data would be much less relevant."
          },
          "isValid": true
        }
      ]
    }
  ]
}
