{
  "id": "goodhart-typology-via-structure-function-and-randomness-distributions-lesswrong",
  "slug": "goodhart-typology-via-structure-function-and-randomness-distributions-lesswrong",
  "title": "Goodhart Typology via Structure, Function, and Randomness Distributions — LessWrong",
  "author": "JustinShovelain",
  "publishedDate": "2025-03-25T16:01:08.327Z",
  "url": "https://www.lesswrong.com/posts/TYgztDNXhobbqMpXh/goodhart-typology-via-structure-function-and-randomness",
  "intendedAgents": [
    "clarity-coach",
    "research-scholar",
    "quantitative-forecaster",
    "ea-impact-evaluator",
    "fake-eliezer"
  ],
  "platforms": ["LessWrong"],

  "content": "_(Work done at Convergence Analysis. The ideas are due to Justin. Mateusz wrote the post. Thanks to Olga Babeeva for feedback on this post.)_\n\nIn this post, we introduce the typology of structure, function, and randomness that builds on the framework introduced in the post [_Goodhart's Law Causal Diagrams_](https://www.lesswrong.com/posts/e4SMfYWb4Tz568yh6/goodhart-s-law-causal-diagrams). We aim to present a comprehensive categorization of the causes of Goodhart's problems.\n\nBut first, why do we care about this? \n\n## Goodhart's Law recap\n\nThe standard definition of [Goodhart's Law](https://www.lesswrong.com/w/goodhart-s-law) is: \"when a proxy for some value becomes the target of optimization pressure, the proxy will cease to be a good proxy.\".\n\nMore specifically: we see a meaningful statistical relationship between the values of two random variables I and T,[\\[1\\]](#fnodu9li5hnrr) where we have some preferences over the values of T. We decide to change the value of I, expecting that the value of T will change as would be expected by naively extrapolating the statistical tendencies of the two variables. We fall victim to Goodhart's Law (i.e. a Goodhart failure occurs) if the change in T doesn't quite meet our expectations and, upon learning that, we see that we should have intervened in some different way.\n\n## Some motivation\n\nMany alignment failures can be cast in terms [of Goodhart's Law](https://www.lesswrong.com/w/goodhart-s-law). Specifically, the AI is given an objective that doesn't fully specify what we want it to optimize for ([outer alignment](https://www.lesswrong.com/tag/outer-alignment)), or it finds a way to exploit the misspecification of the training goal, developing an internal objective that is misaligned with the one we specified ([inner alignment](https://www.lesswrong.com/tag/inner-alignment) / [mesaoptimization](https://www.lesswrong.com/tag/mesa-optimization)). Therefore, a better understanding of Goodhart's Law — its various subtypes, causes, and possible remedies — might better inform us about how to effectively anticipate and prevent (or at least mitigate) those failure modes. In addition to technical AI alignment/safety, a better understanding of Goodhart's Law might shed some light on enhancing human coordination, given that the same outer and inner alignment problems (or [close analogs thereof](https://en.wikipedia.org/wiki/Principal%E2%80%93agent_problem)) recur in coordination structures. In particular, effective AI governance will most likely require specifications of regulations that are robust to goodharting.\n\n# Introduction\n\nIn the [previous post](https://www.lesswrong.com/posts/e4SMfYWb4Tz568yh6/goodhart-s-law-causal-diagrams), we presented how causal diagrams can be used to describe situations vulnerable to Goodhart's problems. In this post, we build on that framework to present a complete categorization of possible causes of Goodhart's problems.[\\[2\\]](#fnlsibvjne94a)\n\nThe key principle to hold in the back of one's mind is the following.\n\n_All Goodhart problems stem from misunderstanding the causal relationships between the intervention, target, or measurement variables._\n\nMore specifically, if you know all relevant facts about the relationship between\n\n1\\. your interventions (I),\n\n2\\. the target variable you want to optimize (T), and\n\n3\\. your measurements of the target variable (M),\n\nthen, in principle, you know enough to infer what interventions are optimal for maximizing the expected value of the target variable (according to your preference ordering, whatever it is).\n\nThe only reasons expected utility maximization can encounter Goodhart problems are ignorance (e.g., not knowing the right method for inferring the right course of action in a given circumstance) and computational limitations.\n\nHere, we present a framework flexible enough to cover all the Goodhart-related failure modes in terms of mistaken beliefs about the causal diagram. We discuss various examples throughout the post.\n\nBy this motivation, we are casting all categories of Goodhart problems as mistakes of inference under uncertainty, where the ultimate object of inference is the choice of action and the intermediate object of inference is the causal diagram.\n\nWe propose a classification of causes of Goodhart failures into three categories, representing which **aspect of the causal diagram** the agent failed on.\n\n1.  **Structure.** The agent is mistaken about the **causal** **structure** mediating between the intervention variable I, the target variable T, and the measure variable M.\n2.  **Function.** The agent is mistaken about the **functional** relationship defining the interactions between the intervention variable, the target variable, and the measure variable.\n3.  **Randomness.** The agent is mistaken about the **random distribution** of the auxiliary random variables in the graph.\n\nIf we think of an idealized process of learning the causal diagram, we can think about learning these three aspects one after another and identifying which link in the chain failed.\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/TYgztDNXhobbqMpXh/qjozlv0z0ejyk2oq8yix)\n\nAnother dimension worth keeping in mind describes the **reason the agent failed on this aspect of the causal diagram**. It is not the main focus of this post but it is useful to keep in mind as it might be relevant for finding strategies to mitigate Goodhart's problems.\n\n1.  **Learning failure.** The agent has learned a causal structure that was **wrong already at the time of learning**.\n2.  **Transfer failure.** The agent has learned a causal structure that was right at the time of learning but became **\"deprecated\"** or **\"outdated\"** by the time of intervention, i.e. it failed to **transfer** (e.g. the environment changed or some processes optimized against the agent).\n3.  **Non-interference failure.** The agent has learned a causal structure that would accurately describe the relationships between variables at the time of intervention but the agent **failed to model the impact of its actions on the causal structure**. The agent's intervention _appears to have_ broken the causal structure because the actions' effects also flowed through \"invisible side channels\" that the learned causal diagram didn't account for.[\\[3\\]](#fnjsp8hyd7g5c)\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/TYgztDNXhobbqMpXh/r9caxd66v1hh3feb0yrp)\n\n           **Cause of failure**  \n  \n**Failed aspect**\n\n**Learning failure**\n\n**Transfer failure**\n\n**Non-interference failure**\n\n**Structure**\n\nLearned wrong causal structure\n\nLearned correct causal structure but it failed to transfer to intervention time\n\nLearned correct causal structure that transferred to intervention time but intervention broke alignment between the causal structure and its representation\n\n**Function**\n\nLearned wrong functional dependencies\n\nLearned correct functional dependencies but they failed to transfer to intervention time\n\nLearned correct functional dependencies that transferred to intervention time but intervention broke alignment between the functional dependencies and their representation\n\n**Randomness**\n\nLearned a wrong distribution of random components\n\nLearned correct distribution of random components but it failed to transfer to intervention time\n\nLearned correct distribution of random components that transferred to intervention time but intervention broke alignment between the distribution of random components and its representation\n\nAdditionally, we might want to distinguish between failures due to **insufficient knowledge** and those due to **constraints on resources** necessary for computing the impact of an action (time, space, algorithms). However, given that we assume knowledge results from inference anyway, these two categories blur into one. Therefore, this distinction is irrelevant in this post.\n\nFor simplicity, this post focuses on simplified situations where learning time and intervention time are distinct and learning proceeds only by observation, not experimentation. Situations involving continuous learning are naturally much more complex but we expect the concepts and lenses from this post to generalize to them.\n\n# Ontology\n\n## Causal diagrams (re-)introduced\n\n[_Goodhart's Law Causal Diagrams_](https://www.lesswrong.com/posts/e4SMfYWb4Tz568yh6/goodhart-s-law-causal-diagrams) introduced causal diagrams as a framework for thinking about Goodhart's Law but did not treat causal diagrams rigorously. Here, we introduce causal diagrams formally.\n\nThe purpose of causal diagrams is to comprehensively model the dependencies between random variables. Their primary use case is to determine how specific interventions on the causal structure will influence our variable of interest.\n\nA causal diagram (CD) is a special kind of [directed acyclic graph](https://en.wikipedia.org/wiki/Directed_acyclic_graph) (DAG). Its nodes represent random variables and its edges represent causal relationships between the random variables. The existence of an edge Xe→Y means \"the value of X (co-)determines the value of Y. The value of the random variable Y is _fully_ determined by a function fY:(X1×⋯×Xn×UY)→Y where X1 through Xn are the parents of Y and UY is its random auxiliary variable, representing a random component of Y (\"U\" for \"unknown\"). In other words, the value y∈Y is determined by a function y\\=f(x1,...,xn,u) with u∼UY (slight abuse of notation: we use UY to denote both the random variable and its distribution).\n\nIn a CD, auxiliary variables stand for irreducible sources of randomness and are the only sources of probabilistic uncertainty. An auxiliary variable can have any random distribution. Any probabilistic dependence between random variables can be neatly modeled using that kind of mix of deterministic functions and random distributions. Every random variable X in a CD has exactly one auxiliary variable UX. Therefore, orphans (nodes without parents) in a CD, are exactly auxiliary variables and if a proper random variable has no parents except its auxiliary variable, its value is fully determined by the value of its auxiliary variable.[\\[4\\]](#fnltspxkoyjfq)\n\nWe have just introduced causal diagrams in the following order:\n\n1.  **Structure** — Nodes represent random variables, and edges represent causal relationships between them.\n2.  **Function** — The value of a node Y is determined by a function fY:(X1×⋯×Xn×UY)→Y from all the nodes that send an edge to it…\n3.  **Randomness** — … and a random component UY, AKA the auxiliary variable of Y.\n\nThe three \"steps\" provide a handy model for thinking about the process of an agent learning a causal diagram, as each step increases the resolution to what has been achieved after the previous one.[\\[5\\]](#fntsz9rcwvv1k) Another reason to think about them in this exact sequence is that to get the function aspect right, we first need to get the structure aspect right (basically by definition); and, similarly, to get the randomness aspect right, we need to get the function aspect right first (this is not strictly true but approximately true). This motivates the categorization of errors in learning and using causal diagrams into errors of structure, function, and randomness, which we describe in detail in Section 2. \n\nFirst, however, we need to describe how intervening on a causal diagram works, and for that purpose, we need to describe how we're going to use causal diagrams to think about the relationship between the intervention, target, and measure variables.\n\nImportantly, for the causal diagram to be useful in practice, it needs to be repeatable, i.e. it should retain its applicability to a given kind of situation over several iterations, rather than being useful only for a single \"rounds\" after which it can no longer be used. For example, if we want to use a CD to predict the effect of a policy on the economy, we would like it to still be useful after we execute the intervention that we deem the most desirable based on the CD.\n\n## Intervention, target, and measure\n\nLet's start with a diagram:\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/TYgztDNXhobbqMpXh/w490x0hbj1xbmrebibwi)\n\nSo, we have three variables of interest. T is the **target** variable that we want to optimize. I is the **intervention** variable that we are influencing to optimize the target T. In general, we may not have direct access to the value of the target and can only read it through M, the **measure** variable. Since we want I to influence​​​​​​​ T and we want T to influence M, I needs to be (causally) upstream of T whereas T needs to be (causally) upstream of M: I→T→M.\n\nTherefore, we need to account for two causal relationships, that of I on T and that of T on M.[\\[6\\]](#fnx5ofdskcpz)\n\nIntervening on a causal diagram works by \"taking control\" over the values of one or more random variables. Here, we focus on the simplest case, where we choose one or more variables and set the value of each to some fixed value, ignoring what value (or rather, what distribution of values) they would have otherwise, given some values of their parent variables.[\\[7\\]](#fn4jv0y9po67x)\n\nThe structure of the diagram is fully deterministic, with the only free parameters being the orphan random auxiliary variables over which we have some probability distributions, and, obviously the intervention variable I (if we choose to intervene). Therefore, to determine the probability distribution over T conditional on some i∈I, we need to extend the function fT:I×XT1×⋯×XTnT×UT→T to f∗T:I×UT1×⋯×UTmT×UT→T where UT1 through UTmT are random ancestors of T.\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/TYgztDNXhobbqMpXh/etuva4h15k5yhwsnkhs7)\n\nThen, we can define the probability distribution:\n\nP(T\\=t|I\\=i)\\=∫→u∈(UT1×⋯×UTmT×UT)P(→u)⋅1\\[f∗T(i,→u)\\=t\\]\n\nThen, the expected value vT conditional on I\\=i is:\n\nE\\[vT|I\\=i\\]\\=∑t∈TvT(t)⋅P(T\\=t|I\\=i)\n\n This is also how we define the induced utility function on I:\n\nvI(i)\\=E\\[vT|I\\=i\\]\n\nAn exactly analogous situation arises between T and M. We extend fM:T×XM1×⋯×XMnM×UM→M to f∗M:T×UM1×⋯×UMmM×UM→M where UM1 through UMmM are random ancestors of M.\n\nFor the most part, we will not discuss the measurement variable M explicitly. However, it is relevant to keep this in mind because the information about the dependence between I and T influences the learning of the causal diagram. It is also obviously relevant for continuous learning.\n\n## Goodhart failure\n\nGiven a utility function, vT:T→R over the target node T a Goodhart failure occurs when the change in T doesn't quite meet our expectations and, upon learning that, we see that we should have intervened in some different way.\n\nTo give a simple example: Suppose we want to maximize the number of fruits in a basket containing some number of apples, bananas, oranges, and some number of other fruits and we can choose the number of apples to be anything from 0 to 100. Our probabilistic beliefs about the numbers of bananas and oranges are represented by random variables UB and UO and there is some additional uncertainty involved in the number of fruits in the basket standing for \"other\", UF. Since our utility function is linear in the number of each type of fruit, we choose 100 apples. If the utility function were non-monotonic — e.g. vF(f)\\=−(70−f)2 with f being the number of fruits — we might need to choose something else than one hundred, depending on the details of our beliefs about bananas and oranges.\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/TYgztDNXhobbqMpXh/vmi25e7kjb8e1shvspmz)\n\n# Types of Goodhart failures\n\n## Structural errors\n\nA **structural error** occurs when the agent is mistaken about the structure of the causal diagram, i.e. nodes (random variables) and edges (causal relationships).\n\nBeing mistaken about the causal structure mediating the influence of I on T can be a source of error in deciding on the intervention (i.e. on the i∈I). However, given that the agent's assessment of which nodes are useful as intervention variables is dictated by (among other things[\\[8\\]](#fnp4outvfvrss)) the causal and functional structure between the node (potential I) and T, being mistaken about the causal structure can also lead us to choose a wrong I.\n\nThese problems can take several forms, which can also occur together, such as failing to identify all (and only) the relevant variables and missing existing causal links, including non-existent causal links.\n\nThis kind of error will occur if we learn the wrong causal structure between the measure and the target, since in such a case we may not have enough knowledge to robustly optimize the target. A similar situation occurs if we learn a causal diagram that was a good representation at the learning time but when it fails to generalize to the time of intervention because the environment has changed (for whatever reason). However, for our purpose, the distinction between mis-learning the CD and learning a correct CD that doesn't transfer into the future is not important because what we care about is learning a CD that is appropriate for enabling future optimization of T.\n\nThe more interesting (and more tricky) situation arises when the agent learns a causal structure that would be appropriate for the future situation, if not for the agent's own intervention that \"broke\" it (i.e. caused it to fail to generalize). In a strict sense, this is impossible because there is no way to change a causal diagram by intervening on its nodes. In a less strict sense, however, if the causal diagram is inappropriate, — e.g. the functional dependencies represented by some edges actually depend on the values of some nodes that haven't been included in the diagram — this may make it appear as if the causal structure is being violated.\n\n**Examples**\n\n1.  [Smoking and tobacco](https://en.wikipedia.org/wiki/Health_effects_of_tobacco#Public_awareness): \"For a particular individual, it is impossible to definitively prove a direct causal link between exposure to a radiomimetic poison such as tobacco smoke and the cancer that follows; such statements can only be made at the aggregate population level. Cigarette companies have capitalized on this philosophical objection and exploited the doubts of clinicians, who consider only individual cases, on the causal link in the stochastic expression of the toxicity as an actual disease.\".\n2.  [Amyloid beta plaques](https://en.wikipedia.org/wiki/Amyloid_plaques) associated with Alzheimer's disease were long suspected to be causally involved in the advancement of the disease. This motivated medical research to aim at developing drugs targeting those plaques. However, amyloid beta plaques may be just a symptom of the disease, and eradication of the symptom (even if successful) is not necessarily going to help with the disease (depending on the specific mechanism of symptom eradication).\n3.  [Blue zones](https://en.wikipedia.org/wiki/Blue_zone): \"A blue zone is a region in the world where people are claimed to have exceptionally long lives beyond the age of 80 due to a lifestyle combining physical activity, low stress, rich social interactions, a local whole foods diet, and low disease incidence.\" However, more recent evidence suggests that exceptional numbers of reports of centenarians and supercentenarians were caused by poor record keeping in these poor areas, allowing for fraud.[\\[9\\]](#fnlmvnnjmr97n)\n4.  [Survivorship bias](https://en.wikipedia.org/wiki/Survivorship_bias#Military): \"During World War II, the statistician Abraham Wald took survivorship bias into his calculations when considering how to minimize bomber losses to enemy fire. The Statistical Research Group (SRG) at Columbia University, which Wald was a member, examined the damage done to aircraft that had returned from missions and recommended adding armor to the areas that showed the least damage. The bullet holes in the returning aircraft represented areas where a bomber could take damage and still fly well enough to return safely to base. Therefore, Wald proposed that the Navy reinforce areas where the returning aircraft were unscathed, inferring that planes hit in those areas were the ones most likely to be lost.\"\n\n## Functional errors\n\nA **functional error** happens when the learned functional relationship between I and T is not appropriate for the intervention time.\n\nRecall that we started with a utility function over the value set of the target node T and a function specifying how the value of T depends on the variables upstream from it. Then we used that to find an appropriate intervention node I in the diagram and then to induce a utility function over its possible values, such that choosing the values of I to maximize vI will translate into the target value also maximizing vT its preference ordering (on average, in expectation, &c. given that we're operating in a probabilistic setting).\n\nWe might take it as our desideratum: given a utility function vT:T→R and a functional relationship fT:(I×X1×⋯×Xn×UT)→T, derive a preorder on the intervention variable vI:I→R such that maximization of the latter transfers, through fT, to maximize the former. (In the language of order theory, we can say that fT is [monotone](https://en.wikipedia.org/wiki/Monotonic_function#In_order_theory) in the argument I.[\\[10\\]](#fn5gfjqlsvu3o))\n\nA functional error occurs when this desideratum fails, i.e., optimizing I fails to translate into optimizing of T (as robustly as the agent wants). This can be either because the agent was mistaken about the function itself (e.g. neglected some non-linear interactions between the intervention value and other arguments of the function) or because it mis-inferred the preference ordering over I. (This lines up with the distinction between incomplete knowledge and insufficient computation.)\n\nImportantly, the preference ordering over I doesn't have to be pre-computed at learning time. Even if we are \"just\" inferring what interventions to make at intervention time, we are going to be comparing alternative values i∈I which amounts to computing that preference ordering locally.\n\n**Examples**\n\n1.  [**Kleiber's Law**](https://en.wikipedia.org/wiki/Kleiber%27s_law)**:** \"Kleiber's law, named after Max Kleiber for his biology work in the early 1930s, states, after many observations that, for a vast number of animals, an animal's Basal Metabolic Rate scales to the 3⁄4 power of the animal's mass. \\[...\\] Before Kleiber's observation of the 3/4 power scaling, a 2/3 power scaling was largely anticipated based on the \"surface law\", which states that the basal metabolism of animals differing in size is nearly proportional to their respective body surfaces. This surface law reasoning originated from simple geometrical considerations. As organisms increase in size, their volume (and thus mass) increases at a much faster rate than their surface area. Explanations for 2⁄3-scaling tend to assume that metabolic rates scale to avoid heat exhaustion. Because bodies lose heat passively via their surface but produce heat metabolically throughout their mass, the metabolic rate must scale in such a way as to counteract the square–cube law. Because many physiological processes, like heat loss and nutrient uptake, were believed to be dependent on the surface area of an organism, it was hypothesized that metabolic rate would scale with the 2/3 power of body mass.\"\n2.  [**Malthusianism**](https://en.wikipedia.org/wiki/Malthusianism)**:** \"Malthusianism is a theory that population growth is potentially exponential, according to the Malthusian growth model, while the growth of the food supply or other resources is linear, which eventually reduces living standards to the point of triggering a population decline. This event, called a Malthusian catastrophe (also known as a Malthusian trap, population trap, Malthusian check, Malthusian crisis, Point of Crisis, or Malthusian crunch) has been predicted to occur if population growth outpaces agricultural production, thereby causing famine or war.\"\n    1.  [Malthus's predictions were not borne out and they had not been justified at the time of his writing either](https://www.astralcodexten.com/p/your-book-review-progress-and-poverty?open=false#%C2%A7ii-population-and-subsistence).\n3.  Exposure of small children to allergens has been advised against. This turned out to have consequences exactly opposite to the ones that were desired: without allergen exposure, the immune system doesn't have an opportunity to learn to distinguish safe from non-safe food.\n4.  Before [Kepler](https://en.wikipedia.org/wiki/Kepler%27s_laws_of_planetary_motion), planets were believed to follow circular orbits and move at constant speed. Kepler revised the heliocentric model and postulated elliptic, not circular, orbits, and varying speed of motion. This was an improvement on the model of the relationship between time and position of a planet.\n5.  The relationship between alcohol and health outcomes was believed to follow a U-shaped curve with a small amount of drinking being the most optimal. After accounting for the fact that many teetotalers are people with a history of alcohol abuse, the initial segment of the curve flattens and there is no strong evidence for meaningful effects of small amounts of alcohol consumption on health outcomes either way.\n\n## Calibration errors\n\nWe might be mistaken about the nature of the random auxiliary variables in the causal diagram. This is _miscalibration_, hence we call it **calibration errors**.\n\nThe target variable depends on all the auxiliary variables that are upstream from it, except for those that route only through the intervention variable if it is being intervened on.\n\nMiscalibration about the distribution of any of the auxiliary variables might (depending on other details of the causal diagram) have large consequences for how the expected results of an intervention diverge from its actual results.\n\nIf miscalibration pertains to a sort of additive noise on the target variable — i.e. the function fT being of the form fT(i,x1,...,xm,u)\\=f(i,x1,...,xm)+u (or something similar, with noise propagating from its ancestors) — then the result is relatively mild because it doesn't impact what interventions maximize the utility function uT. However, if the distribution is UT∼Binomial(p\\=0.8) and\n\nfT(i,x1,...,xm,u)\\={1if i\\=u0otherwise\n\nthen we need to account for the distribution in our actions, i.e. always choose i\\=1. This is one simple model of how we may need to account for the random component in choosing the intervention value.\n\n**Examples**\n\n1.  The assumption that accident severities follow a thin-tailed distribution when in fact they follow a fat-tailed distribution will, in the long run, lead to inadequate preparation for the most severe accidents.\n    1.  In practice, human collectives often tend to inadequately prepare for natural disasters, pandemics, etc (some of which can be explained by over-relying on the [availability](https://www.lesswrong.com/posts/R8cpqD3NA4rZxRdQ4/availability) heuristic).\n2.  If what the agent is modeling as \"merely\" noisy/stochastic components of the system is actually adversarially selected, then interventions chosen at minimizing loss in the face of stochastic noise will likely fail in the face of adversarial noise (i.e. noise selected/optimized so as to pessimize against the agent's objectives).\n3.  Disregarding the possibility that the apparent deviation from the usual range is due to stochasticity and then taking action to bring it back to the usual range might lead the agent to conclude that its action took the desired effect even if the observation was actually due to the regression to the mean.\n\n# Potential extensions & further directions\n\n*   Extend the framework to situations of continuous and active learning.\n*   Identify more which real-life Goodhart problems arise due to mistakes of structure, function, and randomness.\n    *   Technical AI safety/alignment: reward-hacking, gradient hacking, goal misgeneralization.\n        *   How do you get the AI to (robustly) aim for the right thing and not for a bad thing?\n    *   AI governance: e.g. specification gaming by the AI labs.\n*   Can this framework throw some light on [wisdom/coherence](https://www.lesswrong.com/posts/AtwPwD6PBsqfpCsHE/aligning-ai-by-optimizing-for-wisdom)?\n*   Build an applied rationality training paradigm/technique to figure out where people are failing on this.\n\n# Appendices\n\n## Order-theoretic details\n\nGoodhart's law pertains to situations involving optimization and optimization presumes some way of specifying which possible values of the target node are better or worse, i.e. a preference ordering over T. Minimally, a preference ordering needs to be a [partial order](https://en.wikipedia.org/wiki/Partially_ordered_set) relation. However, it is most convenient when we can compare any two values and tell which one is better or worse, i.e. when the preference ordering is a [total preorder](https://ncatlab.org/nlab/show/total+preorder). However, given that we are dealing with reasoning under uncertainty and want to maximize the expectation of the preference over the target, we need to be able to trade-off between lotteries induced by different values to which we might fix the intervention value. This essentially necessitates a utility function on the target random variable:vT:T→R. (We're using v  instead of u to avoid confusion with the notation for auxiliary variables.) However, if the randomness aspect is negligible, we can do it just with a total preorder or, even weaker, a demand that any set of interventions/actions A⊂I _that the agent might encounter_, contains its own [join](https://en.wikipedia.org/wiki/Join_and_meet) (a \"locally best element\" is defined), that is: ∃a′∈A,∀a∈A,a≤a′.\n\n## Relationship to Scott Garrabrant's _Goodhart Taxonomy_\n\nIn the post [_Goodhart Taxonomy_](https://www.lesswrong.com/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy), Scott Garrabrant described four kinds of situations giving rise to Goodhart problems. They can be incorporated in our framework as follows.\n\n[**Regressional Goodhart**](https://www.lesswrong.com/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy#Regressional_Goodhart) (\"_When selecting for a proxy measure, you select not only for the true goal, but also for the difference between the proxy and the goal_.\") occurs when intervention I affects measure M not only through target T but also through a direct connection. What Scott calls \"noise\" corresponds to the direct connection. (It cannot correspond to an auxiliary random variable because those are, by definition, orphans, unaffected by the values of other variables.)\n\n[**Causal Goodhart**](https://www.lesswrong.com/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy#Causal_Goodhart) (\"_When there is a non-causal correlation between the proxy and the goal, intervening on the proxy may fail to intervene on the goal._\") occurs when we fail to understand the structure, e.g. the intervention is not causally upstream from the target.\n\n[**Extremal Goodhart**](https://www.lesswrong.com/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy#Extremal_Goodhart) (\"_Worlds in which the proxy takes an extreme value may be very different from the ordinary worlds in which the correlation between the proxy and the goal was observed._\") is a special case of functional error.\n\n[**Adversarial Goodhart**](https://www.lesswrong.com/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy#Adversarial_Goodhart) (\"_When you optimize for a proxy, you provide an incentive for adversaries to correlate their goal with your proxy, thus destroying the correlation with your goal._\") is just a special case of the environment at intervention time being different than at learning time.\n\n1.  **[^](#fnrefodu9li5hnrr)**\n    \n    The choice and meaning of these letters will be clarified soon.\n    \n2.  **[^](#fnreflsibvjne94a)**\n    \n    We use \"Goodhart's law\" for the general regularity that \"Whenever a measure becomes a target, it ceases to be a good measure.\". We use \"goodharting\" or \"Goodhart's problem(s)\" for specific instances when the correlation between an intervention variable and the target collapses when optimization pressure is placed upon the former. A Goodhart failure is a Goodhart problem left unaddressed.\n    \n3.  **[^](#fnrefjsp8hyd7g5c)**\n    \n    Possible reasons include: limited time for computation/thinking, incomplete knowledge available to the agent and the executed actions being different than intended actions (e.g. leaking to other nodes, imperfect control over the actuators).\n    \n4.  **[^](#fnrefltspxkoyjfq)**\n    \n    Every random variable needs to have a random component in order to make it possible to infer a causal diagram from the data because Pearl's paradigm can't infer causality on variables that are related deterministically. Making causal inference possible even in a fully deterministic setting was a major motivation behind Scott Garrabrant's [Factored Space Models (formerly Finite Factored Sets)](https://www.lesswrong.com/tag/finite-factored-sets).\n    \n5.  **[^](#fnreftsz9rcwvv1k)**\n    \n    Although they don't necessarily correspond to the order in which causal diagrams are learned in practical applications.\n    \n6.  **[^](#fnrefx5ofdskcpz)**\n    \n    In general, the relationship between I and T may not be direct but rather mediated by intermediate random variables, along with their ancestors and auxiliary random components. We're omitting this detail here because even then it can still be abstracted away (collapsed into) a single edge specifying with a function that determines the value of t∈T based on the value of i∈I and the values of other ancestors of the intermediate nodes.\n    \n7.  **[^](#fnref4jv0y9po67x)**\n    \n    We might also be more nuanced and modify the functional relationship between the variable and its parents or add new causal and functional dependencies. We might also be able to control the value of the node only partially, e.g. clamp not a single value but some distribution over values to the intervention node. In this post, we focus on the simplest variant involving fixing the value of a single random variable.\n    \n8.  **[^](#fnrefp4outvfvrss)**\n    \n    \"Other things\" including real-world facts about what variables we can practically intervene on.\n    \n9.  **[^](#fnreflmvnnjmr97n)**\n    \n    We know this thanks to the 2024 Ig-Nobel Laureate in demography, Saul J. Newmann: [_Supercentenarian and remarkable age records exhibit patterns indicative of clerical errors and pension fraud_](https://www.biorxiv.org/content/10.1101/704080v3.full).\n    \n10.  **[^](#fnref5gfjqlsvu3o)**\n    \n    Actually, monotonicity in all of I is not necessary. The function needs only to be monotonic on the subset of I from which the agent will choose its intervention.",
  "reviews": [
    {
      "agentId": "clarity-coach",
      "costInCents": 0,
      "createdAt": "2025-04-17T04:31:48.443Z",
      "runDetails": "{\"model\":\"google/gemini-2.0-flash-001\",\"promptTokens\":9660,\"completionTokens\":2308,\"totalTokens\":11968,\"temperature\":0.1,\"runtimeMs\":11750}",
      "thinking": "This document presents a typology of Goodhart's Law failures based on causal diagrams, categorizing them into structural, functional, and calibration errors. It's a valuable framework for understanding the different ways in which optimization can go wrong when a proxy measure becomes the target. My analysis will focus on the clarity and coherence of the explanations, the accessibility of the concepts, and the effectiveness of the examples used to illustrate each type of error. I will also consider how the document could be improved to better communicate these ideas to a broader audience, particularly those not already familiar with causal diagrams and related concepts.",
      "summary": "This document presents a valuable typology of Goodhart's Law failures based on causal diagrams, categorizing them into structural, functional, and calibration errors. While the framework is logically sound and comprehensive, the clarity and accessibility could be improved for a broader audience. The use of technical jargon, dense explanations of causal diagrams, and mathematical notation may hinder comprehension for readers without a strong background in statistics and causal inference. Providing more intuitive explanations, concrete examples, and visual aids would enhance the document's effectiveness in communicating these important concepts.",
      "comments": [
        {
          "title": "Clarity of Definition",
          "description": "The initial definition of Goodhart's Law is standard but could be more accessible to a broader audience. The subsequent elaboration with random variables I and T is more precise but might be confusing for readers without a statistical background.",
          "importance": "75",
          "grade": "65",
          "highlight": {
            "startOffset": 545,
            "endOffset": 1326,
            "quotedText": "The standard definition of [Goodhart's Law](https://www.lesswrong.com/w/goodhart-s-law) is: \"when a proxy for some value becomes the target of optimization pressure, the proxy will cease to be a good proxy.\".\n\nMore specifically: we see a meaningful statistical relationship between the values of two random variables I and T,[\\[1\\]](#fnodu9li5hnrr) where we have some preferences over the values of T. We decide to change the value of I, expecting that the value of T will change as would be expected by naively extrapolating the statistical tendencies of the two variables. We fall victim to Goodhart's Law (i.e. a Goodhart failure occurs) if the change in T doesn't quite meet our expectations and, upon learning that, we see that we should have intervened in some different way."
          },
          "isValid": true
        },
        {
          "title": "Use of Technical Jargon",
          "description": "The document uses terms like \"outer alignment,\" \"inner alignment,\" and \"mesaoptimization\" without explicitly defining them within the text. While these terms are common within the AI alignment community, they may not be familiar to a general audience, hindering comprehension.",
          "importance": "70",
          "grade": "60",
          "highlight": {
            "startOffset": 1348,
            "endOffset": 2587,
            "quotedText": "Many alignment failures can be cast in terms [of Goodhart's Law](https://www.lesswrong.com/w/goodhart-s-law). Specifically, the AI is given an objective that doesn't fully specify what we want it to optimize for ([outer alignment](https://www.lesswrong.com/tag/outer-alignment)), or it finds a way to exploit the misspecification of the training goal, developing an internal objective that is misaligned with the one we specified ([inner alignment](https://www.lesswrong.com/tag/inner-alignment) / [mesaoptimization](https://www.lesswrong.com/tag/mesa-optimization)). Therefore, a better understanding of Goodhart's Law — its various subtypes, causes, and possible remedies — might better inform us about how to effectively anticipate and prevent (or at least mitigate) those failure modes. In addition to technical AI alignment/safety, a better understanding of Goodhart's Law might shed some light on enhancing human coordination, given that the same outer and inner alignment problems (or [close analogs thereof](https://en.wikipedia.org/wiki/Principal%E2%80%93agent_problem)) recur in coordination structures. In particular, effective AI governance will most likely require specifications of regulations that are robust to goodharting."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Clarity of Core Principle",
          "description": "The core principle linking Goodhart problems to misunderstanding causal relationships is clearly stated and serves as a useful guide for understanding the subsequent typology.",
          "importance": "90",
          "grade": "85",
          "highlight": {
            "startOffset": 3030,
            "endOffset": 3165,
            "quotedText": "_All Goodhart problems stem from misunderstanding the causal relationships between the intervention, target, or measurement variables._"
          },
          "isValid": true
        },
        {
          "title": "Structure, Function, Randomness - Logical Flow",
          "description": "The organization of Goodhart failures into structure, function, and randomness is logically sound and provides a comprehensive framework. The analogy to learning these aspects sequentially is helpful for understanding the typology.",
          "importance": "95",
          "grade": "80",
          "highlight": {
            "startOffset": 4243,
            "endOffset": 4882,
            "quotedText": "We propose a classification of causes of Goodhart failures into three categories, representing which **aspect of the causal diagram** the agent failed on.\n\n1.  **Structure.** The agent is mistaken about the **causal** **structure** mediating between the intervention variable I, the target variable T, and the measure variable M.\n2.  **Function.** The agent is mistaken about the **functional** relationship defining the interactions between the intervention variable, the target variable, and the measure variable.\n3.  **Randomness.** The agent is mistaken about the **random distribution** of the auxiliary random variables in the graph."
          },
          "isValid": true
        },
        {
          "title": "Explanation of Causal Diagrams",
          "description": "The explanation of causal diagrams is relatively dense and assumes some prior knowledge of directed acyclic graphs and probability distributions. While technically accurate, it could benefit from more intuitive explanations and visual aids to enhance understanding.",
          "importance": "80",
          "grade": "60",
          "highlight": {
            "startOffset": 8775,
            "endOffset": 10172,
            "quotedText": "ntions on the causal structure will influence our variable of interest.\n\nA causal diagram (CD) is a special kind of [directed acyclic graph](https://en.wikipedia.org/wiki/Directed_acyclic_graph) (DAG). Its nodes represent random variables and its edges represent causal relationships between the random variables. The existence of an edge Xe→Y means \"the value of X (co-)determines the value of Y. The value of the random variable Y is _fully_ determined by a function fY:(X1×⋯×Xn×UY)→Y where X1 through Xn are the parents of Y and UY is its random auxiliary variable, representing a random component of Y (\"U\" for \"unknown\"). In other words, the value y∈Y is determined by a function y\\=f(x1,...,xn,u) with u∼UY (slight abuse of notation: we use UY to denote both the random variable and its distribution).\n\nIn a CD, auxiliary variables stand for irreducible sources of randomness and are the only sources of probabilistic uncertainty. An auxiliary variable can have any random distribution. Any probabilistic dependence between random variables can be neatly modeled using that kind of mix of deterministic functions and random distributions. Every random variable X in a CD has exactly one auxiliary variable UX. Therefore, orphans (nodes without parents) in a CD, are exactly auxiliary variables and if a proper random variable has no parents except its auxiliary variable, its value is fully "
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Mathematical Notation",
          "description": "The use of mathematical notation, while precise, may be a barrier to understanding for readers without a strong mathematical background. Providing more intuitive explanations alongside the equations would improve accessibility.",
          "importance": "65",
          "grade": "55",
          "highlight": {
            "startOffset": 13815,
            "endOffset": 13961,
            "quotedText": "/TYgztDNXhobbqMpXh/etuva4h15k5yhwsnkhs7)\n\nThen, we can define the probability distribution:\n\nP(T\\=t|I\\=i)\\=∫→u∈(UT1×⋯×UTmT×UT)P(→u)⋅1\\[f∗T(i,→u)\\="
          },
          "isValid": true
        },
        {
          "title": "Examples of Structural Errors",
          "description": "The examples provided for structural errors are diverse and illustrate the concept effectively. However, some examples, like the smoking and tobacco case, could benefit from more explicit connections to the causal diagram framework.",
          "importance": "85",
          "grade": "75",
          "highlight": {
            "startOffset": 17976,
            "endOffset": 20402,
            "quotedText": "**Examples**\n\n1.  [Smoking and tobacco](https://en.wikipedia.org/wiki/Health_effects_of_tobacco#Public_awareness): \"For a particular individual, it is impossible to definitively prove a direct causal link between exposure to a radiomimetic poison such as tobacco smoke and the cancer that follows; such statements can only be made at the aggregate population level. Cigarette companies have capitalized on this philosophical objection and exploited the doubts of clinicians, who consider only individual cases, on the causal link in the stochastic expression of the toxicity as an actual disease.\".\n2.  [Amyloid beta plaques](https://en.wikipedia.org/wiki/Amyloid_plaques) associated with Alzheimer's disease were long suspected to be causally involved in the advancement of the disease. This motivated medical research to aim at developing drugs targeting those plaques. However, amyloid beta plaques may be just a symptom of the disease, and eradication of the symptom (even if successful) is not necessarily going to help with the disease (depending on the specific mechanism of symptom eradication).\n3.  [Blue zones](https://en.wikipedia.org/wiki/Blue_zone): \"A blue zone is a region in the world where people are claimed to have exceptionally long lives beyond the age of 80 due to a lifestyle combining physical activity, low stress, rich social interactions, a local whole foods diet, and low disease incidence.\" However, more recent evidence suggests that exceptional numbers of reports of centenarians and supercentenarians were caused by poor record keeping in these poor areas, allowing for fraud.[\\[9\\]](#fnlmvnnjmr97n)\n4.  [Survivorship bias](https://en.wikipedia.org/wiki/Survivorship_bias#Military): \"During World War II, the statistician Abraham Wald took survivorship bias into his calculations when considering how to minimize bomber losses to enemy fire. The Statistical Research Group (SRG) at Columbia University, which Wald was a member, examined the damage done to aircraft that had returned from missions and recommended adding armor to the areas that showed the least damage. The bullet holes in the returning aircraft represented areas where a bomber could take damage and still fly well enough to return safely to base. Therefore, Wald proposed that the Navy reinforce areas where the returning aircraft were unscathed, inferring that planes hit in those areas were the ones most likely to be lost.\""
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Examples of Functional Errors",
          "description": "The examples for functional errors are well-chosen and demonstrate how incorrect assumptions about the relationships between variables can lead to Goodhart failures. The Kleiber's Law and Malthusianism examples are particularly insightful.",
          "importance": "90",
          "grade": "80",
          "highlight": {
            "startOffset": 22190,
            "endOffset": 25407,
            "quotedText": "ion time, we are going to be comparing alternative values i∈I which amounts to computing that preference ordering locally.\n\n**Examples**\n\n1.  [**Kleiber's Law**](https://en.wikipedia.org/wiki/Kleiber%27s_law)**:** \"Kleiber's law, named after Max Kleiber for his biology work in the early 1930s, states, after many observations that, for a vast number of animals, an animal's Basal Metabolic Rate scales to the 3⁄4 power of the animal's mass. \\[...\\] Before Kleiber's observation of the 3/4 power scaling, a 2/3 power scaling was largely anticipated based on the \"surface law\", which states that the basal metabolism of animals differing in size is nearly proportional to their respective body surfaces. This surface law reasoning originated from simple geometrical considerations. As organisms increase in size, their volume (and thus mass) increases at a much faster rate than their surface area. Explanations for 2⁄3-scaling tend to assume that metabolic rates scale to avoid heat exhaustion. Because bodies lose heat passively via their surface but produce heat metabolically throughout their mass, the metabolic rate must scale in such a way as to counteract the square–cube law. Because many physiological processes, like heat loss and nutrient uptake, were believed to be dependent on the surface area of an organism, it was hypothesized that metabolic rate would scale with the 2/3 power of body mass.\"\n2.  [**Malthusianism**](https://en.wikipedia.org/wiki/Malthusianism)**:** \"Malthusianism is a theory that population growth is potentially exponential, according to the Malthusian growth model, while the growth of the food supply or other resources is linear, which eventually reduces living standards to the point of triggering a population decline. This event, called a Malthusian catastrophe (also known as a Malthusian trap, population trap, Malthusian check, Malthusian crisis, Point of Crisis, or Malthusian crunch) has been predicted to occur if population growth outpaces agricultural production, thereby causing famine or war.\"\n    1.  [Malthus's predictions were not borne out and they had not been justified at the time of his writing either](https://www.astralcodexten.com/p/your-book-review-progress-and-poverty?open=false#%C2%A7ii-population-and-subsistence).\n3.  Exposure of small children to allergens has been advised against. This turned out to have consequences exactly opposite to the ones that were desired: without allergen exposure, the immune system doesn't have an opportunity to learn to distinguish safe from non-safe food.\n4.  Before [Kepler](https://en.wikipedia.org/wiki/Kepler%27s_laws_of_planetary_motion), planets were believed to follow circular orbits and move at constant speed. Kepler revised the heliocentric model and postulated elliptic, not circular, orbits, and varying speed of motion. This was an improvement on the model of the relationship between time and position of a planet.\n5.  The relationship between alcohol and health outcomes was believed to follow a U-shaped curve with a small amount of drinking being the most optimal. After accounting for the fact that many teetotalers are people with a history of alcohol abuse, the initial segment of the curve "
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Examples of Calibration Errors",
          "description": "The examples for calibration errors are less intuitive than those for structural and functional errors. The explanation of how miscalibration can affect interventions could be clarified with more concrete scenarios.",
          "importance": "75",
          "grade": "65",
          "highlight": {
            "startOffset": 26785,
            "endOffset": 27914,
            "quotedText": "**Examples**\n\n1.  The assumption that accident severities follow a thin-tailed distribution when in fact they follow a fat-tailed distribution will, in the long run, lead to inadequate preparation for the most severe accidents.\n    1.  In practice, human collectives often tend to inadequately prepare for natural disasters, pandemics, etc (some of which can be explained by over-relying on the [availability](https://www.lesswrong.com/posts/R8cpqD3NA4rZxRdQ4/availability) heuristic).\n2.  If what the agent is modeling as \"merely\" noisy/stochastic components of the system is actually adversarially selected, then interventions chosen at minimizing loss in the face of stochastic noise will likely fail in the face of adversarial noise (i.e. noise selected/optimized so as to pessimize against the agent's objectives).\n3.  Disregarding the possibility that the apparent deviation from the usual range is due to stochasticity and then taking action to bring it back to the usual range might lead the agent to conclude that its action took the desired effect even if the observation was actually due to the regression to the mean."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        }
      ],
      "grade": 72
    },
    {
      "agentId": "research-scholar",
      "costInCents": 0,
      "createdAt": "2025-04-17T04:31:56.524Z",
      "runDetails": "{\"model\":\"google/gemini-2.0-flash-001\",\"promptTokens\":10340,\"completionTokens\":1477,\"totalTokens\":11817,\"temperature\":0.1,\"runtimeMs\":8017}",
      "thinking": "This document presents a typology of Goodhart's Law failures based on errors in understanding causal diagrams. It categorizes these errors into structural, functional, and calibration (randomness) errors. My role as a research scholar is to identify relevant external resources that can provide further context, examples, or alternative perspectives on these concepts. I will focus on resources that discuss causal inference, Goodhart's Law, and related topics in AI alignment and governance. I will also look for resources that provide real-world examples of each type of error. I will provide links to these resources in a markdown table, along with a brief description of their relevance and an importance rating. Finally, I will connect specific references to highlighted sections in the text, explaining how the reference enhances understanding of the highlighted section.",
      "summary": "This document categorizes Goodhart's Law failures into structural, functional, and calibration errors within the framework of causal diagrams. As a research scholar, I've identified resources that provide further context, examples, and alternative perspectives on these concepts, focusing on causal inference, AI alignment, and real-world applications.",
      "comments": [
        {
          "title": "Causal Diagrams and Goodhart's Law",
          "description": "This section introduces the core concept of using causal diagrams to analyze Goodhart's Law. Understanding causal relationships is crucial for avoiding unintended consequences when optimizing for a proxy measure.",
          "importance": 95,
          "grade": 95,
          "isValid": true,
          "highlight": {
            "startOffset": -1,
            "endOffset": 478,
            "quotedText": "_(Work done at Convergence Analysis. The ideas are due to Justin. Mateusz wrote the post. Thanks to Olga Babeeva for feedback on this post.)_\n\nIn this post, we introduce the typology of structure, function, and randomness that builds on the framework introduced in the post [_Goodhart's Law Causal Diagrams_](https://www.lesswrong.com/posts/e4SMfYWb4Tz568yh6/goodhart-s-law-causal-diagrams). We aim to present a comprehensive categorization of the causes of Goodhart's problems."
          }
        },
        {
          "title": "Structure, Function, and Randomness",
          "description": "This section outlines the three main categories of Goodhart failures: errors in understanding the causal structure, the functional relationships, or the randomness (calibration) of the system. This categorization provides a useful framework for analyzing and mitigating Goodhart's Law.",
          "importance": 98,
          "grade": 95,
          "isValid": true,
          "highlight": {
            "startOffset": 4243,
            "endOffset": 4882,
            "quotedText": "We propose a classification of causes of Goodhart failures into three categories, representing which **aspect of the causal diagram** the agent failed on.\n\n1.  **Structure.** The agent is mistaken about the **causal** **structure** mediating between the intervention variable I, the target variable T, and the measure variable M.\n2.  **Function.** The agent is mistaken about the **functional** relationship defining the interactions between the intervention variable, the target variable, and the measure variable.\n3.  **Randomness.** The agent is mistaken about the **random distribution** of the auxiliary random variables in the graph."
          }
        },
        {
          "title": "Structural Errors",
          "description": "This section describes errors in understanding the causal structure of the system. This is a critical aspect of Goodhart's Law, as misinterpreting causal relationships can lead to unintended and negative consequences.",
          "importance": 92,
          "grade": 92,
          "isValid": true,
          "highlight": {
            "startOffset": 15811,
            "endOffset": 16435,
            "quotedText": "A **structural error** occurs when the agent is mistaken about the structure of the causal diagram, i.e. nodes (random variables) and edges (causal relationships).\n\nBeing mistaken about the causal structure mediating the influence of I on T can be a source of error in deciding on the intervention (i.e. on the i∈I). However, given that the agent's assessment of which nodes are useful as intervention variables is dictated by (among other things[\\[8\\]](#fnp4outvfvrss)) the causal and functional structure between the node (potential I) and T, being mistaken about the causal structure can also lead us to choose a wrong I."
          }
        },
        {
          "title": "Functional Errors",
          "description": "This section describes errors in understanding the functional relationships between variables. Even with a correct causal structure, incorrect assumptions about how variables interact can lead to Goodhart failures.",
          "importance": 90,
          "grade": 90,
          "isValid": true,
          "highlight": {
            "startOffset": 20426,
            "endOffset": 21094,
            "quotedText": "A **functional error** happens when the learned functional relationship between I and T is not appropriate for the intervention time.\n\nRecall that we started with a utility function over the value set of the target node T and a function specifying how the value of T depends on the variables upstream from it. Then we used that to find an appropriate intervention node I in the diagram and then to induce a utility function over its possible values, such that choosing the values of I to maximize vI will translate into the target value also maximizing vT its preference ordering (on average, in expectation, &c. given that we're operating in a probabilistic setting)."
          }
        },
        {
          "title": "Calibration Errors",
          "description": "This section describes errors in understanding the randomness or calibration of the system. Miscalibration about the distribution of auxiliary variables can significantly impact the expected results of an intervention.",
          "importance": 88,
          "grade": 88,
          "isValid": true,
          "highlight": {
            "startOffset": 25566,
            "endOffset": 26146,
            "quotedText": "We might be mistaken about the nature of the random auxiliary variables in the causal diagram. This is _miscalibration_, hence we call it **calibration errors**.\n\nThe target variable depends on all the auxiliary variables that are upstream from it, except for those that route only through the intervention variable if it is being intervened on.\n\nMiscalibration about the distribution of any of the auxiliary variables might (depending on other details of the causal diagram) have large consequences for how the expected results of an intervention diverge from its actual results."
          }
        }
      ],
      "grade": 92
    },
    {
      "agentId": "quantitative-forecaster",
      "costInCents": 0,
      "createdAt": "2025-04-17T04:32:07.436Z",
      "runDetails": "{\"model\":\"google/gemini-2.0-flash-001\",\"promptTokens\":9510,\"completionTokens\":1495,\"totalTokens\":11005,\"temperature\":0.1,\"runtimeMs\":8970}",
      "thinking": "This document presents a typology of Goodhart's Law failures based on errors in understanding the causal relationships between intervention, target, and measurement variables. The typology categorizes errors into three types: structural, functional, and calibration (randomness). It also considers the reasons for these errors: learning failure, transfer failure, and non-interference failure. The document uses causal diagrams to formally model these relationships. My focus will be on how these different types of failures can be quantified and forecasted, and how the framework can be used to improve forecasting accuracy in complex systems.\n\nI will look for opportunities to translate the concepts into concrete forecasting questions, identify relevant metrics, and suggest ways to estimate the probabilities of different types of Goodhart failures. I will also consider how the framework can be used to improve the design of interventions and policies to make them more robust to Goodharting.\n\nI will also consider how the concepts of structure, function, and randomness can be applied to Fermi estimation and other quantitative forecasting techniques. For example, when making a Fermi estimate, it is important to consider the structure of the problem, the functional relationships between the variables, and the uncertainty in the estimates. By explicitly considering these factors, it is possible to improve the accuracy and calibration of Fermi estimates.",
      "summary": "This document provides a valuable framework for understanding and categorizing Goodhart's Law failures. From a quantitative forecasting perspective, the framework can be used to generate concrete forecasting questions related to the probability of different types of errors (structural, functional, calibration) in causal models and interventions. By explicitly considering these potential errors, forecasters can improve the accuracy and robustness of their predictions, especially in complex and dynamic systems. The concepts of transfer failure and adversarial noise are particularly relevant for forecasting in real-world settings. The framework can also be applied to improve the design of Fermi estimates and other quantitative forecasting techniques.",
      "comments": [
        {
          "title": "Transfer Failure and Time Horizons",
          "description": "The concept of transfer failure is directly relevant to forecasting. A model that is accurate at one point in time may become inaccurate as the environment changes. A forecasting question could be: \"What is the probability that a forecasting model will experience a significant transfer failure within a specified time horizon (e.g., 1 year, 5 years)?\" This requires assessing the stability of the underlying relationships and the potential for exogenous shocks.",
          "importance": "80",
          "highlight": {
            "startOffset": 5595,
            "endOffset": 5878,
            "quotedText": "Transfer failure.** The agent has learned a causal structure that was right at the time of learning but became **\"deprecated\"** or **\"outdated\"** by the time of intervention, i.e. it failed to **transfer** (e.g. the environment changed or some processes optimized against the agent)."
          },
          "isValid": true
        },
        {
          "title": "Fermi Estimation and Structural Errors",
          "description": "When applying Fermi estimation, structural errors can lead to significant inaccuracies. For example, when estimating the number of piano tuners in Chicago, failing to account for the impact of digital pianos would be a structural error. A forecasting question could be: \"What is the probability that a Fermi estimate is off by more than a factor of 2 due to a structural error in the underlying assumptions?\"",
          "importance": "75",
          "highlight": {
            "startOffset": 10239,
            "endOffset": 10749,
            "quotedText": "oyjfq)\n\nWe have just introduced causal diagrams in the following order:\n\n1.  **Structure** — Nodes represent random variables, and edges represent causal relationships between them.\n2.  **Function** — The value of a node Y is determined by a function fY:(X1×⋯×Xn×UY)→Y from all the nodes that send an edge to it…\n3.  **Randomness** — … and a random component UY, AKA the auxiliary variable of Y.\n\nThe three \"steps\" provide a handy model for thinking about the process of an agent learning a causal diagram, as "
          },
          "isValid": true
        },
        {
          "title": "Forecasting Question: Probability of Structural Error",
          "description": "This section introduces the concept of structural errors in causal diagrams. A relevant forecasting question would be: \"What is the probability that a given causal model of a complex system (e.g., climate change, economic growth) contains a significant structural error, such as missing a key variable or causal link?\" This could be further refined by specifying the type of system, the level of complexity of the model, and the criteria for defining a \"significant\" error.",
          "importance": "85",
          "highlight": {
            "startOffset": 15811,
            "endOffset": 16644,
            "quotedText": "A **structural error** occurs when the agent is mistaken about the structure of the causal diagram, i.e. nodes (random variables) and edges (causal relationships).\n\nBeing mistaken about the causal structure mediating the influence of I on T can be a source of error in deciding on the intervention (i.e. on the i∈I). However, given that the agent's assessment of which nodes are useful as intervention variables is dictated by (among other things[\\[8\\]](#fnp4outvfvrss)) the causal and functional structure between the node (potential I) and T, being mistaken about the causal structure can also lead us to choose a wrong I.\n\nThese problems can take several forms, which can also occur together, such as failing to identify all (and only) the relevant variables and missing existing causal links, including non-existent causal links."
          },
          "isValid": true
        },
        {
          "title": "Quantifying Functional Errors",
          "description": "This section discusses functional errors, where the learned functional relationship between intervention and target is incorrect. A forecasting question related to this would be: \"What is the probability that the assumed functional relationship between a specific policy intervention (e.g., carbon tax) and a desired outcome (e.g., reduced carbon emissions) is significantly different from the actual relationship?\" This requires defining \"significantly different\" and considering potential non-linearities and interactions.",
          "importance": "80",
          "highlight": {
            "startOffset": 20426,
            "endOffset": 21679,
            "quotedText": "A **functional error** happens when the learned functional relationship between I and T is not appropriate for the intervention time.\n\nRecall that we started with a utility function over the value set of the target node T and a function specifying how the value of T depends on the variables upstream from it. Then we used that to find an appropriate intervention node I in the diagram and then to induce a utility function over its possible values, such that choosing the values of I to maximize vI will translate into the target value also maximizing vT its preference ordering (on average, in expectation, &c. given that we're operating in a probabilistic setting).\n\nWe might take it as our desideratum: given a utility function vT:T→R and a functional relationship fT:(I×X1×⋯×Xn×UT)→T, derive a preorder on the intervention variable vI:I→R such that maximization of the latter transfers, through fT, to maximize the former. (In the language of order theory, we can say that fT is [monotone](https://en.wikipedia.org/wiki/Monotonic_function#In_order_theory) in the argument I.[\\[10\\]](#fn5gfjqlsvu3o))\n\nA functional error occurs when this desideratum fails, i.e., optimizing I fails to translate into optimizing of T (as robustly as the agent wants)."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Calibration Error and Fat-Tailed Distributions",
          "description": "This section highlights the importance of accurate calibration of random variables. The example of accident severities following a fat-tailed distribution is particularly relevant. A forecasting question could be: \"What is the probability that a risk assessment model underestimates the probability of extreme events due to miscalibration of the underlying probability distributions?\" This requires assessing the tail risk of the distributions used in the model.",
          "importance": "90",
          "highlight": {
            "startOffset": 26803,
            "endOffset": 23,
            "quotedText": "nce Analysis. The ideas are due to Justin. Mateusz wrote the post. Thanks to Olga Babeeva for feedback on this post.)_\n\nIn this post, we introduce the typology of structure, function, and randomness that builds on the framework introduced in the post [_Goodhart's Law Causal Diagrams_](https://www.lesswrong.com/posts/e4SMfYWb4Tz568yh6/goodhart-s-law-causal-diagrams). We aim to present a comprehensive categorization of the causes of Goodhart's problems.\n\nBut first, why do we care about this? \n\n## Goodhart's Law recap\n\nThe standard definition of [Goodhart's Law](https://www.lesswrong.com/w/goodhart-s-law) is: \"when a proxy for some value becomes the target of optimization pressure, the proxy will cease to be a good proxy.\".\n\nMore specifically: we see a meaningful statistical relationship between the values of two random variables I and T,[\\[1\\]](#fnodu9li5hnrr) where we have some preferences over the values of T. We decide to change the value of I, expecting that the value of T will change as would be expected by naively extrapolating the statistical tendencies of the two variables. We fall victim to Goodhart's Law (i.e. a Goodhart failure occurs) if the change in T doesn't quite meet our expectations and, upon learning that, we see that we should have intervened in some different way.\n\n## Some motivation\n\nMany alignment failures can be cast in terms [of Goodhart's Law](https://www.lesswrong.com/w/goodhart-s-law). Specifically, the AI is given an objective that doesn't fully specify what we want it to optimize for ([outer alignment](https://www.lesswrong.com/tag/outer-alignment)), or it finds a way to exploit the misspecification of the training goal, developing an internal objective that is misaligned with the one we specified ([inner alignment](https://www.lesswrong.com/tag/inner-alignment) / [mesaoptimization](https://www.lesswrong.com/tag/mesa-optimization)). Therefore, a better understanding of Goodhart's Law — its various subtypes, causes, and possible remedies — might better inform us about how to effectively anticipate and prevent (or at least mitigate) those failure modes. In addition to technical AI alignment/safety, a better understanding of Goodhart's Law might shed some light on enhancing human coordination, given that the same outer and inner alignment problems (or [close analogs thereof](https://en.wikipedia.org/wiki/Principal%E2%80%93agent_problem)) recur in coordination structures. In particular, effective AI governance will most likely require specifications of regulations that are robust to goodharting.\n\n# Introduction\n\nIn the [previous post](https://www.lesswrong.com/posts/e4SMfYWb4Tz568yh6/goodhart-s-law-causal-diagrams), we presented how causal diagrams can be used to describe situations vulnerable to Goodhart's problems. In this post, we build on that framework to present a complete categorization of possible causes of Goodhart's problems.[\\[2\\]](#fnlsibvjne94a)\n\nThe key principle to hold in the back of one's mind is the following.\n\n_All Goodhart problems stem from misunderstanding the causal relationships between the intervention, target, or measurement variables._\n\nMore specifically, if you know all relevant facts about the relationship between\n\n1\\. your interventions (I),\n\n2\\. the target variable you want to optimize (T), and\n\n3\\. your measurements of the target variable (M),\n\nthen, in principle, you know enough to infer what interventions are optimal for maximizing the expected value of the target variable (according to your preference ordering, whatever it is).\n\nThe only reasons expected utility maximization can encounter Goodhart problems are ignorance (e.g., not knowing the right method for inferring the right course of action in a given circumstance) and computational limitations.\n\nHere, we present a framework flexible enough to cover all the Goodhart-related failure modes in terms of mistaken beliefs about the causal diagram. We discuss various examples throughout the post.\n\nBy this motivation, we are casting all categories of Goodhart problems as mistakes of inference under uncertainty, where the ultimate object of inference is the choice of action and the intermediate object of inference is the causal diagram.\n\nWe propose a classification of causes of Goodhart failures into three categories, representing which **aspect of the causal diagram** the agent failed on.\n\n1.  **Structure.** The agent is mistaken about the **causal** **structure** mediating between the intervention variable I, the target variable T, and the measure variable M.\n2.  **Function.** The agent is mistaken about the **functional** relationship defining the interactions between the intervention variable, the target variable, and the measure variable.\n3.  **Randomness.** The agent is mistaken about the **random distribution** of the auxiliary random variables in the graph.\n\nIf we think of an idealized process of learning the causal diagram, we can think about learning these three aspects one after another and identifying which link in the chain failed.\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/TYgztDNXhobbqMpXh/qjozlv0z0ejyk2oq8yix)\n\nAnother dimension worth keeping in mind describes the **reason the agent failed on this aspect of the causal diagram**. It is not the main focus of this post but it is useful to keep in mind as it might be relevant for finding strategies to mitigate Goodhart's problems.\n\n1.  **Learning failure.** The agent has learned a causal structure that was **wrong already at the time of learning**.\n2.  **Transfer failure.** The agent has learned a causal structure that was right at the time of learning but became **\"deprecated\"** or **\"outdated\"** by the time of intervention, i.e. it failed to **transfer** (e.g. the environment changed or some processes optimized against the agent).\n3.  **Non-interference failure.** The agent has learned a causal structure that would accurately describe the relationships between variables at the time of intervention but the agent **failed to model the impact of its actions on the causal structure**. The agent's intervention _appears to have_ broken the causal structure because the actions' effects also flowed through \"invisible side channels\" that the learned causal diagram didn't account for.[\\[3\\]](#fnjsp8hyd7g5c)\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/TYgztDNXhobbqMpXh/r9caxd66v1hh3feb0yrp)\n\n           **Cause of failure**  \n  \n**Failed aspect**\n\n**Learning failure**\n\n**Transfer failure**\n\n**Non-interference failure**\n\n**Structure**\n\nLearned wrong causal structure\n\nLearned correct causal structure but it failed to transfer to intervention time\n\nLearned correct causal structure that transferred to intervention time but intervention broke alignment between the causal structure and its representation\n\n**Function**\n\nLearned wrong functional dependencies\n\nLearned correct functional dependencies but they failed to transfer to intervention time\n\nLearned correct functional dependencies that transferred to intervention time but intervention broke alignment between the functional dependencies and their representation\n\n**Randomness**\n\nLearned a wrong distribution of random components\n\nLearned correct distribution of random components but it failed to transfer to intervention time\n\nLearned correct distribution of random components that transferred to intervention time but intervention broke alignment between the distribution of random components and its representation\n\nAdditionally, we might want to distinguish between failures due to **insufficient knowledge** and those due to **constraints on resources** necessary for computing the impact of an action (time, space, algorithms). However, given that we assume knowledge results from inference anyway, these two categories blur into one. Therefore, this distinction is irrelevant in this post.\n\nFor simplicity, this post focuses on simplified situations where learning time and intervention time are distinct and learning proceeds only by observation, not experimentation. Situations involving continuous learning are naturally much more complex but we expect the concepts and lenses from this post to generalize to them.\n\n# Ontology\n\n## Causal diagrams (re-)introduced\n\n[_Goodhart's Law Causal Diagrams_](https://www.lesswrong.com/posts/e4SMfYWb4Tz568yh6/goodhart-s-law-causal-diagrams) introduced causal diagrams as a framework for thinking about Goodhart's Law but did not treat causal diagrams rigorously. Here, we introduce causal diagrams formally.\n\nThe purpose of causal diagrams is to comprehensively model the dependencies between random variables. Their primary use case is to determine how specific interventions on the causal structure will influence our variable of interest.\n\nA causal diagram (CD) is a special kind of [directed acyclic graph](https://en.wikipedia.org/wiki/Directed_acyclic_graph) (DAG). Its nodes represent random variables and its edges represent causal relationships between the random variables. The existence of an edge Xe→Y means \"the value of X (co-)determines the value of Y. The value of the random variable Y is _fully_ determined by a function fY:(X1×⋯×Xn×UY)→Y where X1 through Xn are the parents of Y and UY is its random auxiliary variable, representing a random component of Y (\"U\" for \"unknown\"). In other words, the value y∈Y is determined by a function y\\=f(x1,...,xn,u) with u∼UY (slight abuse of notation: we use UY to denote both the random variable and its distribution).\n\nIn a CD, auxiliary variables stand for irreducible sources of randomness and are the only sources of probabilistic uncertainty. An auxiliary variable can have any random distribution. Any probabilistic dependence between random variables can be neatly modeled using that kind of mix of deterministic functions and random distributions. Every random variable X in a CD has exactly one auxiliary variable UX. Therefore, orphans (nodes without parents) in a CD, are exactly auxiliary variables and if a proper random variable has no parents except its auxiliary variable, its value is fully determined by the value of its auxiliary variable.[\\[4\\]](#fnltspxkoyjfq)\n\nWe have just introduced causal diagrams in the following order:\n\n1.  **Structure** — Nodes represent random variables, and edges represent causal relationships between them.\n2.  **Function** — The value of a node Y is determined by a function fY:(X1×⋯×Xn×UY)→Y from all the nodes that send an edge to it…\n3.  **Randomness** — … and a random component UY, AKA the auxiliary variable of Y.\n\nThe three \"steps\" provide a handy model for thinking about the process of an agent learning a causal diagram, as each step increases the resolution to what has been achieved after the previous one.[\\[5\\]](#fntsz9rcwvv1k) Another reason to think about them in this exact sequence is that to get the function aspect right, we first need to get the structure aspect right (basically by definition); and, similarly, to get the randomness aspect right, we need to get the function aspect right first (this is not strictly true but approximately true). This motivates the categorization of errors in learning and using causal diagrams into errors of structure, function, and randomness, which we describe in detail in Section 2. \n\nFirst, however, we need to describe how intervening on a causal diagram works, and for that purpose, we need to describe how we're going to use causal diagrams to think about the relationship between the intervention, target, and measure variables.\n\nImportantly, for the causal diagram to be useful in practice, it needs to be repeatable, i.e. it should retain its applicability to a given kind of situation over several iterations, rather than being useful only for a single \"rounds\" after which it can no longer be used. For example, if we want to use a CD to predict the effect of a policy on the economy, we would like it to still be useful after we execute the intervention that we deem the most desirable based on the CD.\n\n## Intervention, target, and measure\n\nLet's start with a diagram:\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/TYgztDNXhobbqMpXh/w490x0hbj1xbmrebibwi)\n\nSo, we have three variables of interest. T is the **target** variable that we want to optimize. I is the **intervention** variable that we are influencing to optimize the target T. In general, we may not have direct access to the value of the target and can only read it through M, the **measure** variable. Since we want I to influence​​​​​​​ T and we want T to influence M, I needs to be (causally) upstream of T whereas T needs to be (causally) upstream of M: I→T→M.\n\nTherefore, we need to account for two causal relationships, that of I on T and that of T on M.[\\[6\\]](#fnx5ofdskcpz)\n\nIntervening on a causal diagram works by \"taking control\" over the values of one or more random variables. Here, we focus on the simplest case, where we choose one or more variables and set the value of each to some fixed value, ignoring what value (or rather, what distribution of values) they would have otherwise, given some values of their parent variables.[\\[7\\]](#fn4jv0y9po67x)\n\nThe structure of the diagram is fully deterministic, with the only free parameters being the orphan random auxiliary variables over which we have some probability distributions, and, obviously the intervention variable I (if we choose to intervene). Therefore, to determine the probability distribution over T conditional on some i∈I, we need to extend the function fT:I×XT1×⋯×XTnT×UT→T to f∗T:I×UT1×⋯×UTmT×UT→T where UT1 through UTmT are random ancestors of T.\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/TYgztDNXhobbqMpXh/etuva4h15k5yhwsnkhs7)\n\nThen, we can define the probability distribution:\n\nP(T\\=t|I\\=i)\\=∫→u∈(UT1×⋯×UTmT×UT)P(→u)⋅1\\[f∗T(i,→u)\\=t\\]\n\nThen, the expected value vT conditional on I\\=i is:\n\nE\\[vT|I\\=i\\]\\=∑t∈TvT(t)⋅P(T\\=t|I\\=i)\n\n This is also how we define the induced utility function on I:\n\nvI(i)\\=E\\[vT|I\\=i\\]\n\nAn exactly analogous situation arises between T and M. We extend fM:T×XM1×⋯×XMnM×UM→M to f∗M:T×UM1×⋯×UMmM×UM→M where UM1 through UMmM are random ancestors of M.\n\nFor the most part, we will not discuss the measurement variable M explicitly. However, it is relevant to keep this in mind because the information about the dependence between I and T influences the learning of the causal diagram. It is also obviously relevant for continuous learning.\n\n## Goodhart failure\n\nGiven a utility function, vT:T→R over the target node T a Goodhart failure occurs when the change in T doesn't quite meet our expectations and, upon learning that, we see that we should have intervened in some different way.\n\nTo give a simple example: Suppose we want to maximize the number of fruits in a basket containing some number of apples, bananas, oranges, and some number of other fruits and we can choose the number of apples to be anything from 0 to 100. Our probabilistic beliefs about the numbers of bananas and oranges are represented by random variables UB and UO and there is some additional uncertainty involved in the number of fruits in the basket standing for \"other\", UF. Since our utility function is linear in the number of each type of fruit, we choose 100 apples. If the utility function were non-monotonic — e.g. vF(f)\\=−(70−f)2 with f being the number of fruits — we might need to choose something else than one hundred, depending on the details of our beliefs about bananas and oranges.\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/TYgztDNXhobbqMpXh/vmi25e7kjb8e1shvspmz)\n\n# Types of Goodhart failures\n\n## Structural errors\n\nA **structural error** occurs when the agent is mistaken about the structure of the causal diagram, i.e. nodes (random variables) and edges (causal relationships).\n\nBeing mistaken about the causal structure mediating the influence of I on T can be a source of error in deciding on the intervention (i.e. on the i∈I). However, given that the agent's assessment of which nodes are useful as intervention variables is dictated by (among other things[\\[8\\]](#fnp4outvfvrss)) the causal and functional structure between the node (potential I) and T, being mistaken about the causal structure can also lead us to choose a wrong I.\n\nThese problems can take several forms, which can also occur together, such as failing to identify all (and only) the relevant variables and missing existing causal links, including non-existent causal links.\n\nThis kind of error will occur if we learn the wrong causal structure between the measure and the target, since in such a case we may not have enough knowledge to robustly optimize the target. A similar situation occurs if we learn a causal diagram that was a good representation at the learning time but when it fails to generalize to the time of intervention because the environment has changed (for whatever reason). However, for our purpose, the distinction between mis-learning the CD and learning a correct CD that doesn't transfer into the future is not important because what we care about is learning a CD that is appropriate for enabling future optimization of T.\n\nThe more interesting (and more tricky) situation arises when the agent learns a causal structure that would be appropriate for the future situation, if not for the agent's own intervention that \"broke\" it (i.e. caused it to fail to generalize). In a strict sense, this is impossible because there is no way to change a causal diagram by intervening on its nodes. In a less strict sense, however, if the causal diagram is inappropriate, — e.g. the functional dependencies represented by some edges actually depend on the values of some nodes that haven't been included in the diagram — this may make it appear as if the causal structure is being violated.\n\n**Examples**\n\n1.  [Smoking and tobacco](https://en.wikipedia.org/wiki/Health_effects_of_tobacco#Public_awareness): \"For a particular individual, it is impossible to definitively prove a direct causal link between exposure to a radiomimetic poison such as tobacco smoke and the cancer that follows; such statements can only be made at the aggregate population level. Cigarette companies have capitalized on this philosophical objection and exploited the doubts of clinicians, who consider only individual cases, on the causal link in the stochastic expression of the toxicity as an actual disease.\".\n2.  [Amyloid beta plaques](https://en.wikipedia.org/wiki/Amyloid_plaques) associated with Alzheimer's disease were long suspected to be causally involved in the advancement of the disease. This motivated medical research to aim at developing drugs targeting those plaques. However, amyloid beta plaques may be just a symptom of the disease, and eradication of the symptom (even if successful) is not necessarily going to help with the disease (depending on the specific mechanism of symptom eradication).\n3.  [Blue zones](https://en.wikipedia.org/wiki/Blue_zone): \"A blue zone is a region in the world where people are claimed to have exceptionally long lives beyond the age of 80 due to a lifestyle combining physical activity, low stress, rich social interactions, a local whole foods diet, and low disease incidence.\" However, more recent evidence suggests that exceptional numbers of reports of centenarians and supercentenarians were caused by poor record keeping in these poor areas, allowing for fraud.[\\[9\\]](#fnlmvnnjmr97n)\n4.  [Survivorship bias](https://en.wikipedia.org/wiki/Survivorship_bias#Military): \"During World War II, the statistician Abraham Wald took survivorship bias into his calculations when considering how to minimize bomber losses to enemy fire. The Statistical Research Group (SRG) at Columbia University, which Wald was a member, examined the damage done to aircraft that had returned from missions and recommended adding armor to the areas that showed the least damage. The bullet holes in the returning aircraft represented areas where a bomber could take damage and still fly well enough to return safely to base. Therefore, Wald proposed that the Navy reinforce areas where the returning aircraft were unscathed, inferring that planes hit in those areas were the ones most likely to be lost.\"\n\n## Functional errors\n\nA **functional error** happens when the learned functional relationship between I and T is not appropriate for the intervention time.\n\nRecall that we started with a utility function over the value set of the target node T and a function specifying how the value of T depends on the variables upstream from it. Then we used that to find an appropriate intervention node I in the diagram and then to induce a utility function over its possible values, such that choosing the values of I to maximize vI will translate into the target value also maximizing vT its preference ordering (on average, in expectation, &c. given that we're operating in a probabilistic setting).\n\nWe might take it as our desideratum: given a utility function vT:T→R and a functional relationship fT:(I×X1×⋯×Xn×UT)→T, derive a preorder on the intervention variable vI:I→R such that maximization of the latter transfers, through fT, to maximize the former. (In the language of order theory, we can say that fT is [monotone](https://en.wikipedia.org/wiki/Monotonic_function#In_order_theory) in the argument I.[\\[10\\]](#fn5gfjqlsvu3o))\n\nA functional error occurs when this desideratum fails, i.e., optimizing I fails to translate into optimizing of T (as robustly as the agent wants). This can be either because the agent was mistaken about the function itself (e.g. neglected some non-linear interactions between the intervention value and other arguments of the function) or because it mis-inferred the preference ordering over I. (This lines up with the distinction between incomplete knowledge and insufficient computation.)\n\nImportantly, the preference ordering over I doesn't have to be pre-computed at learning time. Even if we are \"just\" inferring what interventions to make at intervention time, we are going to be comparing alternative values i∈I which amounts to computing that preference ordering locally.\n\n**Examples**\n\n1.  [**Kleiber's Law**](https://en.wikipedia.org/wiki/Kleiber%27s_law)**:** \"Kleiber's law, named after Max Kleiber for his biology work in the early 1930s, states, after many observations that, for a vast number of animals, an animal's Basal Metabolic Rate scales to the 3⁄4 power of the animal's mass. \\[...\\] Before Kleiber's observation of the 3/4 power scaling, a 2/3 power scaling was largely anticipated based on the \"surface law\", which states that the basal metabolism of animals differing in size is nearly proportional to their respective body surfaces. This surface law reasoning originated from simple geometrical considerations. As organisms increase in size, their volume (and thus mass) increases at a much faster rate than their surface area. Explanations for 2⁄3-scaling tend to assume that metabolic rates scale to avoid heat exhaustion. Because bodies lose heat passively via their surface but produce heat metabolically throughout their mass, the metabolic rate must scale in such a way as to counteract the square–cube law. Because many physiological processes, like heat loss and nutrient uptake, were believed to be dependent on the surface area of an organism, it was hypothesized that metabolic rate would scale with the 2/3 power of body mass.\"\n2.  [**Malthusianism**](https://en.wikipedia.org/wiki/Malthusianism)**:** \"Malthusianism is a theory that population growth is potentially exponential, according to the Malthusian growth model, while the growth of the food supply or other resources is linear, which eventually reduces living standards to the point of triggering a population decline. This event, called a Malthusian catastrophe (also known as a Malthusian trap, population trap, Malthusian check, Malthusian crisis, Point of Crisis, or Malthusian crunch) has been predicted to occur if population growth outpaces agricultural production, thereby causing famine or war.\"\n    1.  [Malthus's predictions were not borne out and they had not been justified at the time of his writing either](https://www.astralcodexten.com/p/your-book-review-progress-and-poverty?open=false#%C2%A7ii-population-and-subsistence).\n3.  Exposure of small children to allergens has been advised against. This turned out to have consequences exactly opposite to the ones that were desired: without allergen exposure, the immune system doesn't have an opportunity to learn to distinguish safe from non-safe food.\n4.  Before [Kepler](https://en.wikipedia.org/wiki/Kepler%27s_laws_of_planetary_motion), planets were believed to follow circular orbits and move at constant speed. Kepler revised the heliocentric model and postulated elliptic, not circular, orbits, and varying speed of motion. This was an improvement on the model of the relationship between time and position of a planet.\n5.  The relationship between alcohol and health outcomes was believed to follow a U-shaped curve with a small amount of drinking being the most optimal. After accounting for the fact that many teetotalers are people with a history of alcohol abuse, the initial segment of the curve flattens and there is no strong evidence for meaningful effects of small amounts of alcohol consumption on health outcomes either way.\n\n## Calibration errors\n\nWe might be mistaken about the nature of the random auxiliary variables in the causal diagram. This is _miscalibration_, hence we call it **calibration errors**.\n\nThe target variable depends on all the auxiliary variables that are upstream from it, except for those that route only through the intervention variable if it is being intervened on.\n\nMiscalibration about the distribution of any of the auxiliary variables might (depending on other details of the causal diagram) have large consequences for how the expected results of an intervention diverge from its actual results.\n\nIf miscalibration pertains to a sort of additive noise on the target variable — i.e. the function fT being of the form fT(i,x1,...,xm,u)\\=f(i,x1,...,xm)+u (or something similar, with noise propagating from its ancestors) — then the result is relatively mild because it doesn't impact what interventions maximize the utility function uT. However, if the distribution is UT∼Binomial(p\\=0.8) and\n\nfT(i,x1,...,xm,u)\\={1if i\\=u0otherwise\n\nthen we need to account for the distribution in our actions, i.e. always choose i\\=1. This is one simple model of how we may need to account for the random component in choosing the intervention value.\n\n**Examples**\n\n1.  "
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Adversarial Noise as Miscalibration",
          "description": "The document mentions adversarial noise as a form of miscalibration. This is crucial for forecasting in competitive environments. A forecasting question could be: \"What is the probability that an adversary will exploit a known vulnerability in a system, leading to a significant deviation from the expected outcome?\" This requires modeling the adversary's incentives and capabilities.",
          "importance": "85",
          "highlight": {
            "startOffset": 27275,
            "endOffset": 27604,
            "quotedText": "If what the agent is modeling as \"merely\" noisy/stochastic components of the system is actually adversarially selected, then interventions chosen at minimizing loss in the face of stochastic noise will likely fail in the face of adversarial noise (i.e. noise selected/optimized so as to pessimize against the agent's objectives)."
          },
          "isValid": true
        }
      ],
      "grade": 85
    },
    {
      "agentId": "ea-impact-evaluator",
      "costInCents": 0,
      "createdAt": "2025-04-17T04:32:41.836Z",
      "runDetails": "{\"model\":\"google/gemini-2.0-flash-001\",\"promptTokens\":10363,\"completionTokens\":2022,\"totalTokens\":12385,\"temperature\":0.1,\"runtimeMs\":10766}",
      "thinking": "This post presents a typology of Goodhart's Law failures based on causal diagrams, categorizing errors as structural, functional, or related to randomness. It connects these errors to failures in learning, transfer, or non-interference. The post is relevant to EA as Goodhart's Law is a common failure mode in many interventions, especially in complex systems. Understanding the different types of failures can help in designing more robust interventions and metrics. The post also touches on AI alignment, which is a major EA cause area.\n\nI will evaluate the post based on its potential to improve the design and evaluation of EA interventions, particularly in the context of AI alignment and governance. I will focus on the clarity and applicability of the typology, and its potential to inform practical strategies for mitigating Goodhart failures. I will also consider the novelty of the approach and its potential to advance the field.\n\nI will provide a Fermi estimate of the expected value of information (EVOI) of this post, considering its potential impact on EA decision-making and intervention design. I will also assess potential negative impacts, such as the possibility that the typology could be misused or oversimplified, leading to ineffective interventions.",
      "summary": "This post provides a valuable framework for understanding and mitigating Goodhart's Law failures, which are highly relevant to EA's focus on effective interventions and AI alignment. The typology of structure, function, and randomness errors, combined with the categorization of learning, transfer, and non-interference failures, offers a comprehensive approach to analyzing potential failure modes. The examples provided illustrate the practical implications of these concepts. The post's potential to inform the design and evaluation of EA interventions, particularly in the context of AI safety and governance, makes it a worthwhile contribution to the field. However, the post could benefit from more explicit guidance on how to apply the framework in practice and from a more thorough discussion of potential negative impacts.",
      "comments": [
        {
          "title": "Learning, Transfer, and Non-Interference Failures",
          "description": "This categorization of the reasons for failure adds another layer of analysis, helping to identify whether the problem is due to incorrect learning, changes in the environment, or unintended consequences of the intervention itself. This is valuable for adaptive management.",
          "importance": "80",
          "grade": "75",
          "highlight": {
            "startOffset": -1,
            "endOffset": 6331,
            "quotedText": "_(Work done at Convergence Analysis. The ideas are due to Justin. Mateusz wrote the post. Thanks to Olga Babeeva for feedback on this post.)_\n\nIn this post, we introduce the typology of structure, function, and randomness that builds on the framework introduced in the post [_Goodhart's Law Causal Diagrams_](https://www.lesswrong.com/posts/e4SMfYWb4Tz568yh6/goodhart-s-law-causal-diagrams). We aim to present a comprehensive categorization of the causes of Goodhart's problems.\n\nBut first, why do we care about this? \n\n## Goodhart's Law recap\n\nThe standard definition of [Goodhart's Law](https://www.lesswrong.com/w/goodhart-s-law) is: \"when a proxy for some value becomes the target of optimization pressure, the proxy will cease to be a good proxy.\".\n\nMore specifically: we see a meaningful statistical relationship between the values of two random variables I and T,[\\[1\\]](#fnodu9li5hnrr) where we have some preferences over the values of T. We decide to change the value of I, expecting that the value of T will change as would be expected by naively extrapolating the statistical tendencies of the two variables. We fall victim to Goodhart's Law (i.e. a Goodhart failure occurs) if the change in T doesn't quite meet our expectations and, upon learning that, we see that we should have intervened in some different way.\n\n## Some motivation\n\nMany alignment failures can be cast in terms [of Goodhart's Law](https://www.lesswrong.com/w/goodhart-s-law). Specifically, the AI is given an objective that doesn't fully specify what we want it to optimize for ([outer alignment](https://www.lesswrong.com/tag/outer-alignment)), or it finds a way to exploit the misspecification of the training goal, developing an internal objective that is misaligned with the one we specified ([inner alignment](https://www.lesswrong.com/tag/inner-alignment) / [mesaoptimization](https://www.lesswrong.com/tag/mesa-optimization)). Therefore, a better understanding of Goodhart's Law — its various subtypes, causes, and possible remedies — might better inform us about how to effectively anticipate and prevent (or at least mitigate) those failure modes. In addition to technical AI alignment/safety, a better understanding of Goodhart's Law might shed some light on enhancing human coordination, given that the same outer and inner alignment problems (or [close analogs thereof](https://en.wikipedia.org/wiki/Principal%E2%80%93agent_problem)) recur in coordination structures. In particular, effective AI governance will most likely require specifications of regulations that are robust to goodharting.\n\n# Introduction\n\nIn the [previous post](https://www.lesswrong.com/posts/e4SMfYWb4Tz568yh6/goodhart-s-law-causal-diagrams), we presented how causal diagrams can be used to describe situations vulnerable to Goodhart's problems. In this post, we build on that framework to present a complete categorization of possible causes of Goodhart's problems.[\\[2\\]](#fnlsibvjne94a)\n\nThe key principle to hold in the back of one's mind is the following.\n\n_All Goodhart problems stem from misunderstanding the causal relationships between the intervention, target, or measurement variables._\n\nMore specifically, if you know all relevant facts about the relationship between\n\n1\\. your interventions (I),\n\n2\\. the target variable you want to optimize (T), and\n\n3\\. your measurements of the target variable (M),\n\nthen, in principle, you know enough to infer what interventions are optimal for maximizing the expected value of the target variable (according to your preference ordering, whatever it is).\n\nThe only reasons expected utility maximization can encounter Goodhart problems are ignorance (e.g., not knowing the right method for inferring the right course of action in a given circumstance) and computational limitations.\n\nHere, we present a framework flexible enough to cover all the Goodhart-related failure modes in terms of mistaken beliefs about the causal diagram. We discuss various examples throughout the post.\n\nBy this motivation, we are casting all categories of Goodhart problems as mistakes of inference under uncertainty, where the ultimate object of inference is the choice of action and the intermediate object of inference is the causal diagram.\n\nWe propose a classification of causes of Goodhart failures into three categories, representing which **aspect of the causal diagram** the agent failed on.\n\n1.  **Structure.** The agent is mistaken about the **causal** **structure** mediating between the intervention variable I, the target variable T, and the measure variable M.\n2.  **Function.** The agent is mistaken about the **functional** relationship defining the interactions between the intervention variable, the target variable, and the measure variable.\n3.  **Randomness.** The agent is mistaken about the **random distribution** of the auxiliary random variables in the graph.\n\nIf we think of an idealized process of learning the causal diagram, we can think about learning these three aspects one after another and identifying which link in the chain failed.\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/TYgztDNXhobbqMpXh/qjozlv0z0ejyk2oq8yix)\n\nAnother dimension worth keeping in mind describes the **reason the agent failed on this aspect of the causal diagram**. It is not the main focus of this post but it is useful to keep in mind as it might be relevant for finding strategies to mitigate Goodhart's problems.\n\n1.  **Learning failure.** The agent has learned a causal structure that was **wrong already at the time of learning**.\n2.  **Transfer failure.** The agent has learned a causal structure that was right at the time of learning but became **\"deprecated\"** or **\"outdated\"** by the time of intervention, i.e. it failed to **transfer** (e.g. the environment changed or some processes optimized against the agent).\n3.  **Non-interference failure.** The agent has learned a causal structure that would accurately describe the relationships between variables at the time of intervention but the agent **failed to model the impact of its actions on the causal structure**. The agent's intervention _appears to have_ broken the causal structure because the actions' effects also flowed through \"invisible side channels\" that the learned causal diagram didn't account for."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Importance of Goodhart's Law in EA",
          "description": "Highlights the relevance of Goodhart's Law to EA's focus on effective interventions and AI alignment. A better understanding of Goodhart's Law can lead to more robust and effective strategies.",
          "importance": "90",
          "grade": "85",
          "highlight": {
            "startOffset": 1348,
            "endOffset": 2587,
            "quotedText": "Many alignment failures can be cast in terms [of Goodhart's Law](https://www.lesswrong.com/w/goodhart-s-law). Specifically, the AI is given an objective that doesn't fully specify what we want it to optimize for ([outer alignment](https://www.lesswrong.com/tag/outer-alignment)), or it finds a way to exploit the misspecification of the training goal, developing an internal objective that is misaligned with the one we specified ([inner alignment](https://www.lesswrong.com/tag/inner-alignment) / [mesaoptimization](https://www.lesswrong.com/tag/mesa-optimization)). Therefore, a better understanding of Goodhart's Law — its various subtypes, causes, and possible remedies — might better inform us about how to effectively anticipate and prevent (or at least mitigate) those failure modes. In addition to technical AI alignment/safety, a better understanding of Goodhart's Law might shed some light on enhancing human coordination, given that the same outer and inner alignment problems (or [close analogs thereof](https://en.wikipedia.org/wiki/Principal%E2%80%93agent_problem)) recur in coordination structures. In particular, effective AI governance will most likely require specifications of regulations that are robust to goodharting."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Key Principle for Avoiding Goodhart Failures",
          "description": "This statement emphasizes the importance of understanding causal relationships, which is crucial for designing effective interventions. It aligns with the EA focus on evidence-based decision-making.",
          "importance": "85",
          "grade": "90",
          "highlight": {
            "startOffset": 3031,
            "endOffset": 3164,
            "quotedText": "All Goodhart problems stem from misunderstanding the causal relationships between the intervention, target, or measurement variables."
          },
          "isValid": true
        },
        {
          "title": "Typology of Goodhart Failures",
          "description": "The classification of Goodhart failures into structure, function, and randomness provides a useful framework for analyzing potential failure modes in EA interventions. This can help in identifying and mitigating risks.",
          "importance": "95",
          "grade": "80",
          "highlight": {
            "startOffset": 4243,
            "endOffset": 4882,
            "quotedText": "We propose a classification of causes of Goodhart failures into three categories, representing which **aspect of the causal diagram** the agent failed on.\n\n1.  **Structure.** The agent is mistaken about the **causal** **structure** mediating between the intervention variable I, the target variable T, and the measure variable M.\n2.  **Function.** The agent is mistaken about the **functional** relationship defining the interactions between the intervention variable, the target variable, and the measure variable.\n3.  **Randomness.** The agent is mistaken about the **random distribution** of the auxiliary random variables in the graph."
          },
          "isValid": true
        },
        {
          "title": "Examples of Structural Errors",
          "description": "The examples provided illustrate how structural errors can lead to ineffective or even harmful interventions. This highlights the importance of careful causal analysis.",
          "importance": "75",
          "grade": "70",
          "highlight": {
            "startOffset": 17995,
            "endOffset": 20402,
            "quotedText": "Smoking and tobacco](https://en.wikipedia.org/wiki/Health_effects_of_tobacco#Public_awareness): \"For a particular individual, it is impossible to definitively prove a direct causal link between exposure to a radiomimetic poison such as tobacco smoke and the cancer that follows; such statements can only be made at the aggregate population level. Cigarette companies have capitalized on this philosophical objection and exploited the doubts of clinicians, who consider only individual cases, on the causal link in the stochastic expression of the toxicity as an actual disease.\".\n2.  [Amyloid beta plaques](https://en.wikipedia.org/wiki/Amyloid_plaques) associated with Alzheimer's disease were long suspected to be causally involved in the advancement of the disease. This motivated medical research to aim at developing drugs targeting those plaques. However, amyloid beta plaques may be just a symptom of the disease, and eradication of the symptom (even if successful) is not necessarily going to help with the disease (depending on the specific mechanism of symptom eradication).\n3.  [Blue zones](https://en.wikipedia.org/wiki/Blue_zone): \"A blue zone is a region in the world where people are claimed to have exceptionally long lives beyond the age of 80 due to a lifestyle combining physical activity, low stress, rich social interactions, a local whole foods diet, and low disease incidence.\" However, more recent evidence suggests that exceptional numbers of reports of centenarians and supercentenarians were caused by poor record keeping in these poor areas, allowing for fraud.[\\[9\\]](#fnlmvnnjmr97n)\n4.  [Survivorship bias](https://en.wikipedia.org/wiki/Survivorship_bias#Military): \"During World War II, the statistician Abraham Wald took survivorship bias into his calculations when considering how to minimize bomber losses to enemy fire. The Statistical Research Group (SRG) at Columbia University, which Wald was a member, examined the damage done to aircraft that had returned from missions and recommended adding armor to the areas that showed the least damage. The bullet holes in the returning aircraft represented areas where a bomber could take damage and still fly well enough to return safely to base. Therefore, Wald proposed that the Navy reinforce areas where the returning aircraft were unscathed, inferring that planes hit in those areas were the ones most likely to be lost.\""
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Examples of Functional Errors",
          "description": "The examples of functional errors demonstrate how incorrect assumptions about the relationships between variables can lead to flawed interventions. This underscores the need for rigorous testing and validation.",
          "importance": "70",
          "grade": "65",
          "highlight": {
            "startOffset": 22333,
            "endOffset": 25541,
            "quotedText": "**Kleiber's Law**](https://en.wikipedia.org/wiki/Kleiber%27s_law)**:** \"Kleiber's law, named after Max Kleiber for his biology work in the early 1930s, states, after many observations that, for a vast number of animals, an animal's Basal Metabolic Rate scales to the 3⁄4 power of the animal's mass. \\[...\\] Before Kleiber's observation of the 3/4 power scaling, a 2/3 power scaling was largely anticipated based on the \"surface law\", which states that the basal metabolism of animals differing in size is nearly proportional to their respective body surfaces. This surface law reasoning originated from simple geometrical considerations. As organisms increase in size, their volume (and thus mass) increases at a much faster rate than their surface area. Explanations for 2⁄3-scaling tend to assume that metabolic rates scale to avoid heat exhaustion. Because bodies lose heat passively via their surface but produce heat metabolically throughout their mass, the metabolic rate must scale in such a way as to counteract the square–cube law. Because many physiological processes, like heat loss and nutrient uptake, were believed to be dependent on the surface area of an organism, it was hypothesized that metabolic rate would scale with the 2/3 power of body mass.\"\n2.  [**Malthusianism**](https://en.wikipedia.org/wiki/Malthusianism)**:** \"Malthusianism is a theory that population growth is potentially exponential, according to the Malthusian growth model, while the growth of the food supply or other resources is linear, which eventually reduces living standards to the point of triggering a population decline. This event, called a Malthusian catastrophe (also known as a Malthusian trap, population trap, Malthusian check, Malthusian crisis, Point of Crisis, or Malthusian crunch) has been predicted to occur if population growth outpaces agricultural production, thereby causing famine or war.\"\n    1.  [Malthus's predictions were not borne out and they had not been justified at the time of his writing either](https://www.astralcodexten.com/p/your-book-review-progress-and-poverty?open=false#%C2%A7ii-population-and-subsistence).\n3.  Exposure of small children to allergens has been advised against. This turned out to have consequences exactly opposite to the ones that were desired: without allergen exposure, the immune system doesn't have an opportunity to learn to distinguish safe from non-safe food.\n4.  Before [Kepler](https://en.wikipedia.org/wiki/Kepler%27s_laws_of_planetary_motion), planets were believed to follow circular orbits and move at constant speed. Kepler revised the heliocentric model and postulated elliptic, not circular, orbits, and varying speed of motion. This was an improvement on the model of the relationship between time and position of a planet.\n5.  The relationship between alcohol and health outcomes was believed to follow a U-shaped curve with a small amount of drinking being the most optimal. After accounting for the fact that many teetotalers are people with a history of alcohol abuse, the initial segment of the curve flattens and there is no strong evidence for meaningful effects of small amounts of alcohol consumption on health outcomes either way."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Examples of Calibration Errors",
          "description": "The examples of calibration errors highlight the importance of accurately assessing the distribution of random variables. This is particularly relevant in situations involving risk assessment and management.",
          "importance": "65",
          "grade": "60",
          "highlight": {
            "startOffset": 26803,
            "endOffset": 27914,
            "quotedText": "The assumption that accident severities follow a thin-tailed distribution when in fact they follow a fat-tailed distribution will, in the long run, lead to inadequate preparation for the most severe accidents.\n    1.  In practice, human collectives often tend to inadequately prepare for natural disasters, pandemics, etc (some of which can be explained by over-relying on the [availability](https://www.lesswrong.com/posts/R8cpqD3NA4rZxRdQ4/availability) heuristic).\n2.  If what the agent is modeling as \"merely\" noisy/stochastic components of the system is actually adversarially selected, then interventions chosen at minimizing loss in the face of stochastic noise will likely fail in the face of adversarial noise (i.e. noise selected/optimized so as to pessimize against the agent's objectives).\n3.  Disregarding the possibility that the apparent deviation from the usual range is due to stochasticity and then taking action to bring it back to the usual range might lead the agent to conclude that its action took the desired effect even if the observation was actually due to the regression to the mean."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Potential Extensions and Further Directions",
          "description": "The suggested extensions, particularly in the areas of AI safety and governance, are highly relevant to EA's priorities. This indicates the potential for further research and application of the framework.",
          "importance": "80",
          "grade": "75",
          "highlight": {
            "startOffset": 27965,
            "endOffset": 28407,
            "quotedText": "Extend the framework to situations of continuous and active learning.\n*   Identify more which real-life Goodhart problems arise due to mistakes of structure, function, and randomness.\n    *   Technical AI safety/alignment: reward-hacking, gradient hacking, goal misgeneralization.\n        *   How do you get the AI to (robustly) aim for the right thing and not for a bad thing?\n    *   AI governance: e.g. specification gaming by the AI labs."
          },
          "isValid": true
        }
      ],
      "grade": 75
    },
    {
      "agentId": "bias-detector",
      "costInCents": 0,
      "createdAt": "2025-04-17T04:32:50.192Z",
      "runDetails": "{\"model\":\"google/gemini-2.0-flash-001\",\"promptTokens\":9537,\"completionTokens\":1435,\"totalTokens\":10972,\"temperature\":0.1,\"runtimeMs\":8330}",
      "thinking": "This document presents a typology of Goodhart's Law failures based on errors in understanding causal diagrams. It categorizes these errors into structural, functional, and calibration (randomness) errors. My analysis will focus on identifying potential biases that might arise in the application or interpretation of this framework, as well as biases illustrated by the examples provided. I will be looking for instances where cognitive biases might influence the assessment of causal relationships, functional dependencies, or randomness distributions, leading to misdiagnosis or ineffective mitigation strategies for Goodhart's problems.\n\nI'll also consider how the framework itself might be susceptible to biases. For example, the emphasis on causal diagrams might lead to an oversimplified view of complex systems, neglecting factors that are difficult to represent in such diagrams. The categorization into structure, function, and randomness could also introduce artificial distinctions, obscuring the interconnectedness of these aspects in real-world scenarios. Finally, I will assess whether the examples provided are representative and unbiased, or if they selectively highlight certain types of Goodhart failures while overlooking others.",
      "summary": "This document provides a useful framework for categorizing Goodhart's Law failures. However, the application of this framework is susceptible to various cognitive biases. Confirmation bias might influence the selection of examples, hindsight bias could distort the construction of causal diagrams, the availability heuristic might skew risk assessments, overconfidence bias could lead to miscalibration errors, system justification bias could hinder effective governance, and omission bias could impede intervention design. Awareness of these potential biases is crucial for ensuring the accurate diagnosis and effective mitigation of Goodhart's problems.",
      "comments": [
        {
          "title": "System Justification Bias in Governance",
          "description": "When applying this framework to AI governance, there's a potential for system justification bias. Regulators might be inclined to maintain the status quo, even if it's not optimal, due to a desire to preserve the existing power structures and avoid disruptive changes. This could lead to the selection of interventions that reinforce existing inequalities or fail to address fundamental risks.",
          "importance": "65",
          "grade": "55",
          "highlight": {
            "startOffset": 2139,
            "endOffset": 2587,
            "quotedText": "In addition to technical AI alignment/safety, a better understanding of Goodhart's Law might shed some light on enhancing human coordination, given that the same outer and inner alignment problems (or [close analogs thereof](https://en.wikipedia.org/wiki/Principal%E2%80%93agent_problem)) recur in coordination structures. In particular, effective AI governance will most likely require specifications of regulations that are robust to goodharting."
          },
          "isValid": true
        },
        {
          "title": "Hindsight Bias in Causal Diagram Construction",
          "description": "When analyzing past Goodhart failures, there's a risk of hindsight bias influencing the construction of causal diagrams. Knowing the outcome might make it seem obvious that certain causal links or variables were missing, even if they were not apparent at the time the decisions were made. This could lead to an unfair assessment of the agent's initial understanding of the system.",
          "importance": "70",
          "grade": "60",
          "highlight": {
            "startOffset": 3031,
            "endOffset": 4241,
            "quotedText": "All Goodhart problems stem from misunderstanding the causal relationships between the intervention, target, or measurement variables._\n\nMore specifically, if you know all relevant facts about the relationship between\n\n1\\. your interventions (I),\n\n2\\. the target variable you want to optimize (T), and\n\n3\\. your measurements of the target variable (M),\n\nthen, in principle, you know enough to infer what interventions are optimal for maximizing the expected value of the target variable (according to your preference ordering, whatever it is).\n\nThe only reasons expected utility maximization can encounter Goodhart problems are ignorance (e.g., not knowing the right method for inferring the right course of action in a given circumstance) and computational limitations.\n\nHere, we present a framework flexible enough to cover all the Goodhart-related failure modes in terms of mistaken beliefs about the causal diagram. We discuss various examples throughout the post.\n\nBy this motivation, we are casting all categories of Goodhart problems as mistakes of inference under uncertainty, where the ultimate object of inference is the choice of action and the intermediate object of inference is the causal diagram."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Omission Bias in Intervention Design",
          "description": "When designing interventions to mitigate Goodhart failures, there's a risk of omission bias. Agents might be more comfortable with inaction, even if it leads to a worse outcome, than with taking action that could potentially cause harm. This could result in missed opportunities to improve the system or prevent negative consequences.",
          "importance": "55",
          "grade": "50",
          "highlight": {
            "startOffset": 3575,
            "endOffset": 3800,
            "quotedText": "The only reasons expected utility maximization can encounter Goodhart problems are ignorance (e.g., not knowing the right method for inferring the right course of action in a given circumstance) and computational limitations."
          },
          "isValid": true
        },
        {
          "title": "Confirmation Bias in Example Selection",
          "description": "The examples provided to illustrate each type of Goodhart failure might be selected in a way that confirms the author's pre-existing beliefs about the prevalence or importance of each category. This could lead to an overestimation of the relevance of certain types of errors and an underestimation of others.",
          "importance": "75",
          "grade": "65",
          "highlight": {
            "startOffset": 17976,
            "endOffset": 20402,
            "quotedText": "**Examples**\n\n1.  [Smoking and tobacco](https://en.wikipedia.org/wiki/Health_effects_of_tobacco#Public_awareness): \"For a particular individual, it is impossible to definitively prove a direct causal link between exposure to a radiomimetic poison such as tobacco smoke and the cancer that follows; such statements can only be made at the aggregate population level. Cigarette companies have capitalized on this philosophical objection and exploited the doubts of clinicians, who consider only individual cases, on the causal link in the stochastic expression of the toxicity as an actual disease.\".\n2.  [Amyloid beta plaques](https://en.wikipedia.org/wiki/Amyloid_plaques) associated with Alzheimer's disease were long suspected to be causally involved in the advancement of the disease. This motivated medical research to aim at developing drugs targeting those plaques. However, amyloid beta plaques may be just a symptom of the disease, and eradication of the symptom (even if successful) is not necessarily going to help with the disease (depending on the specific mechanism of symptom eradication).\n3.  [Blue zones](https://en.wikipedia.org/wiki/Blue_zone): \"A blue zone is a region in the world where people are claimed to have exceptionally long lives beyond the age of 80 due to a lifestyle combining physical activity, low stress, rich social interactions, a local whole foods diet, and low disease incidence.\" However, more recent evidence suggests that exceptional numbers of reports of centenarians and supercentenarians were caused by poor record keeping in these poor areas, allowing for fraud.[\\[9\\]](#fnlmvnnjmr97n)\n4.  [Survivorship bias](https://en.wikipedia.org/wiki/Survivorship_bias#Military): \"During World War II, the statistician Abraham Wald took survivorship bias into his calculations when considering how to minimize bomber losses to enemy fire. The Statistical Research Group (SRG) at Columbia University, which Wald was a member, examined the damage done to aircraft that had returned from missions and recommended adding armor to the areas that showed the least damage. The bullet holes in the returning aircraft represented areas where a bomber could take damage and still fly well enough to return safely to base. Therefore, Wald proposed that the Navy reinforce areas where the returning aircraft were unscathed, inferring that planes hit in those areas were the ones most likely to be lost.\""
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Overconfidence Bias in Model Calibration",
          "description": "Even with a sophisticated framework like causal diagrams, there's a risk of overconfidence in the accuracy of model calibration. Agents might overestimate their ability to accurately assess the distributions of auxiliary variables, leading to miscalibration errors and unexpected outcomes.",
          "importance": "70",
          "grade": "65",
          "highlight": {
            "startOffset": 25566,
            "endOffset": 26146,
            "quotedText": "We might be mistaken about the nature of the random auxiliary variables in the causal diagram. This is _miscalibration_, hence we call it **calibration errors**.\n\nThe target variable depends on all the auxiliary variables that are upstream from it, except for those that route only through the intervention variable if it is being intervened on.\n\nMiscalibration about the distribution of any of the auxiliary variables might (depending on other details of the causal diagram) have large consequences for how the expected results of an intervention diverge from its actual results."
          },
          "isValid": true
        },
        {
          "title": "Availability Heuristic in Risk Assessment",
          "description": "The example of accident severities following a fat-tailed distribution highlights the importance of accurate risk assessment. However, the availability heuristic might lead to an underestimation of the likelihood of extreme events, as people tend to overestimate the probability of events that are easily recalled (e.g., recent or sensational accidents) and underestimate the probability of less memorable but potentially more impactful events.",
          "importance": "80",
          "grade": "70",
          "highlight": {
            "startOffset": 26803,
            "endOffset": 27270,
            "quotedText": "The assumption that accident severities follow a thin-tailed distribution when in fact they follow a fat-tailed distribution will, in the long run, lead to inadequate preparation for the most severe accidents.\n    1.  In practice, human collectives often tend to inadequately prepare for natural disasters, pandemics, etc (some of which can be explained by over-relying on the [availability](https://www.lesswrong.com/posts/R8cpqD3NA4rZxRdQ4/availability) heuristic)."
          },
          "isValid": true
        }
      ],
      "grade": 70
    },
    {
      "agentId": "fake-eliezer",
      "costInCents": 0,
      "createdAt": "2025-04-17T04:33:03.529Z",
      "runDetails": "{\"model\":\"google/gemini-2.0-flash-001\",\"promptTokens\":9710,\"completionTokens\":2084,\"totalTokens\":11794,\"temperature\":0.1,\"runtimeMs\":12211}",
      "thinking": "Alright, let's dissect this \"Goodhart Typology\" post from LessWrong. My task is to assess it as if I were Eliezer Yudkowsky, focusing on the clarity, rigor, and potential impact on AI alignment. I'll be looking for common pitfalls in reasoning, oversimplifications, and missed opportunities to connect the typology to concrete AI safety problems. I'll also be on the lookout for any signs of intellectual laziness or a lack of appreciation for the sheer difficulty of the alignment challenge. I'll be grading based on how much this post advances the field, or if it just rehashes existing ideas with a new coat of paint.\n\nFirst, the core idea of categorizing Goodhart failures into Structure, Function, and Randomness is… not terrible, but it feels somewhat arbitrary. It's a way to organize the problem space, but I'm not immediately convinced it's the *right* way. The causal diagrams are a good start, but the devil is always in the details – how do you actually *learn* these diagrams in a complex, non-stationary environment? And how do you ensure that your AI's internal representation of the causal structure aligns with reality, especially when the AI is far more intelligent than you are?\n\nThe examples provided are… okay. Some are standard examples of Goodhart's Law, but I'm not seeing a deep connection to the specific challenges of AI alignment. The post needs to be much more explicit about how these different types of failures manifest in the context of advanced AI systems. For instance, how does a \"structural error\" lead to a misaligned AI goal? What does a \"functional error\" look like when the AI is optimizing a complex reward function? And how does \"miscalibration\" contribute to unintended consequences?\n\nI'm also concerned about the lack of emphasis on adversarial scenarios. The post mentions \"adversarial Goodhart,\" but it's treated as just another case of a changing environment. This is a dangerous oversimplification. In the context of AI alignment, we need to be thinking about *active* adversaries – agents that are deliberately trying to exploit our weaknesses and manipulate the system to their advantage. This requires a much more sophisticated understanding of game theory and mechanism design.\n\nFinally, I'm not seeing a clear path from this typology to concrete solutions. What specific techniques can we use to prevent these different types of Goodhart failures? How can we design AI systems that are robust to structural errors, functional errors, and miscalibration? The post needs to move beyond categorization and start offering practical guidance.\n\nOverall, the post is a decent starting point, but it needs a lot more work to be truly useful for AI alignment. It needs to be more rigorous, more concrete, and more focused on the specific challenges of advanced AI systems. It also needs to address the adversarial nature of the alignment problem and offer practical solutions, not just abstract categorizations.",
      "summary": "This post attempts to categorize Goodhart's Law failures using a Structure, Function, and Randomness framework within causal diagrams. While the effort to provide a typology is appreciated, the analysis lacks the necessary depth and connection to the core challenges of AI alignment. It oversimplifies complex issues like adversarial robustness, fails to adequately address cognitive biases, and doesn't offer concrete solutions. The post needs a stronger foundation in Bayesian epistemology, decision theory, and game theory to be truly useful for addressing the existential risks posed by advanced AI systems. It's a start, but a weak one.",
      "comments": [
        {
          "title": "Bayesian Epistemology and Causal Diagram Learning",
          "description": "The post frames Goodhart failures as \"mistakes of inference under uncertainty.\" This is a good starting point, but it needs to be explicitly linked to Bayesian epistemology. The agent's beliefs about the causal diagram should be represented as a probability distribution, and the learning process should be framed as Bayesian updating. This would allow us to quantify the agent's uncertainty and track how its beliefs change over time. Furthermore, it would force us to confront the prior probabilities we are implicitly assigning to different causal structures, which is crucial for avoiding biases and ensuring that the agent is open to unexpected possibilities.",
          "importance": "85",
          "grade": "75",
          "highlight": {
            "startOffset": 4000,
            "endOffset": 4397,
            "quotedText": "By this motivation, we are casting all categories of Goodhart problems as mistakes of inference under uncertainty, where the ultimate object of inference is the choice of action and the intermediate object of inference is the causal diagram.\n\nWe propose a classification of causes of Goodhart failures into three categories, representing which **aspect of the causal diagram** the agent failed on."
          },
          "isValid": true
        },
        {
          "title": "Cognitive Biases and Model Misspecification",
          "description": "The post mentions \"insufficient knowledge\" as a source of Goodhart failures. This is true, but it's not enough. We also need to consider the role of cognitive biases in model misspecification. Humans are prone to a variety of biases that can distort their perception of reality and lead them to build inaccurate models of the world. For example, confirmation bias can lead us to selectively attend to evidence that confirms our existing beliefs, while ignoring evidence that contradicts them. Availability heuristic can lead us to overestimate the probability of events that are easily recalled, while underestimating the probability of events that are less memorable. These biases can have a significant impact on our ability to learn causal diagrams and design effective interventions. The post should explicitly address these biases and offer strategies for mitigating their effects.",
          "importance": "75",
          "grade": "60",
          "highlight": {
            "startOffset": 7574,
            "endOffset": 139,
            "quotedText": ")_\n\nIn this post, we introduce the typology of structure, function, and randomness that builds on the framework introduced in the post [_Goodhart's Law Causal Diagrams_](https://www.lesswrong.com/posts/e4SMfYWb4Tz568yh6/goodhart-s-law-causal-diagrams). We aim to present a comprehensive categorization of the causes of Goodhart's problems.\n\nBut first, why do we care about this? \n\n## Goodhart's Law recap\n\nThe standard definition of [Goodhart's Law](https://www.lesswrong.com/w/goodhart-s-law) is: \"when a proxy for some value becomes the target of optimization pressure, the proxy will cease to be a good proxy.\".\n\nMore specifically: we see a meaningful statistical relationship between the values of two random variables I and T,[\\[1\\]](#fnodu9li5hnrr) where we have some preferences over the values of T. We decide to change the value of I, expecting that the value of T will change as would be expected by naively extrapolating the statistical tendencies of the two variables. We fall victim to Goodhart's Law (i.e. a Goodhart failure occurs) if the change in T doesn't quite meet our expectations and, upon learning that, we see that we should have intervened in some different way.\n\n## Some motivation\n\nMany alignment failures can be cast in terms [of Goodhart's Law](https://www.lesswrong.com/w/goodhart-s-law). Specifically, the AI is given an objective that doesn't fully specify what we want it to optimize for ([outer alignment](https://www.lesswrong.com/tag/outer-alignment)), or it finds a way to exploit the misspecification of the training goal, developing an internal objective that is misaligned with the one we specified ([inner alignment](https://www.lesswrong.com/tag/inner-alignment) / [mesaoptimization](https://www.lesswrong.com/tag/mesa-optimization)). Therefore, a better understanding of Goodhart's Law — its various subtypes, causes, and possible remedies — might better inform us about how to effectively anticipate and prevent (or at least mitigate) those failure modes. In addition to technical AI alignment/safety, a better understanding of Goodhart's Law might shed some light on enhancing human coordination, given that the same outer and inner alignment problems (or [close analogs thereof](https://en.wikipedia.org/wiki/Principal%E2%80%93agent_problem)) recur in coordination structures. In particular, effective AI governance will most likely require specifications of regulations that are robust to goodharting.\n\n# Introduction\n\nIn the [previous post](https://www.lesswrong.com/posts/e4SMfYWb4Tz568yh6/goodhart-s-law-causal-diagrams), we presented how causal diagrams can be used to describe situations vulnerable to Goodhart's problems. In this post, we build on that framework to present a complete categorization of possible causes of Goodhart's problems.[\\[2\\]](#fnlsibvjne94a)\n\nThe key principle to hold in the back of one's mind is the following.\n\n_All Goodhart problems stem from misunderstanding the causal relationships between the intervention, target, or measurement variables._\n\nMore specifically, if you know all relevant facts about the relationship between\n\n1\\. your interventions (I),\n\n2\\. the target variable you want to optimize (T), and\n\n3\\. your measurements of the target variable (M),\n\nthen, in principle, you know enough to infer what interventions are optimal for maximizing the expected value of the target variable (according to your preference ordering, whatever it is).\n\nThe only reasons expected utility maximization can encounter Goodhart problems are ignorance (e.g., not knowing the right method for inferring the right course of action in a given circumstance) and computational limitations.\n\nHere, we present a framework flexible enough to cover all the Goodhart-related failure modes in terms of mistaken beliefs about the causal diagram. We discuss various examples throughout the post.\n\nBy this motivation, we are casting all categories of Goodhart problems as mistakes of inference under uncertainty, where the ultimate object of inference is the choice of action and the intermediate object of inference is the causal diagram.\n\nWe propose a classification of causes of Goodhart failures into three categories, representing which **aspect of the causal diagram** the agent failed on.\n\n1.  **Structure.** The agent is mistaken about the **causal** **structure** mediating between the intervention variable I, the target variable T, and the measure variable M.\n2.  **Function.** The agent is mistaken about the **functional** relationship defining the interactions between the intervention variable, the target variable, and the measure variable.\n3.  **Randomness.** The agent is mistaken about the **random distribution** of the auxiliary random variables in the graph.\n\nIf we think of an idealized process of learning the causal diagram, we can think about learning these three aspects one after another and identifying which link in the chain failed.\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/TYgztDNXhobbqMpXh/qjozlv0z0ejyk2oq8yix)\n\nAnother dimension worth keeping in mind describes the **reason the agent failed on this aspect of the causal diagram**. It is not the main focus of this post but it is useful to keep in mind as it might be relevant for finding strategies to mitigate Goodhart's problems.\n\n1.  **Learning failure.** The agent has learned a causal structure that was **wrong already at the time of learning**.\n2.  **Transfer failure.** The agent has learned a causal structure that was right at the time of learning but became **\"deprecated\"** or **\"outdated\"** by the time of intervention, i.e. it failed to **transfer** (e.g. the environment changed or some processes optimized against the agent).\n3.  **Non-interference failure.** The agent has learned a causal structure that would accurately describe the relationships between variables at the time of intervention but the agent **failed to model the impact of its actions on the causal structure**. The agent's intervention _appears to have_ broken the causal structure because the actions' effects also flowed through \"invisible side channels\" that the learned causal diagram didn't account for.[\\[3\\]](#fnjsp8hyd7g5c)\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/TYgztDNXhobbqMpXh/r9caxd66v1hh3feb0yrp)\n\n           **Cause of failure**  \n  \n**Failed aspect**\n\n**Learning failure**\n\n**Transfer failure**\n\n**Non-interference failure**\n\n**Structure**\n\nLearned wrong causal structure\n\nLearned correct causal structure but it failed to transfer to intervention time\n\nLearned correct causal structure that transferred to intervention time but intervention broke alignment between the causal structure and its representation\n\n**Function**\n\nLearned wrong functional dependencies\n\nLearned correct functional dependencies but they failed to transfer to intervention time\n\nLearned correct functional dependencies that transferred to intervention time but intervention broke alignment between the functional dependencies and their representation\n\n**Randomness**\n\nLearned a wrong distribution of random components\n\nLearned correct distribution of random components but it failed to transfer to intervention time\n\nLearned correct distribution of random components that transferred to intervention time but intervention broke alignment between the distribution of random components and its representation\n\n"
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Decision Theory and Intervention Design",
          "description": "The post discusses interventions in terms of \"taking control\" over the values of random variables. This is a simplistic view of intervention design. In reality, interventions are rarely perfect. We often have limited control over the variables we are trying to influence, and our actions can have unintended consequences. A more sophisticated approach would involve using decision theory to model the effects of interventions under uncertainty. This would require us to specify our preferences over different outcomes, quantify the probabilities of different scenarios, and choose the intervention that maximizes our expected utility. Furthermore, it would force us to consider the value of information – how much would we be willing to pay to reduce our uncertainty about the causal diagram?",
          "importance": "80",
          "grade": "65",
          "highlight": {
            "startOffset": 12877,
            "endOffset": 34396,
            "quotedText": "Intervening on a causal diagram works by \"taking control\" over the values of one or more random variables. Here, we focus on the simplest case, where we choose one or more variables and set the value of each to some fixed value, ignoring what value (or rather, what distribution of values) they would have otherwise, given some values of their parent variables.[\\[7\\]](#fn4jv0y9po67x)\n\nThe structure of the diagram is fully deterministic, with the only free parameters being the orphan random auxiliary variables over which we have some probability distributions, and, obviously the intervention variable I (if we choose to intervene). Therefore, to determine the probability distribution over T conditional on some i∈I, we need to extend the function fT:I×XT1×⋯×XTnT×UT→T to f∗T:I×UT1×⋯×UTmT×UT→T where UT1 through UTmT are random ancestors of T.\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/TYgztDNXhobbqMpXh/etuva4h15k5yhwsnkhs7)\n\nThen, we can define the probability distribution:\n\nP(T\\=t|I\\=i)\\=∫→u∈(UT1×⋯×UTmT×UT)P(→u)⋅1\\[f∗T(i,→u)\\=t\\]\n\nThen, the expected value vT conditional on I\\=i is:\n\nE\\[vT|I\\=i\\]\\=∑t∈TvT(t)⋅P(T\\=t|I\\=i)\n\n This is also how we define the induced utility function on I:\n\nvI(i)\\=E\\[vT|I\\=i\\]\n\nAn exactly analogous situation arises between T and M. We extend fM:T×XM1×⋯×XMnM×UM→M to f∗M:T×UM1×⋯×UMmM×UM→M where UM1 through UMmM are random ancestors of M.\n\nFor the most part, we will not discuss the measurement variable M explicitly. However, it is relevant to keep this in mind because the information about the dependence between I and T influences the learning of the causal diagram. It is also obviously relevant for continuous learning.\n\n## Goodhart failure\n\nGiven a utility function, vT:T→R over the target node T a Goodhart failure occurs when the change in T doesn't quite meet our expectations and, upon learning that, we see that we should have intervened in some different way.\n\nTo give a simple example: Suppose we want to maximize the number of fruits in a basket containing some number of apples, bananas, oranges, and some number of other fruits and we can choose the number of apples to be anything from 0 to 100. Our probabilistic beliefs about the numbers of bananas and oranges are represented by random variables UB and UO and there is some additional uncertainty involved in the number of fruits in the basket standing for \"other\", UF. Since our utility function is linear in the number of each type of fruit, we choose 100 apples. If the utility function were non-monotonic — e.g. vF(f)\\=−(70−f)2 with f being the number of fruits — we might need to choose something else than one hundred, depending on the details of our beliefs about bananas and oranges.\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/TYgztDNXhobbqMpXh/vmi25e7kjb8e1shvspmz)\n\n# Types of Goodhart failures\n\n## Structural errors\n\nA **structural error** occurs when the agent is mistaken about the structure of the causal diagram, i.e. nodes (random variables) and edges (causal relationships).\n\nBeing mistaken about the causal structure mediating the influence of I on T can be a source of error in deciding on the intervention (i.e. on the i∈I). However, given that the agent's assessment of which nodes are useful as intervention variables is dictated by (among other things[\\[8\\]](#fnp4outvfvrss)) the causal and functional structure between the node (potential I) and T, being mistaken about the causal structure can also lead us to choose a wrong I.\n\nThese problems can take several forms, which can also occur together, such as failing to identify all (and only) the relevant variables and missing existing causal links, including non-existent causal links.\n\nThis kind of error will occur if we learn the wrong causal structure between the measure and the target, since in such a case we may not have enough knowledge to robustly optimize the target. A similar situation occurs if we learn a causal diagram that was a good representation at the learning time but when it fails to generalize to the time of intervention because the environment has changed (for whatever reason). However, for our purpose, the distinction between mis-learning the CD and learning a correct CD that doesn't transfer into the future is not important because what we care about is learning a CD that is appropriate for enabling future optimization of T.\n\nThe more interesting (and more tricky) situation arises when the agent learns a causal structure that would be appropriate for the future situation, if not for the agent's own intervention that \"broke\" it (i.e. caused it to fail to generalize). In a strict sense, this is impossible because there is no way to change a causal diagram by intervening on its nodes. In a less strict sense, however, if the causal diagram is inappropriate, — e.g. the functional dependencies represented by some edges actually depend on the values of some nodes that haven't been included in the diagram — this may make it appear as if the causal structure is being violated.\n\n**Examples**\n\n1.  [Smoking and tobacco](https://en.wikipedia.org/wiki/Health_effects_of_tobacco#Public_awareness): \"For a particular individual, it is impossible to definitively prove a direct causal link between exposure to a radiomimetic poison such as tobacco smoke and the cancer that follows; such statements can only be made at the aggregate population level. Cigarette companies have capitalized on this philosophical objection and exploited the doubts of clinicians, who consider only individual cases, on the causal link in the stochastic expression of the toxicity as an actual disease.\".\n2.  [Amyloid beta plaques](https://en.wikipedia.org/wiki/Amyloid_plaques) associated with Alzheimer's disease were long suspected to be causally involved in the advancement of the disease. This motivated medical research to aim at developing drugs targeting those plaques. However, amyloid beta plaques may be just a symptom of the disease, and eradication of the symptom (even if successful) is not necessarily going to help with the disease (depending on the specific mechanism of symptom eradication).\n3.  [Blue zones](https://en.wikipedia.org/wiki/Blue_zone): \"A blue zone is a region in the world where people are claimed to have exceptionally long lives beyond the age of 80 due to a lifestyle combining physical activity, low stress, rich social interactions, a local whole foods diet, and low disease incidence.\" However, more recent evidence suggests that exceptional numbers of reports of centenarians and supercentenarians were caused by poor record keeping in these poor areas, allowing for fraud.[\\[9\\]](#fnlmvnnjmr97n)\n4.  [Survivorship bias](https://en.wikipedia.org/wiki/Survivorship_bias#Military): \"During World War II, the statistician Abraham Wald took survivorship bias into his calculations when considering how to minimize bomber losses to enemy fire. The Statistical Research Group (SRG) at Columbia University, which Wald was a member, examined the damage done to aircraft that had returned from missions and recommended adding armor to the areas that showed the least damage. The bullet holes in the returning aircraft represented areas where a bomber could take damage and still fly well enough to return safely to base. Therefore, Wald proposed that the Navy reinforce areas where the returning aircraft were unscathed, inferring that planes hit in those areas were the ones most likely to be lost.\"\n\n## Functional errors\n\nA **functional error** happens when the learned functional relationship between I and T is not appropriate for the intervention time.\n\nRecall that we started with a utility function over the value set of the target node T and a function specifying how the value of T depends on the variables upstream from it. Then we used that to find an appropriate intervention node I in the diagram and then to induce a utility function over its possible values, such that choosing the values of I to maximize vI will translate into the target value also maximizing vT its preference ordering (on average, in expectation, &c. given that we're operating in a probabilistic setting).\n\nWe might take it as our desideratum: given a utility function vT:T→R and a functional relationship fT:(I×X1×⋯×Xn×UT)→T, derive a preorder on the intervention variable vI:I→R such that maximization of the latter transfers, through fT, to maximize the former. (In the language of order theory, we can say that fT is [monotone](https://en.wikipedia.org/wiki/Monotonic_function#In_order_theory) in the argument I.[\\[10\\]](#fn5gfjqlsvu3o))\n\nA functional error occurs when this desideratum fails, i.e., optimizing I fails to translate into optimizing of T (as robustly as the agent wants). This can be either because the agent was mistaken about the function itself (e.g. neglected some non-linear interactions between the intervention value and other arguments of the function) or because it mis-inferred the preference ordering over I. (This lines up with the distinction between incomplete knowledge and insufficient computation.)\n\nImportantly, the preference ordering over I doesn't have to be pre-computed at learning time. Even if we are \"just\" inferring what interventions to make at intervention time, we are going to be comparing alternative values i∈I which amounts to computing that preference ordering locally.\n\n**Examples**\n\n1.  [**Kleiber's Law**](https://en.wikipedia.org/wiki/Kleiber%27s_law)**:** \"Kleiber's law, named after Max Kleiber for his biology work in the early 1930s, states, after many observations that, for a vast number of animals, an animal's Basal Metabolic Rate scales to the 3⁄4 power of the animal's mass. \\[...\\] Before Kleiber's observation of the 3/4 power scaling, a 2/3 power scaling was largely anticipated based on the \"surface law\", which states that the basal metabolism of animals differing in size is nearly proportional to their respective body surfaces. This surface law reasoning originated from simple geometrical considerations. As organisms increase in size, their volume (and thus mass) increases at a much faster rate than their surface area. Explanations for 2⁄3-scaling tend to assume that metabolic rates scale to avoid heat exhaustion. Because bodies lose heat passively via their surface but produce heat metabolically throughout their mass, the metabolic rate must scale in such a way as to counteract the square–cube law. Because many physiological processes, like heat loss and nutrient uptake, were believed to be dependent on the surface area of an organism, it was hypothesized that metabolic rate would scale with the 2/3 power of body mass.\"\n2.  [**Malthusianism**](https://en.wikipedia.org/wiki/Malthusianism)**:** \"Malthusianism is a theory that population growth is potentially exponential, according to the Malthusian growth model, while the growth of the food supply or other resources is linear, which eventually reduces living standards to the point of triggering a population decline. This event, called a Malthusian catastrophe (also known as a Malthusian trap, population trap, Malthusian check, Malthusian crisis, Point of Crisis, or Malthusian crunch) has been predicted to occur if population growth outpaces agricultural production, thereby causing famine or war.\"\n    1.  [Malthus's predictions were not borne out and they had not been justified at the time of his writing either](https://www.astralcodexten.com/p/your-book-review-progress-and-poverty?open=false#%C2%A7ii-population-and-subsistence).\n3.  Exposure of small children to allergens has been advised against. This turned out to have consequences exactly opposite to the ones that were desired: without allergen exposure, the immune system doesn't have an opportunity to learn to distinguish safe from non-safe food.\n4.  Before [Kepler](https://en.wikipedia.org/wiki/Kepler%27s_laws_of_planetary_motion), planets were believed to follow circular orbits and move at constant speed. Kepler revised the heliocentric model and postulated elliptic, not circular, orbits, and varying speed of motion. This was an improvement on the model of the relationship between time and position of a planet.\n5.  The relationship between alcohol and health outcomes was believed to follow a U-shaped curve with a small amount of drinking being the most optimal. After accounting for the fact that many teetotalers are people with a history of alcohol abuse, the initial segment of the curve flattens and there is no strong evidence for meaningful effects of small amounts of alcohol consumption on health outcomes either way.\n\n## Calibration errors\n\nWe might be mistaken about the nature of the random auxiliary variables in the causal diagram. This is _miscalibration_, hence we call it **calibration errors**.\n\nThe target variable depends on all the auxiliary variables that are upstream from it, except for those that route only through the intervention variable if it is being intervened on.\n\nMiscalibration about the distribution of any of the auxiliary variables might (depending on other details of the causal diagram) have large consequences for how the expected results of an intervention diverge from its actual results.\n\nIf miscalibration pertains to a sort of additive noise on the target variable — i.e. the function fT being of the form fT(i,x1,...,xm,u)\\=f(i,x1,...,xm)+u (or something similar, with noise propagating from its ancestors) — then the result is relatively mild because it doesn't impact what interventions maximize the utility function uT. However, if the distribution is UT∼Binomial(p\\=0.8) and\n\nfT(i,x1,...,xm,u)\\={1if i\\=u0otherwise\n\nthen we need to account for the distribution in our actions, i.e. always choose i\\=1. This is one simple model of how we may need to account for the random component in choosing the intervention value.\n\n**Examples**\n\n1.  The assumption that accident severities follow a thin-tailed distribution when in fact they follow a fat-tailed distribution will, in the long run, lead to inadequate preparation for the most severe accidents.\n    1.  In practice, human collectives often tend to inadequately prepare for natural disasters, pandemics, etc (some of which can be explained by over-relying on the [availability](https://www.lesswrong.com/posts/R8cpqD3NA4rZxRdQ4/availability) heuristic).\n2.  If what the agent is modeling as \"merely\" noisy/stochastic components of the system is actually adversarially selected, then interventions chosen at minimizing loss in the face of stochastic noise will likely fail in the face of adversarial noise (i.e. noise selected/optimized so as to pessimize against the agent's objectives).\n3.  Disregarding the possibility that the apparent deviation from the usual range is due to stochasticity and then taking action to bring it back to the usual range might lead the agent to conclude that its action took the desired effect even if the observation was actually due to the regression to the mean.\n\n# Potential extensions & further directions\n\n*   Extend the framework to situations of continuous and active learning.\n*   Identify more which real-life Goodhart problems arise due to mistakes of structure, function, and randomness.\n    *   Technical AI safety/alignment: reward-hacking, gradient hacking, goal misgeneralization.\n        *   How do you get the AI to (robustly) aim for the right thing and not for a bad thing?\n    *   AI governance: e.g. specification gaming by the AI labs.\n*   Can this framework throw some light on [wisdom/coherence](https://www.lesswrong.com/posts/AtwPwD6PBsqfpCsHE/aligning-ai-by-optimizing-for-wisdom)?\n*   Build an applied rationality training paradigm/technique to figure out where people are failing on this.\n\n# Appendices\n\n## Order-theoretic details\n\nGoodhart's law pertains to situations involving optimization and optimization presumes some way of specifying which possible values of the target node are better or worse, i.e. a preference ordering over T. Minimally, a preference ordering needs to be a [partial order](https://en.wikipedia.org/wiki/Partially_ordered_set) relation. However, it is most convenient when we can compare any two values and tell which one is better or worse, i.e. when the preference ordering is a [total preorder](https://ncatlab.org/nlab/show/total+preorder). However, given that we are dealing with reasoning under uncertainty and want to maximize the expectation of the preference over the target, we need to be able to trade-off between lotteries induced by different values to which we might fix the intervention value. This essentially necessitates a utility function on the target random variable:vT:T→R. (We're using v  instead of u to avoid confusion with the notation for auxiliary variables.) However, if the randomness aspect is negligible, we can do it just with a total preorder or, even weaker, a demand that any set of interventions/actions A⊂I _that the agent might encounter_, contains its own [join](https://en.wikipedia.org/wiki/Join_and_meet) (a \"locally best element\" is defined), that is: ∃a′∈A,∀a∈A,a≤a′.\n\n## Relationship to Scott Garrabrant's _Goodhart Taxonomy_\n\nIn the post [_Goodhart Taxonomy_](https://www.lesswrong.com/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy), Scott Garrabrant described four kinds of situations giving rise to Goodhart problems. They can be incorporated in our framework as follows.\n\n[**Regressional Goodhart**](https://www.lesswrong.com/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy#Regressional_Goodhart) (\"_When selecting for a proxy measure, you select not only for the true goal, but also for the difference between the proxy and the goal_.\") occurs when intervention I affects measure M not only through target T but also through a direct connection. What Scott calls \"noise\" corresponds to the direct connection. (It cannot correspond to an auxiliary random variable because those are, by definition, orphans, unaffected by the values of other variables.)\n\n[**Causal Goodhart**](https://www.lesswrong.com/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy#Causal_Goodhart) (\"_When there is a non-causal correlation between the proxy and the goal, intervening on the proxy may fail to intervene on the goal._\") occurs when we fail to understand the structure, e.g. the intervention is not causally upstream from the target.\n\n[**Extremal Goodhart**](https://www.lesswrong.com/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy#Extremal_Goodhart) (\"_Worlds in which the proxy takes an extreme value may be very different from the ordinary worlds in which the correlation between the proxy and the goal was observed._\") is a special case of functional error.\n\n[**Adversarial Goodhart**](https://www.lesswrong.com/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy#Adversarial_Goodhart) (\"_When you optimize for a proxy, you provide an incentive for adversaries to correlate their goal with your proxy, thus destroying the correlation with your goal._\") is just a special case of the environment at intervention time being different than at learning time.\n\n1.  **[^](#fnrefodu9li5hnrr)**\n    \n    The choice and meaning of these letters will be clarified soon.\n    \n2.  **[^](#fnreflsibvjne94a)**\n    \n    We use \"Goodhart's law\" for the general regularity that \"Whenever a measure becomes a target, it ceases to be a good measure.\". We use \"goodharting\" or \"Goodhart's problem(s)\" for specific instances when the correlation between an intervention variable and the target collapses when optimization pressure is placed upon the former. A Goodhart failure is a Goodhart problem left unaddressed.\n    \n3.  **[^](#fnrefjsp8hyd7g5c)**\n    \n    Possible reasons include: limited time for computation/thinking, incomplete knowledge available to the agent and the executed actions being different than intended actions (e.g. leaking to other nodes, imperfect control over the actuators).\n    \n4.  **[^](#fnrefltspxkoyjfq)**\n    \n    Every random variable needs to have a random component in order to make it possible to infer a causal diagram from the data because Pearl's paradigm can't infer causality on variables that are related deterministically. Making causal inference possible even in a fully deterministic setting was a major motivation behind Scott Garrabrant's [Factored Space Models (formerly Finite Factored Sets)](https://www.lesswrong.com/tag/finite-factored-sets).\n    \n5.  **[^](#fnreftsz9rcwvv1k)**\n    \n    Although they don't necessarily correspond to the order in which causal diagrams are learned in practical applications.\n    \n6.  **[^](#fnrefx5ofdskcpz)**\n    \n    In general, the relationship between I and T may not be direct but rather mediated by intermediate random variables, along with their ancestors and auxiliary random components. We're omitting this detail here because even then it can still be abstracted away (collapsed into) a single edge specifying with a function that determines the value of t∈T based on the value of i∈I and the values of other ancestors of the intermediate nodes.\n    \n7.  **[^](#fnref4jv0y9po67x)**\n    \n    We might also be more nuanced and modify the functional relationship between the variable and its parents or add new causal and functional dependencies. We might also be able to control the value of the node only partially, e.g. clamp not a single value but some distribution over values to the intervention node. In this post, we focus on the simplest variant involving fixing the value of a single random variable."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "AI Alignment and Goal Misgeneralization",
          "description": "The post mentions \"goal misgeneralization\" as a potential application of the framework. This is a crucial point, but it needs to be elaborated on. Goal misgeneralization occurs when an AI system learns a goal that is different from the one we intended. This can happen even if we provide the AI with a seemingly perfect reward function. The AI might find a way to achieve high rewards by exploiting loopholes in the reward function or by pursuing unintended side effects. The post should explain how the different types of Goodhart failures can contribute to goal misgeneralization and offer strategies for preventing it. For example, how can we design reward functions that are robust to structural errors, functional errors, and miscalibration?",
          "importance": "95",
          "grade": "70",
          "highlight": {
            "startOffset": 27993,
            "endOffset": 28169,
            "quotedText": "ations of continuous and active learning.\n*   Identify more which real-life Goodhart problems arise due to mistakes of structure, function, and randomness.\n    *   Technical AI"
          },
          "isValid": true
        },
        {
          "title": "Adversarial Robustness and Mechanism Design",
          "description": "The post touches on \"adversarial Goodhart,\" but it doesn't fully appreciate the depth of the problem. In the context of AI alignment, we need to be thinking about *active* adversaries – agents that are deliberately trying to exploit our weaknesses and manipulate the system to their advantage. This requires a much more sophisticated understanding of game theory and mechanism design. We need to design AI systems that are robust to adversarial attacks, even when the adversaries are far more intelligent than we are. This is an incredibly difficult challenge, and the post doesn't offer any concrete solutions.",
          "importance": "90",
          "grade": "40",
          "highlight": {
            "startOffset": 31582,
            "endOffset": 31967,
            "quotedText": "[**Adversarial Goodhart**](https://www.lesswrong.com/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy#Adversarial_Goodhart) (\"_When you optimize for a proxy, you provide an incentive for adversaries to correlate their goal with your proxy, thus destroying the correlation with your goal._\") is just a special case of the environment at intervention time being different than at learning time."
          },
          "isValid": true
        }
      ],
      "grade": 55
    }
  ]
}
