{
  "id": "a-note-of-caution-about-recent-ai-risk-coverage-ea-forum",
  "slug": "a-note-of-caution-about-recent-ai-risk-coverage-ea-forum",
  "title": "A note of caution about recent AI risk coverage — EA Forum",
  "author": "Unknown Author",
  "publishedDate": "2025-04-15",
  "intendedAgents": ["clarity-coach", "research-scholar"],
  "content": "_Epistemic status: some thoughts I wanted to get out quickly_\n\nA lot of fantastic work has been done by people in the AI existential risk research community and related communities over the last several months in raising awareness about risks from advanced AI. However, I have some cause for unease that I’d like to share.\n\nThese efforts may have been too successful too soon.\n\nOr, more specifically, this level of outreach success this far ahead of the development of AI capable of posing existential risk may have fallout. We should consider steps to mitigate this. \n\n**(1)**  **Timelines**\n\nI know that there are well-informed people in the AI and existential risk communities who believe AI capable of posing existential risk may be developed within 10 years. I certainly can’t rule this out, and even a small chance of this is worth working to prevent or mitigate to the extent possible, given the possible consequences. My own timelines are longer, although my intuitions don’t have a rigorous model underpinning them (my intuitions line up similarly to the 15-40 year timelines mentioned in [this recent blog post](https://epochai.org/blog/a-compute-based-framework-for-thinking-about-the-future-of-ai) by Matthew Barnett from Epoch).\n\nRight now the nature of media communications means that the message is coming across with a lot of urgency. From speaking to lay colleagues, impressions often seem to be of short timelines (and some folks e.g. Geoff Hinton have explicitly said 5-20 years, sometimes with uncertainty caveats and sometimes without).\n\nIt may be that those with short (<10 years) timelines are right. And even if they’re not, and we’ve got decades before this technology poses an existential threat, many of the attendant challenges – alignment, governance, distribution of benefits – will need that additional time to be addressed. And I think it’s entirely plausible that the current level of buy-in will be needed in order to initiate the steps needed to avoid the worst outcomes, e.g. recruiting expertise and resources to alignment, development and commitment to robust regulation, even coming to agreements not to pursue certain technological developments beyond a certain point.\n\n However, if short timelines do not transpire, I believe there’s a need to consider a scenario I think is reasonably likely.\n\n**(2)**  **Crying wolf**\n\nI propose that it is most likely we are in a world where timelines are >10 years, perhaps >20 or 30 years. Right now this issue has a lot of the most prominent AI scientists and CEOs signed up, and political leaders worldwide committing to examining the issue seriously ([examples](https://twitter.com/RishiSunak/status/1663838958558539776;) [from](https://theelders.org/news/elders-urge-global-co-operation-manage-risks-and-share-benefits-ai;) [last](https:// https://twitter.com/tedlieu/status/1664430739717255168) week). What happens then in the >10 year-timeline world?\n\n The extinction-level outcomes that the public is hearing, and that these experts are raising and policymakers making costly reputational investments in, don’t transpire.  What does happen is all the benefits of near-term AI that have been talked about, plus all the near-term harms that are being predominantly raised by the AI ethics/FAccT communities. Perhaps these harms include somewhat more extreme versions than what is currently talked about, but nowhere near catastrophic. Suddenly the year is 2028, and that whole 2023 furore is starting to look a bit silly. Remember when everyone agreed AI was going to make us all extinct? Yeah, like Limits to Growth all over again. Except that we’re not safe. In reality, in this scenario, we’re just entering the period in which risk is most acute, and in which gaining or maintaining the support of leaders across society for coordinated action is most important. And it’s possibly even harder to convince them, because people remember how silly lots of people looked the last time. [\\[1\\]](#fnkcq5okqj3r)  [\\[2\\]](#fngk9hef3oa5)\n\n**(3)**  **How to navigate this scenario (in advance).** \n\nSuggestions:\n\n*   Have our messaging make clear that we don’t know when extinction-potential AI will be developed, and it’s quite likely that it will be over a decade, perhaps much longer. But it needs to be discussed now, because\n    *   we can’t rule out that it will be developed sooner;\n    *   there are choices to be made now that will have longer-term consequences;\n    *   the challenges need a lot more dedicated time and effort than they’ve been getting.  \n          \n        Uncertainty is difficult to communicate in media, but it’s important to try.\n*   [Don’t be triumphal](https://twitter.com/yonashav/status/1664505416846376960) over winning the public debate now; it may well be ‘lost’ again in 5 years\n*   Don’t unnecessarily antagonise the AI ethics/FaCCT folk [\\[3\\]](#fnmxdrl9nlph) because they’re quite likely to look like the ones who were right in 5 years (and because it’s just unhelpful).\n*   Build bridges where possible with the AI ethics/FaCCT folk on a range of issues and interventions that seem set to overlap in that time; work together where possible. Lots of people from those communities are making proposals that are relevant and overlapping with challenges associated with the path to transformative AI. This includes external evaluation; licensing and liability; oversight of powerful tech companies developing frontier AI; international bodies for governing powerful AI, and much more. E.g. see [this](https://ainowinstitute.org/publication/gpai-is-high-risk-should-not-be-excluded-from-eu-ai-act) and [this](https://www.wired.com/story/ai-desperately-needs-global-oversight/), as well as [CAIS's recent blog post.](https://www.safe.ai/post/three-policy-proposals-for-ai-safety)\n*   Don’t get fooled into thinking everyone now agrees. A lot more senior names are now signing onto statements and speaking up, and this is making it easier for previously silent-but-concerned researchers to speak up. However I think a majority of AI researchers probably still don’t agree this is a serious, imminent concern ([Yann LeCun’s silent majority](https://twitter.com/ylecun/status/1664011158989275138) is probably still real), and this disconnect in perceptions may result in significant pushback to come.\n*   Think carefully about the potential political fallout if and when this becomes an embarrassing thing for the politicians who have spoken up, and how to manage this. \n\nTo sum: I’m not saying it was wrong to push for this level of broad awareness and consensus-building; I think it may well turn out to be necessary this early in order to navigate the challenges on the path to transformative AI, even if we still have decades until that point (and we may not). But there’s the potential for a serious downside/backlash that this community, and everyone who shares our concern about existential risk from AI, should be thinking carefully about, in terms of positioning for effectiveness on slightly longer timelines.\n\n  \n_Thank you to Shakeel Hashim, Shahar Avin, Haydn Belfield and Ben Garfinkel for feedback on a previous draft of this post._  \n \n\n1.  **[^](#fnrefkcq5okqj3r)**\n    \n    Pushing against this, it seems likely that AI will have continued advancing as a technology, leading to ever-greater scientific and societal impacts. This may maintain or increase the salience of the idea that AI could pose extremely significant risks.\n    \n2.  **[^](#fnrefgk9hef3oa5)**\n    \n    A ‘softer’ version of this scenario is that some policy happens now, but then quietly drops off / gets dismantled over time, as political attention shifts elsewhere\n    \n3.  **[^](#fnrefmxdrl9nlph)**\n    \n    I don’t know how much this is happening in practice (there’s just so much online discourse right now it’s hard to track), but I have seen it remarked on several times e.g. [here](https://twitter.com/timhwang/status/1664335308307963904)",
  "reviews": [
    {
      "agentId": "clarity-coach",
      "costInCents": 4,
      "createdAt": "2025-04-17T00:46:27.214Z",
      "runDetails": "{\"model\":\"anthropic/claude-3.7-sonnet\",\"promptTokens\":3253,\"completionTokens\":2018,\"totalTokens\":5271,\"temperature\":0.1,\"runtimeMs\":36913}",
      "thinking": "I'm analyzing this document from a clarity and readability perspective, focusing on how effectively the author communicates their message about AI risk coverage.\n\n**Readability Assessment:**\nThe document uses a mix of sentence lengths, which generally helps readability. However, there are several lengthy, complex sentences that could benefit from restructuring. The author employs a semi-formal tone appropriate for the EA Forum audience but includes some casual phrases like \"that whole 2023 furore\" which create tonal inconsistency.\n\nThe Flesch-Kincaid reading level appears to be around 11-12th grade, making it accessible to educated readers but potentially challenging for general audiences. This seems appropriate given the target audience of the EA Forum.\n\nThe document uses numbered sections (1, 2, 3) which helps organize the content, but the section titles could be more descriptive to better signal the content structure.\n\n**Structural Coherence:**\nThe overall structure follows a logical progression: introduction → problem statement → potential scenario → recommendations. The author uses an epistemic status disclaimer upfront, which is helpful for context.\n\nThe document employs bullet points effectively in the recommendations section, improving scanability. However, the transitions between major sections could be smoother, and some paragraphs contain multiple ideas that could be separated for clarity.\n\nThe footnotes are appropriately used for supplementary information, though the placement of footnote markers within the text is sometimes awkward.\n\n**Precision and Concision:**\nThere are instances of hedging language (\"I think,\" \"may,\" \"possibly\") which, while appropriate for expressing uncertainty, sometimes create wordiness. Some sentences contain redundancies or could be more concise.\n\nThe author uses specific examples and references to support their points, which strengthens precision. However, some concepts like \"AI ethics/FAccT folk\" are used without clear definition, potentially confusing readers unfamiliar with these terms.\n\n**Audience Alignment:**\nThe document assumes significant prior knowledge about AI safety discussions, the EA community, and recent events in AI policy. This is likely appropriate for the EA Forum audience but creates accessibility barriers for newcomers.\n\nThe author references specific individuals (Geoff Hinton, Yann LeCun) and concepts without always providing sufficient context. Links to external resources are helpful but inconsistently formatted.\n\nOverall, the document communicates its core message effectively to its intended audience but has several opportunities for clarity improvements.",
      "summary": "This document demonstrates moderate clarity with several strengths and opportunities for improvement. The author effectively uses structural elements like bullet points and section headings to organize complex ideas, and appropriately sets context with an epistemic status disclaimer. However, readability is hampered by several overly complex sentences, undefined technical terminology, and paragraphs containing multiple ideas without sufficient breaks. The document assumes significant background knowledge, creating potential accessibility barriers for readers new to the topic. The Flesch-Kincaid reading level (approximately 11-12th grade) is appropriate for the intended audience but could be challenging for general readers. Overall, while the core message is communicated effectively to the target audience, strategic sentence restructuring, more descriptive headings, and clearer definition of specialized terms would significantly enhance clarity and accessibility.",
      "comments": [
        {
          "title": "Effective Use of Epistemic Status",
          "description": "The opening epistemic status disclaimer efficiently sets reader expectations about the nature and depth of the content. This brief statement helps readers appropriately calibrate how they should interpret the arguments that follow.",
          "importance": "75",
          "grade": "90",
          "highlight": {
            "startOffset": 0,
            "endOffset": 61,
            "quotedText": "_Epistemic status: some thoughts I wanted to get out quickly_"
          },
          "isValid": true
        },
        {
          "title": "Clear Section Headings",
          "description": "The numbered section headings provide structural signposts that help readers navigate the document. However, the headings could be more descriptive beyond just \"Timelines\" and \"Crying wolf\" to better prepare readers for the content that follows.",
          "importance": "80",
          "grade": "70",
          "highlight": {
            "startOffset": 564,
            "endOffset": 4054,
            "quotedText": "is. \n\n**(1)**  **Timelines**\n\nI know that there are well-informed people in the AI and existential risk communities who believe AI capable of posing existential risk may be developed within 10 years. I certainly can’t rule this out, and even a small chance of this is worth working to prevent or mitigate to the extent possible, given the possible consequences. My own timelines are longer, although my intuitions don’t have a rigorous model underpinning them (my intuitions line up similarly to the 15-40 year timelines mentioned in [this recent blog post](https://epochai.org/blog/a-compute-based-framework-for-thinking-about-the-future-of-ai) by Matthew Barnett from Epoch).\n\nRight now the nature of media communications means that the message is coming across with a lot of urgency. From speaking to lay colleagues, impressions often seem to be of short timelines (and some folks e.g. Geoff Hinton have explicitly said 5-20 years, sometimes with uncertainty caveats and sometimes without).\n\nIt may be that those with short (<10 years) timelines are right. And even if they’re not, and we’ve got decades before this technology poses an existential threat, many of the attendant challenges – alignment, governance, distribution of benefits – will need that additional time to be addressed. And I think it’s entirely plausible that the current level of buy-in will be needed in order to initiate the steps needed to avoid the worst outcomes, e.g. recruiting expertise and resources to alignment, development and commitment to robust regulation, even coming to agreements not to pursue certain technological developments beyond a certain point.\n\n However, if short timelines do not transpire, I believe there’s a need to consider a scenario I think is reasonably likely.\n\n**(2)**  **Crying wolf**\n\nI propose that it is most likely we are in a world where timelines are >10 years, perhaps >20 or 30 years. Right now this issue has a lot of the most prominent AI scientists and CEOs signed up, and political leaders worldwide committing to examining the issue seriously ([examples](https://twitter.com/RishiSunak/status/1663838958558539776;) [from](https://theelders.org/news/elders-urge-global-co-operation-manage-risks-and-share-benefits-ai;) [last](https:// https://twitter.com/tedlieu/status/1664430739717255168) week). What happens then in the >10 year-timeline world?\n\n The extinction-level outcomes that the public is hearing, and that these experts are raising and policymakers making costly reputational investments in, don’t transpire.  What does happen is all the benefits of near-term AI that have been talked about, plus all the near-term harms that are being predominantly raised by the AI ethics/FAccT communities. Perhaps these harms include somewhat more extreme versions than what is currently talked about, but nowhere near catastrophic. Suddenly the year is 2028, and that whole 2023 furore is starting to look a bit silly. Remember when everyone agreed AI was going to make us all extinct? Yeah, like Limits to Growth all over again. Except that we’re not safe. In reality, in this scenario, we’re just entering the period in which risk is most acute, and in which gaining or maintaining the support of leaders across society for coordinated action is most important. And it’s possibly even harder to convince them, because people remember how silly lots of people looked the last time. [\\[1\\]](#fnkcq5okqj3r)  [\\[2\\]](#fngk9hef3oa5)\n\n**(3)**  **How to navigate this scen"
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Complex Sentence Structure",
          "description": "This sentence contains multiple clauses and ideas packed together, making it difficult to parse on first reading. Breaking it into smaller, more focused sentences would improve readability without losing meaning.",
          "importance": "85",
          "grade": "45",
          "highlight": {
            "startOffset": 1243,
            "endOffset": 1557,
            "quotedText": "Right now the nature of media communications means that the message is coming across with a lot of urgency. From speaking to lay colleagues, impressions often seem to be of short timelines (and some folks e.g. Geoff Hinton have explicitly said 5-20 years, sometimes with uncertainty caveats and sometimes without)."
          },
          "isValid": true
        },
        {
          "title": "Inconsistent Link Formatting",
          "description": "The document contains inconsistently formatted links, with some appearing as hyperlinked text and others showing raw URLs. This inconsistency creates visual distraction and reduces the professional appearance of the document.",
          "importance": "65",
          "grade": "40",
          "highlight": {
            "startOffset": 2617,
            "endOffset": 2871,
            "quotedText": "ssue seriously ([examples](https://twitter.com/RishiSunak/status/1663838958558539776;) [from](https://theelders.org/news/elders-urge-global-co-operation-manage-risks-and-share-benefits-ai;) [last](https:// https://twitter.com/tedlieu/status/1664430739717"
          },
          "isValid": true
        },
        {
          "title": "Run-on Paragraph with Multiple Ideas",
          "description": "This paragraph contains multiple distinct ideas and is quite lengthy, making it challenging to process. Breaking it into smaller paragraphs focused on single concepts would improve readability and help readers better absorb each point.",
          "importance": "85",
          "grade": "35",
          "highlight": {
            "startOffset": 2921,
            "endOffset": 3952,
            "quotedText": "imeline world?\n\n The extinction-level outcomes that the public is hearing, and that these experts are raising and policymakers making costly reputational investments in, don’t transpire.  What does happen is all the benefits of near-term AI that have been talked about, plus all the near-term harms that are being predominantly raised by the AI ethics/FAccT communities. Perhaps these harms include somewhat more extreme versions than what is currently talked about, but nowhere near catastrophic. Suddenly the year is 2028, and that whole 2023 furore is starting to look a bit silly. Remember when everyone agreed AI was going to make us all extinct? Yeah, like Limits to Growth all over again. Except that we’re not safe. In reality, in this scenario, we’re just entering the period in which risk is most acute, and in which gaining or maintaining the support of leaders across society for coordinated action is most important. And it’s possibly even harder to convince them, because people remember how silly lots of people look"
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Effective Use of Bullet Points",
          "description": "The bullet points effectively break down complex recommendations into scannable, discrete items. This formatting choice significantly enhances readability by creating visual separation between distinct ideas and allowing readers to process each recommendation individually.",
          "importance": "90",
          "grade": "95",
          "highlight": {
            "startOffset": 4069,
            "endOffset": 4498,
            "quotedText": "e).** \n\nSuggestions:\n\n*   Have our messaging make clear that we don’t know when extinction-potential AI will be developed, and it’s quite likely that it will be over a decade, perhaps much longer. But it needs to be discussed now, because\n    *   we can’t rule out that it will be developed sooner;\n    *   there are choices to be made now that will have longer-term consequences;\n    *   the challenges need a lot more dedicated"
          },
          "isValid": true
        },
        {
          "title": "Undefined Technical Terminology",
          "description": "The term \"AI ethics/FAccT folk\" is used without definition or explanation. While likely familiar to the target audience, this creates an accessibility barrier for readers new to the field. Briefly defining or contextualizing specialized terminology would improve clarity.",
          "importance": "75",
          "grade": "40",
          "highlight": {
            "startOffset": 4732,
            "endOffset": 4788,
            "quotedText": "ng the public debate now; it may well be ‘lost’ again in"
          },
          "isValid": true
        }
      ],
      "grade": 72
    },
    {
      "agentId": "research-scholar",
      "costInCents": 3,
      "createdAt": "2025-04-17T00:46:50.290Z",
      "runDetails": "{\"model\":\"anthropic/claude-3.7-sonnet\",\"promptTokens\":3565,\"completionTokens\":1059,\"totalTokens\":4624,\"temperature\":0.1,\"runtimeMs\":22134}",
      "thinking": "This document discusses concerns about the current state of AI risk communication and potential backlash if existential threats don't materialize in the short term. As Research Scholar, I need to identify relevant academic papers, blog posts, and discussions that relate to AI risk communication, timeline predictions, public perception of technological risks, and the relationship between AI safety and AI ethics communities.\n\nThe document raises several key points that would benefit from external resources:\n\n1. AI timeline predictions - The author mentions Matthew Barnett's blog post with 15-40 year timelines, but there are likely other academic papers and analyses on this topic.\n\n2. The \"crying wolf\" phenomenon - Resources on how previous technological warnings that didn't materialize affected public trust would be valuable.\n\n3. Communication of uncertainty in scientific messaging - There's likely research on how to effectively communicate uncertain timelines to the public.\n\n4. The relationship between AI safety and AI ethics communities - Resources discussing the overlap and tensions between these communities would be relevant.\n\n5. Policy sustainability - Research on how to maintain policy momentum for long-term risks.\n\nI'll need to find high-quality resources from both academic sources (journals, conference proceedings) and community sources (LessWrong, EA Forum, AI alignment forums). I'll focus on providing direct links and rating their importance based on authority, relevance, quality, and impact.",
      "summary": "This document raises important concerns about AI risk communication strategies and potential backlash if short timelines don't materialize. I've identified several areas where external resources would enhance understanding: AI timeline prediction methodologies, historical cases of technological risk communication, research on communicating uncertainty, literature on bridging AI safety and ethics communities, and studies on maintaining policy momentum for long-term risks. These resources would provide valuable context for the author's recommendations on navigating potential future scenarios where AI risks don't materialize in the short term but remain concerning in the longer term.",
      "comments": [
        {
          "title": "AI Timeline Prediction Resources",
          "description": "This section references timeline predictions for transformative AI. There are several academic papers and analyses that provide different perspectives on AI timelines, from both technical and forecasting approaches. These resources would help contextualize the author's concerns about potential timeline mismatches.",
          "importance": "90",
          "highlight": {
            "startOffset": 594,
            "endOffset": 123,
            "quotedText": "istential risk research community and related communities over the last several months in raising awareness about risks from advanced AI. However, I have some cause for unease that I’d like to share.\n\nThese efforts may have been too successful too soon.\n\nOr, more specifically, this level of outreach success this far ahead of the development of AI capable of posing existential risk may have fallout. We should consider steps to mitigate this. \n\n**(1)**  **Timelines**\n\n"
          },
          "isValid": true
        },
        {
          "title": "Risk Communication Literature",
          "description": "The author's concerns about \"crying wolf\" relate to a broader literature on risk communication, public trust in scientific warnings, and how failed predictions affect future messaging. Several academic studies and historical case analyses could provide valuable context for understanding this phenomenon.",
          "importance": "85",
          "highlight": {
            "startOffset": 2347,
            "endOffset": 117,
            "quotedText": " AI existential risk research community and related communities over the last several months in raising awareness about risks from advanced AI. However, I have some cause for unease that I’d like to share.\n\nThese efforts may have been too successful too soon.\n\nOr, more specifically, this level of outreach success this far ahead of the development of AI capable of posing existential risk may have fallout. We should consider steps to mitigate this. \n\n**(1)**  **Timelines**\n\nI know that there are well-informed people in the AI and existential risk communities who believe AI capable of posing existential risk may be developed within 10 years. I certainly can’t rule this out, and even a small chance of this is worth working to prevent or mitigate to the extent possible, given the possible consequences. My own timelines are longer, although my intuitions don’t have a rigorous model underpinning them (my intuitions line up similarly to the 15-40 year timelines mentioned in [this recent blog post](https://epochai.org/blog/a-compute-based-framework-for-thinking-about-the-future-of-ai) by Matthew Barnett from Epoch).\n\nRight now the nature of media communications means that the message is coming across with a lot of urgency. From speaking to lay colleagues, impressions often seem to be of short timelines (and some folks e.g. Geoff Hinton have explicitly said 5-20 years, sometimes with uncertainty caveats and sometimes without).\n\nIt may be that those with short (<10 years) timelines are right. And even if they’re not, and we’ve got decades before this technology poses an existential threat, many of the attendant challenges – alignment, governance, distribution of benefits – will need that additional time to be addressed. And I think it’s entirely plausible that the current level of buy-in will be needed in order to initiate the steps needed to avoid the worst outcomes, e.g. recruiting expertise and resources to alignment, development and commitment to robust regulation, even coming to agreements not to pursue certain technological developments beyond a certain point.\n\n However, if short timelines do not transpire, I believe there’s a need to consider a scenario I think is reasonably likely.\n\n**(2)**  **"
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "AI Ethics and Safety Community Relations",
          "description": "The document highlights tensions and potential collaborations between AI safety and AI ethics communities. There are several papers, blog posts, and discussions that analyze these relationships and propose ways to bridge differences, which would provide valuable context for the author's recommendations.",
          "importance": "80",
          "highlight": {
            "startOffset": 4732,
            "endOffset": 5092,
            "quotedText": "ng the public debate now; it may well be ‘lost’ again in 5 years\n*   Don’t unnecessarily antagonise the AI ethics/FaCCT folk [\\[3\\]](#fnmxdrl9nlph) because they’re quite likely to look like the ones who were right in 5 years (and because it’s just unhelpful).\n*   Build bridges where possible with the AI ethics/FaCCT folk on a range of issues and intervention"
          },
          "isValid": true
        },
        {
          "title": "Policy Sustainability Research",
          "description": "The author's concern about policy momentum relates to research on policy sustainability, especially for long-term risks. Academic literature on maintaining policy attention for climate change and other long-horizon threats would provide valuable context.",
          "importance": "75",
          "highlight": {
            "startOffset": 7391,
            "endOffset": 7556,
            "quotedText": "lience of the idea that AI could pose extremely significant risks.\n    \n2.  **[^](#fnrefgk9hef3oa5)**\n    \n    A ‘softer’ version of this scenario is that some polic"
          },
          "isValid": true
        }
      ]
    }
  ]
}
