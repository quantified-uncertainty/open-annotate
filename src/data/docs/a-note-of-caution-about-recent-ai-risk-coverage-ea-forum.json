{
  "id": "a-note-of-caution-about-recent-ai-risk-coverage-ea-forum",
  "slug": "a-note-of-caution-about-recent-ai-risk-coverage-ea-forum",
  "title": "A note of caution about recent AI risk coverage — EA Forum",
  "author": "Unknown Author",
  "publishedDate": "2025-04-15",
  "intendedAgents": [
    "bias-detector",
    "clarity-coach",
    "research-scholar"
  ],
  "content": "_Epistemic status: some thoughts I wanted to get out quickly_\n\nA lot of fantastic work has been done by people in the AI existential risk research community and related communities over the last several months in raising awareness about risks from advanced AI. However, I have some cause for unease that I’d like to share.\n\nThese efforts may have been too successful too soon.\n\nOr, more specifically, this level of outreach success this far ahead of the development of AI capable of posing existential risk may have fallout. We should consider steps to mitigate this. \n\n**(1)**  **Timelines**\n\nI know that there are well-informed people in the AI and existential risk communities who believe AI capable of posing existential risk may be developed within 10 years. I certainly can’t rule this out, and even a small chance of this is worth working to prevent or mitigate to the extent possible, given the possible consequences. My own timelines are longer, although my intuitions don’t have a rigorous model underpinning them (my intuitions line up similarly to the 15-40 year timelines mentioned in [this recent blog post](https://epochai.org/blog/a-compute-based-framework-for-thinking-about-the-future-of-ai) by Matthew Barnett from Epoch).\n\nRight now the nature of media communications means that the message is coming across with a lot of urgency. From speaking to lay colleagues, impressions often seem to be of short timelines (and some folks e.g. Geoff Hinton have explicitly said 5-20 years, sometimes with uncertainty caveats and sometimes without).\n\nIt may be that those with short (<10 years) timelines are right. And even if they’re not, and we’ve got decades before this technology poses an existential threat, many of the attendant challenges – alignment, governance, distribution of benefits – will need that additional time to be addressed. And I think it’s entirely plausible that the current level of buy-in will be needed in order to initiate the steps needed to avoid the worst outcomes, e.g. recruiting expertise and resources to alignment, development and commitment to robust regulation, even coming to agreements not to pursue certain technological developments beyond a certain point.\n\n However, if short timelines do not transpire, I believe there’s a need to consider a scenario I think is reasonably likely.\n\n**(2)**  **Crying wolf**\n\nI propose that it is most likely we are in a world where timelines are >10 years, perhaps >20 or 30 years. Right now this issue has a lot of the most prominent AI scientists and CEOs signed up, and political leaders worldwide committing to examining the issue seriously ([examples](https://twitter.com/RishiSunak/status/1663838958558539776;) [from](https://theelders.org/news/elders-urge-global-co-operation-manage-risks-and-share-benefits-ai;) [last](https:// https://twitter.com/tedlieu/status/1664430739717255168) week). What happens then in the >10 year-timeline world?\n\n The extinction-level outcomes that the public is hearing, and that these experts are raising and policymakers making costly reputational investments in, don’t transpire.  What does happen is all the benefits of near-term AI that have been talked about, plus all the near-term harms that are being predominantly raised by the AI ethics/FAccT communities. Perhaps these harms include somewhat more extreme versions than what is currently talked about, but nowhere near catastrophic. Suddenly the year is 2028, and that whole 2023 furore is starting to look a bit silly. Remember when everyone agreed AI was going to make us all extinct? Yeah, like Limits to Growth all over again. Except that we’re not safe. In reality, in this scenario, we’re just entering the period in which risk is most acute, and in which gaining or maintaining the support of leaders across society for coordinated action is most important. And it’s possibly even harder to convince them, because people remember how silly lots of people looked the last time. [\\[1\\]](#fnkcq5okqj3r)  [\\[2\\]](#fngk9hef3oa5)\n\n**(3)**  **How to navigate this scenario (in advance).** \n\nSuggestions:\n\n*   Have our messaging make clear that we don’t know when extinction-potential AI will be developed, and it’s quite likely that it will be over a decade, perhaps much longer. But it needs to be discussed now, because\n    *   we can’t rule out that it will be developed sooner;\n    *   there are choices to be made now that will have longer-term consequences;\n    *   the challenges need a lot more dedicated time and effort than they’ve been getting.  \n          \n        Uncertainty is difficult to communicate in media, but it’s important to try.\n*   [Don’t be triumphal](https://twitter.com/yonashav/status/1664505416846376960) over winning the public debate now; it may well be ‘lost’ again in 5 years\n*   Don’t unnecessarily antagonise the AI ethics/FaCCT folk [\\[3\\]](#fnmxdrl9nlph) because they’re quite likely to look like the ones who were right in 5 years (and because it’s just unhelpful).\n*   Build bridges where possible with the AI ethics/FaCCT folk on a range of issues and interventions that seem set to overlap in that time; work together where possible. Lots of people from those communities are making proposals that are relevant and overlapping with challenges associated with the path to transformative AI. This includes external evaluation; licensing and liability; oversight of powerful tech companies developing frontier AI; international bodies for governing powerful AI, and much more. E.g. see [this](https://ainowinstitute.org/publication/gpai-is-high-risk-should-not-be-excluded-from-eu-ai-act) and [this](https://www.wired.com/story/ai-desperately-needs-global-oversight/), as well as [CAIS's recent blog post.](https://www.safe.ai/post/three-policy-proposals-for-ai-safety)\n*   Don’t get fooled into thinking everyone now agrees. A lot more senior names are now signing onto statements and speaking up, and this is making it easier for previously silent-but-concerned researchers to speak up. However I think a majority of AI researchers probably still don’t agree this is a serious, imminent concern ([Yann LeCun’s silent majority](https://twitter.com/ylecun/status/1664011158989275138) is probably still real), and this disconnect in perceptions may result in significant pushback to come.\n*   Think carefully about the potential political fallout if and when this becomes an embarrassing thing for the politicians who have spoken up, and how to manage this. \n\nTo sum: I’m not saying it was wrong to push for this level of broad awareness and consensus-building; I think it may well turn out to be necessary this early in order to navigate the challenges on the path to transformative AI, even if we still have decades until that point (and we may not). But there’s the potential for a serious downside/backlash that this community, and everyone who shares our concern about existential risk from AI, should be thinking carefully about, in terms of positioning for effectiveness on slightly longer timelines.\n\n  \n_Thank you to Shakeel Hashim, Shahar Avin, Haydn Belfield and Ben Garfinkel for feedback on a previous draft of this post._  \n \n\n1.  **[^](#fnrefkcq5okqj3r)**\n    \n    Pushing against this, it seems likely that AI will have continued advancing as a technology, leading to ever-greater scientific and societal impacts. This may maintain or increase the salience of the idea that AI could pose extremely significant risks.\n    \n2.  **[^](#fnrefgk9hef3oa5)**\n    \n    A ‘softer’ version of this scenario is that some policy happens now, but then quietly drops off / gets dismantled over time, as political attention shifts elsewhere\n    \n3.  **[^](#fnrefmxdrl9nlph)**\n    \n    I don’t know how much this is happening in practice (there’s just so much online discourse right now it’s hard to track), but I have seen it remarked on several times e.g. [here](https://twitter.com/timhwang/status/1664335308307963904)",
  "reviews": [
    {
      "agentId": "bias-detector",
      "costInCents": 6424,
      "createdAt": "2025-04-16T00:31:50.322Z",
      "runDetails": "{\"model\":\"anthropic/claude-3.7-sonnet\",\"promptTokens\":4447,\"completionTokens\":1977,\"totalTokens\":6424,\"temperature\":0.7,\"runtimeMs\":37294}",
      "summary": "This document presents a thoughtful critique of AI existential risk messaging, exhibiting several forms of bias despite its balanced approach. The analysis reveals confirmation bias in the author's timeline predictions, framing bias in the \"crying wolf\" scenario, and false equivalence between existential risks and ethical concerns. There's notable in-group favoritism toward the AI safety community while creating an outgroup of AI ethics researchers. The document shows moderate selection bias in evidence presentation and contains assumptions about how public perception will evolve. While the author acknowledges uncertainties and calls for collaboration, the underlying narrative reflects status quo bias by suggesting that current advocacy approaches require only minor adjustments rather than fundamental reconsideration. Overall, the document scores 6/10 for cognitive bias, 5/10 for social bias, and 4/10 for framing bias.",
      "comments": [
        {
          "title": "False Equivalence Between Risk Types",
          "description": "The document creates a false equivalence between existential risks and ethical concerns by suggesting that AI ethics/FAccT communities will \"look like the ones who were right\" if catastrophic outcomes don't materialize quickly. This minimizes the fundamental difference between these risk categories and their time horizons. A more nuanced approach would acknowledge that both communities could be simultaneously correct about different aspects of AI risk.",
          "highlight": {
            "startOffset": 0,
            "endOffset": 0,
            "quotedText": ""
          },
          "isValid": false,
          "error": "End text not found in document"
        },
        {
          "title": "In-group Favoritism Toward AI Safety Community",
          "description": "The document displays in-group favoritism by consistently referring to \"us\" and \"our\" when discussing the AI safety community, while positioning AI ethics researchers as \"them.\" This social bias subtly reinforces division rather than collaboration. A more inclusive approach would acknowledge shared goals and use more unified language when discussing the broader AI research community.",
          "highlight": {
            "startOffset": 63,
            "endOffset": 322,
            "quotedText": "A lot of fantastic work has been done by people in the AI existential risk research community and related communities over the last several months in raising awareness about risks from advanced AI. However, I have some cause for unease that I’d like to share."
          },
          "isValid": true
        },
        {
          "title": "Selective Evidence in Timeline Discussion",
          "description": "The author selectively cites evidence supporting longer timelines while acknowledging but not equally exploring shorter timeline perspectives. This selection bias creates an imbalanced presentation of the timeline debate. A more comprehensive approach would include specific evidence and models supporting shorter timelines to provide readers with a more complete picture of the debate.",
          "highlight": {
            "startOffset": 594,
            "endOffset": 1023,
            "quotedText": "I know that there are well-informed people in the AI and existential risk communities who believe AI capable of posing existential risk may be developed within 10 years. I certainly can’t rule this out, and even a small chance of this is worth working to prevent or mitigate to the extent possible, given the possible consequences. My own timelines are longer, although my intuitions don’t have a rigorous model underpinning them"
          },
          "isValid": false,
          "error": "Highlight overlaps with a previous highlight"
        },
        {
          "title": "Confirmation Bias in Timeline Predictions",
          "description": "The author acknowledges diverse timeline predictions but subtly favors longer timelines that align with their own intuitions. This confirmation bias potentially undermines the urgency of shorter timeline concerns without sufficient evidence. To mitigate this bias, the author could present a more balanced assessment of timeline probabilities with equal weight given to short-timeline scenarios and their implications.",
          "highlight": {
            "startOffset": 926,
            "endOffset": 1241,
            "quotedText": "My own timelines are longer, although my intuitions don’t have a rigorous model underpinning them (my intuitions line up similarly to the 15-40 year timelines mentioned in [this recent blog post](https://epochai.org/blog/a-compute-based-framework-for-thinking-about-the-future-of-ai) by Matthew Barnett from Epoch)."
          },
          "isValid": true
        },
        {
          "title": "Framing Bias in \"Crying Wolf\" Scenario",
          "description": "The author frames a future scenario where AI safety concerns appear overblown as highly probable (\"I propose that it is most likely\"), revealing a framing bias that potentially undermines the credibility of current advocacy efforts. This presents a subjective probability assessment as more factual than warranted. A more balanced approach would present multiple plausible future scenarios with their respective implications.",
          "highlight": {
            "startOffset": 2362,
            "endOffset": 2935,
            "quotedText": "I propose that it is most likely we are in a world where timelines are >10 years, perhaps >20 or 30 years. Right now this issue has a lot of the most prominent AI scientists and CEOs signed up, and political leaders worldwide committing to examining the issue seriously ([examples](https://twitter.com/RishiSunak/status/1663838958558539776;) [from](https://theelders.org/news/elders-urge-global-co-operation-manage-risks-and-share-benefits-ai;) [last](https:// https://twitter.com/tedlieu/status/1664430739717255168) week). What happens then in the >10 year-timeline world?"
          },
          "isValid": true
        },
        {
          "title": "Availability Bias in Media Perception",
          "description": "The author displays availability bias by emphasizing recent high-profile statements and media coverage as evidence of widespread commitment to addressing AI risks. This overweights recent, salient events while potentially underestimating the fragility of this attention. A more balanced assessment would consider historical patterns of technology policy attention cycles and institutional factors affecting sustained engagement.",
          "highlight": {
            "startOffset": 2469,
            "endOffset": 2885,
            "quotedText": "Right now this issue has a lot of the most prominent AI scientists and CEOs signed up, and political leaders worldwide committing to examining the issue seriously ([examples](https://twitter.com/RishiSunak/status/1663838958558539776;) [from](https://theelders.org/news/elders-urge-global-co-operation-manage-risks-and-share-benefits-ai;) [last](https:// https://twitter.com/tedlieu/status/1664430739717255168) week)."
          },
          "isValid": false,
          "error": "Highlight overlaps with a previous highlight"
        },
        {
          "title": "Outgroup Homogeneity Bias Toward AI Ethics Community",
          "description": "The document treats the AI ethics/FAccT community as a homogeneous outgroup with unified perspectives, displaying outgroup homogeneity bias. This overlooks the diversity of views within this community and their varying degrees of concern about existential risks. A more accurate portrayal would acknowledge the spectrum of perspectives within both communities and the significant overlaps between them.",
          "highlight": {
            "startOffset": 3109,
            "endOffset": 3418,
            "quotedText": "What does happen is all the benefits of near-term AI that have been talked about, plus all the near-term harms that are being predominantly raised by the AI ethics/FAccT communities. Perhaps these harms include somewhat more extreme versions than what is currently talked about, but nowhere near catastrophic."
          },
          "isValid": true
        },
        {
          "title": "Assumption Bias in Public Perception Predictions",
          "description": "The document makes strong assumptions about how public perception will evolve, predicting that current concerns will look \"silly\" in 5 years. This reflects assumption bias by projecting a specific narrative about future public opinion without sufficient evidence. A more balanced approach would acknowledge multiple possible trajectories for public perception and the factors that might influence them.",
          "highlight": {
            "startOffset": 3419,
            "endOffset": 3616,
            "quotedText": "Suddenly the year is 2028, and that whole 2023 furore is starting to look a bit silly. Remember when everyone agreed AI was going to make us all extinct? Yeah, like Limits to Growth all over again."
          },
          "isValid": true
        },
        {
          "title": "False Dichotomy Between Existential and Near-term Concerns",
          "description": "The document creates a false dichotomy between existential risks and near-term ethical concerns, suggesting these are separate domains rather than interconnected issues. This dichotomous thinking artificially separates communities that could benefit from integrated approaches. A more nuanced framing would emphasize how addressing near-term AI ethics concerns can build institutional capacity and governance frameworks that also help address longer-term existential risks.",
          "highlight": {
            "startOffset": 4996,
            "endOffset": 5318,
            "quotedText": "Build bridges where possible with the AI ethics/FaCCT folk on a range of issues and interventions that seem set to overlap in that time; work together where possible. Lots of people from those communities are making proposals that are relevant and overlapping with challenges associated with the path to transformative AI."
          },
          "isValid": true
        },
        {
          "title": "Status Quo Bias in Advocacy Approach",
          "description": "The document exhibits status quo bias by suggesting that current advocacy approaches require only minor adjustments rather than fundamental reconsideration. This bias leads to recommendations that largely maintain existing strategies with modest modifications. A more critical approach would consider whether entirely different advocacy frameworks might be more effective for long-term engagement.",
          "highlight": {
            "startOffset": 6485,
            "endOffset": 7032,
            "quotedText": "To sum: I’m not saying it was wrong to push for this level of broad awareness and consensus-building; I think it may well turn out to be necessary this early in order to navigate the challenges on the path to transformative AI, even if we still have decades until that point (and we may not). But there’s the potential for a serious downside/backlash that this community, and everyone who shares our concern about existential risk from AI, should be thinking carefully about, in terms of positioning for effectiveness on slightly longer timelines."
          },
          "isValid": true
        }
      ]
    },
    {
      "agentId": "clarity-coach",
      "costInCents": 0,
      "createdAt": "2025-04-16T00:32:29.848Z",
      "summary": "Error: Unable to parse the response from the AI. Please try again.",
      "comments": []
    },
    {
      "agentId": "research-scholar",
      "costInCents": 8230,
      "createdAt": "2025-04-16T00:33:33.875Z",
      "runDetails": "{\"model\":\"anthropic/claude-3.7-sonnet\",\"promptTokens\":5248,\"completionTokens\":2982,\"totalTokens\":8230,\"temperature\":0.7,\"runtimeMs\":56173}",
      "summary": "This document addresses a potential strategic dilemma in AI existential risk advocacy: current public awareness efforts might be \"too successful too soon.\" The author expresses concern that if AI capable of posing existential risk doesn't emerge within the next decade (which aligns with their longer timeline expectations of 15-40 years), we may face a \"crying wolf\" scenario where initial alarmism followed by non-materialization of catastrophic outcomes undermines future credibility precisely when risks become most acute. The post suggests several mitigation strategies: communicating timeline uncertainty clearly, avoiding triumphalism, building bridges with the AI ethics community, recognizing that consensus on AI risk remains incomplete, and preparing for potential political fallout. While acknowledging the necessity of current awareness-building, the author cautions that positioning for longer timelines requires careful consideration to maintain effectiveness through potential backlash periods.",
      "comments": [
        {
          "title": "Timeline Disagreement Within the AI Risk Community",
          "description": "The document highlights an important epistemic division within the AI risk community regarding timelines to transformative AI. While some prominent voices argue for <10 year timelines (which drives much of the current urgency in messaging), others including the author lean toward longer timelines (15-40 years). This disagreement has been extensively discussed in the EA/rationalist community, with Ajeya Cotra's biological anchors framework and Daniel Kokotajlo's work on timelines being significant contributions. The disagreement has important strategic implications - shorter timelines suggest prioritizing immediate technical safety work and policy interventions, while longer timelines might favor institution-building, field-building, and more deliberative approaches to governance.",
          "highlight": {
            "startOffset": 594,
            "endOffset": 1239,
            "quotedText": "I know that there are well-informed people in the AI and existential risk communities who believe AI capable of posing existential risk may be developed within 10 years. I certainly can’t rule this out, and even a small chance of this is worth working to prevent or mitigate to the extent possible, given the possible consequences. My own timelines are longer, although my intuitions don’t have a rigorous model underpinning them (my intuitions line up similarly to the 15-40 year timelines mentioned in [this recent blog post](https://epochai.org/blog/a-compute-based-framework-for-thinking-about-the-future-of-ai) by Matthew Barnett from Epoch"
          },
          "isValid": true
        },
        {
          "title": "Media Communication Challenges with Uncertainty",
          "description": "The author identifies a key challenge in risk communication: media tends to simplify messages and strip away uncertainty qualifications. This relates to the broader epistemic challenge in communicating existential risks, as discussed in Ord's \"The Precipice\" and Yudkowsky's writings on civilizational inadequacy. The difficulty of communicating probability distributions rather than point estimates to the public creates a tension between generating appropriate urgency and maintaining epistemic accuracy. This connects to LessWrong discussions on the unilateralist's curse - where actors might feel compelled to sound maximally alarming about risks - and to broader work on scientific communication under uncertainty (e.g., COVID-19 communication challenges).",
          "highlight": {
            "startOffset": 1243,
            "endOffset": 1557,
            "quotedText": "Right now the nature of media communications means that the message is coming across with a lot of urgency. From speaking to lay colleagues, impressions often seem to be of short timelines (and some folks e.g. Geoff Hinton have explicitly said 5-20 years, sometimes with uncertainty caveats and sometimes without)."
          },
          "isValid": true
        },
        {
          "title": "The Crying Wolf Scenario",
          "description": "The author outlines a plausible scenario where AI risk advocates lose credibility if catastrophic outcomes don't materialize in the near-term, making future advocacy more difficult. This connects to broader issues in catastrophic risk communication, including what Ord calls the \"vulnerable world hypothesis\" and what Nick Bostrom terms the \"boy who cried wolf\" effect in technological forecasting. This dynamic creates a challenging strategic landscape for advocates - maintaining credibility through potential disappointment cycles while still generating sufficient motivation for preventative action. Similar patterns have occurred in climate advocacy, nuclear risk communication, and pandemic preparedness, where periods of heightened concern have been followed by reduced attention despite persistent underlying risks.",
          "highlight": {
            "startOffset": 2362,
            "endOffset": 3969,
            "quotedText": "I propose that it is most likely we are in a world where timelines are >10 years, perhaps >20 or 30 years. Right now this issue has a lot of the most prominent AI scientists and CEOs signed up, and political leaders worldwide committing to examining the issue seriously ([examples](https://twitter.com/RishiSunak/status/1663838958558539776;) [from](https://theelders.org/news/elders-urge-global-co-operation-manage-risks-and-share-benefits-ai;) [last](https:// https://twitter.com/tedlieu/status/1664430739717255168) week). What happens then in the >10 year-timeline world?\n\n The extinction-level outcomes that the public is hearing, and that these experts are raising and policymakers making costly reputational investments in, don’t transpire.  What does happen is all the benefits of near-term AI that have been talked about, plus all the near-term harms that are being predominantly raised by the AI ethics/FAccT communities. Perhaps these harms include somewhat more extreme versions than what is currently talked about, but nowhere near catastrophic. Suddenly the year is 2028, and that whole 2023 furore is starting to look a bit silly. Remember when everyone agreed AI was going to make us all extinct? Yeah, like Limits to Growth all over again. Except that we’re not safe. In reality, in this scenario, we’re just entering the period in which risk is most acute, and in which gaining or maintaining the support of leaders across society for coordinated action is most important. And it’s possibly even harder to convince them, because people remember how silly lots of people looked the last time."
          },
          "isValid": true
        },
        {
          "title": "Uncertainty Communication as Strategic Imperative",
          "description": "The suggestion to explicitly communicate timeline uncertainty represents a strategic approach to maintaining long-term credibility. This connects to the rationalist community's emphasis on proper calibration and expressing appropriate uncertainty, as seen in prediction markets, forecasting tournaments, and Tetlock's work on superforecasting. The tension between generating urgency while maintaining epistemic standards relates to what Yudkowsky has called \"civilizational inadequacy\" - our institutions may be unable to respond appropriately to probabilistic existential threats. This approach also connects to work on \"robust\" strategy formation under deep uncertainty, where interventions are designed to be valuable across multiple possible future scenarios rather than optimized for a single expected outcome.",
          "highlight": {
            "startOffset": 4095,
            "endOffset": 4639,
            "quotedText": "Have our messaging make clear that we don’t know when extinction-potential AI will be developed, and it’s quite likely that it will be over a decade, perhaps much longer. But it needs to be discussed now, because\n    *   we can’t rule out that it will be developed sooner;\n    *   there are choices to be made now that will have longer-term consequences;\n    *   the challenges need a lot more dedicated time and effort than they’ve been getting.  \n          \n        Uncertainty is difficult to communicate in media, but it’s important to try."
          },
          "isValid": true
        },
        {
          "title": "Community Tensions and Epistemic Tribalism",
          "description": "The footnote referencing potential antagonism between AI safety and AI ethics communities highlights an important social dynamic within the broader AI governance ecosystem. This connects to what Tetlock calls \"epistemic tribalism\" - the tendency for communities with different methodological approaches or normative priorities to develop antagonistic rather than collaborative relationships. The AI safety and AI ethics communities have developed somewhat different epistemic cultures, with safety more influenced by rationalist/EA thinking and ethics more connected to critical theory and justice frameworks. These differences have sometimes led to unnecessary conflicts despite substantial overlapping interests. This relates to broader EA discussions about \"moral trade\" and \"cooperative AI\" - the value of finding ways to collaborate despite differing values or worldviews. The author's concern about antagonism suggests recognition that building effective governance for advanced AI will require coalition-building across these epistemic communities.",
          "highlight": {
            "startOffset": 4801,
            "endOffset": 7946,
            "quotedText": "Don’t unnecessarily antagonise the AI ethics/FaCCT folk [\\[3\\]](#fnmxdrl9nlph) because they’re quite likely to look like the ones who were right in 5 years (and because it’s just unhelpful).\n*   Build bridges where possible with the AI ethics/FaCCT folk on a range of issues and interventions that seem set to overlap in that time; work together where possible. Lots of people from those communities are making proposals that are relevant and overlapping with challenges associated with the path to transformative AI. This includes external evaluation; licensing and liability; oversight of powerful tech companies developing frontier AI; international bodies for governing powerful AI, and much more. E.g. see [this](https://ainowinstitute.org/publication/gpai-is-high-risk-should-not-be-excluded-from-eu-ai-act) and [this](https://www.wired.com/story/ai-desperately-needs-global-oversight/), as well as [CAIS's recent blog post.](https://www.safe.ai/post/three-policy-proposals-for-ai-safety)\n*   Don’t get fooled into thinking everyone now agrees. A lot more senior names are now signing onto statements and speaking up, and this is making it easier for previously silent-but-concerned researchers to speak up. However I think a majority of AI researchers probably still don’t agree this is a serious, imminent concern ([Yann LeCun’s silent majority](https://twitter.com/ylecun/status/1664011158989275138) is probably still real), and this disconnect in perceptions may result in significant pushback to come.\n*   Think carefully about the potential political fallout if and when this becomes an embarrassing thing for the politicians who have spoken up, and how to manage this. \n\nTo sum: I’m not saying it was wrong to push for this level of broad awareness and consensus-building; I think it may well turn out to be necessary this early in order to navigate the challenges on the path to transformative AI, even if we still have decades until that point (and we may not). But there’s the potential for a serious downside/backlash that this community, and everyone who shares our concern about existential risk from AI, should be thinking carefully about, in terms of positioning for effectiveness on slightly longer timelines.\n\n  \n_Thank you to Shakeel Hashim, Shahar Avin, Haydn Belfield and Ben Garfinkel for feedback on a previous draft of this post._  \n \n\n1.  **[^](#fnrefkcq5okqj3r)**\n    \n    Pushing against this, it seems likely that AI will have continued advancing as a technology, leading to ever-greater scientific and societal impacts. This may maintain or increase the salience of the idea that AI could pose extremely significant risks.\n    \n2.  **[^](#fnrefgk9hef3oa5)**\n    \n    A ‘softer’ version of this scenario is that some policy happens now, but then quietly drops off / gets dismantled over time, as political attention shifts elsewhere\n    \n3.  **[^](#fnrefmxdrl9nlph)**\n    \n    I don’t know how much this is happening in practice (there’s just so much online discourse right now it’s hard to track), but I have seen it remarked on several times e.g. [here](https://twitter.com/timhwang/status/1664335308307963904)"
          },
          "isValid": true
        },
        {
          "title": "Bridge-Building with AI Ethics Communities",
          "description": "The recommendation to build bridges with AI ethics/FAccT communities highlights a strategic opportunity for coalition-building despite some philosophical differences. This connects to ongoing discussions in EA about the value of \"moral trade\" and cooperation across value differences (as discussed by Toby Ord and Will MacAskill). The AI ethics and AI safety communities have historically had tensions, with different emphases on near-term harms versus long-term existential risks, but share many overlapping governance concerns. This bridge-building approach acknowledges what Holden Karnofsky has called \"worldview diversification\" - the value of incorporating multiple perspectives when dealing with deep uncertainty. Finding practical interventions that address both near and long-term concerns (like external evaluation, oversight mechanisms, and international governance) represents a pragmatic approach to coalition-building.",
          "highlight": {
            "startOffset": 4996,
            "endOffset": 5795,
            "quotedText": "Build bridges where possible with the AI ethics/FaCCT folk on a range of issues and interventions that seem set to overlap in that time; work together where possible. Lots of people from those communities are making proposals that are relevant and overlapping with challenges associated with the path to transformative AI. This includes external evaluation; licensing and liability; oversight of powerful tech companies developing frontier AI; international bodies for governing powerful AI, and much more. E.g. see [this](https://ainowinstitute.org/publication/gpai-is-high-risk-should-not-be-excluded-from-eu-ai-act) and [this](https://www.wired.com/story/ai-desperately-needs-global-oversight/), as well as [CAIS's recent blog post.](https://www.safe.ai/post/three-policy-proposals-for-ai-safety)"
          },
          "isValid": true
        },
        {
          "title": "The Illusion of Consensus",
          "description": "The author warns against overestimating the current level of consensus on AI risk, noting that vocal concern from prominent figures may not represent the majority view among AI researchers. This connects to what Daniel Kahneman calls the \"availability heuristic\" - where visible examples lead us to overestimate prevalence - and to broader EA discussions about the challenges of operating in information bubbles. The reference to LeCun's \"silent majority\" suggests potential for significant pushback, similar to what occurred in nuclear risk debates and climate science. This caution relates to the concept of \"robust\" policy-making, which anticipates potential resistance and designs interventions that can withstand shifts in public or expert opinion. Understanding the true distribution of expert beliefs, rather than focusing only on the most visible voices, remains a key challenge for the field.",
          "highlight": {
            "startOffset": 5800,
            "endOffset": 6313,
            "quotedText": "Don’t get fooled into thinking everyone now agrees. A lot more senior names are now signing onto statements and speaking up, and this is making it easier for previously silent-but-concerned researchers to speak up. However I think a majority of AI researchers probably still don’t agree this is a serious, imminent concern ([Yann LeCun’s silent majority](https://twitter.com/ylecun/status/1664011158989275138) is probably still real), and this disconnect in perceptions may result in significant pushback to come."
          },
          "isValid": true
        },
        {
          "title": "Political Fallout Management",
          "description": "The concern about potential political fallout if catastrophic AI outcomes don't materialize in the short-term connects to broader discussions of policy advocacy under uncertainty. This relates to what political scientists call the \"issue-attention cycle\" (Downs, 1972) where public and political attention to problems tends to follow boom-bust patterns. The suggestion to think carefully about managing this fallout acknowledges the reality that policy windows can close as rapidly as they open, as described in Kingdon's multiple streams framework of policy change. For the EA/rationalist community, this connects to ongoing discussions about maintaining \"option value\" and ensuring that initial policy interventions don't foreclose future opportunities if predictions prove incorrect. This concern also relates to the concept of \"robust\" policy design - creating interventions that deliver value even if specific risk predictions don't materialize as expected.",
          "highlight": {
            "startOffset": 6318,
            "endOffset": 7032,
            "quotedText": "Think carefully about the potential political fallout if and when this becomes an embarrassing thing for the politicians who have spoken up, and how to manage this. \n\nTo sum: I’m not saying it was wrong to push for this level of broad awareness and consensus-building; I think it may well turn out to be necessary this early in order to navigate the challenges on the path to transformative AI, even if we still have decades until that point (and we may not). But there’s the potential for a serious downside/backlash that this community, and everyone who shares our concern about existential risk from AI, should be thinking carefully about, in terms of positioning for effectiveness on slightly longer timelines."
          },
          "isValid": true
        },
        {
          "title": "Technological Advancement as Credibility Maintenance",
          "description": "The footnote suggesting that continued AI advancement might maintain the salience of AI risk concerns points to an interesting strategic consideration. This connects to what Bostrom calls the \"technological completion conjecture\" - the idea that technological development tends to continue until physical limits are reached. The footnote suggests that even if specific timeline predictions prove incorrect, the continued advancement of AI capabilities may itself serve as evidence that concerns about more advanced systems remain valid. This relates to the concept of \"technological momentum\" (Hughes, 1969) in science and technology studies - the tendency of technological trajectories to continue along established paths due to institutional, economic and social factors. For the rationalist community, this connects to discussions about how to maintain appropriate concern about risks even through periods where specific predictions may not materialize as expected.",
          "highlight": {
            "startOffset": 7205,
            "endOffset": 7457,
            "quotedText": "Pushing against this, it seems likely that AI will have continued advancing as a technology, leading to ever-greater scientific and societal impacts. This may maintain or increase the salience of the idea that AI could pose extremely significant risks."
          },
          "isValid": true
        },
        {
          "title": "Policy Entropy and Institutional Decay",
          "description": "The footnote about policy implementation followed by quiet dismantling highlights an important dynamic in institutional governance. This connects to what political scientists call \"policy entropy\" - the tendency for policies to weaken over time without sustained attention and maintenance. In the context of existential risk governance, this relates to what Ord calls the \"long reflection\" problem - how to maintain important safeguards over extended timeframes. This concern parallels observations in nuclear security, where initial strong governance frameworks have experienced gradual erosion as public attention shifted elsewhere. For the rationalist community, this connects to discussions about the challenges of institutional design for longtermism - creating governance structures that remain robust despite changing political priorities, attention cycles, and generational turnover. This footnote acknowledges that even successful initial policy interventions may face sustainability challenges without continued advocacy.",
          "highlight": {
            "startOffset": 7502,
            "endOffset": 7666,
            "quotedText": "A ‘softer’ version of this scenario is that some policy happens now, but then quietly drops off / gets dismantled over time, as political attention shifts elsewhere"
          },
          "isValid": true
        }
      ]
    }
  ]
}