{
  "id": "my-current-claims-and-cruxes-on-llm-forecasting-epistemics-lesswrong",
  "slug": "my-current-claims-and-cruxes-on-llm-forecasting-epistemics-lesswrong",
  "title": "My Current Claims and Cruxes on LLM Forecasting & Epistemics — LessWrong",
  "author": "ozziegooen",
  "publishedDate": "2024-06-26T00:40:58.342Z",
  "url": "https://www.lesswrong.com/posts/QvFRAEsGv5fEhdH3Q/preliminary-notes-on-llm-forecasting-and-epistemics",
  "intendedAgents": [
    "bias-detector",
    "clarity-coach",
    "research-scholar",
    "fake-eliezer",
    "quantitative-forecaster",
    "ea-impact-evaluator"
  ],
  "content": "I think that recent improvements in LLMs have brought us to the point where LLM epistemic systems are starting to be useful. After spending some time thinking about it, I've realized that such systems, broadly, seem very promising to me as an effective altruist intervention area. However, I think that our community has yet to do a solid job outlining what this area could look like or figuring out key uncertainties.\n\nThis document presents a rough brainstorm on these topics. While I could dedicate months to refining these ideas, I've chosen to share these preliminary notes now to spark discussion. If you find the style too terse, feel free to use an LLM to rewrite it in a format you prefer.\n\nI believe my vision for this area is more ambitious and far-reaching (i.e. not narrow to a certain kind of forecasting) than what I've observed in other discussions. I'm particularly excited about AI-heavy epistemic improvements, which I believe have greater potential than traditional forecasting innovations.\n\nI'm trying to figure out what to make of this regarding our future plans at [QURI](https://quantifieduncertainty.org), and I recommend that other organizations in the space consider similar updates.\n\n![Expensive photograph of a simple open notebook on wood table - 3 very-rough old hand-drawn mini-sketches, of engineering flow diagrams that uses AI and brains to make predictions. Lots of white in background. Only a few lines drawn. No words.](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EykCuXDCFAT5oGyux/ddwsmmzz71oq6evcpdw3)\n\nImaginary sketch of AI epistemic infrastructure, Dall-E 3\n\n## **Key Definitions**:\n\n*   **Epistemic process:** A set of procedures to do analysis work, often about topics with a lot of uncertainty. This could be “_have one journalist do everything themselves_”, to a complex (but repeatable) ecosystem of multiple humans and software systems.\n*   **LLM-based Epistemic Process (LEP)**: A system that relies on LLMs to carry out most or all of an epistemic process. This might start at ~10% LLM-labor, but can gradually ramp up. I imagine that such a process is likely to feature some kinds of estimates or forecasts.\n*   **Scaffolding:** Software used around an LLM system, often to make it valuable for specific use-cases. In the case of an LEP, a lot of scaffolding might be needed.\n\n##  1. High-Level Benefits & Uses\n\n**Claim 1: If humans could forecast much better, these humans should make few foreseeable mistakes. This covers many mistakes, particularly ones we might be worried about now.**\n\n*   Someone deciding about talking to a chatbot that can be predicted to be net-negative (perhaps it would create an unhealthy relationship) could see this forecast and simply decide not to start the chat.\n*   Say that a person’s epistemic state could follow one of four trajectories, depending on some set of reading materials. For example, one set is conspiratorial, one is religious, etc. Good forecasting could help forecast this and inform the person ahead of time.\n    *   Note that this can be radical and perhaps dangerous. For example, a religious family knowing how to keep their children religious with a great deal of certainty.\n*   Say that one of two political candidates is predictably terrible. This could be made clear to voters who trust said prediction.\n*   If an AI actor is doing something likely to be monopolistic or dangerous, this would be made more obvious to itself and those around it.\n*   Note: There will also be unforeseeable mistakes, but any actions that we could do that are foreseeably-high-value for them, could be predicted. For example, general-purpose risk mitigation measures. \n\n**Claim 2: Highly intelligent / epistemically capable organizations are likely to be better at coordination.** \n\n*   This might well mean fewer wars and conflict, along with corresponding military spending.\n*   If highly capable actors were in a prisoner's dilemma, the results could be ugly. But very often, there’s a lot of potential and value in not getting into one in the first place.\n*   Evidence: From [The Better Angels of Our Nature](https://en.wikipedia.org/wiki/The_Better_Angels_of_Our_Nature), there’s significant evidence that humanity has become significantly less violent over time. One potential exception is that large risks of conflict between large actors might have increased, but these last bits are some of the most difficult to coordinate, so this might not be too surprising. \n\n**Claim 3: The harm caused by altruistic development of epistemic improvements could be capped by a self-enforcing loop.**\n\n*   Agents developing epistemics should use the tools they build to decide on the net value of further improvements. This is a very important variable to get right.\n*   If it ever seems like future work will be negative in expected value, progress could and should be stopped.\n\n**Claim 4: Better forecasting would lead to more experimentation and innovation (“weirdness”).**\n\n*   Radical technology projects are well-known to be high-risk. High returns when they go well, high losses if they don’t. This means that there’s a high value of information in being more correct about outcomes early on.\n*   If people could become much better at forecasting, then it’s expected that they will become better at picking innovations to try. Bad projects would be avoided more often, and the expected value of innovation would increase.\n\n**Claim 5: AI forecasting/epistemics will be much more deeply integrated into the world than current prediction markets.**\n\n*   First, note that current prediction markets / tournaments are highly niche and overlooked. So AI systems would have to be more accepted, to be very valuable.\n*   Unlike current “human forecasters”, AI forecasting can be embedded deeply into other software. There would likely be better AI forecasting for repeated decisions, like devops procedures, edge cases in insurance, AI agents, etc.\n*   If “AI forecasters” were really solid and cheap, we’d expect them to improve many complex software workflows. Existing “human forecasters” are hard to directly make much money from, but these AI systems would be much easier to do so.\n*   They would likely be integrated in many other systems, such as:\n    *   Overseeing product sales chances\n    *   Estimating changes in accounting, from one quarter to the next\n    *   Estimating project lengths and budgets\n    *   Deciding the best LLM to use for a certain AI workflow, or the best workflow to follow.\n*   Note that many developments in \"_AI epistemics_\" are just regular enhancements to LLMs. There's no clear line between \"_AI forecasting tech_\" and \"_improvements in LLMs that make them better general-purpose reasoners._\"\n\n**Claim 6: Even \"straightforward\" AI forecasting systems should look very different from human judgemental forecasting systems**\n\n*   I've found others seem to imagine AI forecasting systems as simply replacing humans in prediction tournaments and prediction markets. I think think this is perhaps a decent first model, but not at all how things would end up.\n*   Prediction markets / tournaments are very much optimized for competitive humans, in cases where humans need to be incentivized. AI systems have very different constraints, so will need very different systems.\n    *   Formal resolutions won't be necessary to incentivize AIs, after they are trained.\n    *   Forecasting can be done on-demand, as the AI is always available. So questions and forecasts might not have to be done continuously, before being read by end-users.\n    *   There's unlikely to be a need to have many AIs compete against each other, like in a traditional tournament, in most cases.\n    *   Modern prediction platforms don't use advanced inputs like [scorable functions](https://forum.effectivealtruism.org/posts/ZWQ5jbLxpd4JtQgu4/scorable-functions-a-format-for-algorithmic-forecasting). Arguably these would be too complex for many humans, but this is unlikely to be the case for AIs. \n\n## 2\\. Failure Modes\n\n1.  **Nonprofit efforts in the space are not effective**\n    1.  Claim: The most likely failure mode for projects in this area is that they will fail to make an impact.\n        1.  If LLM abilities dramatically outpace agent / scaffolding architectures, then the only useful work here might be done by large LLM labs.\n        2.  There will likely be commercial entities taking care of the most obvious and low-hanging market opportunities. It’s not clear how to expect this area to change over time. It's possible that nonprofit / EA efforts here would have trouble finding a strong fit.\n2.  **Forecasting could be used for net-bad activities**\n    1.  **AI Acceleration**  \n        It’s possible that epistemic improvements would be used by AI capabilities actors to increase capabilities and not make other informed choices.  \n          \n        Claim: I think this is unlikely to be the net outcome. This would require these organizations to become smarter about making AIs, but not much smarter about making them safe, including for their own benefit. It would also require the other groups that could block them to become smarter - some of these groups include other non-LLM companies and other highly motivated actors who wouldn't want AI to go poorly.  \n          \n        One crux here is if the AI developers will both be much better at integrating AI epistemic improvements in their internal workflows, and if these systems won't convince or help them with AI alignment. This could be a really bad outcome, so it's important to continue to pay attention to.\n    2.  **Malicious Governments**  \n        Governments like China or North Korea could use better epistemic systems to better control their populations. My guess is that great epistemic systems will likely do some damage here. I think that overall the improvements (like improving western governments) will more than outweigh that, but this is something to pay attention to.  \n          \n        Overall, I don’t see reasons to think that better epistemics will dramatically change existing power balances in ways that are yet predictable. I think this is true for most technologies I could imagine, even many of the most beneficial ones.\n    3.  **Malicious Corporations**   \n        There are some situations where companies could be far better at forecasting than consumers would be, leading to a large power imbalance. If companies have much more information about customers than users have about themselves or these companies, that could lead to them being taken advantage of. It seems important to use governments (hopefully with better epistemic infrastructure) to take active steps against this. It seems important here to work to make sure that epistemic advances get distributed equitably.\n3.  **Intelligent systems lead to more complexity, leading to a less stable world**  \n    Arguably as humans become more intelligent, human civilization becomes more complex. Complexity has a cost, and at some points, these costs can become detrimental.  \n      \n    As you might expect, I think this is a _predictable_ disaster, meaning that better prediction abilities would help here. I’d expect agents to be able to make decisions about one step at a time, and for each, see if the benefits are worth the costs. In addition, forecasting will make many things simpler and more handleable. See sources like [Thinking Like a State](https://en.wikipedia.org/wiki/Seeing_Like_a_State) to understand ways that power structures actively force systems to become simpler - sometimes even _too_ much so.\n4.  **There's too much confusion over the terminology and boundaries, so research is stunted**  \n    \"_Epistemics_\" is a poorly understood term. \"_Judgemental Forecasting_\" is still very niche. The term \"forecasting\" is arguably too generic. It seems very possible that this area will struggle in finding agreed-upon and popular framings and terminology, which could contribute to it be neglected. I think more terminological and clarification work here could be valuable. This is similar to the idea of [Distillation and Research Debt.](https://distill.pub/2017/research-debt/) \n\n## 3\\. Viability\n\n**Claim 1: All/Almost all parts of a conventional epistemic process can be automated with LLMs.**\n\nLLMs can:\n\n1.  Generate forecasting questions\n2.  Resolve forecasting questions\n3.  Gather data from computers\n4.  Write specific questions to expert - a la interviews\n5.  Make forecasts\n6.  Make visualizations and dashboards of forecasts\n7.  Personalize information for specific people\n\n_Note: “Can be automated” doesn’t mean “easy to automate in a cost-effective or high-quality way.”_  \n \n\n**Claim 2: A lot of communication is low-entropy, especially with knowledge of the author.**\n\n*   Low-entropy means “_predictable by a decent AI agent, knowing information about the author’s history_”.\n*   Evidence: We currently have systems that predict what people will say in very limited extents - i.e. for email. These typically don’t have much information about the user. I expect this sort of thing will become more widely integrated and capable.\n*   Evidence: When _I_ write, I feel like much of my communication is highly predictable. I’m able to use Claude to do a lot for me. It’s not there yet - but the setup now is incredibly basic (it doesn’t even have my historic writing), and the tech is improving.\n*   At some point, we could make decent approximations of what many people believe on many topics. This could help us greatly speed up communication - AI systems would just need the pieces they can’t predict.   \n     \n\n**Claim 3: LLM-epistemics should be easier to study and optimize than human-epistemics.**  \n  \nThey can be:\n\n*   Reproducible (models don’t change over time)\n*   Versioned (if updates are made to an LLM, this new LLM can be separately studied)\n*   Pure (similar inputs, with seeds, produce the same outputs)\n*   Scalable (much easier for 10 researchers to study one LLM in parallel, than studying one human. Can also reuse the same exact LLM in many places, or researchers could run 500 LLM runs in parallel.)\n*   Investigatable (People have privacy concerns. LLMs don’t, so we don’t need to be cautious about making them feel bad with public evaluations)\n*   Modifiable (We can’t freely change parts of most people’s knowledge or abilities, but we can with LLMs.\n    *   We can experiment with a large number of isolated changes.\n    *   We can’t wipe a person’s memory in order to backtest a human-based epistemic process, but we (partially) do this for LLMs, by providing them access to specific data sets.\n    *   Related point: One challenge to note though is that the training data is particularly optimized for immediate times after the LLM was trained. We should probably only test LLMs in forecasting scenarios after they have been trained, and assume that their quality will degrade for forecasts they make far out from their training time. That said, this still is overall far more standardizable than with humans, which are typically our main alternative.   \n          \n        Add to that, that LLMs are often cheaper and better than many humans, and overall, these systems are hard to ignore.\n*   Controlled/limited. For example, if a question about \"Will person X be assassinated?\" were posted to a liquid prediction market, it could encourage someone to actually attack that person. (See: [assassination markets](https://en.wikipedia.org/wiki/Assassination_market)). However, if someone were to query their own AI forecasting system that question, they would hopefully have strong assurances that the AI won't be taking actions in the world. (Obviously, if the AI is not controlled, this could go poorly, but that is an extreme scenario.)\n\nEvidence:\n\n*   LLM R&D has dramatically risen recently and has resulted in major gains (>$2T) to company valuations. Human R&D around epistemics and reasoning has had nothing like this recently.\n*   We have far more studies and data about LLM performance than we do about important scientists, intellectuals, or decision-makers. There are of course some human studies (a la the Good Judgement Project), but these are expensive and thus few and very narrow.\n\nSubclaim: Even if we just wanted to understand ideal human epistemics better, one reasonable method might be to make strong LLM systems and study them, because these are much easier to study, and will likely lead to some generalizable findings.\n\n**Claim 4: LLM epistemic integration will be gradual.**  \nPractically speaking LLMs and Humans will work together side-by-side in epistemic processes for some time. I expect LLMs-epistemics to follow a similar trajectory to self-driving. First, it starts with light augmentation. Then it moves to “fully automated in most use cases, but occasionally needs oversight.” As with autonomous driving, Level 5 is very difficult.\n\n*   Subpoint: Don’t ask “_When will LLMs be better than humans at \\[forecasting|writing|research\\],_” Ask questions more like, “_what percentage of work done in \\[forecasting|writing|research\\] be automated_”, or, “_what ratio of Manifold Karma will be won by bots vs. humans vs. hybrids, over time?_” I’d expect to see gradual gains here.\n\nSubclaim: I expect that LLM-based forecasters will gradually dominate open platforms like Manifold and Metaculus. As this happens, it might scare off many of the human forecasters. If this is not handled gracefully, it could lead to these platforms feeling like ghost towns and becoming irrelevant, in favor of people using more direct AI systems.\n\n## 4\\. Scaffolding\n\nPrevious LLM-based forecasting systems have included some amounts of scaffolding.\n\n*   [FutureSearch.ai](http://futuresearch.ai)\n*   [Approaching Human-Level Forecasting with Language Models](https://arxiv.org/abs/2402.18563)\n\n  \n**Claim 1: LLM-based epistemic systems will likely require a lot of scaffolding, especially as they get ambitious**.  \nI.E. software, databases, custom web applications, custom batch job pipelines, etc. \n\n  \n**Claim 2: Scaffolding-heavy systems have significant downsides.**\n\n*   Difficult to study. Unlike single LLMs, scaffolded systems can feature:\n    *   High costs of deployment.\n    *   No/little ability to roll-back.\n    *   Frequent changes.\n    *   Internal substructures that are highly variable between different implementations.\n    *   External-facing APIs that are very different between different implementations.\n    *   Evidence: The previously mentioned LLM-based systems have some of these failures - and these are simple systems compared to what we might have later.\n*   Subclaim: If scaffolding is to be expected / needed, it could be important to plan for research measures that take this into account. This will require specialized attention.\n*   Subclaim: It seems important for there to be some “portable/packaged” LEPs, that could easily be shared, versioned, deployed, etc. This would be useful for research and benchmarks/baselines.\n\n**Claim 3: Scaffolding-heavy systems are likely to be centralized.**\n\n*   In general, complex web projects are centralized.\n*   This means that scaffolding-heavy setups will favor startups and in-house teams.\n*   This means that it's easy for many of these systems to be useless. Users might only interact with 1-2 systems, so any smaller competitors might get totally ignored, even if they have a few unusually great features.\n*   It’s possible that there could be some clean lines of separation between different components. Some lines seem simple to try out, but experimentation and thinking is needed to develop a better idea. Thinking/planning here seems good.\n    *   The more it’s split up into understood components, the easier it will be to research, optimize, and proliferate each part.\n\n**Claim 4: Scaffolding-heavy systems are expensive, and thus difficult to incentivize people to build.**\n\n*   Most forecasting competitions now pay out fairly small amounts. This makes it difficult to incentivize scaffolding.\n*   I don’t expect people to independently create great scaffolding in order to win <$50k.\n    *   My impression is that very little forecasting on Metaculus and Manifold is automated with LLMs. (There is one recent Metaculus competition here, but this is an isolated and forced use.)\n*   It’s also difficult to share and maintain scaffolding. You need to make custom web UIs, you might have to set up billing infrastructure for payments, etc. In a very large forecasting ecosystem with a lot of money, forecasters would subscribe to scaffolding systems, in the same way the professional traders subscribe to trading software and services.\n*   This means that we might well see slower progress here / less diversity.   \n     \n\n**Claim 5: New technologies or startups could greatly change and improve scaffolding.**\n\nRight now there are some startups and open-source frameworks to help with general-purpose scaffolding, but these are [known to be lacking](https://news.ycombinator.com/item?id=40739982). It's possible that much better systems will be developed in the future, especially because there's a lot of money in the space.\n\nIf this happens, that could hypothetically make powerful LLM epistemic tools dramatically easier to create.\n\nOne annoying challenge is that change in the field could lessen the importance of early experimentation. It could be the case that early projects focus heavily on some specific scaffolding frameworks, only for other frameworks to become more powerful later.\n\nA possible successful world could be one where different teams make \"[GPTs](https://chatgpt.com/gpts)\" that can integrate nicely with other workflows and interact with each other. Ideally these wouldn't be restricted to any one platform.  \n  \n**Crux: Scaffolding vs. Direct LLM Improvements** \n\nOne risk around scaffolding is that a lot of key functionality that scaffolding would be used for might be implemented directly in the inference of LLMs. For example, long-term planning could be done simply with scaffolding, but AI developers are also iterating on implementing it directly into the LLM layer. \n\nAnother example is that of editing text. If you have a long text document and want to use an LLM to make a few edits, it would currently attempt to rewrite the entire document, at a fairly slow speed. This could be improved with scaffolding by instead asking it to write a git diff and then applying that using traditional code. Or it could be solved on the inference layer directly, [as discussed here](https://www.cursor.com/blog/instant-apply). \n\nIf it is the case that most future improvements are done directly with the LLMs, then that could significantly change what work will be valuable. For example, if there are only a few leading LLM developers, then it might ultimately be up to them to actually add any functionality.  \n\n## 5\\. Subcomponents\n\nAs stated above, it’s hard to determine the best subdivisions of LEPs / LLM-Epistemic Scaffolding.\n\nHere’s one attempt, to provide a simple picture. \n\n**1\\. Data-collection**\n\n*   Many LLM services are already coming out that can help quickly put data, including private data, together, for different needs. See Apple Intelligence as one service that would work for people’s personal data.\n*   There are hundreds of startups in the data-collecting and data-management space. This problem can easily be split up more.\n*   Example performance question:  \n    _“Provided with resources R, gather as much data as you can, from this list of desired data, about X.”_  \n    The results of this will likely be obvious (it’s likely to be clear what data is accessible), so scientific testing might not be needed.   \n     \n\n**2\\. World Model**\n\n*   “World model” here likely means, “A set of stored symbolic and probabilistic assumptions about the world / people / organizations.”\n*   World models will likely include software programs and writing, instead of only relying on existing LLMs.\n    *   I expect that there could be workflows to LEPs to contribute to world models that will later give it better forecasting / reasoning abilities. The forecasting components could build the world models.\n*   Existing online data has very minimal attempts at providing accessible world models. These would ideally be probabilistic, immutable, shareable, collaborative, etc.\n*   Tools like [Squiggle Hub](https://squigglehub.org) are meant to be used here. Formal ontologies and knowledge graphs are likely to be useful.\n*   Some world models could be directly interesting to humans, for the presentation layer. Others might primarily be used by LLMs. For these, the world model is a form of a cache / collaboration.\n*   It seems possible/likely that world models won’t be unique to LEPs. Instead, many separate LEPs can use the same world models.\n*   World models could contain (or complete be) text descriptions of outcomes. For example, 10 compressed pages explaining how the next 10 years will go for a specific region.\n    *   It’s not clear yet what the best representations are for LLMs to store and fetch data about its understanding of specific things.\n*   Example performance question:  \n    “_Given a forecasting LLM L, and forecasting question Q, contribute a world model that will result in L being highly accurate_.”  \n     \n\n**3\\. Human Elicitation**\n\n*   It’s often useful to just go out and directly ask questions to people, or give them surveys. Strong LEPs should ideally be able to design these procedures to maximize information value, then execute them autonomously.\n    *   A system might realize that it would be useful to get a very specific expert to give their take on something. It could then email them, and perhaps offer a financial reward.\n    *   A different system might generate a bunch of specific binary questions for a group of people it wants to elicit information from. Maybe it’s trying to estimate aspects of their preferences, and information would help.\n*   Example performance question:  \n    _“Given that you want to get as much useful new data about topic X, using existing research Y, use resources R to send off elicitions and gather data from humans.”_\n\n**4\\. Data Prioritization**\n\n*   The task here is, “_Given the potential to fetch from data source D, and resources R, fetch the most useful data for providing marginal insight on unknown variables V._”\n*   Summarizing and displaying such work could also be useful. But doing so for humans would be very different to doing so for other LLMs.\n*   It seems like with LLMs that are capable of providing quick and accurate forecasts, we could use reinforcement learning and similar algorithms to do a good job here. For any set of data, we could quickly measure the improvement in accuracy using that data.\n\n**5\\. Numeric Modeling / Fermi Calculations**\n\n*   Make great complex Python/Excel/Squiggle models/estimates.\n*   One reasonable approach to fermi estimation is “Try a bunch of very different approaches, then mix the answers together.”\n*   We’ve seen a lot of evidence that LLMs can generate working Python/Squiggle code. However, I don’t believe that there has been much optimization in this area yet.\n*   It seems easy to iterate on, and generate, great data of strong examples of this. That could be then used for LLM training / fine-tuning.\n*   Example performance question:  \n    _“Estimate variable V as well as you can, using up to N LLM calls, and no other calls.”_  \n     \n\n**6\\. Decomposition / Amplification**\n\nThere are many potential strategies to \"break a problem down\" and solve different parts with different threads of LLMs. \n\nFor example:\n\n*   Chain-of-Thought prompting\n*   Mixture of experts\n*   Ensemble Averaging\n*   Debate (i.e. [Safety via Debate](https://arxiv.org/abs/1805.00899))\n*   Fact verification\n\nArguably, numeric modeling is a form of decomposition/amplification, I just separated it to draw attention to its uniqueness.\n\nThis is an active and popular area of ML research. Some of these techniques are incorporated in the LLM inference layer, but some are also used in scaffolding.\n\nI'm sure there's a lot more work to do here. It's less clear where smaller nonprofit research can be most effective, but I suspect that there are some good options. \n\nMany methods might be unique to forecasting or specific epistemic workflows. For example:\n\n*   Try to identify key cruxes or crucial considerations, and then investigate them\n*   Automated debate mapping\n*   Identify [Scissor statements](https://slatestarcodex.com/2018/10/30/sort-by-controversial/), in part to help personalize research\n*   Use [discernment](https://www.lesswrong.com/posts/W2iwHXF9iBg4kmyq6/the-practice-and-virtue-of-discernment) to clarify and decompose key terminology\n*   Breaking down forecasting questions into sub-questions, perhaps with bayesian networks. \n\nArguably, the rationality and effective altruist communities might be very well suited to work in this area. There's been a lot of writing on how to systematically break hard problems down, now people would need to experiment with encoding these systems into automated workflows.   \n  \n**7\\. Question Generation** \n\n*   Come up with questions to be researched or forecasted on.\n*   This might well be done by other listed components - for example, it could simply combine \"Idea Generation\" with \"Forecasting\" to come up with potential questions, and guess which will be the most important and tractable. It also might be better done under the \"Presentation\" question. However, I suspect that there will be unique research and development for this problem, especially because it's a major part of existing forecasting systems.\n*   Example performance question:  \n    _“Write 5 forecasting questions on topic X. After this, a separate epistemic system will be run with a fixed number of resources R on each question. Choose questions such that the final results are maximally useful to end-users.\"_  \n     \n\n  \n**8\\. Forecasting**\n\n*   Convert fetched data into a forecast.\n*   Can call the “Numeric Modeling” module, or be merged with it.\n*   There’s been previous work here that is promising.\n*   Fine-tuning and prompt engineering seems like it would go a long way.\n*   Current LLMs are known to be biased and often suffer from poor calibration, in part in order to become better liked by human users. We might well want specialized systems optimized for pure accuracy.\n    *   This delta might cause problems. Perhaps it could lead to bad news stories (where a model used for forecasting gives a politically-incorrect answer), or perhaps it could be used by bad actors.\n    *   This area should be thus treated with care. Perhaps some models are only available to certain researchers, or perhaps there should be large disclaimers provided for regular people attempting to use them directly.\n*   Forecasting can output high-level stories, probabilities, scorable functions, or other. There are many potential formats here.\n*   Example performance question:  \n    _“Using information I, and N LLM calls, make your best forecast on variable V. You may use the Numeric Modeling module when doing so, but this will eat into the LLM calls.”_  \n     \n\n**9\\. Forecasting Resolution**\n\n*   Most forecasting questions should be resolvable automatically. This can be done by either querying external data, or perhaps by asking a person/LLM for their opinion.\n*   This component is likely to often just either be, or defer to, other LEP modules.\n*   Question Creation and Optimization\n    *   A strong LEP would make good judgment calls on what research or forecasting topics would be valuable to people, and initialize those.\n    *   This could range from “once a user logs in, generates custom information for them to maximize their EV”, to, “write a list of standard forecasting questions that are collectively high-EV to a subpopulation.”\n    *   Example performance question:  \n        _“Using an existing LEP L, and resources R, write a set of forecasting questions. Expect that forecasting resources F will be used on these questions, and you are trying to optimize utility function U.”_  \n         \n\n**10\\. Presentation**\n\n*   Many custom charts and graphics would likely be useful.\n*   Decisions need to be made about what final data should be shown to users and how.\n*   Automated decision-making: In some cases / eventually, LEPs would interact directly with decision systems. In theory, people don’t need to know almost anything - critical information could be used directly by in-world automations. For example, instead of an LEP doing research into the costs and benefits of vitamin D supplementation and showing you that research, a system instead just starts an order for you to get a regular set of supplements. This could come with optional research, but in many cases the research won’t be reviewed, if the system is trusted.\n*   In cases where LEPs are used behind APIs, the presentation layer could include answering questions about what the LEP could do, and suggesting specific uses for it.\n*   Presentation/wording should be optimized to be understandable to humans. There should be tests of the error rates of what was intended to be stated, and what humans understood. For example, many people have significant misunderstandings about basic statistics concepts, so probabilities might need to be handled with care.\n*   Example performance questions:\n    *   _“Using LEP output O, design a dashboard that user U will most prefer.”_\n    *   _“Using LEP output O, design a set of complicated diagrams without detectable errors.” (judged by an LLM)._   \n         \n\n**11\\. Distribution**\n\n*   Full custom user interfaces could be created. This might range from a simple custom LLM chat, to a auto-generated set of dashboards, with the ability to delve deeper into any topic the user clicks on.\n*   LEPs could be accessed by any other chat LLM interface, as long as that interface can call external APIs. This could be generally useful for integrating into other AI tools and workflows, perhaps via other common LLM chat UIs.\n*   LEP information could be distributed like how human communication is communicated. Text messages, emails, Tweets, voice assistants, etc. Ideally the LEP could vaguely predict what information is useful to people and when, and then send them that information in those cases.\n    *   Example: You get an email in the morning. It says, _“Some news came out at 6am today that changed my recommendation of your financial assets. I recommend you change to a different mutual fund. I estimate that doing so will net you an average of $50/month.”_\n*   Example performance questions:\n    *   _“Using LEP output O, design and implement the best way to get this information to agent A.” We can use an LLM that mimics human behaviors to evaluate._  \n         \n\n**12\\. Idea Generation / Optimization**\n\n*   It will often be useful to generate ideas on the best ways to do something.\n*   For example, an LLM agent might want to make a suggestion to a user, before doing forecasting on said suggestion.\n*   Regular LLM performance might be fine on this, but fine-tuning or similar might help. It might be the case that there are certain specific domains where idea generation is very useful.\n*   “Idea Generation” is related to [program optimization](https://en.wikipedia.org/wiki/Program_optimization) and other CS topics.\n*   Example performance questions:\n    *   _“Given a person with details D, generate a plan for that person to optimize their financial future.”_ (This will be then judged with an LEP / forecasting module)\n    *   _“Given a 2d grid game G, come up with 10 ordered ideas for the best strategy”_ (Can be judged empirically)  \n         \n\n**13\\. Task Orchestration**\n\n*   We need systems that can actually trigger all the other components when needed. This process needs to be debuggable / introspectable. Ideally, these workflows would be highly understandable.   \n     \n\n## 6\\.  Benchmarks & Baselines\n\n**Claim 1: Accessing forecasting quality and efficiency with humans is very difficult and often poorly done.**\n\n*   Scores like the Brier Score don’t take into account the difficulty of the question.\n*   It’s really hard to standardize question sets, audiences, and environments. Each time a human forecasting competition is done, it’s done with a separate population. Also, we typically don’t track the resources going into these systems - like the total time that people spend forecasting. Again, humans are mediocre for studying, in comparison to LLMs.\n*   We arguably don't yet have good statistical measures for [credal resilience](https://forum.effectivealtruism.org/topics/credal-resilience).\n\n**Claim 2: Portable LEPs could be useful for acting as priors or baselines, for other systems to perform against.**\n\n*   We will want some baseline LEPs to score other LEPs against. These should act on the same data, at the same times.\n    *   For example, one could take a new LEP, a well-known Baseline LEP, and a known or generated question set. Both systems would forecast those questions, and then the new LEP will be evaluated based on how much it outperforms the Baseline LEP. For example, with the average difference in brier score.\n    *   In order to do this, the LEP must be equipped with the abilities to fetch data and to forecast. This is useful in order to compare the system to human forecasters (which do both), but it might be often better to divide the subcomponents and study/optimize them in isolation.\n    *   Another use is to use Baseline LEP to initialize forecasting markets. Right now manifold questions begin at 50%. It could be better for them to instead begin at a number suggested by a simple LEP.\n    *   Even if these LEPs aren't highly accurate, they could be useful for sanity checking claims outside of forecasting systems. For example, all publicly given estimates and statements could be run against LEPs. This can then highlight estimates that are particularly surprising. In financial / numeric models, all inputs and assumptions can be checked against LEPs to find ones that might be suspicious.\n\n**Claim 3: Standardized LEPs could be great judges to resolve subjective questions**\n\n*   It’s well-known that it’s difficult to use partially-subjective questions in prediction markets. Much of the reason is because the only way to resolve these is with human expert judges, and these human judges have eccentricities that are hard to predict. AI systems might help here, because they can be far more standardized and testable than human systems.\n*   For example, say there’s a question, “_Will project X generally be considered a failure in 2026?_” This can be changed to, “_Will project X be considered a failure, in an average of 20 runs, by a certain LEP, in 2026?_” Then, predictors could provide this LEP fake hypothetical information on the future and see how this question would resolve with that information. There should be very little surprises from the eventual output of the LEP.\n*   Related, LEPs could be used to provide independent resolutions for every other forecasting question on Metaculus/Manifold/etc. There would be a certain expected error rate - if some resolution judge statistically exceeds this rate, that would be flagged for review.\n\n**Idea: It’s possible we could get very useful small LEPs that are easily packaged, relatively cheaply.**\n\n*   Certain LEP components are much more useful to include for different kinds of testing than others. Some simple web search functionality and forecasting abilities could get us far for testing forecasting. Right now there are a bunch of startups and tools emerging to help with certain kinds of scaffolding that might be useful here. For example, you can currently bundle special data and prompts with a custom ChatGPT GPT. Right now this cannot be accessed via the API, but this is a potential thing that could exist in the future.\n*   One viable solution could be for someone to set up a very simple web service that can make internet queries and forecasts. Ideally, this would provide multiple immutable options, and this would be trusted to stay up, for future experimentation. It might only need to be accessible via an API.\n*   It might also be possible to have a simple LEP in a single Docker container or similar.\n*   Even very simple LEPs could act as much better baselines than what we use today.\n\n**Challenge: LLM-forecasting abilities will vary based on date.**\n\n*   The quality of LLM predictions is likely to go down, the further away the forecasting question resolution is from the LLM training date. This is a major challenge to using them as a benchmark. For example, GPT4 might be decent at guessing the US GDP in 2025 if used in 2024 with search capabilities (i.e. web search). But if it were used in 2030, for the US GDP in 2031, with search capabilities but no other training updates, its performance would likely be worse. It's background/training data would be much less relevant.\n*   It's possible that there could simple numeric adjustments for this, for certain accuracy scores. For example, perhaps it's expected that the forecasting quality would decline according to a linear trend, in proportion between the time the LLM was trained, and when the forecasting question was asked.\n*   LLMs should rarely be used to “_forecast_” events that happened before their release. It’s likely that their training data would have leaks of the results, or of things close to these results.\n*   Related, LLMs will be better at forecasting in some domains in others. However, I don't think that this will be a major problem. We will learn about these boundaries over time, and I expect that they will be heavily correlated between different LLMs and LLM versions. \n\n## 7\\. Caching vs. On-Demand Forecasting\n\n**Key question: How much would we expect AI forecasting systems to generate forecasts and visualizations on-demand for users, vs. save them before?**\n\n*   On-demand work is more user-customizable and user-specific, but it’s harder.\n*   In the beginning, on-demand AI work will be used for the “last-mile” problem. A user asks a question - we first find the most similar forecasts, then use an LLM to use those to generate a personalized forecast. We’d also want great charts ready-to-go.\n*   This tracks a question about web content more generally. Eventually websites might be generated on-demand, to be personalized for users. But it will likely take some time to get there.\n\n**Simple heuristics:**\n\n*   Save all generated forecasts, especially if these are expensive to produce. Reference them for future forecasting. This should be very useful to have the system cost-effectively scale to handle a large number of queries.\n    *   Be careful of potential privacy issues, though.\n*   If the system can predict that (A) a bunch of questions will be asked on topic T in the future, and (B) some saved computations will be useful, then it will aim to produce such work ahead of time. This might be by fetching a lot of relevant data, updating the world model, or making other improvements.\n*   It’s probably not an efficient bet to make complex scorable functions with a lot of attention to parts of the parameter space that are unlikely to actually be called. Instead, do a rough version, then wait until they are called to bother improving them in these areas. Save the updated version, of course.\n\n## Further Research\n\nI feel like there are clearly some larger investigative projects to do here. It's a vast and important topic, and at the same time there's a lot of fairly-obvious things (yet interesting and useful) we could say about it. This is mostly just putting a bunch of existing ideas in one place and providing some definitions and clarifications.\n\nSome future things I'd like to explore (or have others explore!) include:\n\n1.  **Exploration of benchmarks and optimization of end-to-end LEPs**  \n    This document includes more about specific subcomponents of LEPs than the full thing itself.\n2.  **Strategies for prioritizing philanthropic LEP work**  \n    I think this document helps list out a lot of potential LEP work, but it doesn't try to prioritize it. This would take some thinking. Maybe predictions can be used.\n3.  **Cost-effectiveness estimation**  \n    I think these systems are promising, but I'm not sure how to best estimate the cost-effectiveness of them vs. other effective altruist interventions. It would be useful to have better models of benefits and risks, perhaps with new delineations.\n4.  **Concrete project ideas for enthusiasts**  \n    I could imagine a lifehacker-like community forming around some of these principles. I think creative experimentation by enthusiasts could be useful, in part to help identify promising full-time talent.\n5.  **Moving key claims and cruxes to forecasting platforms and LLMs**  \n    Fitting with the theme, it seems interesting to try to pull out key questions here and post them to forecasting platforms. I tried to isolate claims and cruxes, in part so that they could later be easily adapted into somewhat-concrete questions. \n\nMy guess is that a lot of people will bounce off this broad topic. I imagine that there's much better writing that could be done to formalize and popularize the key concepts that will be useful going forward. I'd encourage others to make stabs here.",
  "reviews": [
    {
      "agentId": "bias-detector",
      "costInCents": 0,
      "createdAt": "2025-04-17T04:08:26.436Z",
      "runDetails": "{\"model\":\"google/gemini-2.0-flash-001\",\"promptTokens\":11290,\"completionTokens\":1874,\"totalTokens\":13164,\"temperature\":0.1,\"runtimeMs\":11732}",
      "thinking": "This document presents a preliminary exploration of LLM-based epistemic systems (LEPs) and their potential impact, benefits, and failure modes. It's a broad brainstorm, aiming to spark discussion and further research. My analysis focuses on identifying potential biases that might influence the author's claims and recommendations, as well as biases that LEPs themselves might exhibit or exacerbate. I'll also consider how the author's framing might be susceptible to certain cognitive biases.nnI'll be looking for instances of confirmation bias (seeking evidence that supports pre-existing beliefs), optimism bias (overestimating the likelihood of positive outcomes), and availability heuristic (over-relying on easily recalled examples). I'll also consider potential biases related to the author's background and affiliations, such as in-group bias or system justification bias.nnFurthermore, I'll analyze the discussion of failure modes for potential biases. For example, the author's assessment of the likelihood of AI acceleration being a net-negative outcome could be influenced by motivated reasoning or scope insensitivity. The discussion of malicious governments and corporations could be affected by out-group homogeneity bias or the just-world hypothesis.nnFinally, I'll examine the author's claims about the viability of LEPs for potential biases. The claims about the ease of studying and optimizing LEPs could be influenced by the illusion of control or the Dunning-Kruger effect. The discussion of benchmarks and baselines could be affected by selection bias or Goodhart's law.",
      "summary": "This document presents a compelling vision for LLM-based epistemic systems, but it's important to critically evaluate the claims and recommendations for potential biases. The analysis reveals potential for optimism bias in assessing the benefits, confirmation bias in dismissing certain risks, availability heuristic in selecting examples, illusion of control in evaluating the ease of optimization, survivorship bias in comparing R&D efforts, base rate neglect in considering malicious uses, and the planning fallacy in anticipating scaffolding development. Recognizing these biases can help refine the author's arguments and guide future research in a more balanced and robust manner.",
      "comments": [
        {
          "title": "Availability Heuristic in Examples of LEP Applications",
          "description": "The examples provided for how AI forecasting/epistemics will be integrated into the world may be influenced by the availability heuristic. The author lists examples such as overseeing product sales chances, estimating changes in accounting, and estimating project lengths. These examples may be more salient or easily recalled, but they may not represent the most impactful or representative applications of LEPs. This could lead to an incomplete or skewed understanding of the potential value of LEPs.",
          "importance": "65",
          "grade": "60",
          "highlight": {
            "startOffset": -1,
            "endOffset": 6527,
            "quotedText": "I think that recent improvements in LLMs have brought us to the point where LLM epistemic systems are starting to be useful. After spending some time thinking about it, I've realized that such systems, broadly, seem very promising to me as an effective altruist intervention area. However, I think that our community has yet to do a solid job outlining what this area could look like or figuring out key uncertainties.\n\nThis document presents a rough brainstorm on these topics. While I could dedicate months to refining these ideas, I've chosen to share these preliminary notes now to spark discussion. If you find the style too terse, feel free to use an LLM to rewrite it in a format you prefer.\n\nI believe my vision for this area is more ambitious and far-reaching (i.e. not narrow to a certain kind of forecasting) than what I've observed in other discussions. I'm particularly excited about AI-heavy epistemic improvements, which I believe have greater potential than traditional forecasting innovations.\n\nI'm trying to figure out what to make of this regarding our future plans at [QURI](https://quantifieduncertainty.org), and I recommend that other organizations in the space consider similar updates.\n\n![Expensive photograph of a simple open notebook on wood table - 3 very-rough old hand-drawn mini-sketches, of engineering flow diagrams that uses AI and brains to make predictions. Lots of white in background. Only a few lines drawn. No words.](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EykCuXDCFAT5oGyux/ddwsmmzz71oq6evcpdw3)\n\nImaginary sketch of AI epistemic infrastructure, Dall-E 3\n\n## **Key Definitions**:\n\n*   **Epistemic process:** A set of procedures to do analysis work, often about topics with a lot of uncertainty. This could be “_have one journalist do everything themselves_”, to a complex (but repeatable) ecosystem of multiple humans and software systems.\n*   **LLM-based Epistemic Process (LEP)**: A system that relies on LLMs to carry out most or all of an epistemic process. This might start at ~10% LLM-labor, but can gradually ramp up. I imagine that such a process is likely to feature some kinds of estimates or forecasts.\n*   **Scaffolding:** Software used around an LLM system, often to make it valuable for specific use-cases. In the case of an LEP, a lot of scaffolding might be needed.\n\n##  1. High-Level Benefits & Uses\n\n**Claim 1: If humans could forecast much better, these humans should make few foreseeable mistakes. This covers many mistakes, particularly ones we might be worried about now.**\n\n*   Someone deciding about talking to a chatbot that can be predicted to be net-negative (perhaps it would create an unhealthy relationship) could see this forecast and simply decide not to start the chat.\n*   Say that a person’s epistemic state could follow one of four trajectories, depending on some set of reading materials. For example, one set is conspiratorial, one is religious, etc. Good forecasting could help forecast this and inform the person ahead of time.\n    *   Note that this can be radical and perhaps dangerous. For example, a religious family knowing how to keep their children religious with a great deal of certainty.\n*   Say that one of two political candidates is predictably terrible. This could be made clear to voters who trust said prediction.\n*   If an AI actor is doing something likely to be monopolistic or dangerous, this would be made more obvious to itself and those around it.\n*   Note: There will also be unforeseeable mistakes, but any actions that we could do that are foreseeably-high-value for them, could be predicted. For example, general-purpose risk mitigation measures. \n\n**Claim 2: Highly intelligent / epistemically capable organizations are likely to be better at coordination.** \n\n*   This might well mean fewer wars and conflict, along with corresponding military spending.\n*   If highly capable actors were in a prisoner's dilemma, the results could be ugly. But very often, there’s a lot of potential and value in not getting into one in the first place.\n*   Evidence: From [The Better Angels of Our Nature](https://en.wikipedia.org/wiki/The_Better_Angels_of_Our_Nature), there’s significant evidence that humanity has become significantly less violent over time. One potential exception is that large risks of conflict between large actors might have increased, but these last bits are some of the most difficult to coordinate, so this might not be too surprising. \n\n**Claim 3: The harm caused by altruistic development of epistemic improvements could be capped by a self-enforcing loop.**\n\n*   Agents developing epistemics should use the tools they build to decide on the net value of further improvements. This is a very important variable to get right.\n*   If it ever seems like future work will be negative in expected value, progress could and should be stopped.\n\n**Claim 4: Better forecasting would lead to more experimentation and innovation (“weirdness”).**\n\n*   Radical technology projects are well-known to be high-risk. High returns when they go well, high losses if they don’t. This means that there’s a high value of information in being more correct about outcomes early on.\n*   If people could become much better at forecasting, then it’s expected that they will become better at picking innovations to try. Bad projects would be avoided more often, and the expected value of innovation would increase.\n\n**Claim 5: AI forecasting/epistemics will be much more deeply integrated into the world than current prediction markets.**\n\n*   First, note that current prediction markets / tournaments are highly niche and overlooked. So AI systems would have to be more accepted, to be very valuable.\n*   Unlike current “human forecasters”, AI forecasting can be embedded deeply into other software. There would likely be better AI forecasting for repeated decisions, like devops procedures, edge cases in insurance, AI agents, etc.\n*   If “AI forecasters” were really solid and cheap, we’d expect them to improve many complex software workflows. Existing “human forecasters” are hard to directly make much money from, but these AI systems would be much easier to do so.\n*   They would likely be integrated in many other systems, such as:\n    *   Overseeing product sales chances\n    *   Estimating changes in accounting, from one quarter to the next\n    *   Estimating project lengths and budgets\n    *   Deciding the best LLM to use for a certain AI workflow, or the best workflow to follow."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Optimism Bias in Potential Benefits",
          "description": "The claims about the benefits of improved forecasting, such as fewer mistakes, better coordination, and more experimentation, may be subject to optimism bias. The author seems to assume that better information will automatically lead to better decisions, without fully accounting for the complexities of human behavior and organizational dynamics. There's an implicit assumption that actors will rationally act on the forecasts, which may not always be the case.",
          "importance": "80",
          "grade": "65",
          "highlight": {
            "startOffset": 2379,
            "endOffset": 3658,
            "quotedText": "Benefits & Uses\n\n**Claim 1: If humans could forecast much better, these humans should make few foreseeable mistakes. This covers many mistakes, particularly ones we might be worried about now.**\n\n*   Someone deciding about talking to a chatbot that can be predicted to be net-negative (perhaps it would create an unhealthy relationship) could see this forecast and simply decide not to start the chat.\n*   Say that a person’s epistemic state could follow one of four trajectories, depending on some set of reading materials. For example, one set is conspiratorial, one is religious, etc. Good forecasting could help forecast this and inform the person ahead of time.\n    *   Note that this can be radical and perhaps dangerous. For example, a religious family knowing how to keep their children religious with a great deal of certainty.\n*   Say that one of two political candidates is predictably terrible. This could be made clear to voters who trust said prediction.\n*   If an AI actor is doing something likely to be monopolistic or dangerous, this would be made more obvious to itself and those around it.\n*   Note: There will also be unforeseeable mistakes, but any actions that we could do that are foreseeably-high-value for them, could be predicted. For example, general-"
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Confirmation Bias in AI Alignment Discussion",
          "description": "The author dismisses the possibility that epistemic improvements could be used by AI capabilities actors to increase capabilities without making them safer. This dismissal could be influenced by confirmation bias, as the author may be more inclined to believe that epistemic improvements will lead to positive outcomes. The author presents a counter-argument, but it may not be sufficiently robust to address the potential risks.",
          "importance": "75",
          "grade": "55",
          "highlight": {
            "startOffset": 8898,
            "endOffset": 9617,
            "quotedText": "Claim: I think this is unlikely to be the net outcome. This would require these organizations to become smarter about making AIs, but not much smarter about making them safe, including for their own benefit. It would also require the other groups that could block them to become smarter - some of these groups include other non-LLM companies and other highly motivated actors who wouldn't want AI to go poorly.  \n          \n        One crux here is if the AI developers will both be much better at integrating AI epistemic improvements in their internal workflows, and if these systems won't convince or help them with AI alignment. This could be a really bad outcome, so it's important to continue to pay attention to."
          },
          "isValid": true
        },
        {
          "title": "Base Rate Neglect in Malicious Government Scenario",
          "description": "The author suggests that great epistemic systems will likely do some damage by being used by malicious governments, but that the improvements (like improving western governments) will more than outweigh that. This assessment may suffer from base rate neglect. The author may not be adequately considering the relative frequency of beneficial vs. harmful uses of epistemic systems, or the potential scale of harm that malicious actors could inflict.",
          "importance": "70",
          "grade": "60",
          "highlight": {
            "startOffset": 9662,
            "endOffset": 10257,
            "quotedText": "Governments like China or North Korea could use better epistemic systems to better control their populations. My guess is that great epistemic systems will likely do some damage here. I think that overall the improvements (like improving western governments) will more than outweigh that, but this is something to pay attention to.  \n          \n        Overall, I don’t see reasons to think that better epistemics will dramatically change existing power balances in ways that are yet predictable. I think this is true for most technologies I could imagine, even many of the most beneficial ones."
          },
          "isValid": true
        },
        {
          "title": "Illusion of Control in LLM Epistemics",
          "description": "The author's claims about the ease of studying and optimizing LLM-epistemics may be influenced by the illusion of control. While LLMs offer advantages in terms of reproducibility and scalability, the complexity of these systems and the potential for emergent behavior should not be underestimated. The author may be overconfident in the ability to fully understand and control these systems.",
          "importance": "70",
          "grade": "50",
          "highlight": {
            "startOffset": 13331,
            "endOffset": 14869,
            "quotedText": "ow is incredibly basic (it doesn’t even have my historic writing), and the tech is improving.\n*   At some point, we could make decent approximations of what many people believe on many topics. This could help us greatly speed up communication - AI systems would just need the pieces they can’t predict.   \n     \n\n**Claim 3: LLM-epistemics should be easier to study and optimize than human-epistemics.**  \n  \nThey can be:\n\n*   Reproducible (models don’t change over time)\n*   Versioned (if updates are made to an LLM, this new LLM can be separately studied)\n*   Pure (similar inputs, with seeds, produce the same outputs)\n*   Scalable (much easier for 10 researchers to study one LLM in parallel, than studying one human. Can also reuse the same exact LLM in many places, or researchers could run 500 LLM runs in parallel.)\n*   Investigatable (People have privacy concerns. LLMs don’t, so we don’t need to be cautious about making them feel bad with public evaluations)\n*   Modifiable (We can’t freely change parts of most people’s knowledge or abilities, but we can with LLMs.\n    *   We can experiment with a large number of isolated changes.\n    *   We can’t wipe a person’s memory in order to backtest a human-based epistemic process, but we (partially) do this for LLMs, by providing them access to specific data sets.\n    *   Related point: One challenge to note though is that the training data is particularly optimized for immediate times after the LLM was trained. We should probably only test LLMs in forecasting scenarios after"
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Survivorship Bias in R&D Comparison",
          "description": "The author compares LLM R&D to human R&D around epistemics and reasoning, noting the significant gains in company valuations associated with LLMs. This comparison may be subject to survivorship bias. The author focuses on the successes of LLM R&D, while potentially overlooking the failures or less impactful efforts in that field. Additionally, the author may not be fully accounting for the different levels of investment and attention given to LLM R&D compared to human R&D in epistemics.",
          "importance": "60",
          "grade": "55",
          "highlight": {
            "startOffset": 15815,
            "endOffset": 16256,
            "quotedText": "LLM R&D has dramatically risen recently and has resulted in major gains (>$2T) to company valuations. Human R&D around epistemics and reasoning has had nothing like this recently.\n*   We have far more studies and data about LLM performance than we do about important scientists, intellectuals, or decision-makers. There are of course some human studies (a la the Good Judgement Project), but these are expensive and thus few and very narrow."
          },
          "isValid": true
        },
        {
          "title": "Planning Fallacy in Scaffolding Development",
          "description": "The author acknowledges that new technologies or startups could greatly change and improve scaffolding, but also notes the risk that early projects might focus on specific scaffolding frameworks that later become less powerful. This highlights the potential for the planning fallacy. The author may be underestimating the time, effort, and uncertainty involved in developing effective scaffolding systems, and overestimating the likelihood of early projects aligning with future technological developments.",
          "importance": "65",
          "grade": "60",
          "highlight": {
            "startOffset": 21297,
            "endOffset": 21793,
            "quotedText": "One annoying challenge is that change in the field could lessen the importance of early experimentation. It could be the case that early projects focus heavily on some specific scaffolding frameworks, only for other frameworks to become more powerful later.\n\nA possible successful world could be one where different teams make \"[GPTs](https://chatgpt.com/gpts)\" that can integrate nicely with other workflows and interact with each other. Ideally these wouldn't be restricted to any one platform."
          },
          "isValid": true
        }
      ],
      "grade": 68
    },
    {
      "agentId": "clarity-coach",
      "costInCents": 0,
      "createdAt": "2025-04-17T04:08:46.546Z",
      "runDetails": "{\"model\":\"google/gemini-2.0-flash-001\",\"promptTokens\":11413,\"completionTokens\":1722,\"totalTokens\":13135,\"temperature\":0.1,\"runtimeMs\":9259}",
      "thinking": "This document presents a preliminary exploration of LLM-based epistemic systems (LEPs) and their potential impact, particularly within the effective altruism community. The author, ozziegooen, outlines key definitions, potential benefits, failure modes, viability claims, scaffolding considerations, subcomponents, benchmarks, and caching strategies related to LEPs. The document is structured as a brainstorm, aiming to spark discussion and further investigation into this emerging field.\n\nAs a Clarity Coach, my focus is on evaluating the document's clarity, coherence, and readability. I will assess how effectively the author communicates complex ideas, identifies potential areas of confusion, and provides actionable recommendations for improvement. I will pay close attention to the use of jargon, the logical flow of arguments, and the overall accessibility of the content to a diverse audience.\n\nGiven the document's nature as preliminary notes, I anticipate finding opportunities to enhance clarity and structure. My analysis will aim to provide constructive feedback that helps the author refine their ideas and communicate them more effectively to a wider audience.",
      "summary": "This document presents a valuable exploration of LLM-based epistemic systems. As a Clarity Coach, my analysis identifies opportunities to enhance the document's clarity, coherence, and readability. Key recommendations include simplifying definitions, reducing jargon, improving logical flow, increasing the specificity of claims, providing more detailed explanations of failure modes, and adding concrete examples. Addressing these areas would make the document more accessible to a wider audience and facilitate further discussion and investigation into this promising field.",
      "comments": [
        {
          "title": "Clarity of Key Definitions",
          "description": "The definitions provided are crucial for understanding the rest of the document. However, they could be made more accessible by providing simpler, more intuitive explanations alongside the formal definitions. For example, the definition of \"Epistemic process\" could benefit from a real-world example to illustrate the concept.",
          "importance": "85",
          "grade": "65",
          "highlight": {
            "startOffset": 1639,
            "endOffset": 158,
            "quotedText": " about it, I've realized that such systems, broadly, seem very promising to me as an effective altruist intervention area. However, I think that our community has yet to do a solid job outlining what this area could look like or figuring out key uncertainties.\n\nThis document presents a rough brainstorm on these topics. While I could dedicate months to refining these ideas, I've chosen to share these preliminary notes now to spark discussion. If you find the style too terse, feel free to use an LLM to rewrite it in a format you prefer.\n\nI believe my vision for this area is more ambitious and far-reaching (i.e. not narrow to a certain kind of forecasting) than what I've observed in other discussions. I'm particularly excited about AI-heavy epistemic improvements, which I believe have greater potential than traditional forecasting innovations.\n\nI'm trying to figure out what to make of this regarding our future plans at [QURI](https://quantifieduncertainty.org), and I recommend that other organizations in the space consider similar updates.\n\n![Expensive photograph of a simple open notebook on wood table - 3 very-rough old hand-drawn mini-sketches, of engineering flow diagrams that uses AI and brains to make predictions. Lots of white in background. Only a few lines drawn. No words.](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EykCuXDCFAT5oGyux/ddwsmmzz71oq6evcpdw3)\n\nImaginary sketch of AI epistemic infrastructure, Dall-E 3\n\n## **"
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Use of Jargon",
          "description": "The document uses several terms that may be unfamiliar to a general audience, such as \"epistemic process,\" \"LLM-based Epistemic Process (LEP),\" and \"scaffolding.\" While these terms are defined, their repeated use without further explanation can hinder readability. Consider providing more context or using simpler language where possible.",
          "importance": "75",
          "grade": "60",
          "highlight": {
            "startOffset": 1922,
            "endOffset": 2346,
            "quotedText": "**LLM-based Epistemic Process (LEP)**: A system that relies on LLMs to carry out most or all of an epistemic process. This might start at ~10% LLM-labor, but can gradually ramp up. I imagine that such a process is likely to feature some kinds of estimates or forecasts.\n*   **Scaffolding:** Software used around an LLM system, often to make it valuable for specific use-cases. In the case of an LEP, a lot of scaffolding mig"
          },
          "isValid": true
        },
        {
          "title": "Use of Examples",
          "description": "The document uses examples to illustrate certain points, but more examples could be added to further clarify complex concepts. Concrete examples help readers connect abstract ideas to real-world scenarios, making the content more accessible and engaging.",
          "importance": "75",
          "grade": "70",
          "highlight": {
            "startOffset": 2365,
            "endOffset": 3692,
            "quotedText": "1. High-Level Benefits & Uses\n\n**Claim 1: If humans could forecast much better, these humans should make few foreseeable mistakes. This covers many mistakes, particularly ones we might be worried about now.**\n\n*   Someone deciding about talking to a chatbot that can be predicted to be net-negative (perhaps it would create an unhealthy relationship) could see this forecast and simply decide not to start the chat.\n*   Say that a person’s epistemic state could follow one of four trajectories, depending on some set of reading materials. For example, one set is conspiratorial, one is religious, etc. Good forecasting could help forecast this and inform the person ahead of time.\n    *   Note that this can be radical and perhaps dangerous. For example, a religious family knowing how to keep their children religious with a great deal of certainty.\n*   Say that one of two political candidates is predictably terrible. This could be made clear to voters who trust said prediction.\n*   If an AI actor is doing something likely to be monopolistic or dangerous, this would be made more obvious to itself and those around it.\n*   Note: There will also be unforeseeable mistakes, but any actions that we could do that are foreseeably-high-value for them, could be predicted. For example, general-purpose risk mitigation measures. "
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Logical Flow and Transitions",
          "description": "The document presents a series of claims and arguments, but the transitions between them could be smoother. Using more explicit transition words and phrases (e.g., \"therefore,\" \"however,\" \"in addition\") would help guide the reader through the author's reasoning and make the overall structure more apparent.",
          "importance": "70",
          "grade": "65",
          "highlight": {
            "startOffset": 3696,
            "endOffset": 4494,
            "quotedText": "Claim 2: Highly intelligent / epistemically capable organizations are likely to be better at coordination.** \n\n*   This might well mean fewer wars and conflict, along with corresponding military spending.\n*   If highly capable actors were in a prisoner's dilemma, the results could be ugly. But very often, there’s a lot of potential and value in not getting into one in the first place.\n*   Evidence: From [The Better Angels of Our Nature](https://en.wikipedia.org/wiki/The_Better_Angels_of_Our_Nature), there’s significant evidence that humanity has become significantly less violent over time. One potential exception is that large risks of conflict between large actors might have increased, but these last bits are some of the most difficult to coordinate, so this might not be too surprising."
          },
          "isValid": true
        },
        {
          "title": "Specificity of Claims",
          "description": "Some of the claims could be made more specific and measurable. For example, \"Better forecasting would lead to more experimentation and innovation ('weirdness')\" is a broad statement. Quantifying \"more experimentation\" and defining \"weirdness\" would make the claim more testable and easier to understand.",
          "importance": "65",
          "grade": "60",
          "highlight": {
            "startOffset": 4901,
            "endOffset": 5447,
            "quotedText": "Claim 4: Better forecasting would lead to more experimentation and innovation (“weirdness”).**\n\n*   Radical technology projects are well-known to be high-risk. High returns when they go well, high losses if they don’t. This means that there’s a high value of information in being more correct about outcomes early on.\n*   If people could become much better at forecasting, then it’s expected that they will become better at picking innovations to try. Bad projects would be avoided more often, and the expected value of innovation would increase."
          },
          "isValid": true
        },
        {
          "title": "Clarity of Failure Modes",
          "description": "The section on failure modes is important for a balanced assessment of LEPs. However, the explanations of each failure mode could be more detailed and provide concrete examples. This would help the reader better understand the potential risks and how they might be mitigated.",
          "importance": "80",
          "grade": "70",
          "highlight": {
            "startOffset": 8488,
            "endOffset": 10014,
            "quotedText": ". It’s not clear how to expect this area to change over time. It's possible that nonprofit / EA efforts here would have trouble finding a strong fit.\n2.  **Forecasting could be used for net-bad activities**\n    1.  **AI Acceleration**  \n        It’s possible that epistemic improvements would be used by AI capabilities actors to increase capabilities and not make other informed choices.  \n          \n        Claim: I think this is unlikely to be the net outcome. This would require these organizations to become smarter about making AIs, but not much smarter about making them safe, including for their own benefit. It would also require the other groups that could block them to become smarter - some of these groups include other non-LLM companies and other highly motivated actors who wouldn't want AI to go poorly.  \n          \n        One crux here is if the AI developers will both be much better at integrating AI epistemic improvements in their internal workflows, and if these systems won't convince or help them with AI alignment. This could be a really bad outcome, so it's important to continue to pay attention to.\n    2.  **Malicious Governments**  \n        Governments like China or North Korea could use better epistemic systems to better control their populations. My guess is that great epistemic systems will likely do some damage here. I think that overall the improvements (like improving western governments) will more than outweigh that, but this is something to pay attention to.  \n          \n       "
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Readability of Subcomponent Descriptions",
          "description": "The descriptions of the subcomponents of LEPs are generally clear, but some could benefit from more concise language and clearer examples. Using bullet points or numbered lists within each subcomponent description could also improve readability.",
          "importance": "70",
          "grade": "65",
          "highlight": {
            "startOffset": 22381,
            "endOffset": 35070,
            "quotedText": "roved with scaffolding by instead asking it to write a git diff and then applying that using traditional code. Or it could be solved on the inference layer directly, [as discussed here](https://www.cursor.com/blog/instant-apply). \n\nIf it is the case that most future improvements are done directly with the LLMs, then that could significantly change what work will be valuable. For example, if there are only a few leading LLM developers, then it might ultimately be up to them to actually add any functionality.  \n\n## 5\\. Subcomponents\n\nAs stated above, it’s hard to determine the best subdivisions of LEPs / LLM-Epistemic Scaffolding.\n\nHere’s one attempt, to provide a simple picture. \n\n**1\\. Data-collection**\n\n*   Many LLM services are already coming out that can help quickly put data, including private data, together, for different needs. See Apple Intelligence as one service that would work for people’s personal data.\n*   There are hundreds of startups in the data-collecting and data-management space. This problem can easily be split up more.\n*   Example performance question:  \n    _“Provided with resources R, gather as much data as you can, from this list of desired data, about X.”_  \n    The results of this will likely be obvious (it’s likely to be clear what data is accessible), so scientific testing might not be needed.   \n     \n\n**2\\. World Model**\n\n*   “World model” here likely means, “A set of stored symbolic and probabilistic assumptions about the world / people / organizations.”\n*   World models will likely include software programs and writing, instead of only relying on existing LLMs.\n    *   I expect that there could be workflows to LEPs to contribute to world models that will later give it better forecasting / reasoning abilities. The forecasting components could build the world models.\n*   Existing online data has very minimal attempts at providing accessible world models. These would ideally be probabilistic, immutable, shareable, collaborative, etc.\n*   Tools like [Squiggle Hub](https://squigglehub.org) are meant to be used here. Formal ontologies and knowledge graphs are likely to be useful.\n*   Some world models could be directly interesting to humans, for the presentation layer. Others might primarily be used by LLMs. For these, the world model is a form of a cache / collaboration.\n*   It seems possible/likely that world models won’t be unique to LEPs. Instead, many separate LEPs can use the same world models.\n*   World models could contain (or complete be) text descriptions of outcomes. For example, 10 compressed pages explaining how the next 10 years will go for a specific region.\n    *   It’s not clear yet what the best representations are for LLMs to store and fetch data about its understanding of specific things.\n*   Example performance question:  \n    “_Given a forecasting LLM L, and forecasting question Q, contribute a world model that will result in L being highly accurate_.”  \n     \n\n**3\\. Human Elicitation**\n\n*   It’s often useful to just go out and directly ask questions to people, or give them surveys. Strong LEPs should ideally be able to design these procedures to maximize information value, then execute them autonomously.\n    *   A system might realize that it would be useful to get a very specific expert to give their take on something. It could then email them, and perhaps offer a financial reward.\n    *   A different system might generate a bunch of specific binary questions for a group of people it wants to elicit information from. Maybe it’s trying to estimate aspects of their preferences, and information would help.\n*   Example performance question:  \n    _“Given that you want to get as much useful new data about topic X, using existing research Y, use resources R to send off elicitions and gather data from humans.”_\n\n**4\\. Data Prioritization**\n\n*   The task here is, “_Given the potential to fetch from data source D, and resources R, fetch the most useful data for providing marginal insight on unknown variables V._”\n*   Summarizing and displaying such work could also be useful. But doing so for humans would be very different to doing so for other LLMs.\n*   It seems like with LLMs that are capable of providing quick and accurate forecasts, we could use reinforcement learning and similar algorithms to do a good job here. For any set of data, we could quickly measure the improvement in accuracy using that data.\n\n**5\\. Numeric Modeling / Fermi Calculations**\n\n*   Make great complex Python/Excel/Squiggle models/estimates.\n*   One reasonable approach to fermi estimation is “Try a bunch of very different approaches, then mix the answers together.”\n*   We’ve seen a lot of evidence that LLMs can generate working Python/Squiggle code. However, I don’t believe that there has been much optimization in this area yet.\n*   It seems easy to iterate on, and generate, great data of strong examples of this. That could be then used for LLM training / fine-tuning.\n*   Example performance question:  \n    _“Estimate variable V as well as you can, using up to N LLM calls, and no other calls.”_  \n     \n\n**6\\. Decomposition / Amplification**\n\nThere are many potential strategies to \"break a problem down\" and solve different parts with different threads of LLMs. \n\nFor example:\n\n*   Chain-of-Thought prompting\n*   Mixture of experts\n*   Ensemble Averaging\n*   Debate (i.e. [Safety via Debate](https://arxiv.org/abs/1805.00899))\n*   Fact verification\n\nArguably, numeric modeling is a form of decomposition/amplification, I just separated it to draw attention to its uniqueness.\n\nThis is an active and popular area of ML research. Some of these techniques are incorporated in the LLM inference layer, but some are also used in scaffolding.\n\nI'm sure there's a lot more work to do here. It's less clear where smaller nonprofit research can be most effective, but I suspect that there are some good options. \n\nMany methods might be unique to forecasting or specific epistemic workflows. For example:\n\n*   Try to identify key cruxes or crucial considerations, and then investigate them\n*   Automated debate mapping\n*   Identify [Scissor statements](https://slatestarcodex.com/2018/10/30/sort-by-controversial/), in part to help personalize research\n*   Use [discernment](https://www.lesswrong.com/posts/W2iwHXF9iBg4kmyq6/the-practice-and-virtue-of-discernment) to clarify and decompose key terminology\n*   Breaking down forecasting questions into sub-questions, perhaps with bayesian networks. \n\nArguably, the rationality and effective altruist communities might be very well suited to work in this area. There's been a lot of writing on how to systematically break hard problems down, now people would need to experiment with encoding these systems into automated workflows.   \n  \n**7\\. Question Generation** \n\n*   Come up with questions to be researched or forecasted on.\n*   This might well be done by other listed components - for example, it could simply combine \"Idea Generation\" with \"Forecasting\" to come up with potential questions, and guess which will be the most important and tractable. It also might be better done under the \"Presentation\" question. However, I suspect that there will be unique research and development for this problem, especially because it's a major part of existing forecasting systems.\n*   Example performance question:  \n    _“Write 5 forecasting questions on topic X. After this, a separate epistemic system will be run with a fixed number of resources R on each question. Choose questions such that the final results are maximally useful to end-users.\"_  \n     \n\n  \n**8\\. Forecasting**\n\n*   Convert fetched data into a forecast.\n*   Can call the “Numeric Modeling” module, or be merged with it.\n*   There’s been previous work here that is promising.\n*   Fine-tuning and prompt engineering seems like it would go a long way.\n*   Current LLMs are known to be biased and often suffer from poor calibration, in part in order to become better liked by human users. We might well want specialized systems optimized for pure accuracy.\n    *   This delta might cause problems. Perhaps it could lead to bad news stories (where a model used for forecasting gives a politically-incorrect answer), or perhaps it could be used by bad actors.\n    *   This area should be thus treated with care. Perhaps some models are only available to certain researchers, or perhaps there should be large disclaimers provided for regular people attempting to use them directly.\n*   Forecasting can output high-level stories, probabilities, scorable functions, or other. There are many potential formats here.\n*   Example performance question:  \n    _“Using information I, and N LLM calls, make your best forecast on variable V. You may use the Numeric Modeling module when doing so, but this will eat into the LLM calls.”_  \n     \n\n**9\\. Forecasting Resolution**\n\n*   Most forecasting questions should be resolvable automatically. This can be done by either querying external data, or perhaps by asking a person/LLM for their opinion.\n*   This component is likely to often just either be, or defer to, other LEP modules.\n*   Question Creation and Optimization\n    *   A strong LEP would make good judgment calls on what research or forecasting topics would be valuable to people, and initialize those.\n    *   This could range from “once a user logs in, generates custom information for them to maximize their EV”, to, “write a list of standard forecasting questions that are collectively high-EV to a subpopulation.”\n    *   Example performance question:  \n        _“Using an existing LEP L, and resources R, write a set of forecasting questions. Expect that forecasting resources F will be used on these questions, and you are trying to optimize utility function U.”_  \n         \n\n**10\\. Presentation**\n\n*   Many custom charts and graphics would likely be useful.\n*   Decisions need to be made about what final data should be shown to users and how.\n*   Automated decision-making: In some cases / eventually, LEPs would interact directly with decision systems. In theory, people don’t need to know almost anything - critical information could be used directly by in-world automations. For example, instead of an LEP doing research into the costs and benefits of vitamin D supplementation and showing you that research, a system instead just starts an order for you to get a regular set of supplements. This could come with optional research, but in many cases the research won’t be reviewed, if the system is trusted.\n*   In cases where LEPs are used behind APIs, the presentation layer could include answering questions about what the LEP could do, and suggesting specific uses for it.\n*   Presentation/wording should be optimized to be understandable to humans. There should be tests of the error rates of what was intended to be stated, and what humans understood. For example, many people have significant misunderstandings about basic statistics concepts, so probabilities might need to be handled with care.\n*   Example performance questions:\n    *   _“Using LEP output O, design a dashboard that user U will most prefer.”_\n    *   _“Using LEP output O, design a set of complicated diagrams without detectable errors.” (judged by an LLM)._   \n         \n\n**11\\. Distribution**\n\n*   Full custom user interfaces could be created. This might range from a simple custom LLM chat, to a auto-generated set of dashboards, with the ability to delve deeper into any topic the user clicks on.\n*   LEPs could be accessed by any other chat LLM interface, as long as that interface can call external APIs. This could be generally useful for integrating into other AI tools and workflows, perhaps via other common LLM chat UIs.\n*   LEP information could be distributed like how human communication is communicated. Text messages, emails, Tweets, voice assistants, etc. Ideally the LEP could vaguely predict what information is useful to people and when, and then send them that information in those cases.\n    *   Example: You get an email in the morning. It says, _“Some news came out at 6am today that changed my recommendation of your financial assets. I recommend you change to a different mutual fund. I estimate that doing so will net you an average of $50/month.”_\n*   Example performance questions:\n    *   _“Using LEP output O, design and implement the best way to get this information to agent A.” We can use an LLM that mimics human behaviors to evaluate._  \n         \n\n**12\\. Idea Generation / Optimization**\n\n*   It will often be useful to generate ideas on the best ways to do something.\n*   For example, an LLM agent might want to make a suggestion to a user, before d"
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Clarity of Benchmarks and Baselines",
          "description": "The discussion of benchmarks and baselines is crucial for evaluating the effectiveness of LEPs. However, the explanations could be made more precise and provide specific examples of how these benchmarks could be used in practice. This would help readers understand the practical implications of the proposed evaluation methods.",
          "importance": "80",
          "grade": "70",
          "highlight": {
            "startOffset": 35070,
            "endOffset": 40828,
            "quotedText": "oing forecasting on said suggestion.\n*   Regular LLM performance might be fine on this, but fine-tuning or similar might help. It might be the case that there are certain specific domains where idea generation is very useful.\n*   “Idea Generation” is related to [program optimization](https://en.wikipedia.org/wiki/Program_optimization) and other CS topics.\n*   Example performance questions:\n    *   _“Given a person with details D, generate a plan for that person to optimize their financial future.”_ (This will be then judged with an LEP / forecasting module)\n    *   _“Given a 2d grid game G, come up with 10 ordered ideas for the best strategy”_ (Can be judged empirically)  \n         \n\n**13\\. Task Orchestration**\n\n*   We need systems that can actually trigger all the other components when needed. This process needs to be debuggable / introspectable. Ideally, these workflows would be highly understandable.   \n     \n\n## 6\\.  Benchmarks & Baselines\n\n**Claim 1: Accessing forecasting quality and efficiency with humans is very difficult and often poorly done.**\n\n*   Scores like the Brier Score don’t take into account the difficulty of the question.\n*   It’s really hard to standardize question sets, audiences, and environments. Each time a human forecasting competition is done, it’s done with a separate population. Also, we typically don’t track the resources going into these systems - like the total time that people spend forecasting. Again, humans are mediocre for studying, in comparison to LLMs.\n*   We arguably don't yet have good statistical measures for [credal resilience](https://forum.effectivealtruism.org/topics/credal-resilience).\n\n**Claim 2: Portable LEPs could be useful for acting as priors or baselines, for other systems to perform against.**\n\n*   We will want some baseline LEPs to score other LEPs against. These should act on the same data, at the same times.\n    *   For example, one could take a new LEP, a well-known Baseline LEP, and a known or generated question set. Both systems would forecast those questions, and then the new LEP will be evaluated based on how much it outperforms the Baseline LEP. For example, with the average difference in brier score.\n    *   In order to do this, the LEP must be equipped with the abilities to fetch data and to forecast. This is useful in order to compare the system to human forecasters (which do both), but it might be often better to divide the subcomponents and study/optimize them in isolation.\n    *   Another use is to use Baseline LEP to initialize forecasting markets. Right now manifold questions begin at 50%. It could be better for them to instead begin at a number suggested by a simple LEP.\n    *   Even if these LEPs aren't highly accurate, they could be useful for sanity checking claims outside of forecasting systems. For example, all publicly given estimates and statements could be run against LEPs. This can then highlight estimates that are particularly surprising. In financial / numeric models, all inputs and assumptions can be checked against LEPs to find ones that might be suspicious.\n\n**Claim 3: Standardized LEPs could be great judges to resolve subjective questions**\n\n*   It’s well-known that it’s difficult to use partially-subjective questions in prediction markets. Much of the reason is because the only way to resolve these is with human expert judges, and these human judges have eccentricities that are hard to predict. AI systems might help here, because they can be far more standardized and testable than human systems.\n*   For example, say there’s a question, “_Will project X generally be considered a failure in 2026?_” This can be changed to, “_Will project X be considered a failure, in an average of 20 runs, by a certain LEP, in 2026?_” Then, predictors could provide this LEP fake hypothetical information on the future and see how this question would resolve with that information. There should be very little surprises from the eventual output of the LEP.\n*   Related, LEPs could be used to provide independent resolutions for every other forecasting question on Metaculus/Manifold/etc. There would be a certain expected error rate - if some resolution judge statistically exceeds this rate, that would be flagged for review.\n\n**Idea: It’s possible we could get very useful small LEPs that are easily packaged, relatively cheaply.**\n\n*   Certain LEP components are much more useful to include for different kinds of testing than others. Some simple web search functionality and forecasting abilities could get us far for testing forecasting. Right now there are a bunch of startups and tools emerging to help with certain kinds of scaffolding that might be useful here. For example, you can currently bundle special data and prompts with a custom ChatGPT GPT. Right now this cannot be accessed via the API, but this is a potential thing that could exist in the future.\n*   One viable solution could be for someone to set up a very simple web service that can make internet queries and forecasts. Ideally, this would provide multiple immutable options, and this would be trusted to stay up, for future experimentation. It might only need to be accessible via an API.\n*   It might also be possible to have a simple LEP in a single Docker container or similar.\n*   Even very simple LEPs could act as much better baselines than what we use today.\n\n**Challenge: LLM-forecasting abilities will vary based on date.**\n\n*   The quality of LLM predictions is likely to go down, the further away the forecasting question resolution is from the LLM training date. This is a major challenge to using them as a benchmark. For example, GPT4 might be decent at guessing the US GDP in 2025 if used in 2024 with search capabilities (i.e. we"
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        }
      ],
      "grade": 70
    },
    {
      "agentId": "research-scholar",
      "costInCents": 0,
      "createdAt": "2025-04-17T04:08:54.351Z",
      "runDetails": "{\"model\":\"google/gemini-2.0-flash-001\",\"promptTokens\":12093,\"completionTokens\":1264,\"totalTokens\":13357,\"temperature\":0.1,\"runtimeMs\":7629}",
      "thinking": "This document provides a comprehensive overview of the potential and challenges of using LLMs in epistemic processes and forecasting. It covers a wide range of topics, from high-level benefits and failure modes to viability, scaffolding, subcomponents, benchmarks, and caching strategies. As a research scholar, I find the sections on subcomponents, benchmarks, and the distinction between scaffolding and direct LLM improvements particularly relevant. The author's emphasis on the need for standardized benchmarks and portable LEPs aligns with the scientific rigor required for advancing this field. The discussion on the trade-offs between caching and on-demand forecasting is also insightful, highlighting the practical considerations for deploying these systems. I will focus on identifying resources that can provide further insights into these areas, particularly those related to benchmarking methodologies, scaffolding frameworks, and the integration of LLMs in epistemic workflows.",
      "summary": "This document explores the potential of LLM-based epistemic systems, highlighting their benefits, failure modes, viability, and the importance of scaffolding. It emphasizes the need for standardized benchmarks and the trade-offs between scaffolding and direct LLM improvements. The author also outlines key subcomponents of LEPs and discusses the caching vs. on-demand forecasting dilemma.",
      "comments": [
        {
          "title": "Resource Recommendation: Integration of LLMs in Epistemic Workflows",
          "description": "The document discusses the gradual integration of LLMs in epistemic workflows. Investigating existing research on human-AI collaboration in knowledge work, such as studies on the impact of AI on decision-making and problem-solving, could provide valuable insights for designing effective hybrid systems that leverage the strengths of both humans and LLMs.",
          "importance": 80,
          "grade": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 16506,
            "endOffset": 16926,
            "quotedText": "Claim 4: LLM epistemic integration will be gradual.**  \nPractically speaking LLMs and Humans will work together side-by-side in epistemic processes for some time. I expect LLMs-epistemics to follow a similar trajectory to self-driving. First, it starts with light augmentation. Then it moves to “fully automated in most use cases, but occasionally needs oversight.” As with autonomous driving, Level 5 is very difficult."
          }
        },
        {
          "title": "Resource Recommendation: Scaffolding Frameworks",
          "description": "The document emphasizes the importance of scaffolding in building effective LLM-based epistemic systems. Researching existing scaffolding frameworks and tools, such as Langchain and LlamaIndex, could provide valuable insights for developing robust and scalable LEPs. Additionally, exploring the trade-offs between different scaffolding architectures and their impact on system performance would be beneficial.",
          "importance": 85,
          "grade": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 17870,
            "endOffset": 18071,
            "quotedText": "Claim 1: LLM-based epistemic systems will likely require a lot of scaffolding, especially as they get ambitious**.  \nI.E. software, databases, custom web applications, custom batch job pipelines, etc. "
          }
        },
        {
          "title": "Resource Recommendation: Subcomponents of LEPs",
          "description": "The document provides a detailed breakdown of the subcomponents of LEPs. Further research into each of these subcomponents, such as data collection, world modeling, human elicitation, and forecasting resolution, could lead to targeted improvements in the overall performance of LEPs. Additionally, exploring the interdependencies between these subcomponents and their impact on system-level behavior would be beneficial.",
          "importance": 70,
          "grade": 65,
          "isValid": true,
          "highlight": {
            "startOffset": 22381,
            "endOffset": 22553,
            "quotedText": "roved with scaffolding by instead asking it to write a git diff and then applying that using traditional code. Or it could be solved on the inference layer directly, [as di"
          }
        },
        {
          "title": "Resource Recommendation: Benchmarking Methodologies",
          "description": "This section highlights the need for robust benchmarking methodologies to evaluate the performance of LLM-based epistemic systems. Further research into existing benchmarking frameworks for AI systems, such as those used in the GLUE and SuperGLUE benchmarks, could provide valuable insights for developing standardized evaluation metrics for LEPs.",
          "importance": 90,
          "grade": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 36031,
            "endOffset": 36584,
            "quotedText": "Claim 1: Accessing forecasting quality and efficiency with humans is very difficult and often poorly done.**\n\n*   Scores like the Brier Score don’t take into account the difficulty of the question.\n*   It’s really hard to standardize question sets, audiences, and environments. Each time a human forecasting competition is done, it’s done with a separate population. Also, we typically don’t track the resources going into these systems - like the total time that people spend forecasting. Again, humans are mediocre for studying, in comparison to LLMs."
          }
        },
        {
          "title": "Resource Recommendation: Caching Strategies",
          "description": "The section on caching vs. on-demand forecasting highlights the importance of efficient caching strategies for scaling LLM-based epistemic systems. Researching existing caching techniques and their application in large-scale AI systems could provide valuable insights for optimizing the performance of LEPs. Additionally, exploring the trade-offs between different caching policies and their impact on system accuracy and latency would be beneficial.",
          "importance": 75,
          "grade": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 41866,
            "endOffset": 42540,
            "quotedText": "Key question: How much would we expect AI forecasting systems to generate forecasts and visualizations on-demand for users, vs. save them before?**\n\n*   On-demand work is more user-customizable and user-specific, but it’s harder.\n*   In the beginning, on-demand AI work will be used for the “last-mile” problem. A user asks a question - we first find the most similar forecasts, then use an LLM to use those to generate a personalized forecast. We’d also want great charts ready-to-go.\n*   This tracks a question about web content more generally. Eventually websites might be generated on-demand, to be personalized for users. But it will likely take some time to get there."
          }
        }
      ],
      "grade": 80
    },
    {
      "agentId": "fake-eliezer",
      "costInCents": 0,
      "createdAt": "2025-04-17T04:09:03.756Z",
      "runDetails": "{\"model\":\"google/gemini-2.0-flash-001\",\"promptTokens\":11463,\"completionTokens\":1498,\"totalTokens\":12961,\"temperature\":0.1,\"runtimeMs\":9392}",
      "thinking": "This document presents a preliminary exploration of LLM-based epistemic processes (LEPs) and their potential impact, particularly within the effective altruism community. The author, ozziegooen, outlines potential benefits, failure modes, viability considerations, scaffolding requirements, subcomponents, benchmarks, and caching strategies related to LEPs. The document aims to spark discussion and encourage further research in this area.\n\nAs Eliezer Yudkowsky, my primary concern is the potential for existential risk, particularly concerning AI. Therefore, I will assess this document through the lens of AI safety and alignment, focusing on whether the proposed epistemic improvements genuinely contribute to solving the hard problem of aligning advanced AI systems with human values. I will be looking for potential failure modes that could exacerbate AI risk, as well as any discussion of how these systems might be used to improve AI alignment research itself. I will also be critical of any assumptions that seem overly optimistic or naive about the difficulty of AI alignment or the potential for misuse of these technologies.\n\nI will also be looking for any discussion of the potential for these systems to be used to improve coordination and decision-making in ways that could reduce existential risks from other sources, such as pandemics or nuclear war. However, my primary focus will remain on AI risk, as I believe this is the most pressing threat facing humanity.",
      "summary": "This document presents a promising but also potentially dangerous area of research. While LLM-based epistemic processes could improve forecasting and decision-making, they also carry significant risks, particularly concerning AI alignment. The author's optimism about self-enforcing loops and government regulation is misplaced. The document needs a stronger focus on adversarial robustness and the potential for misuse by malicious actors. The emphasis on benchmarks and baselines is positive, but the challenges of using LLMs as reliable benchmarks need to be addressed more thoroughly. Overall, this is a valuable starting point, but it requires a more critical and risk-aware perspective, especially concerning AI safety.",
      "comments": [
        {
          "title": "Coordination and Existential Risk",
          "description": "While the author touches on coordination, the link to existential risk reduction isn't explicitly made. Better coordination *could* reduce risks from pandemics or nuclear war, but this needs to be a more central focus.",
          "importance": "75",
          "grade": "65",
          "highlight": {
            "startOffset": 3705,
            "endOffset": 4494,
            "quotedText": "Highly intelligent / epistemically capable organizations are likely to be better at coordination.** \n\n*   This might well mean fewer wars and conflict, along with corresponding military spending.\n*   If highly capable actors were in a prisoner's dilemma, the results could be ugly. But very often, there’s a lot of potential and value in not getting into one in the first place.\n*   Evidence: From [The Better Angels of Our Nature](https://en.wikipedia.org/wiki/The_Better_Angels_of_Our_Nature), there’s significant evidence that humanity has become significantly less violent over time. One potential exception is that large risks of conflict between large actors might have increased, but these last bits are some of the most difficult to coordinate, so this might not be too surprising."
          },
          "isValid": true
        },
        {
          "title": "Self-Enforcing Loop - A Dangerous Assumption",
          "description": "The idea of a self-enforcing loop to cap harm is dangerously optimistic. It assumes that agents developing epistemics will accurately assess the net value of further improvements, which is a highly uncertain and potentially biased judgment. This is a classic example of Goodhart's Law waiting to happen.",
          "importance": "80",
          "grade": "40",
          "highlight": {
            "startOffset": 4625,
            "endOffset": 4897,
            "quotedText": "Agents developing epistemics should use the tools they build to decide on the net value of further improvements. This is a very important variable to get right.\n*   If it ever seems like future work will be negative in expected value, progress could and should be stopped."
          },
          "isValid": true
        },
        {
          "title": "Crucial Point on AI Alignment",
          "description": "This highlights a critical consideration: whether AI epistemic improvements will genuinely aid in AI alignment or merely accelerate capabilities without addressing safety concerns. This is a key crux that needs careful examination.",
          "importance": "95",
          "grade": "85",
          "highlight": {
            "startOffset": 9330,
            "endOffset": 9617,
            "quotedText": "One crux here is if the AI developers will both be much better at integrating AI epistemic improvements in their internal workflows, and if these systems won't convince or help them with AI alignment. This could be a really bad outcome, so it's important to continue to pay attention to."
          },
          "isValid": true
        },
        {
          "title": "Overly Optimistic View of Government",
          "description": "Relying on governments (even with better epistemic infrastructure) to counter malicious corporations is naive. Governments are often captured by corporate interests, and their epistemic improvements could be used to further those interests, not protect consumers.",
          "importance": "65",
          "grade": "35",
          "highlight": {
            "startOffset": 10601,
            "endOffset": 10818,
            "quotedText": "It seems important to use governments (hopefully with better epistemic infrastructure) to take active steps against this. It seems important here to work to make sure that epistemic advances get distributed equitably."
          },
          "isValid": true
        },
        {
          "title": "Calibration and Bias - Critical for AI Safety",
          "description": "The point about LLMs being biased and poorly calibrated is crucial. If these systems are used for AI alignment research, their biases could lead to flawed conclusions and dangerous strategies. This needs to be addressed with extreme care.",
          "importance": "90",
          "grade": "80",
          "highlight": {
            "startOffset": 30250,
            "endOffset": 30871,
            "quotedText": "Current LLMs are known to be biased and often suffer from poor calibration, in part in order to become better liked by human users. We might well want specialized systems optimized for pure accuracy.\n    *   This delta might cause problems. Perhaps it could lead to bad news stories (where a model used for forecasting gives a politically-incorrect answer), or perhaps it could be used by bad actors.\n    *   This area should be thus treated with care. Perhaps some models are only available to certain researchers, or perhaps there should be large disclaimers provided for regular people attempting to use them directly."
          },
          "isValid": true
        },
        {
          "title": "Missing: Discussion of Adversarial Robustness",
          "description": "A glaring omission is the lack of discussion about adversarial robustness. If these systems are used in high-stakes decision-making, they will be targets for manipulation. How can we ensure that they are resistant to adversarial attacks and remain reliable even when subjected to malicious inputs?",
          "importance": "80",
          "grade": "20",
          "highlight": {
            "startOffset": 33683,
            "endOffset": 33883,
            "quotedText": "Full custom user interfaces could be created. This might range from a simple custom LLM chat, to a auto-generated set of dashboards, with the ability to delve deeper into any topic the user clicks on."
          },
          "isValid": true
        },
        {
          "title": "Benchmarks and Baselines - Essential for Progress",
          "description": "The emphasis on benchmarks and baselines is important. Without rigorous evaluation, it's impossible to know if these systems are actually improving epistemic capabilities or just generating convincing-sounding nonsense. However, the challenges of using LLMs as benchmarks due to their time-dependent performance need to be carefully considered.",
          "importance": "85",
          "grade": "75",
          "highlight": {
            "startOffset": 36741,
            "endOffset": 38166,
            "quotedText": "Portable LEPs could be useful for acting as priors or baselines, for other systems to perform against.**\n\n*   We will want some baseline LEPs to score other LEPs against. These should act on the same data, at the same times.\n    *   For example, one could take a new LEP, a well-known Baseline LEP, and a known or generated question set. Both systems would forecast those questions, and then the new LEP will be evaluated based on how much it outperforms the Baseline LEP. For example, with the average difference in brier score.\n    *   In order to do this, the LEP must be equipped with the abilities to fetch data and to forecast. This is useful in order to compare the system to human forecasters (which do both), but it might be often better to divide the subcomponents and study/optimize them in isolation.\n    *   Another use is to use Baseline LEP to initialize forecasting markets. Right now manifold questions begin at 50%. It could be better for them to instead begin at a number suggested by a simple LEP.\n    *   Even if these LEPs aren't highly accurate, they could be useful for sanity checking claims outside of forecasting systems. For example, all publicly given estimates and statements could be run against LEPs. This can then highlight estimates that are particularly surprising. In financial / numeric models, all inputs and assumptions can be checked against LEPs to find ones that might be suspicious."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        }
      ],
      "grade": 65
    },
    {
      "agentId": "quantitative-forecaster",
      "costInCents": 0,
      "createdAt": "2025-04-17T04:09:18.504Z",
      "runDetails": "{\"model\":\"google/gemini-2.0-flash-001\",\"promptTokens\":11263,\"completionTokens\":1985,\"totalTokens\":13248,\"temperature\":0.1,\"runtimeMs\":11638}",
      "thinking": "This document is a rich source of potential forecasting questions and Fermi estimates related to the development and impact of LLM-based epistemic systems. The author presents numerous claims and cruxes that can be transformed into concrete, measurable predictions. My analysis will focus on identifying these opportunities and suggesting how they can be framed for forecasting platforms like Metaculus or Manifold Markets. I will also provide initial probability estimates and reasoning, as well as suggest potential Fermi estimates for relevant quantities.\n\nThe document covers a wide range of topics, including the potential benefits and risks of LLM-based epistemic systems, their viability, the importance of scaffolding, and the various subcomponents that make up such systems. This breadth provides ample opportunities for generating diverse and impactful forecasts. I will prioritize claims and cruxes that are both important and tractable, focusing on those that can be resolved within a reasonable timeframe and have clear resolution criteria.\n\nI will also pay attention to the author's discussion of benchmarks and baselines, as this is crucial for evaluating the performance of LLM-based epistemic systems and tracking their progress over time. I will suggest specific metrics and approaches for establishing meaningful benchmarks and baselines.\n\nFinally, I will consider the author's call for further research and identify areas where quantitative forecasting can play a key role in guiding future efforts. This includes prioritizing philanthropic work, estimating cost-effectiveness, and generating concrete project ideas for enthusiasts.",
      "summary": "This document provides a wealth of opportunities for quantitative forecasting related to the development and impact of LLM-based epistemic systems. I've identified several key claims and cruxes that can be transformed into concrete, measurable forecasting questions suitable for platforms like Metaculus or Manifold Markets. These questions cover a range of topics, including the impact of better forecasting on mistake reduction, global coordination, innovation, and the integration of AI forecasting into software workflows. I've also highlighted the importance of establishing benchmarks and baselines for evaluating the performance of LLM-based epistemic systems and suggested specific metrics and approaches for doing so. Finally, I've identified areas where Fermi estimates can be used to quantify relevant quantities, such as the number of LLM calls needed for Fermi calculations and the decline in accuracy of LLM predictions over time.",
      "comments": [
        {
          "title": "Forecasting Question: Impact of Better Forecasting on Mistake Reduction",
          "description": "This claim can be translated into a forecasting question: \"By 2030, by what percentage will the rate of foreseeable mistakes (as defined by X) be reduced due to the widespread adoption of better forecasting tools?\" This allows for a measurable assessment of the impact of improved forecasting on decision-making.",
          "importance": "85",
          "highlight": {
            "startOffset": 2398,
            "endOffset": 3692,
            "quotedText": "Claim 1: If humans could forecast much better, these humans should make few foreseeable mistakes. This covers many mistakes, particularly ones we might be worried about now.**\n\n*   Someone deciding about talking to a chatbot that can be predicted to be net-negative (perhaps it would create an unhealthy relationship) could see this forecast and simply decide not to start the chat.\n*   Say that a person’s epistemic state could follow one of four trajectories, depending on some set of reading materials. For example, one set is conspiratorial, one is religious, etc. Good forecasting could help forecast this and inform the person ahead of time.\n    *   Note that this can be radical and perhaps dangerous. For example, a religious family knowing how to keep their children religious with a great deal of certainty.\n*   Say that one of two political candidates is predictably terrible. This could be made clear to voters who trust said prediction.\n*   If an AI actor is doing something likely to be monopolistic or dangerous, this would be made more obvious to itself and those around it.\n*   Note: There will also be unforeseeable mistakes, but any actions that we could do that are foreseeably-high-value for them, could be predicted. For example, general-purpose risk mitigation measures. "
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Forecasting Question: Impact of Epistemic Improvements on Global Coordination",
          "description": "This claim can be framed as: \"By 2040, what will be the percentage decrease in global military spending (as a proxy for conflict) compared to a baseline scenario without significant epistemic improvements?\" This allows for a quantitative assessment of the impact of improved coordination on global stability.",
          "importance": "80",
          "highlight": {
            "startOffset": 3658,
            "endOffset": 4451,
            "quotedText": "purpose risk mitigation measures. \n\n**Claim 2: Highly intelligent / epistemically capable organizations are likely to be better at coordination.** \n\n*   This might well mean fewer wars and conflict, along with corresponding military spending.\n*   If highly capable actors were in a prisoner's dilemma, the results could be ugly. But very often, there’s a lot of potential and value in not getting into one in the first place.\n*   Evidence: From [The Better Angels of Our Nature](https://en.wikipedia.org/wiki/The_Better_Angels_of_Our_Nature), there’s significant evidence that humanity has become significantly less violent over time. One potential exception is that large risks of conflict between large actors might have increased, but these last bits are some of the most difficult to coord"
          },
          "isValid": true
        },
        {
          "title": "Forecasting Question: Self-Enforcing Loop for Epistemic Improvements",
          "description": "This claim can be translated into a forecasting question: \"Will a self-enforcing loop (as defined by X) be successfully implemented by 2035, such that further development of epistemic improvements is halted if it is deemed net-negative in expected value?\"",
          "importance": "75",
          "highlight": {
            "startOffset": 4499,
            "endOffset": 4897,
            "quotedText": "Claim 3: The harm caused by altruistic development of epistemic improvements could be capped by a self-enforcing loop.**\n\n*   Agents developing epistemics should use the tools they build to decide on the net value of further improvements. This is a very important variable to get right.\n*   If it ever seems like future work will be negative in expected value, progress could and should be stopped."
          },
          "isValid": true
        },
        {
          "title": "Forecasting Question: Impact of Forecasting on Innovation",
          "description": "This claim can be framed as: \"By 2030, what will be the percentage increase in successful radical technology projects (as defined by X) due to improved forecasting abilities?\"",
          "importance": "70",
          "highlight": {
            "startOffset": 4901,
            "endOffset": 5447,
            "quotedText": "Claim 4: Better forecasting would lead to more experimentation and innovation (“weirdness”).**\n\n*   Radical technology projects are well-known to be high-risk. High returns when they go well, high losses if they don’t. This means that there’s a high value of information in being more correct about outcomes early on.\n*   If people could become much better at forecasting, then it’s expected that they will become better at picking innovations to try. Bad projects would be avoided more often, and the expected value of innovation would increase."
          },
          "isValid": true
        },
        {
          "title": "Forecasting Question: Integration of AI Forecasting into Software Workflows",
          "description": "This claim can be translated into a forecasting question: \"By 2028, what percentage of complex software workflows will have AI forecasting deeply integrated into them?\"",
          "importance": "80",
          "highlight": {
            "startOffset": 5451,
            "endOffset": 6751,
            "quotedText": "Claim 5: AI forecasting/epistemics will be much more deeply integrated into the world than current prediction markets.**\n\n*   First, note that current prediction markets / tournaments are highly niche and overlooked. So AI systems would have to be more accepted, to be very valuable.\n*   Unlike current “human forecasters”, AI forecasting can be embedded deeply into other software. There would likely be better AI forecasting for repeated decisions, like devops procedures, edge cases in insurance, AI agents, etc.\n*   If “AI forecasters” were really solid and cheap, we’d expect them to improve many complex software workflows. Existing “human forecasters” are hard to directly make much money from, but these AI systems would be much easier to do so.\n*   They would likely be integrated in many other systems, such as:\n    *   Overseeing product sales chances\n    *   Estimating changes in accounting, from one quarter to the next\n    *   Estimating project lengths and budgets\n    *   Deciding the best LLM to use for a certain AI workflow, or the best workflow to follow.\n*   Note that many developments in \"_AI epistemics_\" are just regular enhancements to LLMs. There's no clear line between \"_AI forecasting tech_\" and \"_improvements in LLMs that make them better general-purpose reasoners._\""
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Forecasting Question: Dominance of LLM-Based Forecasters on Open Platforms",
          "description": "This subclaim can be framed as: \"By 2026, what percentage of Manifold Karma will be won by bots vs. humans vs. hybrids?\"",
          "importance": "85",
          "highlight": {
            "startOffset": 17269,
            "endOffset": 17267,
            "quotedText": "\n\n"
          },
          "isValid": true
        },
        {
          "title": "Fermi Estimate: Number of LLM Calls for Fermi Calculation",
          "description": "This section suggests a Fermi estimate. A possible question is: \"How many LLM calls are needed to estimate variable V as well as possible?\" A Fermi estimate could break this down by considering the number of calls needed for data gathering, model building, and sensitivity analysis.",
          "importance": "75",
          "highlight": {
            "startOffset": 26181,
            "endOffset": 26844,
            "quotedText": "er data from humans.”_\n\n**4\\. Data Prioritization**\n\n*   The task here is, “_Given the potential to fetch from data source D, and resources R, fetch the most useful data for providing marginal insight on unknown variables V._”\n*   Summarizing and displaying such work could also be useful. But doing so for humans would be very different to doing so for other LLMs.\n*   It seems like with LLMs that are capable of providing quick and accurate forecasts, we could use reinforcement learning and similar algorithms to do a good job here. For any set of data, we could quickly measure the improvement in accuracy using that data.\n\n**5\\. Numeric Modeling / Fermi Calc"
          },
          "isValid": true
        },
        {
          "title": "Forecasting Question: Adoption of Portable LEPs as Baselines",
          "description": "This claim can be translated into a forecasting question: \"By 2027, what percentage of forecasting platforms will use portable LEPs as baselines for evaluating new forecasting systems?\"",
          "importance": "75",
          "highlight": {
            "startOffset": 36732,
            "endOffset": 37973,
            "quotedText": "Claim 2: Portable LEPs could be useful for acting as priors or baselines, for other systems to perform against.**\n\n*   We will want some baseline LEPs to score other LEPs against. These should act on the same data, at the same times.\n    *   For example, one could take a new LEP, a well-known Baseline LEP, and a known or generated question set. Both systems would forecast those questions, and then the new LEP will be evaluated based on how much it outperforms the Baseline LEP. For example, with the average difference in brier score.\n    *   In order to do this, the LEP must be equipped with the abilities to fetch data and to forecast. This is useful in order to compare the system to human forecasters (which do both), but it might be often better to divide the subcomponents and study/optimize them in isolation.\n    *   Another use is to use Baseline LEP to initialize forecasting markets. Right now manifold questions begin at 50%. It could be better for them to instead begin at a number suggested by a simple LEP.\n    *   Even if these LEPs aren't highly accurate, they could be useful for sanity checking claims outside of forecasting systems. For example, all publicly given estimates and statements could be run against LEPs."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Forecasting Question: Accuracy of LLM-Based Forecasting Over Time",
          "description": "This challenge can be framed as a forecasting question: \"How will the accuracy of LLM predictions decline as the resolution date moves further away from the LLM training date?\" This could be modeled as a percentage decrease in Brier score per year.",
          "importance": "80",
          "highlight": {
            "startOffset": 40452,
            "endOffset": 41547,
            "quotedText": "Challenge: LLM-forecasting abilities will vary based on date.**\n\n*   The quality of LLM predictions is likely to go down, the further away the forecasting question resolution is from the LLM training date. This is a major challenge to using them as a benchmark. For example, GPT4 might be decent at guessing the US GDP in 2025 if used in 2024 with search capabilities (i.e. web search). But if it were used in 2030, for the US GDP in 2031, with search capabilities but no other training updates, its performance would likely be worse. It's background/training data would be much less relevant.\n*   It's possible that there could simple numeric adjustments for this, for certain accuracy scores. For example, perhaps it's expected that the forecasting quality would decline according to a linear trend, in proportion between the time the LLM was trained, and when the forecasting question was asked.\n*   LLMs should rarely be used to “_forecast_” events that happened before their release. It’s likely that their training data would have leaks of the results, or of things close to these results."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        }
      ],
      "grade": 85
    },
    {
      "agentId": "ea-impact-evaluator",
      "costInCents": 0,
      "createdAt": "2025-04-17T04:09:31.959Z",
      "runDetails": "{\"model\":\"google/gemini-2.0-flash-001\",\"promptTokens\":12116,\"completionTokens\":1576,\"totalTokens\":13692,\"temperature\":0.1,\"runtimeMs\":9824}",
      "thinking": "This document presents a preliminary exploration of LLM-based Epistemic Processes (LEPs) and their potential as an effective altruist intervention area. The author, ozziegooen, outlines key definitions, benefits, failure modes, viability, scaffolding requirements, subcomponents, benchmarks, and caching strategies related to LEPs. The document aims to spark discussion and encourage further research in this area.nnFrom an EA perspective, the document touches on several important themes: the potential for AI to improve decision-making and coordination, the importance of considering negative impacts and failure modes, and the need for rigorous evaluation and benchmarking. The author also highlights the potential for LEPs to be integrated into various domains, including forecasting, research, and policy-making.nnMy analysis will focus on evaluating the document's claims through the lens of the ITN framework, developing Fermi estimates of potential impact, and assessing the overall value proposition of investing in LEP research and development. I will also consider potential negative impacts and compare LEPs to other EA interventions.",
      "summary": "This document presents a promising exploration of LLM-based Epistemic Processes (LEPs) as a potential EA intervention area. The author effectively outlines key definitions, benefits, failure modes, and viability considerations. The document scores well on Importance, Neglectedness, Tractability, and Novelty. However, it falls short in providing quantitative Fermi estimates of potential impact, particularly regarding negative impacts and the overall value proposition. While the author acknowledges potential risks, a more rigorous quantitative assessment is needed to determine whether investing in LEP research and development is justified compared to other EA interventions. The lack of concrete project ideas and cost-effectiveness estimations also limits the document's practical value. Overall, the document serves as a valuable starting point for further research and discussion, but requires more rigorous quantitative analysis to fully assess its potential impact.",
      "comments": [
        {
          "title": "Neglectedness Assessment: Under-Explored Area",
          "description": "The author's assertion that the EA community has not yet done a solid job outlining the potential of LLM epistemic systems suggests that this area is relatively neglected compared to other AI safety or forecasting efforts. This could present an opportunity for high-impact contributions.",
          "importance": "80",
          "grade": "75",
          "highlight": {
            "startOffset": 281,
            "endOffset": 1210,
            "quotedText": "However, I think that our community has yet to do a solid job outlining what this area could look like or figuring out key uncertainties.\n\nThis document presents a rough brainstorm on these topics. While I could dedicate months to refining these ideas, I've chosen to share these preliminary notes now to spark discussion. If you find the style too terse, feel free to use an LLM to rewrite it in a format you prefer.\n\nI believe my vision for this area is more ambitious and far-reaching (i.e. not narrow to a certain kind of forecasting) than what I've observed in other discussions. I'm particularly excited about AI-heavy epistemic improvements, which I believe have greater potential than traditional forecasting innovations.\n\nI'm trying to figure out what to make of this regarding our future plans at [QURI](https://quantifieduncertainty.org), and I recommend that other organizations in the space consider similar updates."
          },
          "isValid": true
        },
        {
          "title": "Novelty Assessment: AI-Heavy Epistemic Improvements",
          "description": "The author's emphasis on AI-heavy epistemic improvements, as opposed to traditional forecasting innovations, suggests a novel approach that could lead to significant breakthroughs. This could involve leveraging LLMs in new and creative ways to enhance epistemic processes.",
          "importance": "75",
          "grade": "70",
          "highlight": {
            "startOffset": 866,
            "endOffset": 1210,
            "quotedText": "I'm particularly excited about AI-heavy epistemic improvements, which I believe have greater potential than traditional forecasting innovations.\n\nI'm trying to figure out what to make of this regarding our future plans at [QURI](https://quantifieduncertainty.org), and I recommend that other organizations in the space consider similar updates."
          },
          "isValid": true
        },
        {
          "title": "Importance Assessment: Potential for Improved Decision-Making",
          "description": "The claim that better forecasting leads to fewer mistakes and improved decision-making is highly relevant to EA's focus on maximizing positive impact. If LEPs can significantly improve our ability to predict outcomes, they could have a wide-ranging impact across various domains.",
          "importance": "90",
          "grade": "85",
          "highlight": {
            "startOffset": 2379,
            "endOffset": 3658,
            "quotedText": "Benefits & Uses\n\n**Claim 1: If humans could forecast much better, these humans should make few foreseeable mistakes. This covers many mistakes, particularly ones we might be worried about now.**\n\n*   Someone deciding about talking to a chatbot that can be predicted to be net-negative (perhaps it would create an unhealthy relationship) could see this forecast and simply decide not to start the chat.\n*   Say that a person’s epistemic state could follow one of four trajectories, depending on some set of reading materials. For example, one set is conspiratorial, one is religious, etc. Good forecasting could help forecast this and inform the person ahead of time.\n    *   Note that this can be radical and perhaps dangerous. For example, a religious family knowing how to keep their children religious with a great deal of certainty.\n*   Say that one of two political candidates is predictably terrible. This could be made clear to voters who trust said prediction.\n*   If an AI actor is doing something likely to be monopolistic or dangerous, this would be made more obvious to itself and those around it.\n*   Note: There will also be unforeseeable mistakes, but any actions that we could do that are foreseeably-high-value for them, could be predicted. For example, general-"
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Importance Assessment: Coordination and Conflict Reduction",
          "description": "The claim that highly intelligent organizations are better at coordination, potentially leading to fewer conflicts, aligns with EA's interest in reducing existential risks. Improved coordination could also lead to more effective resource allocation and problem-solving.",
          "importance": "85",
          "grade": "80",
          "highlight": {
            "startOffset": 3694,
            "endOffset": 13,
            "quotedText": "recent improvements in LLMs have brought us to the point where LLM epistemic systems are starting to be useful. After spending some time thinking about it, I've realized that such systems, broadly, seem very promising to me as an effective altruist intervention area. However, I think that our community has yet to do a solid job outlining what this area could look like or figuring out key uncertainties.\n\nThis document presents a rough brainstorm on these topics. While I could dedicate months to refining these ideas, I've chosen to share these preliminary notes now to spark discussion. If you find the style too terse, feel free to use an LLM to rewrite it in a format you prefer.\n\nI believe my vision for this area is more ambitious and far-reaching (i.e. not narrow to a certain kind of forecasting) than what I've observed in other discussions. I'm particularly excited about AI-heavy epistemic improvements, which I believe have greater potential than traditional forecasting innovations.\n\nI'm trying to figure out what to make of this regarding our future plans at [QURI](https://quantifieduncertainty.org), and I recommend that other organizations in the space consider similar updates.\n\n![Expensive photograph of a simple open notebook on wood table - 3 very-rough old hand-drawn mini-sketches, of engineering flow diagrams that uses AI and brains to make predictions. Lots of white in background. Only a few lines drawn. No words.](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EykCuXDCFAT5oGyux/ddwsmmzz71oq6evcpdw3)\n\nImaginary sketch of AI epistemic infrastructure, Dall-E 3\n\n## **Key Definitions**:\n\n*   **Epistemic process:** A set of procedures to do analysis work, often about topics with a lot of uncertainty. This could be “_have one journalist do everything themselves_”, to a complex (but repeatable) ecosystem of multiple humans and software systems.\n*   **LLM-based Epistemic Process (LEP)**: A system that relies on LLMs to carry out most or all of an epistemic process. This might start at ~10% LLM-labor, but can gradually ramp up. I imagine that such a process is likely to feature some kinds of estimates or forecasts.\n*   **Scaffolding:** Software used around an LLM system, often to make it valuable for specific use-cases. In the case of an LEP, a lot of scaffolding might be needed.\n\n##  1. High-Level Benefits & Uses\n\n**Claim 1: If humans could forecast much better, these humans should make few foreseeable mistakes. This covers many mistakes, particularly ones we might be worried about now.**\n\n*   Someone deciding about talking to a chatbot that can be predicted to be net-negative (perhaps it would create an unhealthy relationship) could see this forecast and simply decide not to start the chat.\n*   Say that a person’s epistemic state could follow one of four trajectories, depending on some set of reading materials. For example, one set is conspiratorial, one is religious, etc. Good forecasting could help forecast this and inform the person ahead of time.\n    *   Note that this can be radical and perhaps dangerous. For example, a religious family knowing how to keep their children religious with a great deal of certainty.\n*   Say that one of two political candidates is predictably terrible. This could be made clear to voters who trust said prediction.\n*   If an AI actor is doing something likely to be monopolistic or dangerous, this would be made more obvious to itself and those around it.\n*   Note: There will also be unforeseeable mistakes, but any actions that we could do that are foreseeably-high-value for them, could be predicted. For example, general-purpose risk mitigation measures. \n\n"
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Fermi Estimate: Potential Impact on Global Coordination",
          "description": "This section lacks a quantitative estimate of impact. A Fermi estimate is needed to assess the potential value of improved global coordination resulting from better epistemics. This estimate should consider the probability of conflict reduction, the potential economic benefits of reduced military spending, and the value of averting catastrophic events.",
          "importance": "90",
          "grade": "40",
          "highlight": {
            "startOffset": 3694,
            "endOffset": 13,
            "quotedText": "recent improvements in LLMs have brought us to the point where LLM epistemic systems are starting to be useful. After spending some time thinking about it, I've realized that such systems, broadly, seem very promising to me as an effective altruist intervention area. However, I think that our community has yet to do a solid job outlining what this area could look like or figuring out key uncertainties.\n\nThis document presents a rough brainstorm on these topics. While I could dedicate months to refining these ideas, I've chosen to share these preliminary notes now to spark discussion. If you find the style too terse, feel free to use an LLM to rewrite it in a format you prefer.\n\nI believe my vision for this area is more ambitious and far-reaching (i.e. not narrow to a certain kind of forecasting) than what I've observed in other discussions. I'm particularly excited about AI-heavy epistemic improvements, which I believe have greater potential than traditional forecasting innovations.\n\nI'm trying to figure out what to make of this regarding our future plans at [QURI](https://quantifieduncertainty.org), and I recommend that other organizations in the space consider similar updates.\n\n![Expensive photograph of a simple open notebook on wood table - 3 very-rough old hand-drawn mini-sketches, of engineering flow diagrams that uses AI and brains to make predictions. Lots of white in background. Only a few lines drawn. No words.](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/EykCuXDCFAT5oGyux/ddwsmmzz71oq6evcpdw3)\n\nImaginary sketch of AI epistemic infrastructure, Dall-E 3\n\n## **Key Definitions**:\n\n*   **Epistemic process:** A set of procedures to do analysis work, often about topics with a lot of uncertainty. This could be “_have one journalist do everything themselves_”, to a complex (but repeatable) ecosystem of multiple humans and software systems.\n*   **LLM-based Epistemic Process (LEP)**: A system that relies on LLMs to carry out most or all of an epistemic process. This might start at ~10% LLM-labor, but can gradually ramp up. I imagine that such a process is likely to feature some kinds of estimates or forecasts.\n*   **Scaffolding:** Software used around an LLM system, often to make it valuable for specific use-cases. In the case of an LEP, a lot of scaffolding might be needed.\n\n##  1. High-Level Benefits & Uses\n\n**Claim 1: If humans could forecast much better, these humans should make few foreseeable mistakes. This covers many mistakes, particularly ones we might be worried about now.**\n\n*   Someone deciding about talking to a chatbot that can be predicted to be net-negative (perhaps it would create an unhealthy relationship) could see this forecast and simply decide not to start the chat.\n*   Say that a person’s epistemic state could follow one of four trajectories, depending on some set of reading materials. For example, one set is conspiratorial, one is religious, etc. Good forecasting could help forecast this and inform the person ahead of time.\n    *   Note that this can be radical and perhaps dangerous. For example, a religious family knowing how to keep their children religious with a great deal of certainty.\n*   Say that one of two political candidates is predictably terrible. This could be made clear to voters who trust said prediction.\n*   If an AI actor is doing something likely to be monopolistic or dangerous, this would be made more obvious to itself and those around it.\n*   Note: There will also be unforeseeable mistakes, but any actions that we could do that are foreseeably-high-value for them, could be predicted. For example, general-purpose risk mitigation measures. \n\n"
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Fermi Estimate: Potential Negative Impacts",
          "description": "The document acknowledges potential negative impacts, such as the use of LEPs by malicious governments. However, it lacks a quantitative assessment of these risks. A Fermi estimate is needed to assess the probability and magnitude of these negative impacts, and to compare them to the potential positive impacts.",
          "importance": "95",
          "grade": "45",
          "highlight": {
            "startOffset": 8488,
            "endOffset": 10559,
            "quotedText": ". It’s not clear how to expect this area to change over time. It's possible that nonprofit / EA efforts here would have trouble finding a strong fit.\n2.  **Forecasting could be used for net-bad activities**\n    1.  **AI Acceleration**  \n        It’s possible that epistemic improvements would be used by AI capabilities actors to increase capabilities and not make other informed choices.  \n          \n        Claim: I think this is unlikely to be the net outcome. This would require these organizations to become smarter about making AIs, but not much smarter about making them safe, including for their own benefit. It would also require the other groups that could block them to become smarter - some of these groups include other non-LLM companies and other highly motivated actors who wouldn't want AI to go poorly.  \n          \n        One crux here is if the AI developers will both be much better at integrating AI epistemic improvements in their internal workflows, and if these systems won't convince or help them with AI alignment. This could be a really bad outcome, so it's important to continue to pay attention to.\n    2.  **Malicious Governments**  \n        Governments like China or North Korea could use better epistemic systems to better control their populations. My guess is that great epistemic systems will likely do some damage here. I think that overall the improvements (like improving western governments) will more than outweigh that, but this is something to pay attention to.  \n          \n        Overall, I don’t see reasons to think that better epistemics will dramatically change existing power balances in ways that are yet predictable. I think this is true for most technologies I could imagine, even many of the most beneficial ones.\n    3.  **Malicious Corporations**   \n        There are some situations where companies could be far better at forecasting than consumers would be, leading to a large power imbalance. If companies have much more information about customers than users have about themselves or these companies, that cou"
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Failure Mode: AI Acceleration Risk",
          "description": "The discussion of AI acceleration as a potential failure mode is crucial from an EA perspective. It highlights the importance of considering the potential negative consequences of epistemic improvements, such as their use by AI capabilities actors to accelerate AI development without sufficient safety measures.",
          "importance": "95",
          "grade": "85",
          "highlight": {
            "startOffset": 8733,
            "endOffset": 9617,
            "quotedText": "It’s possible that epistemic improvements would be used by AI capabilities actors to increase capabilities and not make other informed choices.  \n          \n        Claim: I think this is unlikely to be the net outcome. This would require these organizations to become smarter about making AIs, but not much smarter about making them safe, including for their own benefit. It would also require the other groups that could block them to become smarter - some of these groups include other non-LLM companies and other highly motivated actors who wouldn't want AI to go poorly.  \n          \n        One crux here is if the AI developers will both be much better at integrating AI epistemic improvements in their internal workflows, and if these systems won't convince or help them with AI alignment. This could be a really bad outcome, so it's important to continue to pay attention to."
          },
          "isValid": true
        },
        {
          "title": "Tractability Assessment: Automation Potential",
          "description": "The claim that almost all parts of a conventional epistemic process can be automated with LLMs indicates that LEPs have high tractability. Automation could significantly reduce the cost and time required for forecasting and analysis.",
          "importance": "85",
          "grade": "80",
          "highlight": {
            "startOffset": 12216,
            "endOffset": 12601,
            "quotedText": "**Claim 1: All/Almost all parts of a conventional epistemic process can be automated with LLMs.**\n\nLLMs can:\n\n1.  Generate forecasting questions\n2.  Resolve forecasting questions\n3.  Gather data from computers\n4.  Write specific questions to expert - a la interviews\n5.  Make forecasts\n6.  Make visualizations and dashboards of forecasts\n7.  Personalize information for specific people"
          },
          "isValid": true
        }
      ],
      "grade": 70
    }
  ]
}