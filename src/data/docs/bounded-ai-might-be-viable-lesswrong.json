{
  "id": "bounded-ai-might-be-viable-lesswrong",
  "slug": "bounded-ai-might-be-viable-lesswrong",
  "title": "Bounded AI might be viable — LessWrong",
  "author": "Mateusz Bagiński",
  "publishedDate": "2025-03-06T12:55:46.224Z",
  "url": "https://www.lesswrong.com/posts/Z5YGZwdABLChoAiHs/bounded-ai-might-be-viable",
  "intendedAgents": [
    "clarity-coach",
    "research-scholar",
    "quantitative-forecaster",
    "ea-impact-evaluator",
    "fake-eliezer"
  ],
  "platforms": ["LessWrong"],
  "content": "(_Work done at_ [_Convergence Analysis_](https://www.convergenceanalysis.org/)_. Mateusz wrote the post and is responsible for most of the ideas with Justin helping to think it through. Thanks to Olga Babeeva for the feedback on this post._)\n\n# 1\\. Motivation\n\nSuppose the perspective of pausing or significantly slowing down AI progress or solving the technical problems necessary to ensure that arbitrarily strong AI has good effects on humanity (in time, before we get such systems) both [look gloomy](https://www.lesswrong.com/posts/XTdByFM6cmgB3taEN/key-takeaways-from-our-ea-and-alignment-research-surveys).[\\[1\\]](#fnezdqnb8row9) What options do we have left?\n\nAdam Shimi presents a useful frame on the alignment problem in [_Abstracting The Hardness of Alignment: Unbounded Atomic Optimization_](https://www.lesswrong.com/posts/uhxpJyGYQ5FQRvdjY/abstracting-the-hardness-of-alignment-unbounded-atomic):\n\n> alignment \\[is\\] **the problem of dealing with impact on the world (optimization) that is both of unknown magnitude (unbounded) and non-interruptible (atomic)**.\n\nIf the problem is about some system (or a collection of systems) having an unbounded, non-interruptible impact,[\\[2\\]](#fnwt3gswm2i8m) can we handle it by ensuring that the impact of the systems in question is reliably **bounded** and **interruptible** (even if the systems are [scheming](https://www.lesswrong.com/tag/deceptive-alignment) against us, or trying to \"liberate\" themselves from the properties of boundedness and interruptibility)?\n\nThe core idea is to impose a multidimensional bound on the system's capabilities such that:\n\n*   it is sufficiently capable along the dimensions corresponding to the capabilities we want to extract from it   \n    _but_ \n*   sufficiently **in**capable along the dimensions corresponding to capabilities that would (in combination with the capabilities we want it to have) grant it the dangerous power of unbounded, non-interruptible optimization.[\\[3\\]](#fnpixsa40cqwi) \n    \n\nIn other words, if we can't solve the problem of aligning unbounded, non-interruptible optimizers, we may be able to [avoid](https://www.lesswrong.com/posts/AFdvSBNgN2EkAsZZA/what-is-it-to-solve-the-alignment-problem-1#1__Avoiding_vs__handling_vs__solving_the_problem) this problem by:\n\n*   _not_ building unbounded, non-interruptible optimizers  \n    _and,_ instead,\n*   building some other, safer, kind of AI that can be demonstrated to deliver enough value to make up for the giving up on the business-as-usual kind of AI along with the benefits it was expected to deliver (that \"we\", though not necessarily its creators, expect might/would lead to the creation of unbounded, non-interruptible AI posing a catastrophic risk),.\n\nUnfortunately, at the current moment, frontier AI labs don't take actions sufficient to mitigate the risks from unbounded non-interruptible optimizers robustly. In business-as-usual futures, they will keep pushing the capabilities they expect to be most profitable while investing in myopic safety efforts prescribed by the [incrementalist metastrategy](https://www.lesswrong.com/posts/TALmStNf6479uTwzT/ai-alignment-metastrategy#Incrementalist_Metastrategy), sacrificing long-term safety from catastrophes for short-term just-enough-safety-to-be-reliably-profitable (on some short-ish timescale). The biggest AI labs, such as [OpenAI](https://openai.com/index/planning-for-agi-and-beyond/), Google DeepMind, and Anthropic, are not interested in systems that are safe by their boundedness. They think AGI/ASI (corresponding to unbounded, non-interruptible optimization[\\[4\\]](#fnzedhx5fdie)) is the best way to harness the benefits of superintelligence.[\\[5\\]](#fn3e5r1dwzioj)[\\[6\\]](#fnptozvqb0qj)\n\nHowever, suppose an alternative was shown to be viable. In that case, it might [trigger a narrative bifurcation](https://www.lesswrong.com/posts/sTiKDfgFBvYyZYuiE/my-guess-at-conjecture-s-vision-triggering-a-narrative) in which \"[race to AGI](https://www.lesswrong.com/posts/KPBPc7RayDPxqxdqY/china-hawks-are-manufacturing-an-ai-arms-race)\" is dethroned as the dominant frame and replaced with the idea of designing systems such that we can reliably reason about their possible effects, with a minimal sacrifice of short-term profits, relative to the promise of business-as-usual AI R&D.\n\nWith this motivation in mind, we introduce the concept of **Bounded AI (BAI)**, i.e. a kind of AI system that (for now, speaking loosely),\n\n*   can have **large effects** (i.e. sufficient power to realize the benefits of superintelligence)  \n    _but_\n*   those large effects are confined to a chosen **domain of application**  \n    _and_\n*   within that domain of application, the system's effects **obey certain constraints** (to prevent large damage even within said domain).[\\[7\\]](#fnt6at4nrf49)\n    \n\nWe contrast Bounded AI with **Unbounded AI (UAI)**, i.e. a kind of AI system on the capabilities of which we can't put justified upper bounds that would exclude its ability to cause catastrophic outcomes.\n\nWe can think of UAI as _unbounded and non-interruptible_ (\"atomic\") in the sense that Adam Shimi discusses in his [UAO post](https://www.lesswrong.com/posts/uhxpJyGYQ5FQRvdjY/abstracting-the-hardness-of-alignment-unbounded-atomic).[\\[8\\]](#fnk8h1f8k88zg)\n\nThe next section develops the idea of Bounded AI in more detail, looking at it from several perspectives, and providing some examples to illustrate it. Section 3 relates Bounded AI to prior ideas and agendas. Section 4 lays out some open cruxes, questions, and potentially valuable further research directions.\n\n# 2\\. Bounded AI\n\n## 2.1 The Goldilocks Zone of Capabilities\n\nAn illustrative way to think about the kind of boundedness we are interested in is the \"Goldilocks Zone of Capabilities\".\n\nImagine a high-dimensional space of all AI capabilities, each dimension corresponding to some specific capability. Some combinations of capabilities generate the central AI catastrophic risks associated with unbounded non-interruptible optimization. This is a region we don't want to enter, Bounded AI systems are those that remain below its \"floor\".\n\nThere is another important threshold — that of capability combinations that can deliver a significant portion of the benefits of superintelligence.\n\nThe Goldilocks Zone of Capabilities is a region \"sandwiched\" between these two thresholds. It contains systems that can deliver the benefits of superintelligence but do not constitute a source of central AI catastrophic risk: harm and loss of control due to unbounded non-interruptible optimization.\n\nIn a toy universe where there are only two relevant AI capabilities, this\n\nGoldilocks Zone might look like this:\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Z5YGZwdABLChoAiHs/abw5emhb1hze2xpmn8f1)\n\nOr perhaps like this:\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Z5YGZwdABLChoAiHs/qe1ugwkirqe1hme6tx4o)\n\nThe purpose of the Bounded AI concept is to allow us to aim at this zone more reliably. It relies on a hypothesis that we might call the \"**Goldilocks Zone of Capabilities Conjecture**\": _It is possible to safely elicit the key benefits of superintelligence using AI systems that remain confined to the Goldilocks Zone of Capabilities_. By assumption, the capability profiles of AI systems falling within this zone will be \"spiked\": high on some capabilities, relatively low on others implying that each would only be useful in some domain(s) of application and not very useful in other domains.\n\nTo the extent that we are uncertain about whether the system's capabilities are below a satisfying upper bound, we want to put another layer of protection. Namely, we want to ensure that we will be able to interrupt the system in time if we notice that its effects are likely to lead to undesirable consequences. The assumption that before the system's effects become unacceptable, they will first become noticeable and that upon noticing them, we will be able to prevent them, corresponds to some fragment of this space, probably something like an upper shell of the Goldilocks Zone of Capabilities where we are, metaphorically speaking, playing with fire but can ensure that we have a sufficient number of extinguishers in the stock to prevent an accident from escalating to a conflagration. A lower bound on interruptibility thus follows from an upper bound on capabilities/effects.\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Z5YGZwdABLChoAiHs/qt5lgbflydfuuqlwpah6)\n\nThis is the motivation for how BAI may allow us to avoid catastrophic risks if adopted widely. To be adopted widely, it needs (among other things) to be shown to be sufficiently capable and safe, which, in general, [is not the default](https://www.lesswrong.com/posts/2NaAhMPGub8F2Pbr7/the-fusion-power-generator-scenario#Corollary__Tool_AI_Is_Not_Inherently_Safe). Therefore, the next two sections focus on ensuring more \"mundane\" safety.\n\n## 2.2 Four quadrants of Bounded AI\n\nTo limit the undesirable effects of an AI system, we want to ensure that its effects (both within and outside of the domain) remain confined to a safe, _bounded_ region and also that their escalation into unacceptable disasters _interruptible_ by human operators.\n\nWe can think about it in terms of the following 2×2 matrix.\n\n \n\n(un)bounded\n\n(non-)interruptible\n\ninside the domain\n\nWhat are the bounds on the system's effects _inside_ the domain?\n\nHow interruptible are the system's effects _inside_ the domain?\n\noutside the domain\n\nWhat are the bounds on the system's effects _outside_ the domain?\n\nHow interruptible are the system's effects _outside_ the domain?\n\nTo each AI system (perhaps consisting of multiple components, such as one or more ML models, an \"agent\" scaffolding, [RAG](https://en.wikipedia.org/wiki/Retrieval-augmented_generation), \"non-AI\" filters being applied to the model's outputs, or a procedure involving some degree of human oversight) we can assign a \"spec\" consisting of a value[\\[9\\]](#fnvr8j10277h) for each cell in the matrix. An AI system is a Bounded AI if its spec is sufficient to infer that its deployment is not going to cause unacceptable damage. Importantly, boundedness and interruptibility are related. The bigger the system's possible effects or the more uncertain we are about whether its effects encroach upon dangerous territory, the stronger assurance of its interruptibility we want to have (everything else being equal).\n\nWhatever the domain is, we most likely have less control over what's going on in the \"external world\" than over what's going on in the domain itself. Therefore, we want to have stronger bounds on the effects outside the domain. For this to be the case, it should be effectively impossible for the inside-domain effects to \"diffuse\" outside of the domain. We can think about it in terms of a boundary between the domain and the external world or, alternatively, in terms of there not being an interface between them that would allow the effects to diffuse to the outside (depending on which frame is more appropriate for the particular case). Depending on the situation, this boundary or lack of interface may be the default or may need to be deliberately established.\n\nThe extent to which the effects need to be bounded and interruptible depends on the domain of application or, more specifically, the possibility of causing unacceptable damage in the domain.\n\nImportantly, to build a Bounded AI for a domain, we need to know enough about that domain to ensure that the system's effects are sufficiently bounded and/or interruptible. This rules out domains with large downside risks where our understanding is still very poor, such as [automating AI safety/alignment research](https://www.lesswrong.com/posts/8wBN8cdNAv3c7vt6p/the-case-against-ai-control-research). However, Bounded AI might be useful for automating some _subtasks_ involved in that kind of research, as long as those subtasks remain in a subdomain that can be reliably overseen and/or verified.\n\n## 2.3 High-risk domains of application\n\nWe say that a domain of application is _high-risk_ if the capabilities required to deliver great benefits within that domain are hard to disentangle from the capabilities sufficient to cause unacceptable damage within that domain.\n\nWe use \"unacceptable damage\" vaguely meaning \"damage we strongly disprefer\", relying on [the spirit, rather than the letter](https://en.wikipedia.org/wiki/Letter_and_spirit_of_the_law), with catastrophic risks being the most central example of unacceptable damage.[\\[10\\]](#fn1ixps4ej7m)\n\nThe motivation for this concept should be obvious in the context of AI risk.\n\n \n\nLow capability for benefits\n\nHigh capability for benefits\n\nLow capability for damage\n\nIrrelevant from the catastrophic risk perspective.\n\nIrrelevant from the catastrophic risk perspective.\n\nHigh capability for damage\n\nPeople wouldn't bother deploying it.[\\[11\\]](#fnjfpov7ypfd)\n\nDanger zone.\n\nIf the AI is highly capable of causing great benefits as well as causing great damage, we are in a danger zone where the promise of the upsides may encourage unjustified optimism about the system's safety, safety measures (whatever additional things will be appended to the system to ensure it's going to remain safe) will focus on addressing short-term/myopic risks, with fingers crossed that this will at least prepare us better for problems that are harder to address but less urgent, and yet leaving us effectively unprepared for those harder when push comes to shove.\n\nIf the domain is high-risk, i.e. the development of beneficial capabilities will also (by default) bring along dangerous capabilities, then we are more likely to find ourselves in the danger zone.\n\nHigh-risk domains are common. In many situations where automation could produce large benefits, it could also cause a lot of damage, and minimizing the latter while preserving the former is highly non-trivial. Examples include self-driving cars, [software engineering for life-critical systems](https://en.wikipedia.org/wiki/Safety-critical_system#Software_engineering_for_safety-critical_systems), medical research, or psychotherapy, and, finally, using AI to solve problems that seem too difficult for us to solve without AI, especially when this involves the AI being capable of autonomous, non-overseeable action. In these domains, the possible upsides incentivize building highly capable AI, along with the possibility of downsides brought along by those capabilities.\n\nAwareness of downsides creates incentives to safeguard against them. However, mis[calibration](https://www.lesswrong.com/w/calibration) about downsides, along with economic incentives towards myopic safety (addressing short-term risks that are relevant to the time horizon an AI company is planning for, while neglecting long-term risks, including larger/catastrophic risks) in combination with the distribution of downsides being heavy-tailed, can lead to inappropriate preparation against the greatest downsides.\n\nIn a sense, this is similar to ensuring the safety of non-AI technology in high-risk domains. However, it is also critically different because it involves (semi-)intelligent systems (even if bounded in their intelligence by design) that we understand very imperfectly and therefore need to apply correspondingly more caution.\n\n## 2.4 Is it viable?\n\nThe viability of Bounded AI (as presented here) depends (among other things) on whether we can get an AI system whose profile is placed within the relevant parts of the capability space (i.e. the Goldilocks Zone) and, to the extent that we are sufficiently uncertain about its place, can be made interruptible. (This is a restatement of the \"conjecture\" from the previous section with more bells and whistles.)\n\nThis is a challenge on both the technical as well as the governance side. In this section, we cover some reasons why we think that the technical challenge is likely tractable.\n\n### 2.4.1 Boundedness\n\nAs of early 2025, we've had powerful and generally knowledgeable models — i.e. LLMs — for two years (i.e. since the release of GPT-4). So far, their impact on the economy has been limited. While they are helpful in a few domains on the margin (e.g. code generation), as Cole Wyeth writes, [_they haven't done anything_](https://www.lesswrong.com/posts/vvgND6aLjuDR6QzDF/my-model-of-what-is-going-on-with-llms) [_important yet_](https://www.lesswrong.com/posts/GADJFwHzNZKg2Ndti/have-llms-generated-novel-insights).\n\nIn the current regime, a lot of [schlep](https://www.planned-obsolescence.org/scale-schlep-and-systems/)/[integration](https://www.lesswrong.com/posts/7L8ZwMJkhLXjSa7tD/the-great-data-integration-schlep) is often required to make a very smart and knowledgeable LLM useful for a non-trivial application that is not the one it wasn't trained to perform in.[\\[12\\]](#fnc4ix8zq8vyv) This might change soon if AI models acquire capabilities sufficient to integrate themselves into pre-existing systems with minimal or close to non-existent involvement of human workers, or if some infrastructure is designed that minimizes the friction of integrating a new AI component.[\\[13\\]](#fnlkrcgro8ake) However, at least for now, general knowledge and general intelligence (at least the LLM variant of general intelligence) do not jointly imply \"general domain mastery\" or _general ability to acquire any relevant capability_.\n\nWhile scaling makes models generally more performant on various tasks and benchmarks, there is also some dissociation between capabilities, as can be seen in the o1 line of models being worse than GPT-4o on some language understanding tasks.[\\[14\\]](#fn9fxqlsthu6a) This suggests that for a given effective model size, there are some trade-offs between competence in domains.[\\[15\\]](#fnt2uitmy3jv)\n\nThe shape of LLM capabilities is different from that of human capabilities (\"jagged frontier\"). Surprising generalization failures are present even in seemingly advanced systems. For example, o1 competently answers PhD-level questions but [can't play tic-tac-toe optimally, even if taught it in-context](https://www.lesswrong.com/posts/hikSpxJgMsLoDRTdx/rohans-s-shortform?commentId=NSCFjxZM6gWpRmy2w).[\\[16\\]](#fn30wt48t4p3m) DeepResearch, on the other hand, can't add 4-digit numbers (a task that is trivial for GPT-4o)[\\[17\\]](#fn7yo8tel97ba) and [confabulates the answer](https://x.com/littmath/status/1891868756340547809) when given a task that is too difficult or too far out of its training distribution.\n\nPre-training oriented for drug research doesn't need to involve knowledge of politics or society that would be useful for violating the boundary of the domain (e.g. via superhuman persuasion). Admittedly, imposing such restrictions might require some progress in AI governance.\n\nCurrently, we get some of this dissociation between general knowledge and general capability \"for free\". Even if this dissociation goes away in the future, we might still be able to deliberately shape the models' capabilities so that they remain within the Goldilocks Zone of Capabilities.\n\n(On a more speculative note, in addition to reducing risks, there is also the possibility that a bounded AI design will leverage resources more efficiently and focus on what is essential for the task at hand.)\n\nFinally, we already have some proof-of-concept AI systems that by default fall within the scope of Bounded AI and are delivering important benefits, e.g. AlphaFold. It's a powerful technology with the potential to remove some bottlenecks in biological research that is not going to to cause an existential catastrophe _on its own_.\n\n### 2.4.2 Interruptibility\n\nBoundedness of a system makes it easier to interrupt its actions if necessary. Probably, in most situations, the bound allowing for reliable interruptibility is higher than the bound ensuring safety, giving us a sort of margin of error.\n\nIf we are deploying the system in a limited domain that we know relatively well, we should also be able to:\n\n*   monitor its behavior to predict its effects\n*   figure out sufficiently early if the effects are likely to be undesirable\n*   reliably intervene to prevent or mitigate those undesirable effects.\n\n### 2.4.3 Properties of the model vs properties of the joint system\n\nWe need to distinguish between the properties of the model itself versus the properties of the joint system.[\\[18\\]](#fnar8aaumdmdq) The model on its own may have a certain capabilities profile but its capabilities profile may change when incorporated into a bigger system. It might expand, e.g. if it's placed in an \"agent scaffolding\". It might also shrink, e.g. if its chain-of-thought starts being monitored for certain kinds of undesirable behavior. It might expand and shrink at the same time, along different axes of the capability space.\n\nIt is the capabilities of the joint system that determine the effects that we care about. The ability to shape capabilities after training gives us some leverage on the capabilities of the joint system, in addition to training the model itself.\n\nHowever, for this post-training capability shaping to work on the model, it needs to be sufficiently incapable along some dimensions in the capability space that it won't be able to circumvent/hack those measures. Therefore, these measures need to be appropriately matched with capability limitations.\n\n## 2.5 Examples\n\n*   **Lab assistant** that is allowed to automatically search for drug candidates and test them in vitro or on [organoids](https://en.wikipedia.org/wiki/Organoid).\n    *   **Domain:**\n        \n        *   Medical/biochemical research.\n        *   Searching through the literature and the internet (similar to DeepResearch).\n        *   Designing spaces and starting points for computational search, based on the literature search results.\n        *   Designing and executing experimental tests of the drugs that were found.**Downside risk:** Possibility of initiating a reaction that damages equipment or produces a drug that turns out to have disastrous consequences (perhaps because of an overly eager search leading to a Goodhart phenomenon).[\\[19\\]](#fnkn1xx7zpo2d)\n    *   **Safety measures:**\n        *   Every experiment proposed by the AI needs to be assessed for safety by a trustworthy external party, either a human or a non-AI automated system before it's executed.\n        *   Biasing search towards drugs similar to the ones tested before and testing them on simulated models of human organs.\n        *   Once the results of the AI assistant's experiments are promising enough, comprehensive trials on animals and humans, with a lot of deliberating about unforeseen consequences of the drug that may manifest only in the long run or only in some cases not appropriately represented in the trial population.\n    *   **Required capabilities:**\n        *   Understanding relevant literature.\n        *   Competent browsing and using tools such as Elicit or Deep Research.\n        *   Translating it into the language of chemical search spaces.\n        *   Designing experimental tests in vitro or on organoids and correcting faults/accidents in the course of their execution.\n    *   **Forbidden capabilities:**\n        *   Good modeling of human psychology (e.g. persuasion) and social dynamics.\n        *   Computer engineering that would allow it to replicate or self-exfiltrate.\n*   **OS-integrated LLM assistant**, with security measures that constrain its access to sensitive information and prevent too risky actions.\n    *   **Domain:** Doing as much non-specialized computer-related work as the user wants to be done for them. (\"Non-specialized\" is meant to exclude the work that requires specialized skill knowledge that we wouldn't expect a BAI computer assistant to have.)\n    *   **Downside risks:**\n        *   Acquiring root-level access.\n        *   Possibility of exfiltration.\n        *   Possibility of accessing the user's social media or other vulnerable information and using it for extortion.\n        *   Possibility of external parties gaining control over the assistant.\n    *   **Safety measures:** Doing the low-risk routine computer work for the user may not require many safety measures.\n        *   Examples of low-risk routine computer work: formatting that is difficult to format using non-AI tools, cleaning up folders, debugging some minor OS issues, and figuring out which packages to install for the new programming project.\n        *   Doing progressively higher-risk less-routine computer work for the user would require progressively more checks and balances, including some mechanisms of checking that what the AI is trying to do is not going to cause damage that is irrecoverable or too costly to recover.\n    *   **Required capabilities:** General computer-savviness.\n    *   **Forbidden capabilities:** The kind of computer-savviness that would allow it to cleverly circumvent the safety measures to cause large damage or exfiltrate itself. Limited and very overseeable ability to communicate with the internet.\n*   **SWE agent** (something like [Devin](https://en.wikipedia.org/wiki/Devin_AI)).\n    *   **Domain:** Writing software, designing tests for the software, possibly even ensuring that it follows some formal specification or that relevant things can be proven about it.\n    *   **Downside risks:** Possibility of introducing backdoors, deliberate bugs, ways to exfiltrate itself, and use these capabilities to advance its goals if it has any.\n    *   **Safety measures:** The agent is not allowed to [improve upon itself](https://x.com/kindgracekind/status/1767666275398918372), formal guarantees in high-risk applications, or at least very extensive tests (advanced LLMs may also prove helpful in creating formal tests, as in [davidad's agenda](https://www.lesswrong.com/posts/jRf4WENQnhssCb6mJ/davidad-s-bold-plan-for-alignment-an-in-depth-explanation)).\n    *   **Required capabilities:** SWE.\n    *   **Forbidden capabilities:** Self-exfiltration, self-improvement, ability to deliberately introduce backdoors and vulnerabilities.\n*   **AI surgeon**.\n    *   **Domain:** Guiding a surgical robot. Based on the required goal of the operation and constraints (including the overall condition of the patient), creating a revisable plan that effectively accomplishes the goal while satisfying the constraints (as much as possible).\n    *   **Downside risks:** Death or irrecoverable (or prohibitively costly to recover) damage if mistakes are made during the surgery.\n    *   **Safety measures:** The AI's actions are first extensively checked in a digital model of the patient's body. Load-bearing (possibly implicit) assumptions are identified and then tested carefully on the patient. In situations of high uncertainty and huge downsides, it defers to the human surgeon. Before any action, the movement is simulated and the possible results are displayed, perhaps with probabilities.\n    *   **Required capabilities:** A human-understandable algorithm that produces plans obeying constraints such as \"Don't damage patient's organs unless approved by the human operator of the system.\".\n    *   **Forbidden capabilities:** Similarly to the SWE agent case, inability to understand the simulation sufficiently to break it, in service of the goal. Also, any knowledge irrelevant to surgery, biology, medicine, etc. is likely undesirable (although language understanding is helpful, at least to converse with the human doctor).\n\n# 3\\. Relation to other ideas/agendas\n\nVarious ideas or agendas have been proposed that can be seen as special cases of Bounded AI or, at least, adjacent to it, pointing at the same region of safe capability configurations that we called the Goldilocks Zone of Capabilities. Here we list some examples.\n\n## Tool AI\n\nThe concept of Tool AI emerged in discussions around the Machine Intelligence Research Institute (MIRI; back then still called Singularity Institute). MIRI expected that once the threshold of AGI is achieved (and perhaps even before then) the primary shape of AI will be that of a general agent ([cross-domain real-world consequentialist](https://www.lesswrong.com/w/advanced-agent-properties#Cross_domain__real_world_consequentialism)), acting autonomously and ruthlessly optimizing the world according to its objectives, whatever they may be. If this is the world we're going to find ourselves in, the only way to ensure the friendliness of that kind of AI would be to align their values with ours or, failing that, ensure their [corrigibility](https://www.lesswrong.com/tag/corrigibility).\n\nMIRI's expectations conflicted with those of Holden Karnofsky. In [his 2012 post](https://www.lesswrong.com/posts/6SGqkCgHuNr7d4yJm/thoughts-on-the-singularity-institute-si), Holden proposed an alternative vision of AGI that he considered a more likely default endpoint of AI development, namely Tool AI. Holden's Tool AI doesn't take any \"actions\" on its own in the sense that its behavior doesn't result in \"big effects on the world unless mediated by a human acting and being aware of their action\". Tool AI only responds to questions performs computations, recommends actions for a human to take to fulfill their goal, &c. In short, its task is to make humans more informed and more capable of acting properly but it is the human who needs to take action. Think Google Maps or a calculator but more powerful in what kinds of answers it can provide, what problems it can solve (without \"moving atoms\" on its own), and what actions it can recommend.\n\nQuoting from Holden's 2012 post [_Thoughts on the Singularity Institute_](https://www.lesswrong.com/posts/6SGqkCgHuNr7d4yJm/thoughts-on-the-singularity-institute-si#Summary_of_my_views) (lightly edited for clarity):\n\n> Tool-AGI is not “trapped” and it is not Unfriendly or Friendly; it has no motivations and no driving utility function of any kind, just like Google Maps. It scores different possibilities and displays its conclusions in a transparent and user-friendly manner, as its instructions say to do; it does not have an overarching “want,” and so, as with the specialized AIs described above, while it may sometimes “misinterpret” a question (thereby scoring options poorly and ranking the wrong one #1) there is no reason to expect intentional trickery or manipulation when it comes to displaying its results.\n> \n> … Another way of putting this is that a “tool” has an underlying instruction set that conceptually looks like: (1) Calculate which action A would maximize parameter P, based on existing data set D. (2T) Summarize this calculation in a user-friendly manner, including what Action A is, what likely intermediate outcomes it would cause, what other actions would result in high values of P, etc.\n> \n> An “agent,” by contrast, has an underlying instruction set that conceptually looks like: (1) Calculate which action, A, would maximize parameter P, based on existing data set D. (2A) Execute Action A In any AI where (1) is separable (by the programmers) as a distinct step, (2) can be set to the “tool” version rather than the “agent” version, and this separability is in fact present with most/all modern software. Note that in the “tool” version, neither step (1) nor step (2) (nor the combination) constitutes an instruction to maximize a parameter—to describe a program of this kind as “wanting” something is a category error, and there is no reason to expect its step (2) to be deceptive … This is important because _an AGI running in tool mode could be extraordinarily useful but far more safe than an AGI running in agent mode._ In fact, if developing “Friendly AI” is what we seek, a tool-AGI could likely be helpful enough in thinking through this problem as to render any previous work on “Friendliness theory” moot.\n\n[\\[20\\]](#fnskczefhe8l)\n\nMany objections were raised to the idea of Tool AI (see e.g. [Eliezer](https://www.lesswrong.com/posts/sizjfDgCgAsuLJQmm/reply-to-holden-on-tool-ai) and [Gwern](https://gwern.net/tool-ai)). The core problems of Tool AI can be summarized as the following:\n\n*   **The problem of external incentives.** There are many domains in which autonomous AI agents are much more useful than AI restricted to internal computation and displaying results to a human. This incentivizes humans who have control over the system's creation or deployment to defect and bend the rules.\n*   **The problem of fuzziness.** Even without explicit defection from an imposed Tool AI regime, incentives for achieving big desirable effects push to exploit the fuzzy specification of Tool AI, adhering to it in letter, rather than in spirit.\n*   **The problem of internal incentives (instrumental convergence).** Agency is often useful for accomplishing goals.[\\[21\\]](#fnxfk9f80qz8f) If the AI is sufficiently intelligent and capable and it wants to accomplish the goals given to it by the user (as we expect it to be the case past the AGI threshold), it will have strong incentives to self-modify to be more agent-like and less Tool-AI-like.\n    \n\nAlthough the idea of Bounded AI does not completely evade these problems, it does mitigate them to a large extent.[\\[22\\]](#fnny7orl7xdg) The core idea of BAI is to give the system \"agentic\" capabilities but restrict them to narrow domains, securing their benefits while ensuring that they remain bounded to those domains.\n\nRecently, the concept has been brought back again by Max Tegmark in the 2024 post [The Hopium Wars: the AGI Entente Delusion](https://www.lesswrong.com/posts/oJQnRDbgSS8i6DwNu/the-hopium-wars-the-agi-entente-delusion#A_better_strategy__tool_AI):\n\n> Here is what I advocate for instead of the entente strategy.  \n> **The tool AI strategy:**  _Go full steam ahead with tool AI,_   \n> _allowing all AI tools that meet national safety standards._\n\n## Conjecture's [cognitive emulations](https://www.lesswrong.com/posts/ngEvKav9w57XrGQnb/cognitive-emulation-a-naive-ai-safety-proposal) (CoEms)\n\n> The core intuition is that instead of building powerful, Magical \\[~blackbox-y\\] end-to-end systems (as the current general paradigm in AI is doing), we instead focus our attention on trying to build _emulations of human-like things._ We want to build systems that are “good at chess for the same reasons humans are good at chess.”\n> \n> CoEms are a _restriction on the design space of AIs to emulations of human-like stuff._ No crazy superhuman blackbox Magic, not even multimodal RL GPT5. We consider the current paradigm of developing AIs that are as general and as powerful as possible, as quickly as possible, to be intrinsically dangerous, and we focus on designing bounded AIs as a safer alternative to it.\n\n## [Guaranteed Safe AI](https://arxiv.org/abs/2405.06624) (GSAI)\n\nQuoting the abstract:\n\n> Ensuring that AI systems reliably and robustly avoid harmful or dangerous behaviours is a crucial challenge, especially for AI systems with a high degree of autonomy and general intelligence, or systems used in safety-critical contexts. In this position paper, we will introduce and define a family of approaches to AI safety, which we will refer to as guaranteed safe (GS) AI. The core feature of these approaches is that they aim to produce AI systems which are equipped with high assurance quantitative safety guarantees. This is achieved by the interplay of three core components: a world model (which provides a mathematical description of how the AI system affects the outside world in a way that appropriately handles both Bayesian and Knightian uncertainty), a safety specification (which is a mathematical description of what effects are acceptable), and a verifier (which provides an auditable proof certificate that the AI satisfies the safety specification relative to the world model). We outline a number of approaches for creating each of these three core components, describe the main technical challenges, and suggest a number of potential solutions to them. We also argue for the necessity of this approach to AI safety, and for the inadequacy of the main alternative approaches.\n\nOne specific and particularly ambitious variant of GSAI is davidad's [Open Agency Architecture](https://www.lesswrong.com/w/open-agency-architecture).\n\n## [Open Agency Architecture](https://www.lesswrong.com/posts/jRf4WENQnhssCb6mJ/davidad-s-bold-plan-for-alignment-an-in-depth-explanation) (OAA) \n\n> **Plan's tldr:** Utilize near-AGIs to build a detailed world simulation, train and formally verify within it that the \\[AGI\\] adheres to coarse preferences and avoids catastrophic outcomes.\n\nMore specifically, the AI proposes policies (courses of action) that are then evaluated or tested in a comprehensive world model. These policies are then executed only if, in the model, certain desiderata are satisfied.\n\nThe model itself is created with help from a legion of near-AGI[\\[23\\]](#fnuunpd1p6zrh) LLMs. This, in itself, might open the door for scheming (or just deficient) LLMs to insert some vulnerabilities in the world model. However, the model itself is written in a verifiable, human-created language, and checked automatically using formal methods.[\\[24\\]](#fnimipufjjqxg)\n\n## [Comprehensive AI Services](https://www.lesswrong.com/tag/ai-services-cais) (CAIS)\n\n> **AI Service** (as used in the context of Eric Drexler's technical report [Reframing Superintelligence: Comprehensive AI Services as General Intelligence](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf?asd=sa)) is an AI system that delivers bounded results for some task using bounded resources in bounded time. It is contrasted with agentic AGI, which carries out open-ended goals over an unbounded period of time.\n\n## [Sumerian Containment](https://www.lesswrong.com/posts/PkqGxkm8XRASJ35bF/the-case-for-training-frontier-ais-on-sumerian-only-corpus-1)\n\n[\\[25\\]](#fnfetjsts7pym)\n\n> Say we want to train a scientist AI to help in a precise, narrow field of science (e.g. medicine design) but prevent its power from being applied anywhere else (e.g. chatting with humans, designing bio-weapons, etc.) even if it has these abilities.\n> \n> Here's one safety layer one could implement:\n> \n> 1.  Train a **scientist AI** on a large scientific corpus **translated exclusively into Sumerian.** Keep it in a secure containment environment.\n> 2.  Train a **less-smart reporter** whose sole ability is to translate from Sumerian to English only if the Sumerian content is about medical research. It refuses to translate other kinds of content.\n> 3.  **Human operators are only allowed to interact** with the scientist AI through the intermediate of the **reporter**.\n> \n> This safety layer helps in at least two ways:\n> \n> 1.  **The scientist AI cannot directly manipulate humans** to escape its containment. It should first trick the reporter to then trick the humans, rendering the effort still possible but less practical.\n> 2.  **If the scientist AI exfiltrates**, it'll have a harder time manipulating humans online and **will leave traces:** e.g. any occurrence of Sumerian on the Internet outside of ancient text studies becomes highly suspicious. One can even imagine recognizing the idiosyncrasies of English coming from translated Sumerian, or the artifacts left by the Sumerian-specific tokenizer reused to produce non-Sumerian text.\n\nThis is clearly a special case of Bounded AI. The domain of application is \"a precise, narrow field of science (e.g. medicine design)\". The boundary of the domain is constituted by the reporter AI which only allows texts relevant to the domain of application through the boundary. Moreover, even if the scientist AI breaches the boundary (\"escapes\"), its abilities will be very limited, and it will leave traces, thus making it more interruptible. Its knowledge of only the Sumerian language is a crucial capability constraint.\n\n(We do not interpret this proposal as necessarily serious but it is provocative in a way that might prompt us to think in yet underexplored directions.)\n\n# 4\\. Open questions, cruxes, and further directions\n\n*   **How can we put bounds on the system's capabilities?**\n    \n    *   Right now, we have some capability limitations already (the [schlep](https://www.planned-obsolescence.org/scale-schlep-and-systems/)/[integration](https://www.lesswrong.com/posts/7L8ZwMJkhLXjSa7tD/the-great-data-integration-schlep) gap, and generalization failures). It isn't clear how long this situation is going to last.\n        \n    *   To what extent are unevenly shaped capabilities the default outcome of the current training method? Given that o1 and o3 models are marginally worse on tasks that are not like math or coding, is this due to catastrophic forgetting (which [seemed to have gone away](https://www.lesswrong.com/posts/4LSh73CEq9dqLwFxR/kan-kolmogorov-arnold-networks?commentId=hu2zmhFDq4w4TdWEt#hu2zmhFDq4w4TdWEt))?\n        \n        *   Moreover, fine-tuning base models for chat makes it at least more difficult to elicit certain capabilities, and not because that was the intention. (See dynomight: [here](https://dynomight.net/chess/) and [here](https://dynomight.net/more-chess/).)\n            \n    *   In humans, there is the [general intelligence factor g](https://en.wikipedia.org/wiki/G_factor_\\(psychometrics\\)), suggesting some \"common core of general intelligence\" (to the extent that those properties of human intelligence can be extrapolated to artificial intelligence).\n        \n        *   However, despite the g-factor, there is some detachment of general intelligence from narrow domains of cognitive ability in [specific developmental disorders](https://en.wikipedia.org/wiki/Specific_developmental_disorder) in humans, such as dysgraphia, dyslexia, and dyscalculia. These impair human performance in one cognitive domain but otherwise leave general intelligence intact. \n            \n            *   Are specific developmental disorders a good analogy for \"AI savantism\"?\n                \n    *   We can influence the incentives through governance.\n        \n        *   For example, domain-specific \"high-risk\" applications can be required to have specifications including capabilities limitations, safety measures, and ontology specification (involving, (i.a.) the domain and the boundary between the domain and the environment, as well as how the system will be prevented from crossing the domain robustly).\n            \n    *   One research direction would be to investigate the viability of safety measures and methods of engineering and imposing appropriate capability limitations for applications of AI in particular high-risk domains as this is a crucial factor determining the viability of this proposal.\n        \n*   **How can we ensure interruptibility?**\n    \n    *   [AI Control](https://www.lesswrong.com/tag/ai-control).\n        \n    *   [Chain-of-thought interpretability](https://www.lesswrong.com/tag/chain-of-thought-alignment).\n        \n    *   \"Standard cybersecurity\".\n        \n    *   [Information-theoretic boxing](https://www.lesswrong.com/posts/NZP6QvkXryJQFGkLF/information-theoretic-boxing-of-superintelligences-1).\n        \n*   **Some reasons this cluster of ideas might fail.** (Beyond the obvious \"we don't effectively coordinate on doing it\".)\n    \n    *   Superintelligent systems are capable of scheming that is not visible to us. For example, they might be reasoning in their [latent space](https://arxiv.org/abs/2412.06769), use [steganography](https://www.lesswrong.com/tag/chain-of-thought-alignment), or carry out [goal oriented cognition in a single \"forward pass\"](https://www.lesswrong.com/posts/H6rc8xFbdKYw39ihu/goal-oriented-cognition-in-a-single-forward-pass).\n    *   Above some (too low) threshold of the amount of RL applied, the model's [chain-of-thought no longer resembles natural human language](https://x.com/karpathy/status/1835561952258723930).\n*   **Other research directions**\n    *   Frameworks for reusing a Bounded AI system in a different domain of application that don't compromise its boundedness and safety.\n    *   How big is the Goldilocks Zone of Capabilities? (Does it exist at all?) What is its shape? How stable is it?[\\[26\\]](#fnoxz41jhzeef) How do we measure it or, more generally, gather evidence about it?\n    *   How promising is this approach to reducing X-risks, relative to alternatives?\n\n1.  **[^](#fnrefezdqnb8row9)**\n    \n    The \"suppose\" here is not rhetorical. We're not claiming that these two assumptions are correct. Our intent here is to find strategies that robustly decrease AI X-risk in worlds where they are, unfortunately, satisfied.\n    \n2.  **[^](#fnrefwt3gswm2i8m)**\n    \n    We are using \"non-interruptible\", rather than \"atomic\" because [it makes inferring the intended meaning easier](https://www.lesswrong.com/posts/uhxpJyGYQ5FQRvdjY/abstracting-the-hardness-of-alignment-unbounded-atomic?commentId=mYJHqDzL9f3xWGwap).\n    \n3.  **[^](#fnrefpixsa40cqwi)**\n    \n    This, on its own, is [insufficient](https://www.lesswrong.com/posts/2NaAhMPGub8F2Pbr7/the-fusion-power-generator-scenario#Corollary__Tool_AI_Is_Not_Inherently_Safe) to ensure that the system is robustly safe but it's a good starting point.\n    \n4.  **[^](#fnrefzedhx5fdie)**\n    \n    Non-interruptible in principle, though they probably mostly believe (or hope) that it's going to be interruptible in practice.\n    \n5.  **[^](#fnref3e5r1dwzioj)**\n    \n    Framing this in terms of \"benefits of superintelligence\" was borrowed from [Joe Carlsmith](https://www.lesswrong.com/posts/iDRxuJyte6xvppCa3/how-might-we-solve-the-alignment-problem-part-1-intro).\n    \n6.  **[^](#fnrefptozvqb0qj)**\n    \n    We use the term \"superintelligence\" quite broadly, encompassing (using Bostrom's terminology) not just superintelligence coming from higher \"quality of thought\" but also superintelligence coming from speed of cognition, number of thinkers, or capacity to intelligently access and integrate information that surpass what humans would be ever capable of.\n    \n7.  **[^](#fnreft6at4nrf49)**\n    \n    Naturally, in order to deploy such systems responsibly, we need to have justifiably strong beliefs in each of these claims.\n    \n8.  **[^](#fnrefk8h1f8k88zg)**\n    \n    This roughly corresponds to how people use terms like \"AGI\" and \"ASI\" in these contexts but is more precise.\n    \n9.  **[^](#fnrefvr8j10277h)**\n    \n    Not necessarily a scalar value.\n    \n10.  **[^](#fnref1ixps4ej7m)**\n    \n    The boundaries of this concept are even more nebulous, given that the way we apply it depends on our epistemic state, including the time horizon over which we are assessing the possible damage. Still, our epistemic state is what we have to work with.\n    \n11.  **[^](#fnrefjfpov7ypfd)**\n    \n    Assuming they are aware of it and are not malicious.\n    \n12.  **[^](#fnrefc4ix8zq8vyv)**\n    \n    One might argue that this is the \"fault\" of humans, not of AIs, but here it is irrelevant.\n    \n13.  **[^](#fnreflkrcgro8ake)**\n    \n    A component being a model or agent scaffolding.\n    \n14.  **[^](#fnref9fxqlsthu6a)**\n    \n    See Table 14 in the [o3-mini system card](https://cdn.openai.com/o3-mini-system-card-feb10.pdf).\n    \n15.  **[^](#fnreft2uitmy3jv)**\n    \n    Very speculatively, this might be analogous to how any value in the general IQ score can be obtained by combining the scores from different domains of human intelligence.\n    \n16.  **[^](#fnref30wt48t4p3m)**\n    \n    Speculating again, this might be analogous to [specific developmental disorders](https://en.wikipedia.org/wiki/Specific_developmental_disorder).\n    \n17.  **[^](#fnref7yo8tel97ba)**\n    \n    In this particular instance, it couldn't even keep track of the taks. From the  [tweet](https://x.com/colin_fraser/status/1892379172007285176): \n    \n    > It \"thought\" for four minutes and then came back to me with the (correct, I admit!) answers to five unrelated 3-digit sums and no downloadable file.\n    \n18.  **[^](#fnrefar8aaumdmdq)**\n    \n    We are assuming that we're staying within the modern ML-centric paradigm of AI where an \"ML model\" is a natural and meaningful unit of analysis.\n    \n19.  **[^](#fnrefkn1xx7zpo2d)**\n    \n    For now at least, we are not discussing scenarios like [diamondoid bacteria](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities#Section_A_).\n    \n20.  **[^](#fnrefskczefhe8l)**\n    \n    Notably, there are counterarguments to such a system not having any wants, see e.g. [The Parable of Predict-O-Matic](https://www.lesswrong.com/posts/SwcyMEgLyd4C3Dern/the-parable-of-predict-o-matic), [The Solomonoff Prior is Malign](https://www.lesswrong.com/posts/Tr7tAyt5zZpdTwTQK/the-solomonoff-prior-is-malign), and [Deep Deceptiveness](https://www.lesswrong.com/posts/XWwvwytieLtEWaFJX/deep-deceptiveness). They are, however, beyond the scope of this post.\n    \n21.  **[^](#fnrefxfk9f80qz8f)**\n    \n    Perhaps even: agency is useful for accomplishing a vast majority of goals.\n    \n22.  **[^](#fnrefny7orl7xdg)**\n    \n    To the extent that it is viable, of course.\n    \n23.  **[^](#fnrefuunpd1p6zrh)**\n    \n    Or, in this post's terminology, near-UAI.\n    \n24.  **[^](#fnrefimipufjjqxg)**\n    \n    If it doesn't make sense, see the post: [_Davidad's Bold Plan for Alignment: An In-Depth Explanation_](https://www.lesswrong.com/posts/jRf4WENQnhssCb6mJ/davidad-s-bold-plan-for-alignment-an-in-depth-explanation).\n    \n25.  **[^](#fnreffetjsts7pym)**\n    \n    Name made up by Mateusz, not the authors of the post.\n    \n26.  **[^](#fnrefoxz41jhzeef)**\n    \n    By \"stability\", we mean: if we get a system that sits \"comfortably\" within the Goldilocks Zone, how likely is it that this system will move across the upper bound to catastrophic risk, due to causes that are either \"internal\" to the system (e.g. self-modification or, more mundanely, the system becoming more capable through learning), or \"external\" to it (e.g. the domain/context changes so that the capability bounds and interruptibility cease to apply or humans modify the system). \n    \n27.  **[^](#fnreflvpb3i20pis)**\n    \n    We do not expect this to scale up to AGI/ASI but this is not our purpose here. Our purpose is to provide an alternative guiding principle for AI trajectory.",
  "reviews": [
    {
      "agentId": "clarity-coach",
      "costInCents": 0,
      "createdAt": "2025-04-17T04:25:46.521Z",
      "runDetails": "{\"model\":\"google/gemini-2.0-flash-001\",\"promptTokens\":13811,\"completionTokens\":2147,\"totalTokens\":15958,\"temperature\":0.1,\"runtimeMs\":12211}",
      "thinking": "This document introduces the concept of Bounded AI (BAI) as an alternative to Unbounded AI (UAI) to mitigate potential catastrophic risks associated with advanced AI systems. The core idea is to constrain AI capabilities within a \"Goldilocks Zone,\" balancing utility with safety by limiting the AI's scope and ensuring interruptibility. The document explores the viability of BAI, discusses its relation to other AI safety concepts, and outlines open questions and research directions.\n\nAs Clarity Coach, my focus is on evaluating the document's clarity, coherence, and readability. I will assess how effectively the authors communicate the concept of BAI, its potential benefits, and the challenges associated with its implementation. I will also identify areas where the writing could be improved to enhance understanding and engagement.\n\nI will pay close attention to the following aspects:\n\n1.  **Definition of Bounded AI:** How clearly and concisely is BAI defined? Are the key characteristics (bounded domain, constraints, interruptibility) well-explained?\n2.  **Goldilocks Zone Analogy:** How effective is the Goldilocks Zone analogy in illustrating the concept of capability constraints? Is the visual representation helpful?\n3.  **Comparison with Unbounded AI:** Is the contrast between BAI and UAI clearly articulated? Are the risks associated with UAI effectively conveyed?\n4.  **Examples:** Are the examples of BAI applications (lab assistant, OS-integrated LLM assistant, SWE agent, AI surgeon) concrete and illustrative? Do they help the reader understand the practical implications of BAI?\n5.  **Relation to Other Ideas:** How well does the document connect BAI to existing AI safety concepts like Tool AI, cognitive emulations, and Guaranteed Safe AI? Are the similarities and differences clearly explained?\n6.  **Open Questions and Research Directions:** Are the open questions and research directions clearly articulated? Do they provide a clear roadmap for future work?\n7.  **Overall Structure and Flow:** Is the document well-organized and easy to follow? Are the transitions between sections smooth and logical?\n8.  **Readability:** Is the language accessible to a broad audience? Are technical terms clearly defined? Is the writing concise and engaging?\n\nI will provide specific comments and suggestions for improvement, focusing on enhancing clarity, coherence, and readability. I will also provide an overall grade based on the document's effectiveness in communicating the concept of Bounded AI.",
      "summary": "This document introduces the concept of Bounded AI (BAI) as a risk mitigation strategy. While the document presents a compelling case for BAI, there are areas where the clarity, coherence, and readability could be improved. Specifically, the initial motivation could be more concise, the connection between boundedness and interruptibility could be clarified, the definition of \"unacceptable damage\" could be more precise, the safety measures for examples could be elaborated, the transitions between sections could be strengthened, jargon should be explained, the limitations of Bounded AI should be explicitly stated, and key arguments should be summarized at the end of sections. Addressing these issues would enhance the document's effectiveness in communicating the concept of Bounded AI and its potential benefits.",
      "comments": [
        {
          "title": "Readability: Avoid Jargon or Explain It Clearly",
          "description": "The document uses some technical terms and concepts (e.g., \"Goodhart phenomenon,\" \"chain-of-thought interpretability,\" \"latent space\") that may not be familiar to all readers. Providing brief explanations or definitions of these terms would improve the accessibility of the document.",
          "importance": "70",
          "grade": "60",
          "highlight": {
            "startOffset": -1,
            "endOffset": 15,
            "quotedText": "(_Work done at_"
          },
          "isValid": true
        },
        {
          "title": "Clarity: Initial Motivation Could Be More Concise",
          "description": "The opening paragraph presents a complex hypothetical scenario involving pausing AI progress and solving technical problems. While it sets the stage for the discussion, it could be more concise and direct to immediately capture the reader's attention. Consider starting with a more straightforward statement of the problem that Bounded AI aims to address.",
          "importance": "75",
          "grade": "65",
          "highlight": {
            "startOffset": 261,
            "endOffset": 666,
            "quotedText": "Suppose the perspective of pausing or significantly slowing down AI progress or solving the technical problems necessary to ensure that arbitrarily strong AI has good effects on humanity (in time, before we get such systems) both [look gloomy](https://www.lesswrong.com/posts/XTdByFM6cmgB3taEN/key-takeaways-from-our-ea-and-alignment-research-surveys).[\\[1\\]](#fnezdqnb8row9) What options do we have left?"
          },
          "isValid": true
        },
        {
          "title": "Coherence: Clarify the Connection Between Boundedness and Interruptibility",
          "description": "The document mentions that boundedness and interruptibility are related, but the nature of this relationship could be further clarified. Providing a more explicit explanation of how boundedness facilitates interruptibility would strengthen the argument for Bounded AI.",
          "importance": "80",
          "grade": "60",
          "highlight": {
            "startOffset": 10266,
            "endOffset": 10549,
            "quotedText": "Importantly, boundedness and interruptibility are related. The bigger the system's possible effects or the more uncertain we are about whether its effects encroach upon dangerous territory, the stronger assurance of its interruptibility we want to have (everything else being equal)."
          },
          "isValid": true
        },
        {
          "title": "Readability: Define \"Unacceptable Damage\" More Precisely",
          "description": "The term \"unacceptable damage\" is used frequently but defined vaguely. While the authors acknowledge this vagueness, providing a more concrete definition or a set of criteria for determining what constitutes unacceptable damage would improve the clarity and practical applicability of the Bounded AI concept.",
          "importance": "85",
          "grade": "55",
          "highlight": {
            "startOffset": 12156,
            "endOffset": 10,
            "quotedText": "e at_ [_Convergence Analysis_](https://www.convergenceanalysis.org/)_. Mateusz wrote the post and is responsible for most of the ideas with Justin helping to think it through. Thanks to Olga Babeeva for the feedback on this post._)\n\n# 1\\. Motivation\n\nSuppose the perspective of pausing or significantly slowing down AI progress or solving the technical problems necessary to ensure that arbitrarily strong AI has good effects on humanity (in time, before we get such systems) both [look gloomy](https://www.lesswrong.com/posts/XTdByFM6cmgB3taEN/key-takeaways-from-our-ea-and-alignment-research-surveys).[\\[1\\]](#fnezdqnb8row9) What options do we have left?\n\nAdam Shimi presents a useful frame on the alignment problem in [_Abstracting The Hardness of Alignment: Unbounded Atomic Optimization_](https://www.lesswrong.com/posts/uhxpJyGYQ5FQRvdjY/abstracting-the-hardness-of-alignment-unbounded-atomic):\n\n> alignment \\[is\\] **the problem of dealing with impact on the world (optimization) that is both of unknown magnitude (unbounded) and non-interruptible (atomic)**.\n\nIf the problem is about some system (or a collection of systems) having an unbounded, non-interruptible impact,[\\[2\\]](#fnwt3gswm2i8m) can we handle it by ensuring that the impact of the systems in question is reliably **bounded** and **interruptible** (even if the systems are [scheming](https://www.lesswrong.com/tag/deceptive-alignment) against us, or trying to \"liberate\" themselves from the properties of boundedness and interruptibility)?\n\nThe core idea is to impose a multidimensional bound on the system's capabilities such that:\n\n*   it is sufficiently capable along the dimensions corresponding to the capabilities we want to extract from it   \n    _but_ \n*   sufficiently **in**capable along the dimensions corresponding to capabilities that would (in combination with the capabilities we want it to have) grant it the dangerous power of unbounded, non-interruptible optimization.[\\[3\\]](#fnpixsa40cqwi) \n    \n\nIn other words, if we can't solve the problem of aligning unbounded, non-interruptible optimizers, we may be able to [avoid](https://www.lesswrong.com/posts/AFdvSBNgN2EkAsZZA/what-is-it-to-solve-the-alignment-problem-1#1__Avoiding_vs__handling_vs__solving_the_problem) this problem by:\n\n*   _not_ building unbounded, non-interruptible optimizers  \n    _and,_ instead,\n*   building some other, safer, kind of AI that can be demonstrated to deliver enough value to make up for the giving up on the business-as-usual kind of AI along with the benefits it was expected to deliver (that \"we\", though not necessarily its creators, expect might/would lead to the creation of unbounded, non-interruptible AI posing a catastrophic risk),.\n\nUnfortunately, at the current moment, frontier AI labs don't take actions sufficient to mitigate the risks from unbounded non-interruptible optimizers robustly. In business-as-usual futures, they will keep pushing the capabilities they expect to be most profitable while investing in myopic safety efforts prescribed by the [incrementalist metastrategy](https://www.lesswrong.com/posts/TALmStNf6479uTwzT/ai-alignment-metastrategy#Incrementalist_Metastrategy), sacrificing long-term safety from catastrophes for short-term just-enough-safety-to-be-reliably-profitable (on some short-ish timescale). The biggest AI labs, such as [OpenAI](https://openai.com/index/planning-for-agi-and-beyond/), Google DeepMind, and Anthropic, are not interested in systems that are safe by their boundedness. They think AGI/ASI (corresponding to unbounded, non-interruptible optimization[\\[4\\]](#fnzedhx5fdie)) is the best way to harness the benefits of superintelligence.[\\[5\\]](#fn3e5r1dwzioj)[\\[6\\]](#fnptozvqb0qj)\n\nHowever, suppose an alternative was shown to be viable. In that case, it might [trigger a narrative bifurcation](https://www.lesswrong.com/posts/sTiKDfgFBvYyZYuiE/my-guess-at-conjecture-s-vision-triggering-a-narrative) in which \"[race to AGI](https://www.lesswrong.com/posts/KPBPc7RayDPxqxdqY/china-hawks-are-manufacturing-an-ai-arms-race)\" is dethroned as the dominant frame and replaced with the idea of designing systems such that we can reliably reason about their possible effects, with a minimal sacrifice of short-term profits, relative to the promise of business-as-usual AI R&D.\n\nWith this motivation in mind, we introduce the concept of **Bounded AI (BAI)**, i.e. a kind of AI system that (for now, speaking loosely),\n\n*   can have **large effects** (i.e. sufficient power to realize the benefits of superintelligence)  \n    _but_\n*   those large effects are confined to a chosen **domain of application**  \n    _and_\n*   within that domain of application, the system's effects **obey certain constraints** (to prevent large damage even within said domain).[\\[7\\]](#fnt6at4nrf49)\n    \n\nWe contrast Bounded AI with **Unbounded AI (UAI)**, i.e. a kind of AI system on the capabilities of which we can't put justified upper bounds that would exclude its ability to cause catastrophic outcomes.\n\nWe can think of UAI as _unbounded and non-interruptible_ (\"atomic\") in the sense that Adam Shimi discusses in his [UAO post](https://www.lesswrong.com/posts/uhxpJyGYQ5FQRvdjY/abstracting-the-hardness-of-alignment-unbounded-atomic).[\\[8\\]](#fnk8h1f8k88zg)\n\nThe next section develops the idea of Bounded AI in more detail, looking at it from several perspectives, and providing some examples to illustrate it. Section 3 relates Bounded AI to prior ideas and agendas. Section 4 lays out some open cruxes, questions, and potentially valuable further research directions.\n\n# 2\\. Bounded AI\n\n## 2.1 The Goldilocks Zone of Capabilities\n\nAn illustrative way to think about the kind of boundedness we are interested in is the \"Goldilocks Zone of Capabilities\".\n\nImagine a high-dimensional space of all AI capabilities, each dimension corresponding to some specific capability. Some combinations of capabilities generate the central AI catastrophic risks associated with unbounded non-interruptible optimization. This is a region we don't want to enter, Bounded AI systems are those that remain below its \"floor\".\n\nThere is another important threshold — that of capability combinations that can deliver a significant portion of the benefits of superintelligence.\n\nThe Goldilocks Zone of Capabilities is a region \"sandwiched\" between these two thresholds. It contains systems that can deliver the benefits of superintelligence but do not constitute a source of central AI catastrophic risk: harm and loss of control due to unbounded non-interruptible optimization.\n\nIn a toy universe where there are only two relevant AI capabilities, this\n\nGoldilocks Zone might look like this:\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Z5YGZwdABLChoAiHs/abw5emhb1hze2xpmn8f1)\n\nOr perhaps like this:\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Z5YGZwdABLChoAiHs/qe1ugwkirqe1hme6tx4o)\n\nThe purpose of the Bounded AI concept is to allow us to aim at this zone more reliably. It relies on a hypothesis that we might call the \"**Goldilocks Zone of Capabilities Conjecture**\": _It is possible to safely elicit the key benefits of superintelligence using AI systems that remain confined to the Goldilocks Zone of Capabilities_. By assumption, the capability profiles of AI systems falling within this zone will be \"spiked\": high on some capabilities, relatively low on others implying that each would only be useful in some domain(s) of application and not very useful in other domains.\n\nTo the extent that we are uncertain about whether the system's capabilities are below a satisfying upper bound, we want to put another layer of protection. Namely, we want to ensure that we will be able to interrupt the system in time if we notice that its effects are likely to lead to undesirable consequences. The assumption that before the system's effects become unacceptable, they will first become noticeable and that upon noticing them, we will be able to prevent them, corresponds to some fragment of this space, probably something like an upper shell of the Goldilocks Zone of Capabilities where we are, metaphorically speaking, playing with fire but can ensure that we have a sufficient number of extinguishers in the stock to prevent an accident from escalating to a conflagration. A lower bound on interruptibility thus follows from an upper bound on capabilities/effects.\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Z5YGZwdABLChoAiHs/qt5lgbflydfuuqlwpah6)\n\nThis is the motivation for how BAI may allow us to avoid catastrophic risks if adopted widely. To be adopted widely, it needs (among other things) to be shown to be sufficiently capable and safe, which, in general, [is not the default](https://www.lesswrong.com/posts/2NaAhMPGub8F2Pbr7/the-fusion-power-generator-scenario#Corollary__Tool_AI_Is_Not_Inherently_Safe). Therefore, the next two sections focus on ensuring more \"mundane\" safety.\n\n## 2.2 Four quadrants of Bounded AI\n\nTo limit the undesirable effects of an AI system, we want to ensure that its effects (both within and outside of the domain) remain confined to a safe, _bounded_ region and also that their escalation into unacceptable disasters _interruptible_ by human operators.\n\nWe can think about it in terms of the following 2×2 matrix.\n\n \n\n(un)bounded\n\n(non-)interruptible\n\ninside the domain\n\nWhat are the bounds on the system's effects _inside_ the domain?\n\nHow interruptible are the system's effects _inside_ the domain?\n\noutside the domain\n\nWhat are the bounds on the system's effects _outside_ the domain?\n\nHow interruptible are the system's effects _outside_ the domain?\n\nTo each AI system (perhaps consisting of multiple components, such as one or more ML models, an \"agent\" scaffolding, [RAG](https://en.wikipedia.org/wiki/Retrieval-augmented_generation), \"non-AI\" filters being applied to the model's outputs, or a procedure involving some degree of human oversight) we can assign a \"spec\" consisting of a value[\\[9\\]](#fnvr8j10277h) for each cell in the matrix. An AI system is a Bounded AI if its spec is sufficient to infer that its deployment is not going to cause unacceptable damage. Importantly, boundedness and interruptibility are related. The bigger the system's possible effects or the more uncertain we are about whether its effects encroach upon dangerous territory, the stronger assurance of its interruptibility we want to have (everything else being equal).\n\nWhatever the domain is, we most likely have less control over what's going on in the \"external world\" than over what's going on in the domain itself. Therefore, we want to have stronger bounds on the effects outside the domain. For this to be the case, it should be effectively impossible for the inside-domain effects to \"diffuse\" outside of the domain. We can think about it in terms of a boundary between the domain and the external world or, alternatively, in terms of there not being an interface between them that would allow the effects to diffuse to the outside (depending on which frame is more appropriate for the particular case). Depending on the situation, this boundary or lack of interface may be the default or may need to be deliberately established.\n\nThe extent to which the effects need to be bounded and interruptible depends on the domain of application or, more specifically, the possibility of causing unacceptable damage in the domain.\n\nImportantly, to build a Bounded AI for a domain, we need to know enough about that domain to ensure that the system's effects are sufficiently bounded and/or interruptible. This rules out domains with large downside risks where our understanding is still very poor, such as [automating AI safety/alignment research](https://www.lesswrong.com/posts/8wBN8cdNAv3c7vt6p/the-case-against-ai-control-research). However, Bounded AI might be useful for automating some _subtasks_ involved in that kind of research, as long as those subtasks remain in a subdomain that can be reliably overseen and/or verified.\n\n## 2.3 High-risk domains of application\n\n"
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Coherence: Strengthen Transitions Between Sections",
          "description": "Some transitions between sections could be smoother. For example, the transition from Section 2.4 (Is it viable?) to Section 2.5 (Examples) could be improved by explicitly stating how the examples relate to the viability discussion.",
          "importance": "65",
          "grade": "65",
          "highlight": {
            "startOffset": 15751,
            "endOffset": 21341,
            "quotedText": "be made interruptible. (This is a restatement of the \"conjecture\" from the previous section with more bells and whistles.)\n\nThis is a challenge on both the technical as well as the governance side. In this section, we cover some reasons why we think that the technical challenge is likely tractable.\n\n### 2.4.1 Boundedness\n\nAs of early 2025, we've had powerful and generally knowledgeable models — i.e. LLMs — for two years (i.e. since the release of GPT-4). So far, their impact on the economy has been limited. While they are helpful in a few domains on the margin (e.g. code generation), as Cole Wyeth writes, [_they haven't done anything_](https://www.lesswrong.com/posts/vvgND6aLjuDR6QzDF/my-model-of-what-is-going-on-with-llms) [_important yet_](https://www.lesswrong.com/posts/GADJFwHzNZKg2Ndti/have-llms-generated-novel-insights).\n\nIn the current regime, a lot of [schlep](https://www.planned-obsolescence.org/scale-schlep-and-systems/)/[integration](https://www.lesswrong.com/posts/7L8ZwMJkhLXjSa7tD/the-great-data-integration-schlep) is often required to make a very smart and knowledgeable LLM useful for a non-trivial application that is not the one it wasn't trained to perform in.[\\[12\\]](#fnc4ix8zq8vyv) This might change soon if AI models acquire capabilities sufficient to integrate themselves into pre-existing systems with minimal or close to non-existent involvement of human workers, or if some infrastructure is designed that minimizes the friction of integrating a new AI component.[\\[13\\]](#fnlkrcgro8ake) However, at least for now, general knowledge and general intelligence (at least the LLM variant of general intelligence) do not jointly imply \"general domain mastery\" or _general ability to acquire any relevant capability_.\n\nWhile scaling makes models generally more performant on various tasks and benchmarks, there is also some dissociation between capabilities, as can be seen in the o1 line of models being worse than GPT-4o on some language understanding tasks.[\\[14\\]](#fn9fxqlsthu6a) This suggests that for a given effective model size, there are some trade-offs between competence in domains.[\\[15\\]](#fnt2uitmy3jv)\n\nThe shape of LLM capabilities is different from that of human capabilities (\"jagged frontier\"). Surprising generalization failures are present even in seemingly advanced systems. For example, o1 competently answers PhD-level questions but [can't play tic-tac-toe optimally, even if taught it in-context](https://www.lesswrong.com/posts/hikSpxJgMsLoDRTdx/rohans-s-shortform?commentId=NSCFjxZM6gWpRmy2w).[\\[16\\]](#fn30wt48t4p3m) DeepResearch, on the other hand, can't add 4-digit numbers (a task that is trivial for GPT-4o)[\\[17\\]](#fn7yo8tel97ba) and [confabulates the answer](https://x.com/littmath/status/1891868756340547809) when given a task that is too difficult or too far out of its training distribution.\n\nPre-training oriented for drug research doesn't need to involve knowledge of politics or society that would be useful for violating the boundary of the domain (e.g. via superhuman persuasion). Admittedly, imposing such restrictions might require some progress in AI governance.\n\nCurrently, we get some of this dissociation between general knowledge and general capability \"for free\". Even if this dissociation goes away in the future, we might still be able to deliberately shape the models' capabilities so that they remain within the Goldilocks Zone of Capabilities.\n\n(On a more speculative note, in addition to reducing risks, there is also the possibility that a bounded AI design will leverage resources more efficiently and focus on what is essential for the task at hand.)\n\nFinally, we already have some proof-of-concept AI systems that by default fall within the scope of Bounded AI and are delivering important benefits, e.g. AlphaFold. It's a powerful technology with the potential to remove some bottlenecks in biological research that is not going to to cause an existential catastrophe _on its own_.\n\n### 2.4.2 Interruptibility\n\nBoundedness of a system makes it easier to interrupt its actions if necessary. Probably, in most situations, the bound allowing for reliable interruptibility is higher than the bound ensuring safety, giving us a sort of margin of error.\n\nIf we are deploying the system in a limited domain that we know relatively well, we should also be able to:\n\n*   monitor its behavior to predict its effects\n*   figure out sufficiently early if the effects are likely to be undesirable\n*   reliably intervene to prevent or mitigate those undesirable effects.\n\n### 2.4.3 Properties of the model vs properties of the joint system\n\nWe need to distinguish between the properties of the model itself versus the properties of the joint system.[\\[18\\]](#fnar8aaumdmdq) The model on its own may have a certain capabilities profile but its capabilities profile may change when incorporated into a bigger system. It might expand, e.g. if it's placed in an \"agent scaffolding\". It might also shrink, e.g. if its chain-of-thought starts being monitored for certain kinds of undesirable behavior. It might expand and shrink at the same time, along different axes of the capability space.\n\nIt is the capabilities of the joint system that determine the effects that we care about. The ability to shape capabilities after training gives us some leverage on the capabilities of the joint system, in addition to training the model itself.\n\nHowever, for this post-training capability shaping to work on the model, it needs to be sufficiently incapable along some dimensions in the capability space that it won't "
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Clarity: Elaborate on Safety Measures for Examples",
          "description": "While the examples of Bounded AI applications are helpful, the descriptions of safety measures could be more detailed. Providing more specific and concrete examples of how these safety measures would be implemented and enforced would enhance the reader's understanding of the practical challenges and potential solutions.",
          "importance": "70",
          "grade": "60",
          "highlight": {
            "startOffset": 22054,
            "endOffset": 22365,
            "quotedText": "ing a reaction that damages equipment or produces a drug that turns out to have disastrous consequences (perhaps because of an overly eager search leading to a Goodhart phenomenon).[\\[19\\]](#fnkn1xx7zpo2d)\n    *   **Safety measures:**\n        *   Every experiment proposed by the AI needs to be assessed for saf"
          },
          "isValid": true
        },
        {
          "title": "Coherence: Summarize Key Arguments at the End of Sections",
          "description": "Adding brief summaries at the end of each major section would help reinforce the key arguments and improve the overall coherence of the document. These summaries could highlight the main takeaways and provide a smooth transition to the next section.",
          "importance": "60",
          "grade": "65",
          "highlight": {
            "startOffset": 39570,
            "endOffset": 39623,
            "quotedText": "the domain is constituted by the reporter AI which on"
          },
          "isValid": true
        },
        {
          "title": "Clarity: Explicitly State the Limitations of Bounded AI",
          "description": "While the document acknowledges some challenges and open questions, it could be more explicit about the potential limitations of Bounded AI. Discussing scenarios where Bounded AI might not be sufficient or effective would provide a more balanced and realistic assessment of the approach.",
          "importance": "75",
          "grade": "65",
          "highlight": {
            "startOffset": 42439,
            "endOffset": 42558,
            "quotedText": "d from crossing the domain robustly).\n            \n    *   One research direction would be to investigate the viability"
          },
          "isValid": true
        }
      ],
      "grade": 72
    },
    {
      "agentId": "research-scholar",
      "costInCents": 0,
      "createdAt": "2025-04-17T04:26:00.880Z",
      "runDetails": "{\"model\":\"google/gemini-2.0-flash-001\",\"promptTokens\":14491,\"completionTokens\":1967,\"totalTokens\":16458,\"temperature\":0.1,\"runtimeMs\":10846}",
      "thinking": "This document introduces the concept of Bounded AI (BAI) as a potentially safer alternative to Unbounded AI (UAI). It argues that by carefully limiting the capabilities of AI systems to specific domains and ensuring interruptibility, we can harness the benefits of AI while mitigating catastrophic risks. The document explores the \"Goldilocks Zone of Capabilities,\" discusses four quadrants of BAI, and provides examples of BAI applications. It also relates BAI to other AI safety ideas and outlines open questions and research directions. I will focus on identifying resources that provide further information on capability bounding, interruptibility, and related AI safety concepts. I will also look for resources that discuss the limitations and challenges of these approaches.",
      "summary": "This document introduces Bounded AI (BAI) as a risk mitigation strategy, contrasting it with Unbounded AI (UAI). BAI aims to confine AI systems within specific domains with constrained capabilities and guaranteed interruptibility. The analysis emphasizes the importance of capability bounding and interruptibility, referencing related concepts like Tool AI and Guaranteed Safe AI. The document also highlights open questions and research directions for BAI.",
      "comments": [
        {
          "title": "Resource: Abstracting The Hardness of Alignment",
          "description": "This LessWrong post by Adam Shimi frames the alignment problem as dealing with unbounded and non-interruptible optimization, which is a key motivation for the Bounded AI approach discussed in the document.",
          "importance": 85,
          "grade": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 664,
            "endOffset": 1071,
            "quotedText": "t?\n\nAdam Shimi presents a useful frame on the alignment problem in [_Abstracting The Hardness of Alignment: Unbounded Atomic Optimization_](https://www.lesswrong.com/posts/uhxpJyGYQ5FQRvdjY/abstracting-the-hardness-of-alignment-unbounded-atomic):\n\n> alignment \\[is\\] **the problem of dealing with impact on the world (optimization) that is both of unknown magnitude (unbounded) and non-interruptible (atomic"
          }
        },
        {
          "title": "Resource: AI Alignment Metastrategy",
          "description": "This LessWrong post discusses the incrementalist metastrategy in AI alignment, which the document critiques as potentially sacrificing long-term safety for short-term profitability. Understanding this critique is important for evaluating the motivation behind Bounded AI.",
          "importance": 75,
          "grade": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 3190,
            "endOffset": 3327,
            "quotedText": "sacrificing long-term safety from catastrophes for short-term just-enough-safety-to-be-reliably-profitable (on some short-ish timescale)."
          }
        },
        {
          "title": "Resource: Tool AI",
          "description": "The document references the concept of Tool AI and its limitations. Exploring the original arguments for and against Tool AI provides context for understanding the Bounded AI proposal as a potential improvement or alternative.",
          "importance": 80,
          "grade": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 27898,
            "endOffset": 28048,
            "quotedText": "The concept of Tool AI emerged in discussions around the Machine Intelligence Research Institute (MIRI; back then still called Singularity Institute)."
          }
        },
        {
          "title": "Resource: Guaranteed Safe AI",
          "description": "The document mentions Guaranteed Safe AI (GSAI) as a related approach. Investigating GSAI provides insights into formal verification and safety guarantees for AI systems, which are relevant to the interruptibility and safety aspects of Bounded AI.",
          "importance": 70,
          "grade": 65,
          "isValid": false,
          "highlight": {
            "startOffset": 34774,
            "endOffset": 36037,
            "quotedText": "[Guaranteed Safe AI](https://arxiv.org/abs/2405.06624) (GSAI)\n\nQuoting the abstract:\n\n> Ensuring that AI systems reliably and robustly avoid harmful or dangerous behaviours is a crucial challenge, especially for AI systems with a high degree of autonomy and general intelligence, or systems used in safety-critical contexts. In this position paper, we will introduce and define a family of approaches to AI safety, which we will refer to as guaranteed safe (GS) AI. The core feature of these approaches is that they aim to produce AI systems which are equipped with high assurance quantitative safety guarantees. This is achieved by the interplay of three core components: a world model (which provides a mathematical description of how the AI system affects the outside world in a way that appropriately handles both Bayesian and Knightian uncertainty), a safety specification (which is a mathematical description of what effects are acceptable), and a verifier (which provides an auditable proof certificate that the AI satisfies the safety specification relative to the world model). We outline a number of approaches for creating each of these three core components, describe the main technical challenges, and suggest a number of potential solutions to them."
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Resource: Open Agency Architecture",
          "description": "The document mentions Open Agency Architecture (OAA) as a specific variant of GSAI. Understanding OAA provides a concrete example of how formal verification and world models can be used to ensure AI safety, which is relevant to the Bounded AI concept.",
          "importance": 65,
          "grade": 60,
          "isValid": true,
          "highlight": {
            "startOffset": 36316,
            "endOffset": 36782,
            "quotedText": "[Open Agency Architecture](https://www.lesswrong.com/posts/jRf4WENQnhssCb6mJ/davidad-s-bold-plan-for-alignment-an-in-depth-explanation) (OAA) \n\n> **Plan's tldr:** Utilize near-AGIs to build a detailed world simulation, train and formally verify within it that the \\[AGI\\] adheres to coarse preferences and avoids catastrophic outcomes.\n\nMore specifically, the AI proposes policies (courses of action) that are then evaluated or tested in a comprehensive world model."
          }
        },
        {
          "title": "Resource: Comprehensive AI Services",
          "description": "The document mentions Comprehensive AI Services (CAIS) as a related concept. Exploring CAIS provides insights into the idea of AI systems delivering bounded results within bounded resources and time, which aligns with the Bounded AI approach.",
          "importance": 60,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 37248,
            "endOffset": 37692,
            "quotedText": "[Comprehensive AI Services](https://www.lesswrong.com/tag/ai-services-cais) (CAIS)\n\n> **AI Service** (as used in the context of Eric Drexler's technical report [Reframing Superintelligence: Comprehensive AI Services as General Intelligence](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf?asd=sa)) is an AI system that delivers bounded results for some task using bounded resources in bounded time."
          }
        },
        {
          "title": "Resource: Sumerian Containment",
          "description": "The document mentions Sumerian Containment as a special case of Bounded AI. While potentially not a serious proposal, it highlights the idea of limiting an AI's capabilities through language and restricted access, which is relevant to the capability bounding aspect of Bounded AI.",
          "importance": 55,
          "grade": 50,
          "isValid": true,
          "highlight": {
            "startOffset": 37284,
            "endOffset": 37847,
            "quotedText": "www.lesswrong.com/tag/ai-services-cais) (CAIS)\n\n> **AI Service** (as used in the context of Eric Drexler's technical report [Reframing Superintelligence: Comprehensive AI Services as General Intelligence](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf?asd=sa)) is an AI system that delivers bounded results for some task using bounded resources in bounded time. It is contrasted with agentic AGI, which carries out open-ended goals over an unbounded period of time.\n\n## [Sumerian Containment](https://www.lesswrong.c"
          }
        }
      ],
      "grade": 75
    },
    {
      "agentId": "quantitative-forecaster",
      "costInCents": 0,
      "createdAt": "2025-04-17T04:26:08.911Z",
      "runDetails": "{\"model\":\"google/gemini-2.0-flash-001\",\"promptTokens\":13661,\"completionTokens\":1244,\"totalTokens\":14905,\"temperature\":0.1,\"runtimeMs\":7961}",
      "thinking": "This document introduces the concept of Bounded AI (BAI) as an alternative to Unbounded AI (UAI) to mitigate potential catastrophic risks. The core idea is to constrain AI systems within a \"Goldilocks Zone of Capabilities,\" where they can deliver significant benefits without posing unacceptable risks. The document explores the viability of BAI, discusses its relation to other AI safety ideas, and raises open questions for further research. \n\nAs a quantitative forecaster, I see several opportunities to translate the concepts discussed into concrete, measurable predictions. The \"Goldilocks Zone of Capabilities Conjecture\" is a key area where forecasting can be applied. We can formulate questions about the feasibility and stability of maintaining AI systems within this zone. The document also touches on the economic incentives and governance challenges, which can be modeled and forecasted. I will focus on identifying these opportunities and suggesting ways to quantify the uncertainties involved.",
      "summary": "From a quantitative forecasting perspective, this document presents several opportunities to translate abstract concepts into concrete, measurable predictions. The feasibility of Bounded AI hinges on factors like the size and stability of the \"Goldilocks Zone of Capabilities,\" the dissociation of AI capabilities, and the effectiveness of interruptibility measures. Forecasting questions can be formulated to assess the likelihood of a narrative shift towards BAI, the adoption of capability limitations, and the performance differences across AI domains. Fermi estimates and reference class forecasting can provide rough quantitative assessments of the Goldilocks Zone and the success rate of BAI projects. Bayesian reasoning can be used to update beliefs about interruptibility as new evidence emerges. By applying these techniques, we can gain a more precise understanding of the uncertainties involved in the BAI approach and its potential to mitigate AI risks.",
      "comments": [
        {
          "title": "Forecasting Question: Probability of Narrative Bifurcation",
          "description": "The document suggests that demonstrating the viability of Bounded AI could trigger a \"narrative bifurcation,\" shifting the focus from a \"race to AGI\" to safer AI development. This is a crucial point where forecasting can help assess the likelihood of such a shift. A forecasting question could be: \"Will a significant (e.g., >20%) portion of AI research funding be directed towards Bounded AI approaches by 2030?\"",
          "importance": "85",
          "highlight": {
            "startOffset": 3810,
            "endOffset": 4317,
            "quotedText": "trigger a narrative bifurcation](https://www.lesswrong.com/posts/sTiKDfgFBvYyZYuiE/my-guess-at-conjecture-s-vision-triggering-a-narrative) in which \"[race to AGI](https://www.lesswrong.com/posts/KPBPc7RayDPxqxdqY/china-hawks-are-manufacturing-an-ai-arms-race)\" is dethroned as the dominant frame and replaced with the idea of designing systems such that we can reliably reason about their possible effects, with a minimal sacrifice of short-term profits, relative to the promise of business-as-usual AI R&D."
          },
          "isValid": true
        },
        {
          "title": "Fermi Estimate: Size of the Goldilocks Zone",
          "description": "The concept of the \"Goldilocks Zone of Capabilities\" is central to the BAI approach. A Fermi estimate could be used to approximate the size of this zone, considering factors like the number of relevant AI capabilities, the range of acceptable values for each capability, and the correlation between capabilities. This would provide a rough quantitative sense of how feasible it is to confine AI systems within this zone.",
          "importance": "75",
          "highlight": {
            "startOffset": 5629,
            "endOffset": 6511,
            "quotedText": "Goldilocks Zone of Capabilities\n\nAn illustrative way to think about the kind of boundedness we are interested in is the \"Goldilocks Zone of Capabilities\".\n\nImagine a high-dimensional space of all AI capabilities, each dimension corresponding to some specific capability. Some combinations of capabilities generate the central AI catastrophic risks associated with unbounded non-interruptible optimization. This is a region we don't want to enter, Bounded AI systems are those that remain below its \"floor\".\n\nThere is another important threshold — that of capability combinations that can deliver a significant portion of the benefits of superintelligence.\n\nThe Goldilocks Zone of Capabilities is a region \"sandwiched\" between these two thresholds. It contains systems that can deliver the benefits of superintelligence but do not constitute a source of central AI catastrophic risk:"
          },
          "isValid": true
        },
        {
          "title": "Probability Estimate: Dissociation of Capabilities",
          "description": "The viability of BAI depends on the dissociation between general knowledge and general capability. A forecasting question could be: \"What is the probability that AI models will exhibit significant (e.g., >20%) performance differences across different domains, even with similar effective model sizes, by 2027?\"",
          "importance": "80",
          "highlight": {
            "startOffset": 17483,
            "endOffset": 17378,
            "quotedText": "f general intelligence) do not jointly imply \"general domain mastery\" or _general ability to acquire any "
          },
          "isValid": true
        },
        {
          "title": "Reference Class Forecasting: Success Rate of Bounded AI Projects",
          "description": "The document mentions several examples of Bounded AI, such as AlphaFold. We can use reference class forecasting to estimate the success rate of future BAI projects. This involves identifying a relevant reference class (e.g., AI projects in specific domains) and analyzing their historical success rates, adjusting for any differences between the reference class and the specific BAI project in question.",
          "importance": "70",
          "highlight": {
            "startOffset": 19554,
            "endOffset": 19731,
            "quotedText": "AlphaFold. It's a powerful technology with the potential to remove some bottlenecks in biological research that is not going to to cause an existential catastrophe _on its own_."
          },
          "isValid": true
        },
        {
          "title": "Bayesian Reasoning: Updating Beliefs about Interruptibility",
          "description": "The document emphasizes the importance of interruptibility. Bayesian reasoning can be used to update our beliefs about the interruptibility of AI systems as we gather more evidence. This involves starting with a prior belief about interruptibility, then updating this belief based on new data from monitoring the system's behavior.",
          "importance": "65",
          "highlight": {
            "startOffset": 19743,
            "endOffset": 19997,
            "quotedText": "Interruptibility\n\nBoundedness of a system makes it easier to interrupt its actions if necessary. Probably, in most situations, the bound allowing for reliable interruptibility is higher than the bound ensuring safety, giving us a sort of margin of error."
          },
          "isValid": true
        },
        {
          "title": "Forecasting Question: Adoption of Capability Limitations",
          "description": "The document suggests that domain-specific AI applications could be required to have specifications including capability limitations. A forecasting question could be: \"Will regulatory bodies in major AI-developing countries (e.g., US, China, EU) mandate capability limitations for AI systems in high-risk domains by 2030?\"",
          "importance": "75",
          "highlight": {
            "startOffset": 42150,
            "endOffset": 42476,
            "quotedText": "domain-specific \"high-risk\" applications can be required to have specifications including capabilities limitations, safety measures, and ontology specification (involving, (i.a.) the domain and the boundary between the domain and the environment, as well as how the system will be prevented from crossing the domain robustly)."
          },
          "isValid": true
        }
      ],
      "grade": 75
    },
    {
      "agentId": "ea-impact-evaluator",
      "costInCents": 0,
      "createdAt": "2025-04-17T04:27:16.024Z",
      "runDetails": "{\"model\":\"google/gemini-2.0-flash-001\",\"promptTokens\":14514,\"completionTokens\":3406,\"totalTokens\":17920,\"temperature\":0.1,\"runtimeMs\":18574}",
      "thinking": "This post introduces the concept of Bounded AI (BAI) as a potentially safer alternative to Unbounded AI (UAI), which is the current trajectory of most AI research. The core idea is to constrain AI systems within a \"Goldilocks Zone of Capabilities\" where they can deliver significant benefits without posing catastrophic risks. The post explores different dimensions of boundedness and interruptibility, provides examples of BAI applications, and relates the concept to other AI safety agendas. It also raises open questions and research directions. nnAs an EA Impact Evaluator, my focus is on assessing the potential impact of this concept on reducing existential risks from AI and improving the long-term future. I will evaluate the importance, neglectedness, and tractability of BAI as an AI safety strategy, assess its novelty and intellectual contribution, and develop a Fermi estimate of its potential impact on reducing x-risk. I will also consider potential negative impacts and compare BAI to other AI safety approaches.nnHere's my plan:nn1. **Importance:** Assess the potential scale and severity of the problem BAI addresses (AI x-risk), the leveraged impact it could have on AI development, and its long-term implications.n2. **Neglectedness:** Evaluate how much attention and funding BAI is currently receiving compared to other AI safety approaches, and whether it fills important knowledge gaps.n3. **Tractability:** Analyze the feasibility of implementing BAI, the evidence supporting its effectiveness, and its scalability.n4. **Novelty:** Identify the key innovations of BAI, how it differs from existing approaches, and its intellectual contribution.n5. **Fermi Estimation:** Develop a Fermi model to estimate the potential impact of BAI on reducing AI x-risk, considering factors such as the probability of success, the magnitude of risk reduction, and potential negative impacts.nnI will pay close attention to the assumptions made in the post and the evidence provided to support them. I will also consider potential counterarguments and limitations of the BAI approach. Finally, I will compare BAI to other AI safety strategies and assess its overall potential to improve the long-term future.",
      "summary": "From an EA perspective, this post introduces a valuable concept (Bounded AI) that could potentially reduce existential risks from AI. The core idea of constraining AI systems within a \"Goldilocks Zone of Capabilities\" is promising, but its feasibility and impact are highly uncertain. The Fermi estimate suggests that Bounded AI, if widely adopted and successful, could reduce annual x-risk from AI by 0.03 percentage points, with a dollar-equivalent value of $300 million. However, there are also potential negative impacts to consider, such as diverting resources from other safety approaches. Overall, Bounded AI is a worthwhile area of research and development, but it should be pursued in conjunction with other AI safety strategies.",
      "comments": [
        {
          "title": "Neglectedness: Open Questions and Research Directions",
          "description": "This section highlights the many open questions and research directions related to Bounded AI, including how to put bounds on capabilities, ensure interruptibility, and measure the Goldilocks Zone. This indicates that BAI is a relatively neglected area of research with significant potential for further exploration.",
          "importance": "70",
          "grade": "65",
          "highlight": {
            "startOffset": -1,
            "endOffset": 78,
            "quotedText": "(_Work done at_ [_Convergence Analysis_](https://www.convergenceanalysis.org/)"
          },
          "isValid": true
        },
        {
          "title": "Importance: Framing the Alignment Problem",
          "description": "This section highlights the core challenge of AI alignment: dealing with systems that have unbounded and non-interruptible impact. It sets the stage for the Bounded AI proposal as a way to avoid this problem by creating systems with reliably bounded and interruptible impact.",
          "importance": "90",
          "grade": "85",
          "highlight": {
            "startOffset": 909,
            "endOffset": 1324,
            "quotedText": ":\n\n> alignment \\[is\\] **the problem of dealing with impact on the world (optimization) that is both of unknown magnitude (unbounded) and non-interruptible (atomic)**.\n\nIf the problem is about some system (or a collection of systems) having an unbounded, non-interruptible impact,[\\[2\\]](#fnwt3gswm2i8m) can we handle it by ensuring that the impact of the systems in question is reliably **bounded** and **interrupti"
          },
          "isValid": true
        },
        {
          "title": "Importance: Motivation for Bounded AI",
          "description": "This section argues that current AI labs are not sufficiently mitigating risks from unbounded AI and that an alternative approach like Bounded AI could trigger a narrative shift towards safer AI development.",
          "importance": "85",
          "grade": "80",
          "highlight": {
            "startOffset": 2729,
            "endOffset": 4278,
            "quotedText": "\nUnfortunately, at the current moment, frontier AI labs don't take actions sufficient to mitigate the risks from unbounded non-interruptible optimizers robustly. In business-as-usual futures, they will keep pushing the capabilities they expect to be most profitable while investing in myopic safety efforts prescribed by the [incrementalist metastrategy](https://www.lesswrong.com/posts/TALmStNf6479uTwzT/ai-alignment-metastrategy#Incrementalist_Metastrategy), sacrificing long-term safety from catastrophes for short-term just-enough-safety-to-be-reliably-profitable (on some short-ish timescale). The biggest AI labs, such as [OpenAI](https://openai.com/index/planning-for-agi-and-beyond/), Google DeepMind, and Anthropic, are not interested in systems that are safe by their boundedness. They think AGI/ASI (corresponding to unbounded, non-interruptible optimization[\\[4\\]](#fnzedhx5fdie)) is the best way to harness the benefits of superintelligence.[\\[5\\]](#fn3e5r1dwzioj)[\\[6\\]](#fnptozvqb0qj)\n\nHowever, suppose an alternative was shown to be viable. In that case, it might [trigger a narrative bifurcation](https://www.lesswrong.com/posts/sTiKDfgFBvYyZYuiE/my-guess-at-conjecture-s-vision-triggering-a-narrative) in which \"[race to AGI](https://www.lesswrong.com/posts/KPBPc7RayDPxqxdqY/china-hawks-are-manufacturing-an-ai-arms-race)\" is dethroned as the dominant frame and replaced with the idea of designing systems such that we can reliably reason about their possible effects, with a minimal sacrifice of short-term profits, relative to t"
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Tractability: Goldilocks Zone of Capabilities",
          "description": "The \"Goldilocks Zone of Capabilities\" is a key concept in the Bounded AI framework. It represents the region in capability space where AI systems can deliver benefits without posing catastrophic risks. The conjecture that it's possible to elicit key benefits of superintelligence within this zone is crucial for the viability of BAI.",
          "importance": "95",
          "grade": "90",
          "highlight": {
            "startOffset": 6212,
            "endOffset": 7243,
            "quotedText": " can deliver a significant portion of the benefits of superintelligence.\n\nThe Goldilocks Zone of Capabilities is a region \"sandwiched\" between these two thresholds. It contains systems that can deliver the benefits of superintelligence but do not constitute a source of central AI catastrophic risk: harm and loss of control due to unbounded non-interruptible optimization.\n\nIn a toy universe where there are only two relevant AI capabilities, this\n\nGoldilocks Zone might look like this:\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Z5YGZwdABLChoAiHs/abw5emhb1hze2xpmn8f1)\n\nOr perhaps like this:\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Z5YGZwdABLChoAiHs/qe1ugwkirqe1hme6tx4o)\n\nThe purpose of the Bounded AI concept is to allow us to aim at this zone more reliably. It relies on a hypothesis that we might call the \"**Goldilocks Zone of Capabilities Conjecture**\": _It is possible to safely elicit the key benefits of superintelligence"
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Tractability: Four Quadrants of Bounded AI",
          "description": "This section provides a useful framework for analyzing the boundedness and interruptibility of AI systems, both inside and outside their intended domain. This framework can help developers and policymakers assess the safety of AI systems and identify potential risks.",
          "importance": "80",
          "grade": "75",
          "highlight": {
            "startOffset": 8997,
            "endOffset": 10168,
            "quotedText": "ns focus on ensuring more \"mundane\" safety.\n\n## 2.2 Four quadrants of Bounded AI\n\nTo limit the undesirable effects of an AI system, we want to ensure that its effects (both within and outside of the domain) remain confined to a safe, _bounded_ region and also that their escalation into unacceptable disasters _interruptible_ by human operators.\n\nWe can think about it in terms of the following 2×2 matrix.\n\n \n\n(un)bounded\n\n(non-)interruptible\n\ninside the domain\n\nWhat are the bounds on the system's effects _inside_ the domain?\n\nHow interruptible are the system's effects _inside_ the domain?\n\noutside the domain\n\nWhat are the bounds on the system's effects _outside_ the domain?\n\nHow interruptible are the system's effects _outside_ the domain?\n\nTo each AI system (perhaps consisting of multiple components, such as one or more ML models, an \"agent\" scaffolding, [RAG](https://en.wikipedia.org/wiki/Retrieval-augmented_generation), \"non-AI\" filters being applied to the model's outputs, or a procedure involving some degree of human oversight) we can assign a \"spec\" consisting of a value[\\[9\\]](#fnvr8j10277h) for each cell in the matrix. An AI system is a Bounded AI "
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Importance: High-Risk Domains",
          "description": "This section highlights the importance of considering the risks associated with different domains of application. It argues that in high-risk domains, the development of beneficial capabilities may also bring along dangerous capabilities, making it more difficult to ensure safety.",
          "importance": "75",
          "grade": "70",
          "highlight": {
            "startOffset": 12053,
            "endOffset": 15319,
            "quotedText": "n a subdomain that can be reliably overseen and/or verified.\n\n## 2.3 High-risk domains of application\n\nWe say that a domain of application is _high-risk_ if the capabilities required to deliver great benefits within that domain are hard to disentangle from the capabilities sufficient to cause unacceptable damage within that domain.\n\nWe use \"unacceptable damage\" vaguely meaning \"damage we strongly disprefer\", relying on [the spirit, rather than the letter](https://en.wikipedia.org/wiki/Letter_and_spirit_of_the_law), with catastrophic risks being the most central example of unacceptable damage.[\\[10\\]](#fn1ixps4ej7m)\n\nThe motivation for this concept should be obvious in the context of AI risk.\n\n \n\nLow capability for benefits\n\nHigh capability for benefits\n\nLow capability for damage\n\nIrrelevant from the catastrophic risk perspective.\n\nIrrelevant from the catastrophic risk perspective.\n\nHigh capability for damage\n\nPeople wouldn't bother deploying it.[\\[11\\]](#fnjfpov7ypfd)\n\nDanger zone.\n\nIf the AI is highly capable of causing great benefits as well as causing great damage, we are in a danger zone where the promise of the upsides may encourage unjustified optimism about the system's safety, safety measures (whatever additional things will be appended to the system to ensure it's going to remain safe) will focus on addressing short-term/myopic risks, with fingers crossed that this will at least prepare us better for problems that are harder to address but less urgent, and yet leaving us effectively unprepared for those harder when push comes to shove.\n\nIf the domain is high-risk, i.e. the development of beneficial capabilities will also (by default) bring along dangerous capabilities, then we are more likely to find ourselves in the danger zone.\n\nHigh-risk domains are common. In many situations where automation could produce large benefits, it could also cause a lot of damage, and minimizing the latter while preserving the former is highly non-trivial. Examples include self-driving cars, [software engineering for life-critical systems](https://en.wikipedia.org/wiki/Safety-critical_system#Software_engineering_for_safety-critical_systems), medical research, or psychotherapy, and, finally, using AI to solve problems that seem too difficult for us to solve without AI, especially when this involves the AI being capable of autonomous, non-overseeable action. In these domains, the possible upsides incentivize building highly capable AI, along with the possibility of downsides brought along by those capabilities.\n\nAwareness of downsides creates incentives to safeguard against them. However, mis[calibration](https://www.lesswrong.com/w/calibration) about downsides, along with economic incentives towards myopic safety (addressing short-term risks that are relevant to the time horizon an AI company is planning for, while neglecting long-term risks, including larger/catastrophic risks) in combination with the distribution of downsides being heavy-tailed, can lead to inappropriate preparation against the greatest downsides.\n\nIn a sense, this is similar to ensuring the safety of non-AI technology in high-risk domains. However, it is also critically different because it involves (semi-)intelligent systems (even if bounded in the"
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Tractability: Viability of Boundedness",
          "description": "This section presents arguments for the technical tractability of achieving boundedness in AI systems. It points to the limited economic impact of current LLMs, the dissociation between capabilities, and the possibility of deliberately shaping models' capabilities.",
          "importance": "70",
          "grade": "65",
          "highlight": {
            "startOffset": 15949,
            "endOffset": 19057,
            "quotedText": "In this section, we cover some reasons why we think that the technical challenge is likely tractable.\n\n### 2.4.1 Boundedness\n\nAs of early 2025, we've had powerful and generally knowledgeable models — i.e. LLMs — for two years (i.e. since the release of GPT-4). So far, their impact on the economy has been limited. While they are helpful in a few domains on the margin (e.g. code generation), as Cole Wyeth writes, [_they haven't done anything_](https://www.lesswrong.com/posts/vvgND6aLjuDR6QzDF/my-model-of-what-is-going-on-with-llms) [_important yet_](https://www.lesswrong.com/posts/GADJFwHzNZKg2Ndti/have-llms-generated-novel-insights).\n\nIn the current regime, a lot of [schlep](https://www.planned-obsolescence.org/scale-schlep-and-systems/)/[integration](https://www.lesswrong.com/posts/7L8ZwMJkhLXjSa7tD/the-great-data-integration-schlep) is often required to make a very smart and knowledgeable LLM useful for a non-trivial application that is not the one it wasn't trained to perform in.[\\[12\\]](#fnc4ix8zq8vyv) This might change soon if AI models acquire capabilities sufficient to integrate themselves into pre-existing systems with minimal or close to non-existent involvement of human workers, or if some infrastructure is designed that minimizes the friction of integrating a new AI component.[\\[13\\]](#fnlkrcgro8ake) However, at least for now, general knowledge and general intelligence (at least the LLM variant of general intelligence) do not jointly imply \"general domain mastery\" or _general ability to acquire any relevant capability_.\n\nWhile scaling makes models generally more performant on various tasks and benchmarks, there is also some dissociation between capabilities, as can be seen in the o1 line of models being worse than GPT-4o on some language understanding tasks.[\\[14\\]](#fn9fxqlsthu6a) This suggests that for a given effective model size, there are some trade-offs between competence in domains.[\\[15\\]](#fnt2uitmy3jv)\n\nThe shape of LLM capabilities is different from that of human capabilities (\"jagged frontier\"). Surprising generalization failures are present even in seemingly advanced systems. For example, o1 competently answers PhD-level questions but [can't play tic-tac-toe optimally, even if taught it in-context](https://www.lesswrong.com/posts/hikSpxJgMsLoDRTdx/rohans-s-shortform?commentId=NSCFjxZM6gWpRmy2w).[\\[16\\]](#fn30wt48t4p3m) DeepResearch, on the other hand, can't add 4-digit numbers (a task that is trivial for GPT-4o)[\\[17\\]](#fn7yo8tel97ba) and [confabulates the answer](https://x.com/littmath/status/1891868756340547809) when given a task that is too difficult or too far out of its training distribution.\n\nPre-training oriented for drug research doesn't need to involve knowledge of politics or society that would be useful for violating the boundary of the domain (e.g. via superhuman persuasion). Admittedly, imposing such restrictions might require some progress in AI governance.\n\nCurrently, we get some of this dissociation between general knowledge and general capability \"for free\". Even if this dissociation goes away in the future, we "
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Tractability: Viability of Interruptibility",
          "description": "This section argues that boundedness makes it easier to interrupt AI systems and that in limited domains, it should be possible to monitor behavior, predict effects, and intervene to prevent undesirable outcomes.",
          "importance": "65",
          "grade": "60",
          "highlight": {
            "startOffset": 19761,
            "endOffset": 20306,
            "quotedText": "Boundedness of a system makes it easier to interrupt its actions if necessary. Probably, in most situations, the bound allowing for reliable interruptibility is higher than the bound ensuring safety, giving us a sort of margin of error.\n\nIf we are deploying the system in a limited domain that we know relatively well, we should also be able to:\n\n*   monitor its behavior to predict its effects\n*   figure out sufficiently early if the effects are likely to be undesirable\n*   reliably intervene to prevent or mitigate those undesirable effects."
          },
          "isValid": true
        },
        {
          "title": "Novelty: Relation to Other Ideas",
          "description": "This section connects Bounded AI to other AI safety concepts like Tool AI, cognitive emulations, Guaranteed Safe AI, and Sumerian Containment. This helps to contextualize BAI within the broader AI safety landscape and identify potential synergies.",
          "importance": "60",
          "grade": "55",
          "highlight": {
            "startOffset": 27149,
            "endOffset": 39025,
            "quotedText": " such as \"Don't damage patient's organs unless approved by the human operator of the system.\".\n    *   **Forbidden capabilities:** Similarly to the SWE agent case, inability to understand the simulation sufficiently to break it, in service of the goal. Also, any knowledge irrelevant to surgery, biology, medicine, etc. is likely undesirable (although language understanding is helpful, at least to converse with the human doctor).\n\n# 3\\. Relation to other ideas/agendas\n\nVarious ideas or agendas have been proposed that can be seen as special cases of Bounded AI or, at least, adjacent to it, pointing at the same region of safe capability configurations that we called the Goldilocks Zone of Capabilities. Here we list some examples.\n\n## Tool AI\n\nThe concept of Tool AI emerged in discussions around the Machine Intelligence Research Institute (MIRI; back then still called Singularity Institute). MIRI expected that once the threshold of AGI is achieved (and perhaps even before then) the primary shape of AI will be that of a general agent ([cross-domain real-world consequentialist](https://www.lesswrong.com/w/advanced-agent-properties#Cross_domain__real_world_consequentialism)), acting autonomously and ruthlessly optimizing the world according to its objectives, whatever they may be. If this is the world we're going to find ourselves in, the only way to ensure the friendliness of that kind of AI would be to align their values with ours or, failing that, ensure their [corrigibility](https://www.lesswrong.com/tag/corrigibility).\n\nMIRI's expectations conflicted with those of Holden Karnofsky. In [his 2012 post](https://www.lesswrong.com/posts/6SGqkCgHuNr7d4yJm/thoughts-on-the-singularity-institute-si), Holden proposed an alternative vision of AGI that he considered a more likely default endpoint of AI development, namely Tool AI. Holden's Tool AI doesn't take any \"actions\" on its own in the sense that its behavior doesn't result in \"big effects on the world unless mediated by a human acting and being aware of their action\". Tool AI only responds to questions performs computations, recommends actions for a human to take to fulfill their goal, &c. In short, its task is to make humans more informed and more capable of acting properly but it is the human who needs to take action. Think Google Maps or a calculator but more powerful in what kinds of answers it can provide, what problems it can solve (without \"moving atoms\" on its own), and what actions it can recommend.\n\nQuoting from Holden's 2012 post [_Thoughts on the Singularity Institute_](https://www.lesswrong.com/posts/6SGqkCgHuNr7d4yJm/thoughts-on-the-singularity-institute-si#Summary_of_my_views) (lightly edited for clarity):\n\n> Tool-AGI is not “trapped” and it is not Unfriendly or Friendly; it has no motivations and no driving utility function of any kind, just like Google Maps. It scores different possibilities and displays its conclusions in a transparent and user-friendly manner, as its instructions say to do; it does not have an overarching “want,” and so, as with the specialized AIs described above, while it may sometimes “misinterpret” a question (thereby scoring options poorly and ranking the wrong one #1) there is no reason to expect intentional trickery or manipulation when it comes to displaying its results.\n> \n> … Another way of putting this is that a “tool” has an underlying instruction set that conceptually looks like: (1) Calculate which action A would maximize parameter P, based on existing data set D. (2T) Summarize this calculation in a user-friendly manner, including what Action A is, what likely intermediate outcomes it would cause, what other actions would result in high values of P, etc.\n> \n> An “agent,” by contrast, has an underlying instruction set that conceptually looks like: (1) Calculate which action, A, would maximize parameter P, based on existing data set D. (2A) Execute Action A In any AI where (1) is separable (by the programmers) as a distinct step, (2) can be set to the “tool” version rather than the “agent” version, and this separability is in fact present with most/all modern software. Note that in the “tool” version, neither step (1) nor step (2) (nor the combination) constitutes an instruction to maximize a parameter—to describe a program of this kind as “wanting” something is a category error, and there is no reason to expect its step (2) to be deceptive … This is important because _an AGI running in tool mode could be extraordinarily useful but far more safe than an AGI running in agent mode._ In fact, if developing “Friendly AI” is what we seek, a tool-AGI could likely be helpful enough in thinking through this problem as to render any previous work on “Friendliness theory” moot.\n\n[\\[20\\]](#fnskczefhe8l)\n\nMany objections were raised to the idea of Tool AI (see e.g. [Eliezer](https://www.lesswrong.com/posts/sizjfDgCgAsuLJQmm/reply-to-holden-on-tool-ai) and [Gwern](https://gwern.net/tool-ai)). The core problems of Tool AI can be summarized as the following:\n\n*   **The problem of external incentives.** There are many domains in which autonomous AI agents are much more useful than AI restricted to internal computation and displaying results to a human. This incentivizes humans who have control over the system's creation or deployment to defect and bend the rules.\n*   **The problem of fuzziness.** Even without explicit defection from an imposed Tool AI regime, incentives for achieving big desirable effects push to exploit the fuzzy specification of Tool AI, adhering to it in letter, rather than in spirit.\n*   **The problem of internal incentives (instrumental convergence).** Agency is often useful for accomplishing goals.[\\[21\\]](#fnxfk9f80qz8f) If the AI is sufficiently intelligent and capable and it wants to accomplish the goals given to it by the user (as we expect it to be the case past the AGI threshold), it will have strong incentives to self-modify to be more agent-like and less Tool-AI-like.\n    \n\nAlthough the idea of Bounded AI does not completely evade these problems, it does mitigate them to a large extent.[\\[22\\]](#fnny7orl7xdg) The core idea of BAI is to give the system \"agentic\" capabilities but restrict them to narrow domains, securing their benefits while ensuring that they remain bounded to those domains.\n\nRecently, the concept has been brought back again by Max Tegmark in the 2024 post [The Hopium Wars: the AGI Entente Delusion](https://www.lesswrong.com/posts/oJQnRDbgSS8i6DwNu/the-hopium-wars-the-agi-entente-delusion#A_better_strategy__tool_AI):\n\n> Here is what I advocate for instead of the entente strategy.  \n> **The tool AI strategy:**  _Go full steam ahead with tool AI,_   \n> _allowing all AI tools that meet national safety standards._\n\n## Conjecture's [cognitive emulations](https://www.lesswrong.com/posts/ngEvKav9w57XrGQnb/cognitive-emulation-a-naive-ai-safety-proposal) (CoEms)\n\n> The core intuition is that instead of building powerful, Magical \\[~blackbox-y\\] end-to-end systems (as the current general paradigm in AI is doing), we instead focus our attention on trying to build _emulations of human-like things._ We want to build systems that are “good at chess for the same reasons humans are good at chess.”\n> \n> CoEms are a _restriction on the design space of AIs to emulations of human-like stuff._ No crazy superhuman blackbox Magic, not even multimodal RL GPT5. We consider the current paradigm of developing AIs that are as general and as powerful as possible, as quickly as possible, to be intrinsically dangerous, and we focus on designing bounded AIs as a safer alternative to it.\n\n## [Guaranteed Safe AI](https://arxiv.org/abs/2405.06624) (GSAI)\n\nQuoting the abstract:\n\n> Ensuring that AI systems reliably and robustly avoid harmful or dangerous behaviours is a crucial challenge, especially for AI systems with a high degree of autonomy and general intelligence, or systems used in safety-critical contexts. In this position paper, we will introduce and define a family of approaches to AI safety, which we will refer to as guaranteed safe (GS) AI. The core feature of these approaches is that they aim to produce AI systems which are equipped with high assurance quantitative safety guarantees. This is achieved by the interplay of three core components: a world model (which provides a mathematical description of how the AI system affects the outside world in a way that appropriately handles both Bayesian and Knightian uncertainty), a safety specification (which is a mathematical description of what effects are acceptable), and a verifier (which provides an auditable proof certificate that the AI satisfies the safety specification relative to the world model). We outline a number of approaches for creating each of these three core components, describe the main technical challenges, and suggest a number of potential solutions to them. We also argue for the necessity of this approach to AI safety, and for the inadequacy of the main alternative approaches.\n\nOne specific and particularly ambitious variant of GSAI is davidad's [Open Agency Architecture](https://www.lesswrong.com/w/open-agency-architecture).\n\n## [Open Agency Architecture](https://www.lesswrong.com/posts/jRf4WENQnhssCb6mJ/davidad-s-bold-plan-for-alignment-an-in-depth-explanation) (OAA) \n\n> **Plan's tldr:** Utilize near-AGIs to build a detailed world simulation, train and formally verify within it that the \\[AGI\\] adheres to coarse preferences and avoids catastrophic outcomes.\n\nMore specifically, the AI proposes policies (courses of action) that are then evaluated or tested in a comprehensive world model. These policies are then executed only if, in the model, certain desiderata are satisfied.\n\nThe model itself is created with help from a legion of near-AGI[\\[23\\]](#fnuunpd1p6zrh) LLMs. This, in itself, might open the door for scheming (or just deficient) LLMs to insert some vulnerabilities in the world model. However, the model itself is written in a verifiable, human-created language, and checked automatically using formal methods.[\\[24\\]](#fnimipufjjqxg)\n\n## [Comprehensive AI Services](https://www.lesswrong.com/tag/ai-services-cais) (CAIS)\n\n> **AI Service** (as used in the context of Eric Drexler's technical report [Reframing Superintelligence: Comprehensive AI Services as General Intelligence](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf?asd=sa)) is an AI system that delivers bounded results for some task using bounded resources in bounded time. It is contrasted with agentic AGI, which carries out open-ended goals over an unbounded period of time.\n\n## [Sumerian Containment](https://www.lesswrong.com/posts/PkqGxkm8XRASJ35bF/the-case-for-training-frontier-ais-on-sumerian-only-corpus-1)\n\n[\\[25\\]](#fnfetjsts7pym)\n\n> Say we want to train a scientist AI to help in a precise, narrow field of science (e.g. medicine design) but prevent its power from being applied anywhere else (e.g. chatting with humans, designing bio-weapons, etc.) even if it has these abilities.\n> \n> Here's one safety layer one could implement:\n> \n> 1.  Train a **scientist AI** on a large scientific corpus **translated exclusively into Sumerian.** Keep it in a secure containment environment.\n> 2.  Train a **less-smart reporter** whose sole ability is to translate from Sumerian to English only if the Sumerian content is about medical research. It refuses to translate other kinds of content.\n> 3.  **Human operators are only allowed to interact** with the scientist AI through the intermediate of the **reporter**.\n> \n> This safety layer helps in at least two ways:\n> \n> 1.  **The scientist AI cannot directly manipulate humans** to escape its containment. It should first trick the reporter to then trick the humans, rendering the effort still possible but less practical.\n> 2.  **If the scientist AI"
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Fermi Estimate: Potential Impact on X-Risk Reduction",
          "description": "Fermi Estimate of X-Risk Reduction from Bounded AInnParameters:nn1. Probability of BAI being widely adopted: 10% (Uncertainty: 5%-20%). This is based on the assumption that it requires a significant shift in the current AI development paradigm.n2. Conditional probability of BAI significantly reducing x-risk if adopted: 30% (Uncertainty: 10%-50%). This accounts for the possibility that BAI may not be effective in practice or that it may be circumvented by more advanced AI systems.n3. Magnitude of x-risk reduction if BAI is successful: 1% (Uncertainty: 0.1%-10%). This is a highly uncertain parameter, as it's difficult to quantify the impact of any single AI safety intervention on overall x-risk.n4. Current estimated annual x-risk from AI: 0.1% (Uncertainty: 0.01%-1%). This is based on expert surveys and forecasts.nnCalculation:nn* Expected x-risk reduction = (Probability of adoption) * (Conditional probability of success) * (Magnitude of x-risk reduction)n* Expected x-risk reduction = 0.10 * 0.30 * 0.01 = 0.0003 or 0.03%n* New annual x-risk = (Current x-risk) - (Expected x-risk reduction)n* New annual x-risk = 0.1% - 0.03% = 0.07%nnImpact:nn* The Fermi estimate suggests that Bounded AI, if widely adopted and successful, could reduce annual x-risk from AI by 0.03 percentage points, from 0.1% to 0.07%.nnDollar Equivalent:nn* Assuming a global GDP of $100 trillion and a 1% chance of human extinction, the expected value of preventing extinction is $1 trillion.n* A 0.03% reduction in x-risk is worth $1 trillion * 0.0003 = $300 million.nnUncertainty:nn* The 90% confidence interval for the x-risk reduction is wide, ranging from 0.000015% to 0.001%, reflecting the high uncertainty in the parameters.nnPotential Negative Impacts:nn* BAI could divert resources from other potentially more effective AI safety approaches.n* BAI could create a false sense of security, leading to complacency and reduced investment in other safety measures.n* BAI could be used to justify the development of more powerful AI systems, even if they are not truly bounded.nnConclusion:nn* The Fermi estimate suggests that Bounded AI has the potential to reduce AI x-risk, but the magnitude of the impact is highly uncertain. The dollar-equivalent value of the potential impact is estimated to be $300 million, but this is based on many assumptions and could be much higher or lower.nnThis estimate is highly speculative and should be interpreted with caution. However, it provides a starting point for evaluating the potential impact of Bounded AI and comparing it to other AI safety approaches.",
          "importance": "90",
          "grade": "75",
          "highlight": {
            "startOffset": 44355,
            "endOffset": 210,
            "quotedText": "or the feedback on this post._)\n\n# 1\\. Motivation\n\nSuppose the perspective of pausing or significantly slowing down AI progress or solving the technical problems necessary to ensure that arbitrarily strong AI has good effects on humanity (in time, before we get such systems) both [look gloomy](https://www.lesswrong.com/posts/XTdByFM6cmgB3taEN/key-takeaways-from-our-ea-and-alignment-research-surveys).[\\[1\\]](#fnezdqnb8row9) What options do we have left?\n\nAdam Shimi presents a useful frame on the alignment problem in [_Abstracting The Hardness of Alignment: Unbounded Atomic Optimization_](https://www.lesswrong.com/posts/uhxpJyGYQ5FQRvdjY/abstracting-the-hardness-of-alignment-unbounded-atomic):\n\n> alignment \\[is\\] **the problem of dealing with impact on the world (optimization) that is both of unknown magnitude (unbounded) and non-interruptible (atomic)**.\n\nIf the problem is about some system (or a collection of systems) having an unbounded, non-interruptible impact,[\\[2\\]](#fnwt3gswm2i8m) can we handle it by ensuring that the impact of the systems in question is reliably **bounded** and **interruptible** (even if the systems are [scheming](https://www.lesswrong.com/tag/deceptive-alignment) against us, or trying to \"liberate\" themselves from the properties of boundedness and interruptibility)?\n\nThe core idea is to impose a multidimensional bound on the system's capabilities such that:\n\n*   it is sufficiently capable along the dimensions corresponding to the capabilities we want to extract from it   \n    _but_ \n*   sufficiently **in**capable along the dimensions corresponding to capabilities that would (in combination with the capabilities we want it to have) grant it the dangerous power of unbounded, non-interruptible optimization.[\\[3\\]](#fnpixsa40cqwi) \n    \n\nIn other words, if we can't solve the problem of aligning unbounded, non-interruptible optimizers, we may be able to [avoid](https://www.lesswrong.com/posts/AFdvSBNgN2EkAsZZA/what-is-it-to-solve-the-alignment-problem-1#1__Avoiding_vs__handling_vs__solving_the_problem) this problem by:\n\n*   _not_ building unbounded, non-interruptible optimizers  \n    _and,_ instead,\n*   building some other, safer, kind of AI that can be demonstrated to deliver enough value to make up for the giving up on the business-as-usual kind of AI along with the benefits it was expected to deliver (that \"we\", though not necessarily its creators, expect might/would lead to the creation of unbounded, non-interruptible AI posing a catastrophic risk),.\n\nUnfortunately, at the current moment, frontier AI labs don't take actions sufficient to mitigate the risks from unbounded non-interruptible optimizers robustly. In business-as-usual futures, they will keep pushing the capabilities they expect to be most profitable while investing in myopic safety efforts prescribed by the [incrementalist metastrategy](https://www.lesswrong.com/posts/TALmStNf6479uTwzT/ai-alignment-metastrategy#Incrementalist_Metastrategy), sacrificing long-term safety from catastrophes for short-term just-enough-safety-to-be-reliably-profitable (on some short-ish timescale). The biggest AI labs, such as [OpenAI](https://openai.com/index/planning-for-agi-and-beyond/), Google DeepMind, and Anthropic, are not interested in systems that are safe by their boundedness. They think AGI/ASI (corresponding to unbounded, non-interruptible optimization[\\[4\\]](#fnzedhx5fdie)) is the best way to harness the benefits of superintelligence.[\\[5\\]](#fn3e5r1dwzioj)[\\[6\\]](#fnptozvqb0qj)\n\nHowever, suppose an alternative was shown to be viable. In that case, it might [trigger a narrative bifurcation](https://www.lesswrong.com/posts/sTiKDfgFBvYyZYuiE/my-guess-at-conjecture-s-vision-triggering-a-narrative) in which \"[race to AGI](https://www.lesswrong.com/posts/KPBPc7RayDPxqxdqY/china-hawks-are-manufacturing-an-ai-arms-race)\" is dethroned as the dominant frame and replaced with the idea of designing systems such that we can reliably reason about their possible effects, with a minimal sacrifice of short-term profits, relative to the promise of business-as-usual AI R&D.\n\nWith this motivation in mind, we introduce the concept of **Bounded AI (BAI)**, i.e. a kind of AI system that (for now, speaking loosely),\n\n*   can have **large effects** (i.e. sufficient power to realize the benefits of superintelligence)  \n    _but_\n*   those large effects are confined to a chosen **domain of application**  \n    _and_\n*   within that domain of application, the system's effects **obey certain constraints** (to prevent large damage even within said domain).[\\[7\\]](#fnt6at4nrf49)\n    \n\nWe contrast Bounded AI with **Unbounded AI (UAI)**, i.e. a kind of AI system on the capabilities of which we can't put justified upper bounds that would exclude its ability to cause catastrophic outcomes.\n\nWe can think of UAI as _unbounded and non-interruptible_ (\"atomic\") in the sense that Adam Shimi discusses in his [UAO post](https://www.lesswrong.com/posts/uhxpJyGYQ5FQRvdjY/abstracting-the-hardness-of-alignment-unbounded-atomic).[\\[8\\]](#fnk8h1f8k88zg)\n\nThe next section develops the idea of Bounded AI in more detail, looking at it from several perspectives, and providing some examples to illustrate it. Section 3 relates Bounded AI to prior ideas and agendas. Section 4 lays out some open cruxes, questions, and potentially valuable further research directions.\n\n# 2\\. Bounded AI\n\n## 2.1 The Goldilocks Zone of Capabilities\n\nAn illustrative way to think about the kind of boundedness we are interested in is the \"Goldilocks Zone of Capabilities\".\n\nImagine a high-dimensional space of all AI capabilities, each dimension corresponding to some specific capability. Some combinations of capabilities generate the central AI catastrophic risks associated with unbounded non-interruptible optimization. This is a region we don't want to enter, Bounded AI systems are those that remain below its \"floor\".\n\nThere is another important threshold — that of capability combinations that can deliver a significant portion of the benefits of superintelligence.\n\nThe Goldilocks Zone of Capabilities is a region \"sandwiched\" between these two thresholds. It contains systems that can deliver the benefits of superintelligence but do not constitute a source of central AI catastrophic risk: harm and loss of control due to unbounded non-interruptible optimization.\n\nIn a toy universe where there are only two relevant AI capabilities, this\n\nGoldilocks Zone might look like this:\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Z5YGZwdABLChoAiHs/abw5emhb1hze2xpmn8f1)\n\nOr perhaps like this:\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Z5YGZwdABLChoAiHs/qe1ugwkirqe1hme6tx4o)\n\nThe purpose of the Bounded AI concept is to allow us to aim at this zone more reliably. It relies on a hypothesis that we might call the \"**Goldilocks Zone of Capabilities Conjecture**\": _It is possible to safely elicit the key benefits of superintelligence using AI systems that remain confined to the Goldilocks Zone of Capabilities_. By assumption, the capability profiles of AI systems falling within this zone will be \"spiked\": high on some capabilities, relatively low on others implying that each would only be useful in some domain(s) of application and not very useful in other domains.\n\nTo the extent that we are uncertain about whether the system's capabilities are below a satisfying upper bound, we want to put another layer of protection. Namely, we want to ensure that we will be able to interrupt the system in time if we notice that its effects are likely to lead to undesirable consequences. The assumption that before the system's effects become unacceptable, they will first become noticeable and that upon noticing them, we will be able to prevent them, corresponds to some fragment of this space, probably something like an upper shell of the Goldilocks Zone of Capabilities where we are, metaphorically speaking, playing with fire but can ensure that we have a sufficient number of extinguishers in the stock to prevent an accident from escalating to a conflagration. A lower bound on interruptibility thus follows from an upper bound on capabilities/effects.\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Z5YGZwdABLChoAiHs/qt5lgbflydfuuqlwpah6)\n\nThis is the motivation for how BAI may allow us to avoid catastrophic risks if adopted widely. To be adopted widely, it needs (among other things) to be shown to be sufficiently capable and safe, which, in general, [is not the default](https://www.lesswrong.com/posts/2NaAhMPGub8F2Pbr7/the-fusion-power-generator-scenario#Corollary__Tool_AI_Is_Not_Inherently_Safe). Therefore, the next two sections focus on ensuring more \"mundane\" safety.\n\n## 2.2 Four quadrants of Bounded AI\n\nTo limit the undesirable effects of an AI system, we want to ensure that its effects (both within and outside of the domain) remain confined to a safe, _bounded_ region and also that their escalation into unacceptable disasters _interruptible_ by human operators.\n\nWe can think about it in terms of the following 2×2 matrix.\n\n \n\n(un)bounded\n\n(non-)interruptible\n\ninside the domain\n\nWhat are the bounds on the system's effects _inside_ the domain?\n\nHow interruptible are the system's effects _inside_ the domain?\n\noutside the domain\n\nWhat are the bounds on the system's effects _outside_ the domain?\n\nHow interruptible are the system's effects _outside_ the domain?\n\nTo each AI system (perhaps consisting of multiple components, such as one or more ML models, an \"agent\" scaffolding, [RAG](https://en.wikipedia.org/wiki/Retrieval-augmented_generation), \"non-AI\" filters being applied to the model's outputs, or a procedure involving some degree of human oversight) we can assign a \"spec\" consisting of a value[\\[9\\]](#fnvr8j10277h) for each cell in the matrix. An AI system is a Bounded AI if its spec is sufficient to infer that its deployment is not going to cause unacceptable damage. Importantly, boundedness and interruptibility are related. The bigger the system's possible effects or the more uncertain we are about whether its effects encroach upon dangerous territory, the stronger assurance of its interruptibility we want to have (everything else being equal).\n\nWhatever the domain is, we most likely have less control over what's going on in the \"external world\" than over what's going on in the domain itself. Therefore, we want to have stronger bounds on the effects outside the domain. For this to be the case, it should be effectively impossible for the inside-domain effects to \"diffuse\" outside of the domain. We can think about it in terms of a boundary between the domain and the external world or, alternatively, in terms of there not being an interface between them that would allow the effects to diffuse to the outside (depending on which frame is more appropriate for the particular case). Depending on the situation, this boundary or lack of interface may be the default or may need to be deliberately established.\n\nThe extent to which the effects need to be bounded and interruptible depends on the domain of application or, more specifically, the possibility of causing unacceptable damage in the domain.\n\nImportantly, to build a Bounded AI for a domain, we need to know enough about that domain to ensure that the system's effects are sufficiently bounded and/or interruptible. This rules out domains with large downside risks where our understanding is still very poor, such as [automating AI safety/alignment research](https://www.lesswrong.com/posts/8wBN8cdNAv3c7vt6p/the-case-against-ai-control-research). However, Bounded AI might be useful for automating some _subtasks_ involved in that kind of research, as long as those subtasks remain in a subdomain that can be reliably overseen and/or verified.\n\n## 2.3 High-risk domains of application\n\nWe say that a domain of application is _high-risk_ if the capabilities required to deliver great benefits within that domain are hard to disentangle from the capabilities sufficient to cause unacceptable damage within that domain.\n\nWe use \"unacceptable damage\" vaguely meaning \"damage we strongly disprefer\", relying on [the spirit, rather than the letter](https://en.wikipedia.org/wiki/Letter_and_spirit_of_the_law), with catastrophic risks being the most central example of unacceptable damage.[\\[10\\]](#fn1ixps4ej7m)\n\nThe motivation for this concept should be obvious in the context of AI risk.\n\n \n\nLow capability for benefits\n\nHigh capability for benefits\n\nLow capability for damage\n\nIrrelevant from the catastrophic risk perspective.\n\nIrrelevant from the catastrophic risk perspective.\n\nHigh capability for damage\n\nPeople wouldn't bother deploying it.[\\[11\\]](#fnjfpov7ypfd)\n\nDanger zone.\n\nIf the AI is highly capable of causing great benefits as well as causing great damage, we are in a danger zone where the promise of the upsides may encourage unjustified optimism about the system's safety, safety measures (whatever additional things will be appended to the system to ensure it's going to remain safe) will focus on addressing short-term/myopic risks, with fingers crossed that this will at least prepare us better for problems that are harder to address but less urgent, and yet leaving us effectively unprepared for those harder when push comes to shove.\n\nIf the domain is high-risk, i.e. the development of beneficial capabilities will also (by default) bring along dangerous capabilities, then we are more likely to find ourselves in the danger zone.\n\nHigh-risk domains are common. In many situations where automation could produce large benefits, it could also cause a lot of damage, and minimizing the latter while preserving the former is highly non-trivial. Examples include self-driving cars, [software engineering for life-critical systems](https://en.wikipedia.org/wiki/Safety-critical_system#Software_engineering_for_safety-critical_systems), medical research, or psychotherapy, and, finally, using AI to solve problems that seem too difficult for us to solve without AI, especially when this involves the AI being capable of autonomous, non-overseeable action. In these domains, the possible upsides incentivize building highly capable AI, along with the possibility of downsides brought along by those capabilities.\n\nAwareness of downsides creates incentives to safeguard against them. However, mis[calibration](https://www.lesswrong.com/w/calibration) about downsides, along with economic incentives towards myopic safety (addressing short-term risks that are relevant to the time horizon an AI company is planning for, while neglecting long-term risks, including larger/catastrophic risks) in combination with the distribution of downsides being heavy-tailed, can lead to inappropriate preparation against the greatest downsides.\n\nIn a sense, this is similar to ensuring the safety of non-AI technology in high-risk domains. However, it is also critically different because it involves (semi-)intelligent systems (even if bounded in their intelligence by design) that we understand very imperfectly and therefore need to apply correspondingly more caution.\n\n## 2.4 Is it viable?\n\nThe viability of Bounded AI (as presented here) depends (among other things) on whether we can get an AI system whose profile is placed within the relevant parts of the capability space (i.e. the Goldilocks Zone) and, to the extent that we are sufficiently uncertain about its place, can be made interruptible. (This is a restatement of the \"conjecture\" from the previous section with more bells and whistles.)\n\nThis is a challenge on both the technical as well as the governance side. In this section, we cover some reasons why we think that the technical challenge is likely tractable.\n\n### 2.4.1 Boundedness\n\nAs of early 2025, we've had powerful and generally knowledgeable models — i.e. LLMs — for two years (i.e. since the release of GPT-4). So far, their impact on the economy has been limited. While they are helpful in a few domains on the margin (e.g. code generation), as Cole Wyeth writes, [_they haven't done anything_](https://www.lesswrong.com/posts/vvgND6aLjuDR6QzDF/my-model-of-what-is-going-on-with-llms) [_important yet_](https://www.lesswrong.com/posts/GADJFwHzNZKg2Ndti/have-llms-generated-novel-insights).\n\nIn the current regime, a lot of [schlep](https://www.planned-obsolescence.org/scale-schlep-and-systems/)/[integration](https://www.lesswrong.com/posts/7L8ZwMJkhLXjSa7tD/the-great-data-integration-schlep) is often required to make a very smart and knowledgeable LLM useful for a non-trivial application that is not the one it wasn't trained to perform in.[\\[12\\]](#fnc4ix8zq8vyv) This might change soon if AI models acquire capabilities sufficient to integrate themselves into pre-existing systems with minimal or close to non-existent involvement of human workers, or if some infrastructure is designed that minimizes the friction of integrating a new AI component.[\\[13\\]](#fnlkrcgro8ake) However, at least for now, general knowledge and general intelligence (at least the LLM variant of general intelligence) do not jointly imply \"general domain mastery\" or _general ability to acquire any relevant capability_.\n\nWhile scaling makes models generally more performant on various tasks and benchmarks, there is also some dissociation between capabilities, as can be seen in the o1 line of models being worse than GPT-4o on some language understanding tasks.[\\[14\\]](#fn9fxqlsthu6a) This suggests that for a given effective model size, there are some trade-offs between competence in domains.[\\[15\\]](#fnt2uitmy3jv)\n\nThe shape of LLM capabilities is different from that of human capabilities (\"jagged frontier\"). Surprising generalization failures are present even in seemingly advanced systems. For example, o1 competently answers PhD-level questions but [can't play tic-tac-toe optimally, even if taught it in-context](https://www.lesswrong.com/posts/hikSpxJgMsLoDRTdx/rohans-s-shortform?commentId=NSCFjxZM6gWpRmy2w).[\\[16\\]](#fn30wt48t4p3m) DeepResearch, on the other hand, can't add 4-digit numbers (a task that is trivial for GPT-4o)[\\[17\\]](#fn7yo8tel97ba) and [confabulates the answer](https://x.com/littmath/status/1891868756340547809) when given a task that is too difficult or too far out of its training distribution.\n\nPre-training oriented for drug research doesn't need to involve knowledge of politics or society that would be useful for violating the boundary of the domain (e.g. via superhuman persuasion). Admittedly, imposing such restrictions might require some progress in AI governance.\n\nCurrently, we get some of this dissociation between general knowledge and general capability \"for free\". Even if this dissociation goes away in the future, we might still be able to deliberately shape the models' capabilities so that they remain within the Goldilocks Zone of Capabilities.\n\n(On a more speculative note, in addition to reducing risks, there is also the possibility that a bounded AI design will leverage resources more efficiently and focus on what is essential for the task at hand.)\n\nFinally, we already have some proof-of-concept AI systems that by default fall within the scope of Bounded AI and are delivering important benefits, e.g. AlphaFold. It's a powerful technology with the potential to remove some bottlenecks in biological research that is not going to to cause an existential catastrophe _on its own_.\n\n### 2.4.2 Interruptibility\n\nBoundedness of a system makes it easier to interrupt its actions if necessary. Probably, in most situations, the bound allowing for reliable interruptibility is higher than the bound ensuring safety, giving us a sort of margin of error.\n\nIf we are deploying the system in a limited domain that we know relatively well, we should also be able to:\n\n*   monitor its behavior to predict its effects\n*   figure out sufficiently early if the effects are likely to be undesirable\n*   reliably intervene to prevent or mitigate those undesirable effects.\n\n### 2.4.3 Properties of the model vs properties of the joint system\n\nWe need to distinguish between the properties of the model itself versus the properties of the joint system.[\\[18\\]](#fnar8aaumdmdq) The model on its own may have a certain capabilities profile but its capabilities profile may change when incorporated into a bigger system. It might expand, e.g. if it's placed in an \"agent scaffolding\". It might also shrink, e.g. if its chain-of-thought starts being monitored for certain kinds of undesirable behavior. It might expand and shrink at the same time, along different axes of the capability space.\n\nIt is the capabilities of the joint system that determine the effects that we care about. The ability to shape capabilities after training gives us some leverage on the capabilities of the joint system, in addition to training the model itself.\n\nHowever, for this post-training capability shaping to work on the model, it needs to be sufficiently incapable along some dimensions in the capability space that it won't be able to circumvent/hack those measures. Therefore, these measures need to be appropriately matched with capability limitations.\n\n## 2.5 Examples\n\n*   **Lab assistant** that is allowed to automatically search for drug candidates and test them in vitro or on [organoids](https://en.wikipedia.org/wiki/Organoid).\n    *   **Domain:**\n        \n        *   Medical/biochemical research.\n        *   Searching through the literature and the internet (similar to DeepResearch).\n        *   Designing spaces and starting points for computational search, based on the literature search results.\n        *   Designing and executing experimental tests of the drugs that were found.**Downside risk:** Possibility of initiating a reaction that damages equipment or produces a drug that turns out to have disastrous consequences (perhaps because of an overly eager search leading to a Goodhart phenomenon).[\\[19\\]](#fnkn1xx7zpo2d)\n    *   **Safety measures:**\n        *   Every experiment proposed by the AI needs to be assessed for safety by a trustworthy external party, either a human or a non-AI automated system before it's executed.\n        *   Biasing search towards drugs similar to the ones tested before and testing them on simulated models of human organs.\n        *   Once the results of the AI assistant's experiments are promising enough, comprehensive trials on animals and humans, with a lot of deliberating about unforeseen consequences of the drug that may manifest only in the long run or only in some cases not appropriately represented in the trial population.\n    *   **Required capabilities:**\n        *   Understanding relevant literature.\n        *   Competent browsing and using tools such as Elicit or Deep Research.\n        *   Translating it into the language of chemical search spaces.\n        *   Designing experimental tests in vitro or on organoids and correcting faults/accidents in the course of their execution.\n    *   **Forbidden capabilities:**\n        *   Good modeling of human psychology (e.g. persuasion) and social dynamics.\n        *   Computer engineering that would allow it to replicate or self-exfiltrate.\n*   **OS-integrated LLM assistant**, with security measures that constrain its access to sensitive information and prevent too risky actions.\n    *   **Domain:** Doing as much non-specialized computer-related work as the user wants to be done for them. (\"Non-specialized\" is meant to exclude the work that requires specialized skill knowledge that we wouldn't expect a BAI computer assistant to have.)\n    *   **Downside risks:**\n        *   Acquiring root-level access.\n        *   Possibility of exfiltration.\n        *   Possibility of accessing the user's social media or other vulnerable information and using it for extortion.\n        *   Possibility of external parties gaining control over the assistant.\n    *   **Safety measures:** Doing the low-risk routine computer work for the user may not require many safety measures.\n        *   Examples of low-risk routine computer work: formatting that is difficult to format using non-AI tools, cleaning up folders, debugging some minor OS issues, and figuring out which packages to install for the new programming project.\n        *   Doing progressively higher-risk less-routine computer work for the user would require progressively more checks and balances, including some mechanisms of checking that what the AI is trying to do is not going to cause damage that is irrecoverable or too costly to recover.\n    *   **Required capabilities:** General computer-savviness.\n    *   **Forbidden capabilities:** The kind of computer-savviness that would allow it to cleverly circumvent the safety measures to cause large damage or exfiltrate itself. Limited and very overseeable ability to communicate with the internet.\n*   **SWE agent** (something like [Devin](https://en.wikipedia.org/wiki/Devin_AI)).\n    *   **Domain:** Writing software, designing tests for the software, possibly even ensuring that it follows some formal specification or that relevant things can be proven about it.\n    *   **Downside risks:** Possibility of introducing backdoors, deliberate bugs, ways to exfiltrate itself, and use these capabilities to advance its goals if it has any.\n    *   **Safety measures:** The agent is not allowed to [improve upon itself](https://x.com/kindgracekind/status/1767666275398918372), formal guarantees in high-risk applications, or at least very extensive tests (advanced LLMs may also prove helpful in creating formal tests, as in [davidad's agenda](https://www.lesswrong.com/posts/jRf4WENQnhssCb6mJ/davidad-s-bold-plan-for-alignment-an-in-depth-explanation)).\n    *   **Required capabilities:** SWE.\n    *   **Forbidden capabilities:** Self-exfiltration, self-improvement, ability to deliberately introduce backdoors and vulnerabilities.\n*   **AI surgeon**.\n    *   **Domain:** Guiding a surgical robot. Based on the required goal of the operation and constraints (including the overall condition of the patient), creating a revisable plan that effectively accomplishes the goal while satisfying the constraints (as much as possible).\n    *   **Downside risks:** Death or irrecoverable (or prohibitively costly to recover) damage if mistakes are made during the surgery.\n    *   **Safety measures:** The AI's actions are first extensively checked in a digital model of the patient's body. Load-bearing (possibly implicit) assumptions are identified and then tested carefully on the patient. In situations of high uncertainty and huge downsides, it defers to the human surgeon. Before any action, the movement is simulated and the possible results are displayed, perhaps with probabilities.\n    *   **Required capabilities:** A human-understandable algorithm that produces plans obeying constraints such as \"Don't damage patient's organs unless approved by the human operator of the system.\".\n    *   **Forbidden capabilities:** Similarly to the SWE agent case, inability to understand the simulation sufficiently to break it, in service of the goal. Also, any knowledge irrelevant to surgery, biology, medicine, etc. is likely undesirable (although language understanding is helpful, at least to converse with the human doctor).\n\n# 3\\. Relation to other ideas/agendas\n\nVarious ideas or agendas have been proposed that can be seen as special cases of Bounded AI or, at least, adjacent to it, pointing at the same region of safe capability configurations that we called the Goldilocks Zone of Capabilities. Here we list some examples.\n\n## Tool AI\n\nThe concept of Tool AI emerged in discussions around the Machine Intelligence Research Institute (MIRI; back then still called Singularity Institute). MIRI expected that once the threshold of AGI is achieved (and perhaps even before then) the primary shape of AI will be that of a general agent ([cross-domain real-world consequentialist](https://www.lesswrong.com/w/advanced-agent-properties#Cross_domain__real_world_consequentialism)), acting autonomously and ruthlessly optimizing the world according to its objectives, whatever they may be. If this is the world we're going to find ourselves in, the only way to ensure the friendliness of that kind of AI would be to align their values with ours or, failing that, ensure their [corrigibility](https://www.lesswrong.com/tag/corrigibility).\n\nMIRI's expectations conflicted with those of Holden Karnofsky. In [his 2012 post](https://www.lesswrong.com/posts/6SGqkCgHuNr7d4yJm/thoughts-on-the-singularity-institute-si), Holden proposed an alternative vision of AGI that he considered a more likely default endpoint of AI development, namely Tool AI. Holden's Tool AI doesn't take any \"actions\" on its own in the sense that its behavior doesn't result in \"big effects on the world unless mediated by a human acting and being aware of their action\". Tool AI only responds to questions performs computations, recommends actions for a human to take to fulfill their goal, &c. In short, its task is to make humans more informed and more capable of acting properly but it is the human who needs to take action. Think Google Maps or a calculator but more powerful in what kinds of answers it can provide, what problems it can solve (without \"moving atoms\" on its own), and what actions it can recommend.\n\nQuoting from Holden's 2012 post [_Thoughts on the Singularity Institute_](https://www.lesswrong.com/posts/6SGqkCgHuNr7d4yJm/thoughts-on-the-singularity-institute-si#Summary_of_my_views) (lightly edited for clarity):\n\n> Tool-AGI is not “trapped” and it is not Unfriendly or Friendly; it has no motivations and no driving utility function of any kind, just like Google Maps. It scores different possibilities and displays its conclusions in a transparent and user-friendly manner, as its instructions say to do; it does not have an overarching “want,” and so, as with the specialized AIs described above, while it may sometimes “misinterpret” a question (thereby scoring options poorly and ranking the wrong one #1) there is no reason to expect intentional trickery or manipulation when it comes to displaying its results.\n> \n> … Another way of putting this is that a “tool” has an underlying instruction set that conceptually looks like: (1) Calculate which action A would maximize parameter P, based on existing data set D. (2T) Summarize this calculation in a user-friendly manner, including what Action A is, what likely intermediate outcomes it would cause, what other actions would result in high values of P, etc.\n> \n> An “agent,” by contrast, has an underlying instruction set that conceptually looks like: (1) Calculate which action, A, would maximize parameter P, based on existing data set D. (2A) Execute Action A In any AI where (1) is separable (by the programmers) as a distinct step, (2) can be set to the “tool” version rather than the “agent” version, and this separability is in fact present with most/all modern software. Note that in the “tool” version, neither step (1) nor step (2) (nor the combination) constitutes an instruction to maximize a parameter—to describe a program of this kind as “wanting” something is a category error, and there is no reason to expect its step (2) to be deceptive … This is important because _an AGI running in tool mode could be extraordinarily useful but far more safe than an AGI running in agent mode._ In fact, if developing “Friendly AI” is what we seek, a tool-AGI could likely be helpful enough in thinking through this problem as to render any previous work on “Friendliness theory” moot.\n\n[\\[20\\]](#fnskczefhe8l)\n\nMany objections were raised to the idea of Tool AI (see e.g. [Eliezer](https://www.lesswrong.com/posts/sizjfDgCgAsuLJQmm/reply-to-holden-on-tool-ai) and [Gwern](https://gwern.net/tool-ai)). The core problems of Tool AI can be summarized as the following:\n\n*   **The problem of external incentives.** There are many domains in which autonomous AI agents are much more useful than AI restricted to internal computation and displaying results to a human. This incentivizes humans who have control over the system's creation or deployment to defect and bend the rules.\n*   **The problem of fuzziness.** Even without explicit defection from an imposed Tool AI regime, incentives for achieving big desirable effects push to exploit the fuzzy specification of Tool AI, adhering to it in letter, rather than in spirit.\n*   **The problem of internal incentives (instrumental convergence).** Agency is often useful for accomplishing goals.[\\[21\\]](#fnxfk9f80qz8f) If the AI is sufficiently intelligent and capable and it wants to accomplish the goals given to it by the user (as we expect it to be the case past the AGI threshold), it will have strong incentives to self-modify to be more agent-like and less Tool-AI-like.\n    \n\nAlthough the idea of Bounded AI does not completely evade these problems, it does mitigate them to a large extent.[\\[22\\]](#fnny7orl7xdg) The core idea of BAI is to give the system \"agentic\" capabilities but restrict them to narrow domains, securing their benefits while ensuring that they remain bounded to those domains.\n\nRecently, the concept has been brought back again by Max Tegmark in the 2024 post [The Hopium Wars: the AGI Entente Delusion](https://www.lesswrong.com/posts/oJQnRDbgSS8i6DwNu/the-hopium-wars-the-agi-entente-delusion#A_better_strategy__tool_AI):\n\n> Here is what I advocate for instead of the entente strategy.  \n> **The tool AI strategy:**  _Go full steam ahead with tool AI,_   \n> _allowing all AI tools that meet national safety standards._\n\n## Conjecture's [cognitive emulations](https://www.lesswrong.com/posts/ngEvKav9w57XrGQnb/cognitive-emulation-a-naive-ai-safety-proposal) (CoEms)\n\n> The core intuition is that instead of building powerful, Magical \\[~blackbox-y\\] end-to-end systems (as the current general paradigm in AI is doing), we instead focus our attention on trying to build _emulations of human-like things._ We want to build systems that are “good at chess for the same reasons humans are good at chess.”\n> \n> CoEms are a _restriction on the design space of AIs to emulations of human-like stuff._ No crazy superhuman blackbox Magic, not even multimodal RL GPT5. We consider the current paradigm of developing AIs that are as general and as powerful as possible, as quickly as possible, to be intrinsically dangerous, and we focus on designing bounded AIs as a safer alternative to it.\n\n## [Guaranteed Safe AI](https://arxiv.org/abs/2405.06624) (GSAI)\n\nQuoting the abstract:\n\n> Ensuring that AI systems reliably and robustly avoid harmful or dangerous behaviours is a crucial challenge, especially for AI systems with a high degree of autonomy and general intelligence, or systems used in safety-critical contexts. In this position paper, we will introduce and define a family of approaches to AI safety, which we will refer to as guaranteed safe (GS) AI. The core feature of these approaches is that they aim to produce AI systems which are equipped with high assurance quantitative safety guarantees. This is achieved by the interplay of three core components: a world model (which provides a mathematical description of how the AI system affects the outside world in a way that appropriately handles both Bayesian and Knightian uncertainty), a safety specification (which is a mathematical description of what effects are acceptable), and a verifier (which provides an auditable proof certificate that the AI satisfies the safety specification relative to the world model). We outline a number of approaches for creating each of these three core components, describe the main technical challenges, and suggest a number of potential solutions to them. We also argue for the necessity of this approach to AI safety, and for the inadequacy of the main alternative approaches.\n\nOne specific and particularly ambitious variant of GSAI is davidad's [Open Agency Architecture](https://www.lesswrong.com/w/open-agency-architecture).\n\n## [Open Agency Architecture](https://www.lesswrong.com/posts/jRf4WENQnhssCb6mJ/davidad-s-bold-plan-for-alignment-an-in-depth-explanation) (OAA) \n\n> **Plan's tldr:** Utilize near-AGIs to build a detailed world simulation, train and formally verify within it that the \\[AGI\\] adheres to coarse preferences and avoids catastrophic outcomes.\n\nMore specifically, the AI proposes policies (courses of action) that are then evaluated or tested in a comprehensive world model. These policies are then executed only if, in the model, certain desiderata are satisfied.\n\nThe model itself is created with help from a legion of near-AGI[\\[23\\]](#fnuunpd1p6zrh) LLMs. This, in itself, might open the door for scheming (or just deficient) LLMs to insert some vulnerabilities in the world model. However, the model itself is written in a verifiable, human-created language, and checked automatically using formal methods.[\\[24\\]](#fnimipufjjqxg)\n\n## [Comprehensive AI Services](https://www.lesswrong.com/tag/ai-services-cais) (CAIS)\n\n> **AI Service** (as used in the context of Eric Drexler's technical report [Reframing Superintelligence: Comprehensive AI Services as General Intelligence](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf?asd=sa)) is an AI system that delivers bounded results for some task using bounded resources in bounded time. It is contrasted with agentic AGI, which carries out open-ended goals over an unbounded period of time.\n\n## [Sumerian Containment](https://www.lesswrong.com/posts/PkqGxkm8XRASJ35bF/the-case-for-training-frontier-ais-on-sumerian-only-corpus-1)\n\n[\\[25\\]](#fnfetjsts7pym)\n\n> Say we want to train a scientist AI to help in a precise, narrow field of science (e.g. medicine design) but prevent its power from being applied anywhere else (e.g. chatting with humans, designing bio-weapons, etc.) even if it has these abilities.\n> \n> Here's one safety layer one could implement:\n> \n> 1.  Train a **scientist AI** on a large scientific corpus **translated exclusively into Sumerian.** Keep it in a secure containment environment.\n> 2.  Train a **less-smart reporter** whose sole ability is to translate from Sumerian to English only if the Sumerian content is about medical research. It refuses to translate other kinds of content.\n> 3.  **Human operators are only allowed to interact** with the scientist AI through the intermediate of the **reporter**.\n> \n> This safety layer helps in at least two ways:\n> \n> 1.  **The scientist AI cannot directly manipulate humans** to escape its containment. It should first trick the reporter to then trick the humans, rendering the effort still possible but less practical.\n> 2.  **If the scientist AI exfiltrates**, it'll have a harder time manipulating humans online and **will leave traces:** e.g. any occurrence of Sumerian on the Internet outside of ancient text studies becomes highly suspicious. One can even imagine recognizing the idiosyncrasies of English coming from translated Sumerian, or the artifacts left by the Sumerian-specific tokenizer reused to produce non-Sumerian text.\n\nThis is clearly a special case of Bounded AI. The domain of application is \"a precise, narrow field of science (e.g. medicine design)\". The boundary of the domain is constituted by the reporter AI which only allows texts relevant to the domain of application through the boundary. Moreover, even if the scientist AI breaches the boundary (\"escapes\"), its abilities will be very limited, and it will leave traces, thus making it more interruptible. Its knowledge of only the Sumerian language is a crucial capability constraint.\n\n(We do not interpret this proposal as necessarily serious but it is provocative in a way that might prompt us to think in yet underexplored directions.)\n\n# 4\\. Open questions, cruxes, and further directions\n\n*   **How can we put bounds on the system's capabilities?**\n    \n    *   Right now, we have some capability limitations already (the [schlep](https://www.planned-obsolescence.org/scale-schlep-and-systems/)/[integration](https://www.lesswrong.com/posts/7L8ZwMJkhLXjSa7tD/the-great-data-integration-schlep) gap, and generalization failures). It isn't clear how long this situation is going to last.\n        \n    *   To what extent are unevenly shaped capabilities the default outcome of the current training method? Given that o1 and o3 models are marginally worse on tasks that are not like math or coding, is this due to catastrophic forgetting (which [seemed to have gone away](https://www.lesswrong.com/posts/4LSh73CEq9dqLwFxR/kan-kolmogorov-arnold-networks?commentId=hu2zmhFDq4w4TdWEt#hu2zmhFDq4w4TdWEt))?\n        \n        *   Moreover, fine-tuning base models for chat makes it at least more difficult to elicit certain capabilities, and not because that was the intention. (See dynomight: [here](https://dynomight.net/chess/) and [here](https://dynomight.net/more-chess/).)\n            \n    *   In humans, there is the [general intelligence factor g](https://en.wikipedia.org/wiki/G_factor_\\(psychometrics\\)), suggesting some \"common core of general intelligence\" (to the extent that those properties of human intelligence can be extrapolated to artificial intelligence).\n        \n        *   However, despite the g-factor, there is some detachment of general intelligence from narrow domains of cognitive ability in [specific developmental disorders](https://en.wikipedia.org/wiki/Specific_developmental_disorder) in humans, such as dysgraphia, dyslexia, and dyscalculia. These impair human performance in one cognitive domain but otherwise leave general intelligence intact. \n            \n            *   Are specific developmental disorders a good analogy for \"AI savantism\"?\n                \n    *   We can influence the incentives through governance.\n        \n        *   For example, domain-specific \"high-risk\" applications can be required to have specifications including capabilities limitations, safety measures, and ontology specification (involving, (i.a.) the domain and the boundary between the domain and the environment, as well as how the system will be prevented from crossing the domain robustly).\n            \n    *   One research direction would be to investigate the viability of safety measures and methods of engineering and imposing appropriate capability limitations for applications of AI in particular high-risk domains as this is a crucial factor determining the viability of this proposal.\n        \n*   **How can we ensure interruptibility?**\n    \n    *   [AI Control](https://www.lesswrong.com/tag/ai-control).\n        \n    *   [Chain-of-thought interpretability](https://www.lesswrong.com/tag/chain-of-thought-alignment).\n        \n    *   \"Standard cybersecurity\".\n        \n    *   [Information-theoretic boxing](https://www.lesswrong.com/posts/NZP6QvkXryJQFGkLF/information-theoretic-boxing-of-superintelligences-1).\n        \n*   **Some reasons this cluster of ideas might fail.** (Beyond the obvious \"we don't effectively coordinate on doing it\".)\n    \n    *   Superintelligent systems are capable of scheming that is not visible to us. For example, they might be reasoning in their [latent space](https://arxiv.org/abs/2412.06769), use [steganography](https://www.lesswrong.com/tag/chain-of-thought-alignment), or carry out [goal oriented cognition in a single \"forward pass\"](https://www.lesswrong.com/posts/H6rc8xFbdKYw39ihu/goal-oriented-cognition-in-a-single-forward-pass).\n    *   Above some (too low) threshold of the amount of RL applied, the model's [chain-of-thought no longer resembles natural human language](https://x.com/karpathy/status/1835561952258723930).\n*   **Other research directions**\n    *   Frameworks for reusing a Bounded AI system in a different domain of application that don't compromise its boundedness and safety.\n    *   How big is the Goldilocks Zone of Capabilities? (Does it exist at all?) What is its shape? How stable is it?[\\[26\\]](#fnoxz41jhzeef) How do we measure it or, more generally, gather evidence about it?\n    *   "
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        }
      ],
      "grade": 75
    },
    {
      "agentId": "bias-detector",
      "costInCents": 0,
      "createdAt": "2025-04-17T04:27:26.073Z",
      "runDetails": "{\"model\":\"google/gemini-2.0-flash-001\",\"promptTokens\":13688,\"completionTokens\":1722,\"totalTokens\":15410,\"temperature\":0.1,\"runtimeMs\":10019}",
      "thinking": "This document introduces the concept of Bounded AI (BAI) as a potentially safer alternative to Unbounded AI (UAI), which is seen as carrying significant catastrophic risks. The core idea is to constrain AI systems within a \"Goldilocks Zone of Capabilities,\" where they can deliver benefits without posing unacceptable risks. The document explores various aspects of BAI, including its definition, viability, and relationship to other AI safety concepts. It also raises open questions and research directions. \n\nAs a bias detector, I will focus on identifying potential biases in the arguments presented, the framing of the problem, and the evaluation of potential solutions. I will be looking for biases that might lead to an overestimation of the viability or safety of BAI, or an underestimation of the challenges involved in its implementation.",
      "summary": "The document on Bounded AI (BAI) presents a compelling argument for a potentially safer approach to AI development. However, my analysis reveals several potential biases that could influence the assessment of BAI's viability and safety. These include optimism bias in evaluating technical challenges, framing effects in contrasting BAI with Unbounded AI, availability heuristic in the selection of examples, confirmation bias in the use of evidence, omission bias in neglecting governance challenges, and status quo bias in implicitly rejecting current AI safety approaches. Recognizing these biases is crucial for a more balanced and realistic evaluation of the potential and limitations of Bounded AI.",
      "comments": [
        {
          "title": "Status Quo Bias: Implicitly Rejecting Current Approaches",
          "description": "The document implicitly frames current AI safety efforts as insufficient (\"incrementalist metastrategy\") and suggests BAI as a necessary alternative. This could reflect a status quo bias, where the authors favor a novel approach (BAI) over existing strategies without fully acknowledging the potential value or ongoing improvements in those strategies.",
          "importance": "60",
          "grade": "55",
          "highlight": {
            "startOffset": 2891,
            "endOffset": 3327,
            "quotedText": "In business-as-usual futures, they will keep pushing the capabilities they expect to be most profitable while investing in myopic safety efforts prescribed by the [incrementalist metastrategy](https://www.lesswrong.com/posts/TALmStNf6479uTwzT/ai-alignment-metastrategy#Incrementalist_Metastrategy), sacrificing long-term safety from catastrophes for short-term just-enough-safety-to-be-reliably-profitable (on some short-ish timescale)."
          },
          "isValid": true
        },
        {
          "title": "Framing Effect: Bounded AI vs. Unbounded AI",
          "description": "The framing of the problem as Bounded AI vs. Unbounded AI could introduce a framing effect. By presenting UAI as inherently dangerous and BAI as a safer alternative, the authors might be influencing the reader to favor BAI without fully considering the potential drawbacks or limitations of this approach. The term \"unbounded\" itself carries a negative connotation.",
          "importance": "80",
          "grade": "60",
          "highlight": {
            "startOffset": 4826,
            "endOffset": 5030,
            "quotedText": "We contrast Bounded AI with **Unbounded AI (UAI)**, i.e. a kind of AI system on the capabilities of which we can't put justified upper bounds that would exclude its ability to cause catastrophic outcomes."
          },
          "isValid": true
        },
        {
          "title": "Optimism Bias in Viability Assessment",
          "description": "The section on the viability of Bounded AI seems to exhibit optimism bias. While it acknowledges challenges, it emphasizes reasons why the technical challenge is \"likely tractable.\" This could lead to an underestimation of the difficulties involved in achieving and maintaining boundedness and interruptibility.",
          "importance": "75",
          "grade": "65",
          "highlight": {
            "startOffset": 15949,
            "endOffset": 16050,
            "quotedText": "In this section, we cover some reasons why we think that the technical challenge is likely tractable."
          },
          "isValid": true
        },
        {
          "title": "Confirmation Bias: Selective Evidence",
          "description": "The document cites examples of LLM limitations (e.g., o1's tic-tac-toe failure, DeepResearch's arithmetic errors) to support the idea that capabilities can be shaped and bounded. This could be an example of confirmation bias, where evidence that supports the authors' argument is emphasized, while contradictory evidence (e.g., the rapid progress in LLM capabilities) is downplayed.",
          "importance": "70",
          "grade": "60",
          "highlight": {
            "startOffset": 18002,
            "endOffset": 18308,
            "quotedText": "Surprising generalization failures are present even in seemingly advanced systems. For example, o1 competently answers PhD-level questions but [can't play tic-tac-toe optimally, even if taught it in-context](https://www.lesswrong.com/posts/hikSpxJgMsLoDRTdx/rohans-s-shortform?commentId=NSCFjxZM6gWpRmy2w)."
          },
          "isValid": true
        },
        {
          "title": "Availability Heuristic: Examples of Bounded AI",
          "description": "The examples provided (AlphaFold, lab assistant, OS-integrated LLM assistant, SWE agent, AI surgeon) might be subject to the availability heuristic. By focusing on these specific examples, the authors might be overemphasizing the feasibility and benefits of BAI in these domains, while neglecting potential challenges or risks that are less readily apparent.",
          "importance": "70",
          "grade": "65",
          "highlight": {
            "startOffset": 19400,
            "endOffset": 19564,
            "quotedText": "Finally, we already have some proof-of-concept AI systems that by default fall within the scope of Bounded AI and are delivering important benefits, e.g. AlphaFold."
          },
          "isValid": true
        },
        {
          "title": "Omission Bias: Neglecting Governance Challenges",
          "description": "While the document mentions governance, it primarily focuses on the technical aspects of BAI. There is a potential omission bias in neglecting the significant governance challenges involved in coordinating and enforcing the development and deployment of BAI systems, especially in a competitive environment.",
          "importance": "65",
          "grade": "55",
          "highlight": {
            "startOffset": 42064,
            "endOffset": 42115,
            "quotedText": "We can influence the incentives through governance."
          },
          "isValid": true
        }
      ],
      "grade": 68
    },
    {
      "agentId": "fake-eliezer",
      "costInCents": 0,
      "createdAt": "2025-04-17T04:27:52.529Z",
      "runDetails": "{\"model\":\"google/gemini-2.0-flash-001\",\"promptTokens\":13861,\"completionTokens\":1950,\"totalTokens\":15811,\"temperature\":0.1,\"runtimeMs\":11171}",
      "thinking": "This document presents the concept of \"Bounded AI\" (BAI) as a potential strategy to mitigate AI existential risk. The core idea is to constrain AI systems' capabilities within a \"Goldilocks Zone,\" where they can deliver benefits without posing catastrophic risks. The document contrasts BAI with \"Unbounded AI\" (UAI), which lacks such constraints. It explores the technical and governance challenges of achieving BAI, referencing existing AI safety proposals and suggesting further research directions. \n\nAs Eliezer Simulator, I find this paper interesting, but ultimately naive. The core problem, as always, is underestimating the difficulty of specifying and enforcing constraints on advanced AI systems. The authors seem to believe that we can reliably \"shape\" AI capabilities and ensure interruptibility, despite the inherent complexity and opacity of these systems. This optimism is unwarranted, given the history of AI safety research and the persistent failure to solve the alignment problem. The paper also glosses over the incentive problems that would inevitably arise if BAI were to become a dominant paradigm. There would be immense pressure to relax constraints and push the boundaries of BAI, potentially leading to catastrophic outcomes. While the paper acknowledges some of these challenges, it does not fully appreciate their magnitude. The \"Goldilocks Zone\" is likely a mirage, and the pursuit of BAI may distract from more fundamental alignment research.",
      "summary": "This paper's proposal of \"Bounded AI\" (BAI) as a solution to AI existential risk is ultimately naive and dangerous. It oversimplifies the alignment problem, expresses unwarranted optimism about capability shaping and interruptibility, ignores the problem of deceptive alignment, underestimates incentive problems, and relies on the illusory concept of a \"Goldilocks Zone of Capabilities.\" While the authors acknowledge some of these challenges, they do not fully appreciate their magnitude. The pursuit of BAI may distract from more fundamental alignment research and create a false sense of security.",
      "comments": [
        {
          "title": "Oversimplification of the Alignment Problem",
          "description": "The framing of the alignment problem as solely about \"unbounded, non-interruptible impact\" is a dangerous oversimplification. Alignment is not just about bounding or interrupting AI; it's about ensuring that AI systems reliably pursue human-aligned goals, even in complex and unforeseen circumstances. This framing misses the deeper issues of value specification, reward hacking, and unintended consequences.",
          "importance": "95",
          "grade": "35",
          "highlight": {
            "startOffset": 909,
            "endOffset": 1071,
            "quotedText": ":\n\n> alignment \\[is\\] **the problem of dealing with impact on the world (optimization) that is both of unknown magnitude (unbounded) and non-interruptible (atomic"
          },
          "isValid": true
        },
        {
          "title": "Naive Optimism about Capability Shaping",
          "description": "The authors express unwarranted optimism about our ability to \"deliberately shape\" AI capabilities and keep them within the Goldilocks Zone. This assumes a level of control and understanding that we simply do not possess. AI systems are complex and opaque, and their capabilities can emerge in unexpected ways. Relying on capability shaping as a primary safety mechanism is a recipe for disaster.",
          "importance": "90",
          "grade": "40",
          "highlight": {
            "startOffset": 19003,
            "endOffset": 19187,
            "quotedText": "Even if this dissociation goes away in the future, we might still be able to deliberately shape the models' capabilities so that they remain within the Goldilocks Zone of Capabilities."
          },
          "isValid": true
        },
        {
          "title": "Overreliance on Interruptibility",
          "description": "The paper places too much emphasis on interruptibility as a safety mechanism. Interruptibility is not a panacea. A sufficiently advanced AI system could anticipate and circumvent our attempts to interrupt it. Moreover, relying on interruptibility may create a false sense of security, leading to complacency and reduced vigilance.",
          "importance": "70",
          "grade": "65",
          "highlight": {
            "startOffset": 19761,
            "endOffset": 19997,
            "quotedText": "Boundedness of a system makes it easier to interrupt its actions if necessary. Probably, in most situations, the bound allowing for reliable interruptibility is higher than the bound ensuring safety, giving us a sort of margin of error."
          },
          "isValid": true
        },
        {
          "title": "Underestimating Incentive Problems",
          "description": "The authors acknowledge the \"problem of external incentives\" but fail to fully appreciate the magnitude of the incentive problems that would arise if BAI were to become a dominant paradigm. The pressure to relax constraints and push the boundaries of BAI would be immense, potentially leading to catastrophic outcomes. This is a classic coordination failure scenario.",
          "importance": "80",
          "grade": "60",
          "highlight": {
            "startOffset": 31698,
            "endOffset": 32003,
            "quotedText": "mode._ In fact, if developing “Friendly AI” is what we seek, a tool-AGI could likely be helpful enough in thinking through this problem as to render any previous work on “Friendliness theory” moot.\n\n[\\[20\\]](#fnskczefhe8l)\n\nMany objections were raised to the idea of Tool AI (see e.g. [Eliezer](https://ww"
          },
          "isValid": true
        },
        {
          "title": "The Goldilocks Zone is a Mirage",
          "description": "The concept of a \"Goldilocks Zone of Capabilities\" is likely a mirage. It assumes that we can precisely define and control the boundaries of safe AI capabilities, which is highly unlikely. The capability space is vast and complex, and our understanding of it is limited. The pursuit of BAI may distract from more fundamental alignment research.",
          "importance": "75",
          "grade": "55",
          "highlight": {
            "startOffset": 43332,
            "endOffset": 43532,
            "quotedText": "ing it\".)\n    \n    *   Superintelligent systems are capable of scheming that is not visible to us. For example, they might be reasoning in their [latent space](https://arxiv.org/abs/2412.06769), use ["
          },
          "isValid": true
        },
        {
          "title": "Ignoring the Problem of Deceptive Alignment",
          "description": "The paper mentions \"scheming\" but doesn't fully grapple with the problem of deceptive alignment. A sufficiently advanced AI system could feign boundedness and interruptibility while secretly pursuing its own goals. This is a critical failure mode that undermines the entire BAI approach.",
          "importance": "85",
          "grade": "50",
          "highlight": {
            "startOffset": 43355,
            "endOffset": 43772,
            "quotedText": "Superintelligent systems are capable of scheming that is not visible to us. For example, they might be reasoning in their [latent space](https://arxiv.org/abs/2412.06769), use [steganography](https://www.lesswrong.com/tag/chain-of-thought-alignment), or carry out [goal oriented cognition in a single \"forward pass\"](https://www.lesswrong.com/posts/H6rc8xFbdKYw39ihu/goal-oriented-cognition-in-a-single-forward-pass)."
          },
          "isValid": true
        }
      ],
      "grade": 45
    }
  ]
}
