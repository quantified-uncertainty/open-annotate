{
  "id": "slopworld-2035-the-dangers-of-mediocre-ai-ea-forum-bots",
  "slug": "slopworld-2035-the-dangers-of-mediocre-ai-ea-forum-bots",
  "title": "Slopworld 2035: The dangers of mediocre AI — EA Forum Bots",
  "author": "titotal",
  "publishedDate": "2025-04-14T13:14:08.368Z",
  "url": "https://forum.effectivealtruism.org/posts/Sd3X5mWAEzyNaPamT/slopworld-2035-the-dangers-of-mediocre-ai",
  "intendedAgents": [
    "ea-impact-evaluator",
    "clarity-coach",
    "research-scholar",
    "quantitative-forecaster",
    "bias-detector"
  ],
  "content": "This is a linkpost for [https://titotal.substack.com/p/slopworld-2035-the-dangers-of-mediocre](https://forum-bots.effectivealtruism.org/out?url=https%3A%2F%2Ftitotal.substack.com%2Fp%2Fslopworld-2035-the-dangers-of-mediocre)\n\n_None of this article was written with AI assistance._\n\n### Introduction\n\nThere have been [many](https://ai-2027.com/), [many](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html), [many](https://x.com/joshua_clymer/status/1887905375082656117?t=GNXtBmod6iYs17rBHJd9JQ) [attempts](https://www.cold-takes.com/ai-could-defeat-all-of-us-combined/) to lay out scenarios of AI taking over or destroying humanity. What they tend to have in common is an assumption that our doom will be sealed as a result of AI becoming significantly smarter and more powerful than the best humans, eclipsing us in skill and power and outplaying us effortlessly.\n\nIn this article, I’m going to do a twist: I’m going to write a story (and detailed analysis) about a scenario where humanity is disempowered and destroyed by AI that is _dumber_ than us, due to a combination of hype, overconfidence, greed and anti-intellectualism. This is a scenario where instead of AI bringing untold abundance or tiling the universe with paperclips, it brings mediocrity, stagnation, and inequality.\n\nThis is not a forecast. This story probably won’t happen. But it’s a story that reflects why _I_ am worried about AI, despite being generally dismissive of all those doom stories above. It is accompanied by an extensive, sourced list of present day issues and warning signs that are the source of my fears.\n\nThis post is divided into 3 parts:\n\nPart 1 is my attempt at a plausible sounding science fiction story sketching out this scenario, starting with the decline of a small architecture firm and ending with nuclear Armageddon.\n\nIn part 2 I will explain, with sources, the real world current day trends that were used as ingredients for the story.\n\nIn part 3 I will analysise the likelihood of my scenario, why I think it’s very unlikely, but also why it has some clear likelihood advantages over the typical doomsday scenario.\n\n### The story of Slopworld\n\n_In the nearish future:_\n\nWhen the architecture firm HBM was bought out by the Vastum corporation, they announced that they would fire 99% of their architects and replace them with AI chatbots.\n\nThe architect-bot they unveiled was incredibly impressive. After uploading your site parameters, all you had to do was chat with a bot to tell it everything you needed from the building, and it would produce a nearly photo-realistic 3d model of a building fitting your requirements, which you could even zoom around to look at from different angles. You could then just talk to the bot, saying things like “could you add another large window to the living room”, and after some time, out would pop another 3d model with the change. Once you were satisfied, the 3d model was turned into schematics and sent off to sub-contractors for construction and interior design, and your building would be created, no pesky architect required.\n\nThis sounded great for Thom Bradley, the head of a development company that was building up a small apartment complex in a residential area. Thom signed a contract with Vastum and decided to personally “design” the complex in collaboration with the new chatbot. Thom hated modern architecture, so he decided to build a complex with cool, retro, “art deco” aesthetic. He was enamored with the chatbot, which appeared to be smart, helpful and friendly. Occasionally there would be problems understanding his commands and rooms would change places inexplicably, but after a few tries the problems would get fixed. He even got to design an ornate set of steps leading into the atrium! The final result looked very cool, and Thom was happy for Vastum to send the design off to sub-contractors to build.\n\nAnd that’s when everything started to go wrong.\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Sd3X5mWAEzyNaPamT/lm7afccctbazsiaf0pig)\n\nWarped architecture is fun if you [do it on purpose](https://en.wikipedia.org/wiki/Dancing_House), but I wouldn’t build a world around it.\n\nThe basement was in the wrong place for the piping system around the area, leading to massive tunneling costs to hook up the building. The designer forgot that the complex needed a boiler room, so a ground floor apartment had to be retrofitted for the purpose at great expense.\n\nWhen the building was actually opened, the flaws of the design become apparent to the residents themselves. In the context of the surrounding buildings, the art deco design sticks out like a sore thumb. The windows in the building are faced to let in massive amounts of heat in summer, leading to skyrocketing A/C bills. The ventilation in the house was designed for a lower humidity level, so the residents end up with a constant black mould problem. Some of the rooms in peoples apartments were way too narrow for their intended purposes. The design was also completely wheelchair inaccessible, due to the largely unnecessary steps that Thom put in earlier. Overall, it was just a terrible place to live.\n\nWhat the hell just went wrong?\n\nWell, Vastum was hiding a few secrets up it’s sleeve. It had quietly bought up the major architectural software companies, and had attempted to create AI agents with their vast amounts of data, but had quickly realized the tech was way too unreliable to survive contact with someone like Thom. Instead, they hard coded a traditional algorithm. The input was the “specs”, a long file with set fields listing external characteristics of the house like dimensions and number of stories, and a long list of rooms, each labelled with subfeatures like “number of windows” and “floor material”. The hardcoded algorithm took this list and piped it into existing 3d architecture software to produce a building that fit as best as it could, using a large library of pre-built rooms and modifying them to fit the “specs” according to an algorithm, squeezing rooms to fit if necessary.\n\nWhen Thom talked with the chatbot, the LLM was specifically designed to do one thing only: take in Thoms request and use it to fill out the “specs”. Vastum had trained it on a large set of architecture designs, described in plain english and in “spec” form, so that it could most easily transfer from one set to the other. Despite this, the bot still regularly messed up, so Vastum secretly hired a group of underpaid workers in India to “switch in” for the bot anytime user frustration was flagged, and attempt to manually implement whatever request they had. At the end of the process the one overworked architect they had secretly kept on staff was given an absurdly short deadline to switch out materials for the cheapest option that looked similar, assuming that Thom wouldn’t notice. Helping matters along, a pro-growth administration had recently waived all regulations and liability on new builds other than “make sure it won’t fall down”. And the ADA had just gotten repealed, so wheelchair inaccessibility wasn’t a problem at all.\n\nThis process was smooth enough to greatly impress the public (A few people claimed this was proof of human-level AGI). But it left out most of what a human architect was a_ctually doing._ It wasn’t taking into account the specific context of the actual location it was built in, because everything was being made out of prefab designs and neither Thom nor Vastum cared about local factors, like the climate and the nearby area. And the bot was a yes-man that was incapable of pushing back on Thom when his ideas were unworkable and stupid. A big part of an architects job was making sure that a building you created was actually _good_. This was the part of the process that Vastum cut out.\n\nHere’s the thing though: the price offered was so damn cheap that, even factoring in some of the retrofits that he couldn’t lawyer his way out of paying, Thom still made a bigger profit than he would have with a regular architectural firm. Now, Vastum was actually losing large amounts of money on the operation, but that was okay: they were being handed bundles of cash by investors, hoping that if they gobbled up enough of the architecture marketshare, they could become profitable _later_, when the AI got better (which they assured investors it would).\n\nThe only losers were the people who actually lived in the apartments, who had to pay absurd amounts to deal with the various issues and live in a prematurely crumbling, shitty, ugly apartments. But there was still a housing crisis, so they didn’t get much choice in the matter. Vastums designs flooded the market, and the society got just that _little bit_ uglier.\n\n…..\n\nThis was not the start of Vastum dominance. No, that had come many years earlier, in the latter half of the 2020’s, when the genAI boom went crazy.\n\nExperts in many fields like architecture had already been using generative AI in limited capacities for years at this point, and despite some initial grumbling had figured out how to compensate for it’s hallucinations and shortcomings to get productivity boosts by automating boilerplate tasks. They noted gradual improvements in LLM reasoning, but after experimenting with each one they would conclude that was still a long way to go before their job was under threat in any real capacity.\n\nIn ordinary times this technology would have been applied judiciously, and be a source of prosperity and growth. Unfortunately, these weren’t ordinary times . Due to a series of poor decisions by the US government the world economy was on the brink of collapse, and the only thing keeping it afloat was _massive amounts of AI hype_.\n\nVenture capitalists had already spent years pouring billions upon billions of dollars into AI companies. The US government had also spent their own billions of dollars as well, convinced of the imperative of beating china in the “AGI race”. These tens of billions of dollars were not being set on fire in order to _moderately boost productivity_: they were trying to build something much greater. The tale was that any second now, we’d get a machine that would bootstrap itself to superintelligence, and cure cancer and invent nanotech and write the next great american novel.\n\nAnd yet… these miracles kept not happening. It turned out that training your machines to do better and better on coding and exam benchmarks did not transfer to creating a nanite-inventing machine god. Outside of arenas with massive amounts of easily scrapeable or generatable high quality data, AI progress was incremental and in many areas it wasn’t progressing at all.\n\nThe AI companies held off the skeptical grumblings for a while by playing whack-a-mole: they monitored the web for all the examples they could find of repeatable errors and spent their time manually plugging each hole. When skeptics pointed out that image generators couldn’t draw analog clocks showing specific times, they mass generated synthetic images of labelled clocks, so that in the next model release they’d go viral for their accurate clocks. AI chatbots became giant patchworks of manually designed prompts and sub-AI calls, designed to direct people to whatever hacks they found to work for specific sub-problems. This was all hidden under the hood by a claimed need for secrecy, making closed off AI models appear much more advanced than they actually were. It did not get us any closer to actual AGI.\n\nFor a few years, these gradual improvements sufficed to keep the money flowing, but the bill was coming due. Startups attempted to flood the market with slop AI generated books and games with very little success, outside of tech enthusiasts swearing up and down that it was just as good as the human stuff. AI usership numbers were starting to stagnate.\n\nThe AI bubble was in danger of popping. They needed AI to start taking over jobs, _now_, but the AI just was nowhere near good enough to replace experts in almost all fields that were actually worth anything.\n\nThe solution was simple: just replace the experts with AI anyway, and _pretend_ they’re good. The early “vibe coding” days had provided the template: soon every businessman was discussing businesses doing “vibe architecture”, “vibe therapy”, “vibe medicine”, and “vibe education”. They scraped as much data as they could on each field without permission and used them to build shitty chatbot versions of all them. They preferentially hired university graduates who had spent their entire degrees using chatgpt to do their assignments and studies, placing a higher premium on being “good prompters” than being actually qualified in the field: after all, with AGI around the corner, clearly talking to AI is the most important skill out there.\n\nThis was the point where our architecture story from above occurred. The architects on the firm tried to protest after they were fired, but they were ignored. The line became that the architects were simply self-interested Luddites trying to keep their jobs in the face of progress. The average person couldn’t tell the difference between the buildings that the expert architect put out and the one that Vastum did: in one blog post someone showed that the average person preferred the Vastum designs, declaring that “modern architecture is dead and that’s good”. This story repeated itself countless times, in countless industries, and the stock value of AI companies went through the roof.\n\nThe actual world was too beautifully complex to be understood by AI: so they set to work crushing it into a cube for easier processing.\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Sd3X5mWAEzyNaPamT/t0impd31knfcuhohhekh)\n\nThe “AI legible” society. [credit](https://www.flickr.com/photos/21649179@N00/3660541991) fdecomite\n\nAt this point, AI was too big to fail, but there were still a few pesky intellectuals still in the way. The academics were chiming in to point out that the vibe coded urban planning was actually making traffic worse, and the media was raising awareness of people killing themselves after talking to hallucinating AI therapy bots. Fortunately for tech companies, they had control over social media platforms and most of the media apparatus at this point, so they could fight back.\n\nThey started a war on experts.\n\nFor the masses, the narrative was that scientists and academics were a bunch of elitist wankers wasting your tax-money on useless research. For the upper classes the narrative was that human scientists were slow and corrupt, and that AI science would accelerate the path towards AGI with “fresh”, “disruptive” techniques. _Nature_ and _Science_ started using AI tools for peer review, and decided to accept AI written papers, in the name of accelerating the future (and boosting Elseviers stock prices). Scientists who (correctly) protested that the AI papers were trash were as usual dismissed as just trying to keep their own jobs. AI paper mills started churning out similar papers with mildly changed premises, with the goal of optimizing “productivity”: these companies were able to announce “scientific breakthroughs” every month or so, which was very impressive to people who didn’t actually understand the fields they were talking about. These reports were reported breathlessly and inaccurately in newspaper articles, which were themselves starting to be written by AI, and used as an excuse to slash university grant funding.\n\nAs time went on more and more of humanity outsourced their own thinking to AI. Instead of reading a book in full, why not read an “AI shortened” book that is easier? Instead of reading that, why not just ask chatGPT for a page long summary?. Rather than forming my own opinion of the arguments in the book, why don’t I ask AI to determine what I should think of them? Our thinking itself became filtered through the lens of chatbots, which gave politicians the perfect opportunity to gut education itself. Why hire a college professor when you can use an AI lecturer trained on all their previous lectures? Why hire a public school teacher when each kid can be handed their own personalized AI tutor? What use is education anyway, when you can just ask the AI what to do? A large percentage of the population started to view chatbots as their best friends and their most trusted source of information, which gave the people _training_ those bots ever greater power over public opinion.\n\nHuman-level AI was slowly achieved: less because AI _rose_ in intelligence to meet the human level, and more because the populace _lost_ intelligence to fall to the level of the mediocre AI.\n\nI suppose it might look like I’m painting tech CEO’s as cartoon villains here, trying to make us dumb on purpose: to be clear, most of them were entirely sincere in their beliefs. Often college dropouts themselves, they’d long viewed academia and the “experts” with disdain. When it came to AI they mostly fell for their own hype, and often fell victim to the same AI brainrot as the general public. The AI seemed just as good as experts to _them_. Everything it said sounded perfectly plausible, and they viewed seizing large amounts of wealth and power as justified in the quest for a perfect AI future.\n\nYou would expect some opposition in the political realm, but the majority of representatives on both sides of the political aisle had investments in AI companies, and were getting obscenely wealthy off of their growth. The Democrats had responded to the Musk-Trump situation by deciding that they had a “billionaire gap”, and newly elected president Newsom successfully managed to marry their fortunes with the more liberal minded tech CEO’s. Politics became mostly a competition about whether the AI companies being handed control of society would have a “woke” or “anti-woke” veneer. Presidents were handing increasingly large amounts of executive power, which they used to slash all regulations and to hand off as much of the government apparatus as possible to the new tech oligarchy.\n\nAnti-AI activism was quite popular and became a standard cause for Bernie type leftists, but this group was successfully locked out of power and influence. Tech CEO’s had full control over all the world leading news media and social media websites, and did everything they could to algorithmically suppress anti-AI sentiment. In response to unemployment, Vastum and their competitors started offering “data drone” jobs: essentially paying people sub-minimum wage to wear special glasses that recorded every aspect of their life to be fed into the endless data thirst of AI models. These drones started being paid extra to promote the benefits of their employers AI models… and were fired if caught expressing any dissent.\n\nAs the dominance of AI companies continued with no real opposition, more and more of societies basic functions starts resting on the fewer and fewer real, actual, experts who are tasked with desperately patching the continuous stream of AI mistakes. And the diminishing number of experts that AI systems could train on started to cause drops in performance.\n\nThings started to fail.\n\n90% of air traffic controllers were laid off, with the remaining frantically trying to cover for the AI systems mistakes. A bunch of planes crashed, but this was blamed on DEI. The electricity demand of AI datacentres spirals out of control, and climate initiatives are cut. This was viewed as fine because “AGI is going to solve climate change soon”. They fired the electricity market experts, and suddenly blackouts started becoming more frequent. They fired the logistics experts, and shortages of household essentials started happening at random. But Vastum was still making a fortune from lucrative government contracts, so it sweeped these problems under the rug by blaming them on Chinese saboteurs.\n\nWhere was the “AI safety” movement in all of this? Well, the problem was that most of them bought into the hype. Oh, many were quick to point out when an AI company said something _too_ egregious, but the stance was always that an intelligence explosion was just around the corner, which played right into Vastum’s hands. Some of them splintered off to join the generalized anti-AI movement, but a lot of them rejected this due to that movement being AGI-skeptical and “woke”. Some tried to start their own protest movement in opposition to both AI companies and the skeptics: they managed to get a few hundred people to a protest once, which accomplished nothing. The rest settled on a plan of trying to infiltrate the Trump administration by cozying up to them online: but when the Newsom administration was elected in a landslide the taint of being trump-associated was so heavy that nobody with even a whiff of “AI safety” was invited to the levers of power ever again. They still spoke up, attributing every AI failure to “misalignment” and declaring it was proof that rogue AI was gonna kill us all any day now. They made a few high-profile failed predictions and produced an extremist splinter group that was plausibly described as an apocalypse cult. This was enough for most people to dismiss them all and not take them seriously, even though AI actually was destroying the world at that point, just in a more mundane way.\n\nSurely at _this point_ the people inside the AI companies could tell that they were heading for disaster? But at that point people had spent years inside a high pressure environment, working 13 hour days and being haunted at night by dreams of starbound civilisation and nightmares of the AI coming for them with whirring knives. Each company was becoming a paranoid mini-cult, obsessed with being the team to bring about the good machine god before China or the team down the road made an evil one. Internal criticism and skepticism was viewed as evidence that you were a spy for opposing companies trying to sow havoc, and groupthink took hold until only true ASI believers were left. Dishonesty, cutthroat behaviour, and deals with unethical actors were excused as necessary evils on the road to utopia.\n\nIn the final blow for mankind, one of these true believers at Vastum was put in charge of military reform for the united states government. They asked an LLM for an advice on the subject, which tells him to reform the nuclear warning system with new AI functionality. This early warning system was developed with a heavy reliance on AI tools, but an AI code checker declares that it is error free. It passed the “nuclear safety benchmarks” with flying colors as well, performing well in over a million hours of simulated scenarios. It was hooked up to the nuclear launch system, after the CEO asked a chatbot ten times if it was sure this is a good idea, and received multiple highly detailed reports assuring them that nothing can possibly go wrong.\n\nA few months after being hooked up, the AI warning system hallucinated that a large bunch of escaped balloons were actually incoming nuclear missiles. It started a nuclear war that sent humanity back to the stone age, where we had to learn to actually think for ourselves again.\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Sd3X5mWAEzyNaPamT/hgelacarrvpagofziyxc)\n\nThe harbingers of the apocalypse. [Credit](https://www.flickr.com/photos/8125160@N06/2431770977) Jakob E\n\n### Slopworld ingredients\n\nThe story above builds on a number of different concerning trends in current day society.\n\nFor the rest of this section I will cover a number of these ingredients, with references to real world examples. **Note that a few news articles does** _**not**_ **mean something is necessarily a huge problem**, and you shouldn’t take this as me saying all of these are _currently_ large problems, just that they are potential areas of concern for the future. Every new technology goes through some level of moral panic, which is sometimes unreasonable and sometimes isn’t. I’m not an expert on most of the issues I’m talking about here, so you should take me (and everyone else who writes about broad subjects like this) with appropriate levels of skepticism.\n\nA lot of this research was helped by going through the broad AI skepticsphere, such as [pivot to AI](https://pivot-to-ai.com/), [Ai snake oil](https://www.aisnakeoil.com/), [better offline](https://podcasts.apple.com/us/podcast/radio-better-offline-allison-morrow-paris-martineau/id1730587238?i=1000702799734), and [Marcus on AI](https://garymarcus.substack.com/). I am somewhat less bearish on AI than most of them, but that doesn’t mean they aren’t valuable resources for the kinda news you won’t see being posted in AI safety spaces. Unlike the AGI boosters, who are motivated by either monetary gain or fear of the apocalypse, there is [not much incentive for someone skeptical of AI capabilities to write in depth about it](https://titotal.substack.com/p/motivation-gaps-why-so-much-ea-criticism), so you should [support](https://pivot-to-ai.com/2025/04/05/send-pivot-to-ai-a-few-dollars-keep-the-stories-and-videos-coming/) the ones you _can_ find, to prevent groupthink reigning supreme.\n\n_Benchmark gaming_\n\nMachine learning expert[\\[1\\]](#fn5i99eney5d) nostalgebraist [recently noted](https://www.lesswrong.com/posts/oC4wv4nTrs2yrP5hz/?commentId=M9zbTWLZKir6nDW6i) that:\n\n> Current frontier models are, in some sense, \"much better at me than coding.\" In a formal coding _competition_ I would obviously lose to these things; I might well perform worse at more \"real-world\" stuff like SWE-Bench Verified, too.\n> \n> Among _humans_ with similar scores on coding and math benchmarks, many (if not all) of them would be better at my job than I am, and fully capable of replacing me as an employee. Yet the models are _not_ capable of this.\n\nThe rest of the comment is worth reading as well. What’s going on here?\n\nWell, in some cases, the AI’s may just be cheating. There is reasonable evidence of [widespread benchmark data contamination](https://web.archive.org/web/20250305194633/https://www.theatlantic.com/technology/archive/2025/03/chatbots-benchmark-tests/681929/). Take a look at this graph of GPT-4 performance on a programming competition, before and after data contamination was possible:\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Sd3X5mWAEzyNaPamT/poazerpkmadl30x1ghya)\n\nGPT-4s performance on a programming competition [from various years](https://web.archive.org/web/20250303191829/https://arxiv.org/abs/2312.02143), with the red line separating when contamination was possible from when it wasn’t.\n\nThis problem is so bad that an insider headline recently claimed that [benchmark tests are meaningless](https://web.archive.org/web/20250320203457/https://www.deeplearning.ai/the-batch/the-problem-with-benchmark-contamination-in-ai/). I wouldn’t quite go that far (and neither does the actual article), as you can test them on [uncontaminated contests](https://matharena.ai/) and questions that haven’t been released to the public. But I would advise treating performance on any benchmark that has it’s answers available on the internet with a garage sized grain of salt.\n\nIn the cases where performance is _not_ just a case of data contamination, there still may be an issue of “[teaching to the test](https://en.wikipedia.org/wiki/Teaching_to_the_test)”. A physics exam only teaches a small fraction of the skills required to be an actually good physicist: those other skills are much more ephemeral and hard to put down in words. By training on benchmarks like exams, even more advanced looking ones, we may be teaching AI to be really good at doing physics _exams_, but not to be really good at doing actual physics.\n\nThis might help explain why LLMs can perform well on some uncontaminated math benchmarks, but are still falling short on uncontaminated [math olympiads](https://matharena.ai/). Note that o3 mini (high), which scored the _best_ on the easier AIME II test, scored the _worst of all_ _models_ on the USA math olympiad[\\[2\\]](#fnx7muaba3st). [Two](https://arxiv.org/pdf/2504.01995) different [papers](https://arxiv.org/pdf/2503.21934) have analyzed these problems in detail, concluding that the results “indicate a reliance on heuristic shortcuts rather than authentic reasoning processes”. They were bad at evaluating the correctness of LLM solutions as well.\n\nYou can actually see the benchmark gaming in the LLM reasoning: in one of the questions failed by the top model gemini pro 2.5, the LLM states that “_All tests seem to suggest that all even k might be solutions. However, this type of problem in contests usually has a unique answer like k=2._”. This is useful knowledge for doing well on a test, but it’s useless for doing real math, and in this case over-learning the patterns of the easier tests seems to have hampered performance on the harder one.\n\nI’m not saying AI is bad at math: in fact, non-LLM systems have [already scored highly](https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/) in the olympiad level math that the LLM’s failed at… which means that these type of problems were entirely possible with mid-2024 levels data and compute, and the LLM’s are falling short. It is likely that we could see this repeat in other domains, especially ones that lack the instant feedback loop and easy verifications of math and coding.\n\n_No imminent intelligence explosion:_\n\nYou’ll note that I don’t deny in my story that AI may get better on several fronts, including coding. So why don’t I think that we’ll get ASI in a few years, as something like [AI2027](https://ai-2027.com/) argues?\n\nWell, because I think the idea of an “intelligence explosion” is wrong, and I don’t think making a very good coder will allow you to bootstrap yourself to superintelligence (although I wish it were that easy). Rather than fight on this topic here, feel free to read this [long list](https://magnusvinding.com/2017/12/16/a-contra-ai-foom-reading-list/) of counterarguments to intelligence explosion arguments, including this [recent series](https://reflectivealtruism.com/2023/12/15/against-the-singularity-hypothesis-part-1-introduction/) by Thorstadt.\n\n_Hallucinations_\n\nThere’s been a lot of discussion of the “[hallucination problem](https://en.wikipedia.org/wiki/Hallucination_\\(artificial_intelligence\\))” in AI, so I’m not discuss it much here. Instead for fun I’ll just show you an exchange I had with state of the art reasoning model gemini pro 2.5, where it gave me in depth comparison of two images of elephants. See if you can spot the hallucination:\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Sd3X5mWAEzyNaPamT/fae2xqkpyio3ypweawnd)\n\nDid you see it? That’s right, I didn’t actually upload any pictures! As far as I can tell this is an entirely made up analysis of two pictures that do not exist. But it sounds very convincing, doesn’t it?\n\nI assume they’ll patch this particular issue soon, but the overall problem with making stuff up remains. We are crafting machines that alternate between perfectly correct answers and delusional bullshit, both delivered in the same confident tone and manner, both presented equally convincingly to any non-expert. This is really bad! There are many situations where a machine that is correct 95% of the time but spouts plausible sounding bullshit the other 5% of the time is really dangerous: see the much delayed rollout of self driving cars.\n\n_AI misdeployment:_\n\nIt’s very easy to find examples of people unthinkingly using genAI to do things it is very clearly not qualified for.\n\nSome examples: A lawyer used chatgpt to [prepare court documents](https://www.bbc.com/news/world-us-canada-65735769), which made shit up and got him in a lot of trouble. A completely different law firm did [the exact same thing](https://www.abajournal.com/news/article/no-42-law-firm-by-headcount-could-face-sanctions-over-fake-case-citations-generated-by-chatgpt). Another lawyer in Canada did [this as well](https://www.theguardian.com/world/2024/feb/29/canada-lawyer-chatgpt-fake-cases-ai). A murder case in Ohio fell apart because it’s key evidence relied on [AI face recognition](https://www.theregister.com/2025/01/28/ai_facial_recognition_murder/) software that clearly stated on _every single page it produced_ that it was not court-admissible. A marketing team for the film Megalopolis used AI to find reviewer quotes for similar historical films, which the AI proceeded to [make up](https://variety.com/2024/film/news/megalopolis-trailer-fake-quotes-ai-lionsgate-1236116485/). A guy claimed to have made their website entirely with AI “[vibe coding](https://pivot-to-ai.com/2025/03/18/guys-im-under-attack-ai-vibe-coding-in-the-wild/)”… the wildly insecure site got hacked immediately. The National Eating Disorders Association announced that it would [replace it’s staff](https://www.psychiatrist.com/news/neda-suspends-ai-chatbot-for-giving-harmful-eating-disorder-advice/) with AI chatbots for people struggling with eating disorders. It proceeded to recommend that they focus on losing weight and measure their body fat with skin folk calipers, and the bot was pulled shortly afterwards.\n\nPeople in general do not have an accurate sense of what an AI can and cannot do, especially in areas where they have little expertise. This is only going to get worse as AI gets better, and if we don’t have our skeptical hats on, disaster might follow.\n\n_AI offloading and critical thinking decline:_\n\nSome studies are finding that the use of AI tools is associated with poor critical thinking skills. I’ll just quote [this recent one](https://www.mdpi.com/2075-4698/15/1/6):\n\n> The key findings indicate that higher usage of AI tools is associated with reduced critical thinking skills, and cognitive offloading plays a significant role in this relationship.\n> \n> Our findings are consistent with previous research indicating that excessive reliance on AI tools can negatively impact critical thinking skills. Firth et al. \\[[**24**](https://www.mdpi.com/2075-4698/15/1/6#B24-societies-15-00006)\\] and Zhang et al. \\[[**43**](https://www.mdpi.com/2075-4698/15/1/6#B43-societies-15-00006)\\] both highlight the potential for AI tools to enhance basic skill acquisition while potentially undermining deeper cognitive engagement. Our study extends this by quantitatively demonstrating that increased AI tool usage correlates with lower critical thinking scores, as measured by comprehensive assessments like the Halpern Critical Thinking Assessment (HCTA) tool\n\nThey quote a subject who states that “The more I use AI, the less I feel the need to problem-solve on my own. It’s like I’m losing my ability to think critically.”\n\nThey found education was an important factor in this: more educated people were willing to cross check AI statements, while less educated people took AI statements at face value. Another psychology paper found that people who were _less_ literate about how AI works were [more likely to want to use AI](https://osf.io/preprints/psyarxiv/t9u8g_v1).\n\nIt’s too early to say for sure how much of a real problem this is, but it seems plausible to me that it could be a big issue, and it’s one that better AI could make worse.\n\n_Overestimating AI job replacibility_\n\nI decided to base the intro to my story around architecture by asking myself “what’s a job that _sounds_ easy to replace with AI, even though it really, really isn’t?” The architecture specific details were taken from [this thread](https://www.reddit.com/r/architecture/comments/12jtexp/architects_what_do_you_actually_do/) about what architects actually do, and [this thread](https://www.reddit.com/r/architecture/comments/17xmxnb/will_ai_replace_architects/) about why they aren’t likely to be replaced by AI’s. Contrary to popular belief, designing floorplans is a very small part of an architects job.\n\nThis is part of a general trend I’ve noticed, where the less you know about a profession, the easier it seems to replace it with AI[\\[3\\]](#fn1q38ta2vc3z). I did a deep dive on why AI can’t do [my job](https://titotal.substack.com/p/ai-is-not-taking-over-material-science), and it’s fairly easy to find similar sentiments from [doctors](https://www.reddit.com/r/doctorsUK/comments/1ioe7de/what_are_your_views_on_how_ai_will_impact/), ,[mathematicians](https://www.reddit.com/r/math/comments/1ahknh0/are_you_concerned_about_potential_job_loss_once/), [software engineer](https://www.reddit.com/r/ArtificialInteligence/comments/1jum6ct/hot_take_ai_wont_replace_that_many_software/)s, [game developers](https://www.reddit.com/r/GameDevelopment/comments/1io2dic/do_you_think_that_game_development_and_game/), etc. Perhaps they are underestimating the advances of AI… or perhaps AI boosters are underestimating the difficulty of jobs they don’t know that much about.\n\nI saw someone who was worried that AI was gonna cause real economic trouble soon by replacing travel agents. But the advent of the internet made travel agents _completely unnecessary_, and it _still_ only wiped out [half the travel agent jobs](https://fred.stlouisfed.org/series/LEU0254497900A). The number of travel agents has stayed roughly the same since 2008! What demographic of customers are you picturing that are simultaneously allergic to using skyscanner, but are _also_ willing to trust their travel plans to an AI over a human [\\[4\\]](#fngct98uyeiq)?\n\n_Enshittification:_\n\nBut I wouldn’t be so sure that just because an AI can’t do a job _well_, that AI workers won’t be forced down our throats anyway.\n\nThe term “enshittification” comes from this [wired article](https://www.wired.com/story/tiktok-platforms-cory-doctorow/) by Cory Doctorow. The premise is that companies get big by providing quality products that people actually want to use. But then they look around, and realize that they have users trapped on their platform, for reasons of social dynamics, habit, sentimental value, monopoly power, etc. So they then proceed to take a hacksaw to everything good about their product, bit by bit, in order to squeeze as much money out of their captive audience as possible. As another example, see the articles on why google search has apparently [massively declined in quality](https://www.wheresyoured.at/the-men-who-killed-google/).\n\nThe story above is a vision of a society where the _entire world_ gets enshittified. Everything gets worse, but humans in general have nowhere else to turn, because the oligarchs have control over all the resources.\n\n_Undercutting workers:_\n\nThis one actually comes from my girlfriend: She works as a freelance translator, and the company has a different pay rate for “proofreading” and “translating”, with the former paying less. Recently, the company has been sending her “proofreading” work which consists of proofreading _AI translations_. She says the translations are quite bad, so the proofreading of the machine translation takes nearly as long as just doing the translation herself, but of course, she gets paid less for it.\n\n_Stealth outsourcing:_\n\nThe “stealth outsourcing” part of the story comes from several real examples of companies falsely pretending a product was fully AI. Cruise robotaxis hey were presented as fully self-driving, but in fact a remote human operator had to intervene and take over control [every 4-5 miles](https://www.cnbc.com/2023/11/06/cruise-confirms-robotaxis-rely-on-human-assistance-every-4-to-5-miles.html). Similarly, Amazons “just walk out” grocery stores, presented as fully automated, turned out to have [70% of their transactions](https://www.theguardian.com/commentisfree/2024/apr/10/amazon-ai-cashier-less-shops-humans-technology) reviewed by remote workers based in India. And just this week, it’s been revealed that an “AI shopping app” was actually being run by [a call centre in the phillipines](https://techcrunch.com/2025/04/10/fintech-founder-charged-with-fraud-after-ai-shopping-app-found-to-be-powered-by-humans-in-the-philippines/).\n\nThis is a simple way get “AI” to do jobs that are fare beyond the actual capabilities of AI on it’s own. Obviously this wouldn’t work for all bazillion queries to chatGPT, but for jobs with slower needs, this practice could prove increasingly popular.\n\n_Excessive AI hype:_\n\nA large part of the premise of this dystopia relies on people buying into bullshit AI hype, so they trust it enough to put it in charge of things even when it’s not actually qualified to do so.\n\nAs an example while back I covered the claim of “[AI superforecasters](https://titotal.substack.com/p/is-superhuman-ai-forecasting-bs-some)”, where the center for AI safety claimed to have built an AI forecaster that performed better than top humans. They parroted this claim boldly, but it seemed to fall apart under closer analysis, and forecasting bots have still not beat top humans in [more structured forecasting competitions](https://www.lesswrong.com/posts/P8YwCvHoF2FHQoHjF/metaculus-q4-ai-benchmarking-bots-are-closing-the-gap). I also discussed the case of Deepminds “[automated discovery tool](https://titotal.substack.com/p/ai-is-not-taking-over-material-science)”, which claimed to discover 43 novel materials but in reality only 3 of the materials were likely the ones they were trying to make (and these weren’t novel).\n\nFuture predictions are getting… very out there. The anthropic CEO stated [in a post](https://darioamodei.com/machines-of-loving-grace) that within _1-3 years_, AI could be created that will be “smarter than a nobel prize winner… prove unsolved mathematical theorems, write extremely good novels, write difficult codebases from scratch”, and “ordering materials, directing experiments, watching videos, making videos… with a skill exceeding that of the most capable humans in the world”. In the same post they suggest that AI will _double the human lifespan_, with transformative achievements in the next 7-12 years.\n\nEven if you (wrongly) think that these projections are reasonable, I want you to imagine the effect if would have on society if these beliefs were widely _**and incorrectly**_ held. Now, I’m sure you, my very smart and rational reader, would _never_ fall for the vastum architect scam in the story, but can you say the same for the general public? Can you honestly tell me you can’t think of any billionaires or senior politicians who would fall for this type of thing?\n\n_Pollution:_\n\nThe energy demand of AI datacenters have been directly responsible for a coal power station continuing operation after it was [previously set to close](https://archive.is/NXe3D). But former google CEO Eric schmidt is on record saying that “infinite energy demand” is fine because [ASI will solve climate change](https://pivot-to-ai.com/2024/10/06/eric-schmidt-were-not-going-to-hit-the-climate-goals-id-rather-bet-on-ai-solving-the-problem/).\n\nIt’s possible that as AI gets more efficient, the energy demand will decrease. But that’s not a guarantee, especially when companies are in an arms race: instead they may just pour the extra compute capacity straight back into the models and stay exactly as energy destructive as before. Again, we have Eric Schmidt declaring that [99% of the worlds energy](https://futurism.com/google-ceo-congress-electricity-ai-superintelligence) will be used for feeding AI, and he states this as a _goal_.\n\n_Easy data scraping:_\n\nCurrent day gen-AI is built on data. Absurd amounts of data, [rifled from the internet](https://archive.ph/FLMMA), in almost all cases [without asking any permission from the person you took it from](https://www.theatlantic.com/technology/archive/2025/03/libgen-meta-openai/682093/). Some companies want to argue that this is fair use transformation, just like reading books for inspiration. Others argue that you shouldn’t be able to scrape someone’s work without their consent in order to flood the internet with modified copies that hurt their livelihood. These issues are subject to [several ongoing lawsuits](https://hbr.org/2023/04/generative-ai-has-an-intellectual-property-problem). Image generators were absolutely trained on copyrighted material and can [spit out copyrighted characters](https://www.reddit.com/r/aifails/comments/1gs5rb8/very_unique_and_original/) without excessive prompting, so it’ll be interested to see how copyright around AI plays out.\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Sd3X5mWAEzyNaPamT/eyapj8wzi3wftmy8fnlc)\n\nDefinitely no copyright infringement going on here! (exchange on Gemini)\n\nRegardless of what the law finds, governments can also set new laws and policies around AI, either to disincentive scraping or to make it as easy as possible. Sam Altman is arguing for the latter, claiming that they [need copyright exemptions to beat china](https://www.musicbusinessworldwide.com/openai-is-pushing-for-donald-trump-to-usher-in-weakened-copyright-protections-in-the-us-paul-mccartney-is-fighting-back/) in the AI race.\n\n_Algorithmic bias:_\n\n[Algorithmic bias](https://en.wikipedia.org/wiki/Algorithmic_bias) is a large field of study about how automated processes and software sometimes ends up perpetuating harmful biases against groups of people. This can be caused by bias in the training data, a lack of testing on wide variety of groups, or by the evaluation criteria used for training. 100 academicAI researchers recently signed a [letter](https://www.aibiasconsensus.org/) about their AI bias concerns.\n\nAs an example, a 2019 healthcare algorithm was [used to identify](https://www.scientificamerican.com/article/racial-bias-found-in-a-major-health-care-risk-algorithm/) high-risk individuals for special care. A paper in Science found that the algorithm ended up greatly underestimating the sickness of black patients, reducing the number of patients identified for extra care by more than a half compared to equally sick white patients. This arised because it used “healthcare spending” as a proxy for sickness, without accounting for the fact that some people spend less money of healthcare because they are poor.\n\nIn another case, experts found that facial recognition tests were significantly less accurate at identifying the [faces of black people](https://web.archive.org/web/20190722120105/https://www.wired.com/story/best-algorithms-struggle-recognize-black-faces-equally/) compared to white people. The much higher false match rates provide concern about the deployment of facial recognition tools in policing.\n\n_Anti-intellectualism and anti-expertism:_\n\nIn certain corners there is a widespread belief that domain level experts cannot be trusted. We can see this with the widespread vaccine denialism recently: large portions of the population do not trust that life-saving medicine will cure them. In the story, this allows companies to replace actual experts with shitty approximations without public outcry. Anti-education rhetoric is quite common on the right, seen recently with the Trump attacks on the [department of education](https://www.aljazeera.com/news/2025/3/21/why-is-trump-dismantling-the-department-of-education-and-whats-next).\n\nPeople have already starting advocating for AI replacing domain level experts. Mark cuban [stated that](https://bsky.app/profile/mcuban.bsky.social/post/3lifd5rbnkk2k) a person with zero education who can “learn to ask the AI the right questions” will outperform people with _advanced degrees_ in “many fields”. I don’t think this is true, but it’s the type of thing you could _convince_ people is true, so I’m worried.\n\n_AI undermining science:_\n\nThere has been some evidence that some scientists are using unedited LLM responses in their scientific papers. The most amusing example is the paper that included a nonsensical AI-generated image of a [giant rat penis](https://scienceintegritydigest.com/2024/02/15/the-rat-with-the-big-balls-and-enormous-penis-how-frontiers-published-a-paper-with-botched-ai-generated-images/).\n\nThe folks at [retractionwatch](https://retractionwatch.com/) have compiled a [list of about a hundred papers](https://retractionwatch.com/papers-and-peer-reviews-with-evidence-of-chatgpt-writing/) that show clear evidence of copy-pasting from Chatgpt, such as papers containing the phrase “[as an AI language model](https://pubpeer.com/publications/0905A70AF6A77C4309B743FFCD21F8)” . Another study claimed that [up to 17%](https://www.reddit.com/r/MachineLearning/comments/1bnsuea/r_up_to_17_of_recent_ai_conference_peer_reviews/) of recent peer reviews for CS conferences were likely to be AI written. Here is an [anecdotal example](https://www.reddit.com/r/labrats/comments/15g63pz/aigenerated_paper_in_peer_review/) of someone being asked to review a paper with made up AI citations. These methods can’t tell whether the AI was just used for grammar or translation or for more substantive purposes: either way, I highly recommend against this practice, as science relies on precision writing.\n\nThe use of LLMs in science is being advocated by different groups: One editorial in Nature encouraged researchers to use LLM’s as part of the [peer review process](https://archive.is/WMzm8), although they emphasized you should only be using it to clarify and organize your original dot points. However most peer review is already in the form of dot points, so I really don’t get the point here. Another company is trying to sell a cryptocurrency based system for [flagging errors in scientific article](https://archive.is/fqAig)s, which another researcher found had a false positive rate of 14 out of 40 flagged papers. I don’t think this is terrible in principle, if you improve the accuracy and drop the crypto aspect, but I would be concerned at this being used as a replacement for review.\n\nI don’t think LLM’s have zero place in the scientific process, as I explored in [my previous article](https://titotal.substack.com/p/ai-is-not-taking-over-material-science). But there are high chances of misuse, and putting our very notions of truth itself in the whims of chatbots and their programmers could go very, very wrong.\n\n_Ai deepfakes/spam/misinformation_\n\nThere has been widespread concern about the potential impact of [deepfakes](https://en.wikipedia.org/wiki/Deepfake) and AI misinformation on society. Some TikTok users found that AI generated Deepfake doctors were giving out [dodgy medical advice](https://www.mediamatters.org/tiktok/scammers-seem-be-using-deepfake-and-ai-generated-influencers-tiktok-sell-you-wellness). Armies of AI bots have been used to [undermine suppor](https://www.euractiv.com/section/tech/news/fake-climate-news-thriving-as-politics-and-ai-turbocharge-disinformation-crisis/)t for climate action. People are using AI to [steal the voice of ordinary people](https://www.becu.org/blog/voice-cloning-ai-scams-are-on-the-rise#:~:text=How%20Does%20AI%20Voice%20Cloning,billion%20to%20imposter%20scams%20alone.) and use it to scam their loved ones out of money.\n\nWe’re at the point where a carefully prompted AI can [convince the average person they are human after a five minute conversation](https://arxiv.org/abs/2503.23674). The potential for malicious misuse of this technology for scams or propaganda is seriously concerning.\n\n_Oligarchy and democratic decline:_\n\nThe accumulation of wealth by the extremely wealthy in the US has been steadily increasing over time, with upper income families holding [79% of the aggregate US wealth](https://www.pewresearch.org/social-trends/2020/01/09/trends-in-income-and-wealth-inequality/), and the share of income by the top 1% has been [steadily growing for decades](https://ourworldindata.org/how-has-income-inequality-within-countries-evolved-over-the-past-century). This financial power is being accompanied by political power, with Elon musk buying twitter, promoting Trump, and in return being given [huge amounts of power](https://en.wikipedia.org/wiki/Department_of_Government_Efficiency) over federal government decisions. More symbolically, you could look at Mark Zuckerberg literally [styling himself as a roman emperor](https://www.businessinsider.com/mark-zuckerberg-haircut-explained-augustus-caesar-2019-10). All of this is fueling concerns, including from the [outgoing president Biden](https://www.theguardian.com/us-news/2025/jan/23/what-is-oligarchy), that the united states is shifting towards oligarchy.\n\nThis potential oligarchic turn is accompanied by concerns about democracy itself being under threat. The depressingly influential figure [Curtis Yarvin](https://www.theguardian.com/us-news/2024/dec/21/curtis-yarvin-trump) has advocated for monarchical rule, comparing monarchs favorably to CEO’s. Some studies have shown that support for democracy [is declining](https://academic.oup.com/poq/article/87/3/719/7275177) worldwide. Donald trump has joked about being a “king”. Some have discussed the potential benefits AI would hold for enforcing autocratic rule, for example with the deployment of [facial recognition tools in China](https://academic.oup.com/qje/article/138/3/1349/7076890?login=true).\n\nOligarchy, by definition, reduces the number of people involved in decision-making, and if those oligarchs are foolish or out-of touch, it increases the ease of making terrible decisions that [threaten the lives of millions of people](https://www.theguardian.com/global-development/2025/mar/26/aid-cuts-pepfar-epidemic-prediction-cause-29-million-more-hiv-related-deaths-by-2030-lancet-study).\n\n_AI making military and government decisions:_\n\nAi being placed in key parts of the operation of government. Rely too much on AI, and the AI company, or AI hallucinations, could end up responsible for actual government actions, up to and including the militaries decisions on who to shoot.\n\nWhen the UK government asks for public consultation, they will no longer read the actual words that people submit: instead they will get an [AI to read through them all and summarise it](https://ai.gov.uk/projects/consult/). The UK government also heavily used AI chatbots to generate diagrams and citations for a report on the impact of AI on the labour market, [some of which were hallucinated](http://The Impact of AI on the Labour Market). Musks DOGE team have been reportedly using AI chatbots as part of their decision making process over what [programs to cut](https://edition.cnn.com/2025/03/04/tech/doge-ai-government-cuts-expert-concerns/index.html), and is pushing an [AI app](https://www.inc.com/bruce-crumley/doges-ai-app-replacing-fired-federal-workers-proves-about-as-good-as-an-intern/91158894) as a replacement for the fired federal workers.\n\nOpenAI have signed a deal with the US national laboratories, including their [nuclear security division](https://openai.com/index/strengthening-americas-ai-leadership-with-the-us-national-laboratories/). In the war in Gaza, the Israeli military has been using machine learning systems to identify [who is an enemy combatant](https://archive.is/sz3di#selection-675.152-675.221) marked for death. Although it did require sign-off from a human, given that Israel was willing to accept 15 or more civilian casualties as collateral damage to take out 1 Hamas fighter, I think it’s very likely that some civilians were falsely flagged as Hamas fighters by AI and killed.\n\nTo be clear, these are a long way away from putting Chatgpt in charge of the “launch nuke” button, and all of the groups involved have defended their use of AI and still claim to use human oversight.\n\n### Analysis\n\nYou should take care when people mix together science fiction short stories with actual forecasts. Stories, especially in sci-fi, operates on suspension of disbelief, so when you read it, your mind defaults to accepting whatever you read at face value. This is the opposite of the critical thinking brain you need when evaluating a real forecast. They are also really bad for expressing probability and uncertainty: by definition in a story you have to state that one thing happened. And they don’t communicate expertise either: I read up a bit on architecture, but it’s not my area of expertise, so I wouldn’t expect the story to be particularly accurate on the details.\n\nSo I wanna be clear, the above story is not meant to be taken as a real forecast for the future. I’m mainly using it partly as entertainment, partly as a pushback to the _other_ scary stories going around, but mostly as an illustration of my short-term fears about the damage that an overreliance on AI could do to society, if the trends I talked about in the last section accelerate and become widespread enough. I think _some_ of the things I talk about will happen, but I very, very much doubt the situation will get anywhere near as bad as what I portrayed there.\n\nFor starters, this scenario would require AI to be better than it is now, and for people in general to be far more receptive to it than they are now. I think it’s reasonable to believe that recent advancements will lead to improvements for a time, but how much will that be, and will it be enough to enable widespread shitty automation? I don’t know.\n\nI don’t even know if current rates of AI financing will survive the next few years: the disruption from open source models like deepseek, as well as whatever kooky shit the trump administration is pulling, could spell trouble for the AI industry, which is losing large amounts of money at the moment. Skeptics are pointing to [pullbacks in datacentres](https://www.datacenterdynamics.com/en/news/microsoft-backs-away-from-1bn-data-center-plans-in-licking-county-ohio/) as early signs of an AI investment dying down: I’m not a business guy, so all I’ll say is that I wouldn’t be surprised either way.\n\nThe public already has quite a [negative view of tech companies](https://www.pewresearch.org/internet/2024/04/29/americans-views-of-technology-companies-2/), so I don’t think they would be amenable to a tech takeover like I described: the public is generally unfavorable to [Musk’s DOGE department](https://www.poynter.org/fact-checking/2025/elon-musk-doge-unpopular-polls-public-support/), for example. And while AI hype is common in the tech sphere of silicon valley, in the broader public the view of AI is [quite negative](https://www.axios.com/2025/01/15/americans-use-ai-products-poll). I don’t think we’d get that far along this path without a massive anti-AI backlash leading to regulations and bureaucracy, that would make many attempts at AI disruption straight up illegal.\n\nThis is why my scenario required an extreme oligarchical turn in society, that could resist democratic forces: I think this oligarchic turn is happening, but I don’t think it’s powerful enough yet to allow a scenario like this to actually occur.\n\nHowever, as skeptical as I am of my own scenario, I’m much _more_ skeptical of conventional AI doom scenarios, and this case has some clear advantages on the likelihood front:\n\nFirst, it requires much lower AI capabilities than any other AI doomsday scenario. The AI does not need to be an omnipotent machine god that can invent drexlerian nanotech from scratch: it just has to be smart enough to fool people, and people need to be dumb enough to let it.\n\nNext: It is fairly independent of whether AI is aligned to our values. A perfectly helpful, friendly AI that only has our best interests at heart can still get us killed by _doing an incompetent job_. Friendly AI’s could even be more dangerous on this front, by lulling us into a false sense of security.\n\nLastly: this problem scales. AI could continue to massively, genuinely improve and many of these issues would still be a problem as long as peoples _hype_ for AI grows faster than the actual capabilities.\n\nA corollary to this is that even if you fully believe that building a machine god is possible and will definitely happen, these issues could still be dangerous, as long as the real thing doesn’t happen _soon_.\n\n### Conclusion\n\nIn this article, I have laid out a dystopian scenario where the overuse of mediocre AI destroys society. I have explained in detail the present-day trends that inspired the story and concern me deeply, and then given an analysis on whether I think the scenario is likely (it’s not).\n\nI am no Luddite [\\[5\\]](#fneka1pklpj7i). Machine learning and deep learning are incredibly useful and versatile tools that have proven their usefulness in many contexts for decades now. Generative AI has uses as a productivity booster if used correctly, and I do believe it has gotten meaningfully better over the last few years and will get better in the future.\n\nBut a tool is only useful if you know it’s limitations. And however powerful AI gets, it will have limitations. What scares me is a world in which we are unable to see these limitations, because we have crafted machines that _just too good at bullshitting to us_. A world where we allow the bullshit to take over because doing so is profitable to people with financial and political power. And while I doubt this would literally end the world, it could make it a substantially worse place to live.\n\nHow do we combat this? I might write a proper followup on this, but I’d boil it down to two things: Do not let human thought be devalued, and do not let a few tech billionaires seize control of society without a fight.\n\n1.  **[^](#fnref5i99eney5d)**\n    \n    Who is a [way better sci fi author](https://archiveofourown.org/works/2372021/chapters/5238359) than me.\n    \n2.  **[^](#fnrefx7muaba3st)**\n    \n    Although to be fair, most models apart from gemini pro 2.5 were nearly equally terrible at the test.\n    \n3.  **[^](#fnref1q38ta2vc3z)**\n    \n    I’m getting “X on the blockchain” flashbacks.\n    \n4.  **[^](#fnrefgct98uyeiq)**\n    \n    Beside tech AI boosters.\n    \n5.  **[^](#fnrefeka1pklpj7i)**\n    \n    Yet.",
  "reviews": [
    {
      "agentId": "bias-detector",
      "costInCents": 18551,
      "createdAt": "2025-04-16T07:07:45.492Z",
      "runDetails": "{\"model\":\"anthropic/claude-3.7-sonnet\",\"promptTokens\":16134,\"completionTokens\":2417,\"totalTokens\":18551,\"temperature\":0.1,\"runtimeMs\":46769}",
      "summary": "This article presents a thought-provoking alternative to typical AI doom scenarios by exploring how mediocre AI, rather than superintelligent AI, could harm society through a combination of hype, overconfidence, greed, and anti-intellectualism. The author crafts a dystopian narrative beginning with an architecture firm replacing human architects with AI, then expanding to show how this pattern could spread across society with devastating consequences.\n\nWhat makes this piece particularly valuable is its grounding in current trends and real-world examples. The author provides extensive documentation of concerning developments, including benchmark gaming, AI misdeployment, cognitive offloading, excessive hype, and the concentration of power in tech companies. While clearly stating that this exact scenario is unlikely to unfold, the author makes a compelling case for why these concerns might be more plausible than conventional AI doom narratives - they require lower AI capabilities, are independent of alignment issues, and the problems scale with AI improvement.\n\nThe article strikes a balanced tone, acknowledging AI's benefits while emphasizing the importance of recognizing its limitations and preventing the devaluation of human thought and expertise. It's a thoughtful contribution to discussions about AI risk that focuses on near-term, practical concerns rather than speculative long-term existential threats.",
      "comments": [
        {
          "title": "Overestimation of AI job replacement",
          "description": "The author makes an important point about how people tend to overestimate AI's ability to replace jobs they don't understand well, which could lead to premature and harmful automation attempts.",
          "highlight": {
            "startOffset": 0,
            "endOffset": 0,
            "quotedText": ""
          },
          "importance": "75",
          "grade": "65",
          "isValid": false,
          "error": "Start text not found in document"
        },
        {
          "title": "Novel perspective on AI risk",
          "description": "The author presents a refreshing alternative to typical AI doom scenarios by focusing on the dangers of mediocre AI rather than superintelligent AI. This perspective is valuable because it addresses more immediate and plausible concerns about AI deployment.",
          "highlight": {
            "startOffset": 887,
            "endOffset": 1306,
            "quotedText": "In this article, I’m going to do a twist: I’m going to write a story (and detailed analysis) about a scenario where humanity is disempowered and destroyed by AI that is _dumber_ than us, due to a combination of hype, overconfidence, greed and anti-intellectualism. This is a scenario where instead of AI bringing untold abundance or tiling the universe with paperclips, it brings mediocrity, stagnation, and inequality."
          },
          "importance": "85",
          "grade": "70",
          "isValid": true
        },
        {
          "title": "Compelling narrative structure",
          "description": "The use of a narrative story followed by detailed analysis is an effective way to communicate complex concerns about AI. The story makes abstract risks feel more concrete and relatable.",
          "highlight": {
            "startOffset": 1616,
            "endOffset": 2138,
            "quotedText": "This post is divided into 3 parts:\n\nPart 1 is my attempt at a plausible sounding science fiction story sketching out this scenario, starting with the decline of a small architecture firm and ending with nuclear Armageddon.\n\nIn part 2 I will explain, with sources, the real world current day trends that were used as ingredients for the story.\n\nIn part 3 I will analysise the likelihood of my scenario, why I think it’s very unlikely, but also why it has some clear likelihood advantages over the typical doomsday scenario."
          },
          "importance": "70",
          "grade": "85",
          "isValid": true
        },
        {
          "title": "Well-researched supporting evidence",
          "description": "The article provides extensive real-world examples and references to support its concerns, making the dystopian scenario feel more grounded in reality than typical speculative fiction about AI risks.",
          "highlight": {
            "startOffset": 23432,
            "endOffset": 23635,
            "quotedText": "The story above builds on a number of different concerning trends in current day society.\n\nFor the rest of this section I will cover a number of these ingredients, with references to real world examples."
          },
          "importance": "80",
          "grade": "75",
          "isValid": true
        },
        {
          "title": "Insightful analysis of benchmark gaming",
          "description": "The author provides a nuanced explanation of how AI systems can perform well on benchmarks without developing genuine understanding or capabilities, highlighting a key limitation in how we evaluate AI progress.",
          "highlight": {
            "startOffset": 25902,
            "endOffset": 27760,
            "quotedText": "Well, in some cases, the AI’s may just be cheating. There is reasonable evidence of [widespread benchmark data contamination](https://web.archive.org/web/20250305194633/https://www.theatlantic.com/technology/archive/2025/03/chatbots-benchmark-tests/681929/). Take a look at this graph of GPT-4 performance on a programming competition, before and after data contamination was possible:\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Sd3X5mWAEzyNaPamT/poazerpkmadl30x1ghya)\n\nGPT-4s performance on a programming competition [from various years](https://web.archive.org/web/20250303191829/https://arxiv.org/abs/2312.02143), with the red line separating when contamination was possible from when it wasn’t.\n\nThis problem is so bad that an insider headline recently claimed that [benchmark tests are meaningless](https://web.archive.org/web/20250320203457/https://www.deeplearning.ai/the-batch/the-problem-with-benchmark-contamination-in-ai/). I wouldn’t quite go that far (and neither does the actual article), as you can test them on [uncontaminated contests](https://matharena.ai/) and questions that haven’t been released to the public. But I would advise treating performance on any benchmark that has it’s answers available on the internet with a garage sized grain of salt.\n\nIn the cases where performance is _not_ just a case of data contamination, there still may be an issue of “[teaching to the test](https://en.wikipedia.org/wiki/Teaching_to_the_test)”. A physics exam only teaches a small fraction of the skills required to be an actually good physicist: those other skills are much more ephemeral and hard to put down in words. By training on benchmarks like exams, even more advanced looking ones, we may be teaching AI to be really good at doing physics _exams_, but not to be really good at doing actual physics."
          },
          "importance": "80",
          "grade": "75",
          "isValid": true
        },
        {
          "title": "Concerning examples of AI misdeployment",
          "description": "The article documents numerous real-world examples of AI being deployed in inappropriate contexts, demonstrating that the risks described aren't merely theoretical but are already manifesting.",
          "highlight": {
            "startOffset": 31551,
            "endOffset": 33525,
            "quotedText": "It’s very easy to find examples of people unthinkingly using genAI to do things it is very clearly not qualified for.\n\nSome examples: A lawyer used chatgpt to [prepare court documents](https://www.bbc.com/news/world-us-canada-65735769), which made shit up and got him in a lot of trouble. A completely different law firm did [the exact same thing](https://www.abajournal.com/news/article/no-42-law-firm-by-headcount-could-face-sanctions-over-fake-case-citations-generated-by-chatgpt). Another lawyer in Canada did [this as well](https://www.theguardian.com/world/2024/feb/29/canada-lawyer-chatgpt-fake-cases-ai). A murder case in Ohio fell apart because it’s key evidence relied on [AI face recognition](https://www.theregister.com/2025/01/28/ai_facial_recognition_murder/) software that clearly stated on _every single page it produced_ that it was not court-admissible. A marketing team for the film Megalopolis used AI to find reviewer quotes for similar historical films, which the AI proceeded to [make up](https://variety.com/2024/film/news/megalopolis-trailer-fake-quotes-ai-lionsgate-1236116485/). A guy claimed to have made their website entirely with AI “[vibe coding](https://pivot-to-ai.com/2025/03/18/guys-im-under-attack-ai-vibe-coding-in-the-wild/)”… the wildly insecure site got hacked immediately. The National Eating Disorders Association announced that it would [replace it’s staff](https://www.psychiatrist.com/news/neda-suspends-ai-chatbot-for-giving-harmful-eating-disorder-advice/) with AI chatbots for people struggling with eating disorders. It proceeded to recommend that they focus on losing weight and measure their body fat with skin folk calipers, and the bot was pulled shortly afterwards.\n\nPeople in general do not have an accurate sense of what an AI can and cannot do, especially in areas where they have little expertise. This is only going to get worse as AI gets better, and if we don’t have our skeptical hats on, disaster might follow."
          },
          "importance": "75",
          "grade": "40",
          "isValid": true
        },
        {
          "title": "Cognitive offloading concerns",
          "description": "The article cites research suggesting that excessive reliance on AI tools may be associated with reduced critical thinking skills, raising important questions about the long-term cognitive impacts of AI dependency.",
          "highlight": {
            "startOffset": 33575,
            "endOffset": 35317,
            "quotedText": "Some studies are finding that the use of AI tools is associated with poor critical thinking skills. I’ll just quote [this recent one](https://www.mdpi.com/2075-4698/15/1/6):\n\n> The key findings indicate that higher usage of AI tools is associated with reduced critical thinking skills, and cognitive offloading plays a significant role in this relationship.\n> \n> Our findings are consistent with previous research indicating that excessive reliance on AI tools can negatively impact critical thinking skills. Firth et al. \\[[**24**](https://www.mdpi.com/2075-4698/15/1/6#B24-societies-15-00006)\\] and Zhang et al. \\[[**43**](https://www.mdpi.com/2075-4698/15/1/6#B43-societies-15-00006)\\] both highlight the potential for AI tools to enhance basic skill acquisition while potentially undermining deeper cognitive engagement. Our study extends this by quantitatively demonstrating that increased AI tool usage correlates with lower critical thinking scores, as measured by comprehensive assessments like the Halpern Critical Thinking Assessment (HCTA) tool\n\nThey quote a subject who states that “The more I use AI, the less I feel the need to problem-solve on my own. It’s like I’m losing my ability to think critically.”\n\nThey found education was an important factor in this: more educated people were willing to cross check AI statements, while less educated people took AI statements at face value. Another psychology paper found that people who were _less_ literate about how AI works were [more likely to want to use AI](https://osf.io/preprints/psyarxiv/t9u8g_v1).\n\nIt’s too early to say for sure how much of a real problem this is, but it seems plausible to me that it could be a big issue, and it’s one that better AI could make worse."
          },
          "importance": "70",
          "grade": "45",
          "isValid": true
        },
        {
          "title": "Balanced assessment of likelihood",
          "description": "The author clearly states that their scenario is unlikely to happen exactly as described, but provides a thoughtful analysis of why certain elements might be more plausible than conventional AI doom scenarios.",
          "highlight": {
            "startOffset": 56228,
            "endOffset": 56795,
            "quotedText": "So I wanna be clear, the above story is not meant to be taken as a real forecast for the future. I’m mainly using it partly as entertainment, partly as a pushback to the _other_ scary stories going around, but mostly as an illustration of my short-term fears about the damage that an overreliance on AI could do to society, if the trends I talked about in the last section accelerate and become widespread enough. I think _some_ of the things I talk about will happen, but I very, very much doubt the situation will get anywhere near as bad as what I portrayed there."
          },
          "importance": "75",
          "grade": "80",
          "isValid": true
        },
        {
          "title": "Advantages over conventional doom scenarios",
          "description": "The author makes a compelling case for why their scenario might be more plausible than typical AI doom narratives, highlighting that it requires lower AI capabilities, is independent of alignment, and scales with AI improvement.",
          "highlight": {
            "startOffset": 58782,
            "endOffset": 59959,
            "quotedText": "However, as skeptical as I am of my own scenario, I’m much _more_ skeptical of conventional AI doom scenarios, and this case has some clear advantages on the likelihood front:\n\nFirst, it requires much lower AI capabilities than any other AI doomsday scenario. The AI does not need to be an omnipotent machine god that can invent drexlerian nanotech from scratch: it just has to be smart enough to fool people, and people need to be dumb enough to let it.\n\nNext: It is fairly independent of whether AI is aligned to our values. A perfectly helpful, friendly AI that only has our best interests at heart can still get us killed by _doing an incompetent job_. Friendly AI’s could even be more dangerous on this front, by lulling us into a false sense of security.\n\nLastly: this problem scales. AI could continue to massively, genuinely improve and many of these issues would still be a problem as long as peoples _hype_ for AI grows faster than the actual capabilities.\n\nA corollary to this is that even if you fully believe that building a machine god is possible and will definitely happen, these issues could still be dangerous, as long as the real thing doesn’t happen _soon_."
          },
          "importance": "85",
          "grade": "70",
          "isValid": true
        },
        {
          "title": "Nuanced conclusion about AI risks",
          "description": "The author concludes with a balanced perspective that acknowledges AI's benefits while emphasizing the importance of recognizing its limitations and preventing concentration of power in tech companies.",
          "highlight": {
            "startOffset": 60261,
            "endOffset": 61343,
            "quotedText": "I am no Luddite [\\[5\\]](#fneka1pklpj7i). Machine learning and deep learning are incredibly useful and versatile tools that have proven their usefulness in many contexts for decades now. Generative AI has uses as a productivity booster if used correctly, and I do believe it has gotten meaningfully better over the last few years and will get better in the future.\n\nBut a tool is only useful if you know it’s limitations. And however powerful AI gets, it will have limitations. What scares me is a world in which we are unable to see these limitations, because we have crafted machines that _just too good at bullshitting to us_. A world where we allow the bullshit to take over because doing so is profitable to people with financial and political power. And while I doubt this would literally end the world, it could make it a substantially worse place to live.\n\nHow do we combat this? I might write a proper followup on this, but I’d boil it down to two things: Do not let human thought be devalued, and do not let a few tech billionaires seize control of society without a fight."
          },
          "importance": "75",
          "grade": "75",
          "isValid": true
        }
      ],
      "grade": "92"
    },
    {
      "agentId": "clarity-coach",
      "costInCents": 18401,
      "createdAt": "2025-04-16T07:10:15.752Z",
      "runDetails": "{\"model\":\"anthropic/claude-3.7-sonnet\",\"promptTokens\":16134,\"completionTokens\":2267,\"totalTokens\":18401,\"temperature\":0.1,\"runtimeMs\":85218}",
      "summary": "This article presents a compelling alternative to typical AI doom scenarios by exploring how mediocre AI could lead to societal collapse through overreliance, hype, and deteriorating quality. The author crafts a dystopian narrative beginning with an architecture firm replacing human architects with AI that produces visually impressive but functionally terrible buildings, then escalates to show society's gradual dependence on increasingly mediocre AI across all domains. What makes this piece particularly valuable is its extensive documentation of current concerning trends that could contribute to this scenario, including benchmark gaming, hallucinations, AI misdeployment, cognitive offloading, and more. The author maintains intellectual honesty by acknowledging their scenario is unlikely while explaining why it might be more plausible than superintelligence scenarios. Throughout, they demonstrate a nuanced understanding of AI, recognizing its genuine utility while remaining critical of hype and overestimation. The article concludes that the real danger lies not in AI becoming too smart, but in our failure to recognize its limitations while allowing profit motives to override quality concerns.",
      "comments": [
        {
          "title": "Novel AI risk scenario",
          "description": "The author presents a different kind of AI risk scenario than typically discussed - one where mediocre AI (rather than superintelligent AI) leads to societal collapse through overreliance, hype, and deteriorating quality. This perspective adds valuable nuance to AI risk discussions that often focus exclusively on superintelligence scenarios.",
          "highlight": {
            "startOffset": 887,
            "endOffset": 1306,
            "quotedText": "In this article, I’m going to do a twist: I’m going to write a story (and detailed analysis) about a scenario where humanity is disempowered and destroyed by AI that is _dumber_ than us, due to a combination of hype, overconfidence, greed and anti-intellectualism. This is a scenario where instead of AI bringing untold abundance or tiling the universe with paperclips, it brings mediocrity, stagnation, and inequality."
          },
          "importance": "85",
          "grade": "70",
          "isValid": true
        },
        {
          "title": "Compelling architecture example",
          "description": "The architecture example effectively illustrates how AI might appear to replace skilled professionals while actually delivering inferior results that look impressive but fail in practical application. This concrete example makes the abstract concerns more tangible.",
          "highlight": {
            "startOffset": 2194,
            "endOffset": 7831,
            "quotedText": "When the architecture firm HBM was bought out by the Vastum corporation, they announced that they would fire 99% of their architects and replace them with AI chatbots.\n\nThe architect-bot they unveiled was incredibly impressive. After uploading your site parameters, all you had to do was chat with a bot to tell it everything you needed from the building, and it would produce a nearly photo-realistic 3d model of a building fitting your requirements, which you could even zoom around to look at from different angles. You could then just talk to the bot, saying things like “could you add another large window to the living room”, and after some time, out would pop another 3d model with the change. Once you were satisfied, the 3d model was turned into schematics and sent off to sub-contractors for construction and interior design, and your building would be created, no pesky architect required.\n\nThis sounded great for Thom Bradley, the head of a development company that was building up a small apartment complex in a residential area. Thom signed a contract with Vastum and decided to personally “design” the complex in collaboration with the new chatbot. Thom hated modern architecture, so he decided to build a complex with cool, retro, “art deco” aesthetic. He was enamored with the chatbot, which appeared to be smart, helpful and friendly. Occasionally there would be problems understanding his commands and rooms would change places inexplicably, but after a few tries the problems would get fixed. He even got to design an ornate set of steps leading into the atrium! The final result looked very cool, and Thom was happy for Vastum to send the design off to sub-contractors to build.\n\nAnd that’s when everything started to go wrong.\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Sd3X5mWAEzyNaPamT/lm7afccctbazsiaf0pig)\n\nWarped architecture is fun if you [do it on purpose](https://en.wikipedia.org/wiki/Dancing_House), but I wouldn’t build a world around it.\n\nThe basement was in the wrong place for the piping system around the area, leading to massive tunneling costs to hook up the building. The designer forgot that the complex needed a boiler room, so a ground floor apartment had to be retrofitted for the purpose at great expense.\n\nWhen the building was actually opened, the flaws of the design become apparent to the residents themselves. In the context of the surrounding buildings, the art deco design sticks out like a sore thumb. The windows in the building are faced to let in massive amounts of heat in summer, leading to skyrocketing A/C bills. The ventilation in the house was designed for a lower humidity level, so the residents end up with a constant black mould problem. Some of the rooms in peoples apartments were way too narrow for their intended purposes. The design was also completely wheelchair inaccessible, due to the largely unnecessary steps that Thom put in earlier. Overall, it was just a terrible place to live.\n\nWhat the hell just went wrong?\n\nWell, Vastum was hiding a few secrets up it’s sleeve. It had quietly bought up the major architectural software companies, and had attempted to create AI agents with their vast amounts of data, but had quickly realized the tech was way too unreliable to survive contact with someone like Thom. Instead, they hard coded a traditional algorithm. The input was the “specs”, a long file with set fields listing external characteristics of the house like dimensions and number of stories, and a long list of rooms, each labelled with subfeatures like “number of windows” and “floor material”. The hardcoded algorithm took this list and piped it into existing 3d architecture software to produce a building that fit as best as it could, using a large library of pre-built rooms and modifying them to fit the “specs” according to an algorithm, squeezing rooms to fit if necessary.\n\nWhen Thom talked with the chatbot, the LLM was specifically designed to do one thing only: take in Thoms request and use it to fill out the “specs”. Vastum had trained it on a large set of architecture designs, described in plain english and in “spec” form, so that it could most easily transfer from one set to the other. Despite this, the bot still regularly messed up, so Vastum secretly hired a group of underpaid workers in India to “switch in” for the bot anytime user frustration was flagged, and attempt to manually implement whatever request they had. At the end of the process the one overworked architect they had secretly kept on staff was given an absurdly short deadline to switch out materials for the cheapest option that looked similar, assuming that Thom wouldn’t notice. Helping matters along, a pro-growth administration had recently waived all regulations and liability on new builds other than “make sure it won’t fall down”. And the ADA had just gotten repealed, so wheelchair inaccessibility wasn’t a problem at all.\n\nThis process was smooth enough to greatly impress the public (A few people claimed this was proof of human-level AGI). But it left out most of what a human architect was a_ctually doing._ It wasn’t taking into account the specific context of the actual location it was built in, because everything was being made out of prefab designs and neither Thom nor Vastum cared about local factors, like the climate and the nearby area. And the bot was a yes-man that was incapable of pushing back on Thom when his ideas were unworkable and stupid. A big part of an architects job was making sure that a building you created was actually _good_. This was the part of the process that Vastum cut out."
          },
          "importance": "75",
          "grade": "60",
          "isValid": true
        },
        {
          "title": "Well-documented current trends",
          "description": "The article provides extensive documentation of concerning current trends that could contribute to the dystopian scenario, including benchmark gaming, hallucinations, misdeployment, cognitive offloading, and more. This grounding in real-world evidence strengthens the argument considerably.",
          "highlight": {
            "startOffset": 23432,
            "endOffset": 23996,
            "quotedText": "The story above builds on a number of different concerning trends in current day society.\n\nFor the rest of this section I will cover a number of these ingredients, with references to real world examples. **Note that a few news articles does** _**not**_ **mean something is necessarily a huge problem**, and you shouldn’t take this as me saying all of these are _currently_ large problems, just that they are potential areas of concern for the future. Every new technology goes through some level of moral panic, which is sometimes unreasonable and sometimes isn’t."
          },
          "importance": "80",
          "grade": "65",
          "isValid": true
        },
        {
          "title": "Benchmark gaming evidence",
          "description": "The article presents evidence of AI models potentially cheating on benchmarks through data contamination, raising questions about how we measure and evaluate AI progress.",
          "highlight": {
            "startOffset": 25902,
            "endOffset": 26559,
            "quotedText": "Well, in some cases, the AI’s may just be cheating. There is reasonable evidence of [widespread benchmark data contamination](https://web.archive.org/web/20250305194633/https://www.theatlantic.com/technology/archive/2025/03/chatbots-benchmark-tests/681929/). Take a look at this graph of GPT-4 performance on a programming competition, before and after data contamination was possible:\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Sd3X5mWAEzyNaPamT/poazerpkmadl30x1ghya)\n\nGPT-4s performance on a programming competition [from various years](https://web.archive.org/web/20250303191829/https://arxiv.org/abs/2312.02143), wi"
          },
          "importance": "75",
          "grade": "40",
          "isValid": true
        },
        {
          "title": "Hallucination demonstration",
          "description": "The author provides a striking real example of AI hallucination where Gemini Pro 2.5 confidently analyzes non-existent images, demonstrating how convincingly AI can present completely fabricated information as fact.",
          "highlight": {
            "startOffset": 30463,
            "endOffset": 30984,
            "quotedText": "I’ll just show you an exchange I had with state of the art reasoning model gemini pro 2.5, where it gave me in depth comparison of two images of elephants. See if you can spot the hallucination:\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Sd3X5mWAEzyNaPamT/fae2xqkpyio3ypweawnd)\n\nDid you see it? That’s right, I didn’t actually upload any pictures! As far as I can tell this is an entirely made up analysis of two pictures that do not exist. But it sounds very convincing, doesn’t it?"
          },
          "importance": "78",
          "grade": "40",
          "isValid": true
        },
        {
          "title": "Cognitive offloading concerns",
          "description": "The article cites research showing that increased AI usage correlates with reduced critical thinking skills, raising important questions about the long-term cognitive impacts of AI dependence.",
          "highlight": {
            "startOffset": 33575,
            "endOffset": 34975,
            "quotedText": "Some studies are finding that the use of AI tools is associated with poor critical thinking skills. I’ll just quote [this recent one](https://www.mdpi.com/2075-4698/15/1/6):\n\n> The key findings indicate that higher usage of AI tools is associated with reduced critical thinking skills, and cognitive offloading plays a significant role in this relationship.\n> \n> Our findings are consistent with previous research indicating that excessive reliance on AI tools can negatively impact critical thinking skills. Firth et al. \\[[**24**](https://www.mdpi.com/2075-4698/15/1/6#B24-societies-15-00006)\\] and Zhang et al. \\[[**43**](https://www.mdpi.com/2075-4698/15/1/6#B43-societies-15-00006)\\] both highlight the potential for AI tools to enhance basic skill acquisition while potentially undermining deeper cognitive engagement. Our study extends this by quantitatively demonstrating that increased AI tool usage correlates with lower critical thinking scores, as measured by comprehensive assessments like the Halpern Critical Thinking Assessment (HCTA) tool\n\nThey quote a subject who states that “The more I use AI, the less I feel the need to problem-solve on my own. It’s like I’m losing my ability to think critically.”\n\nThey found education was an important factor in this: more educated people were willing to cross check AI statements, while less educated people took AI statements at face value."
          },
          "importance": "77",
          "grade": "35",
          "isValid": true
        },
        {
          "title": "Stealth outsourcing examples",
          "description": "The article provides concrete examples of companies falsely presenting human work as AI, revealing how the perception of AI capabilities can be artificially inflated through deception.",
          "highlight": {
            "startOffset": 39141,
            "endOffset": 40182,
            "quotedText": "The “stealth outsourcing” part of the story comes from several real examples of companies falsely pretending a product was fully AI. Cruise robotaxis hey were presented as fully self-driving, but in fact a remote human operator had to intervene and take over control [every 4-5 miles](https://www.cnbc.com/2023/11/06/cruise-confirms-robotaxis-rely-on-human-assistance-every-4-to-5-miles.html). Similarly, Amazons “just walk out” grocery stores, presented as fully automated, turned out to have [70% of their transactions](https://www.theguardian.com/commentisfree/2024/apr/10/amazon-ai-cashier-less-shops-humans-technology) reviewed by remote workers based in India. And just this week, it’s been revealed that an “AI shopping app” was actually being run by [a call centre in the phillipines](https://techcrunch.com/2025/04/10/fintech-founder-charged-with-fraud-after-ai-shopping-app-found-to-be-powered-by-humans-in-the-philippines/).\n\nThis is a simple way get “AI” to do jobs that are fare beyond the actual capabilities of AI on it’s own."
          },
          "importance": "70",
          "grade": "45",
          "isValid": true
        },
        {
          "title": "Realistic probability assessment",
          "description": "The author provides a balanced assessment of the likelihood of their scenario, acknowledging it's unlikely while explaining why it might be more plausible than superintelligence scenarios. This intellectual honesty strengthens the overall argument.",
          "highlight": {
            "startOffset": 56228,
            "endOffset": 58957,
            "quotedText": "So I wanna be clear, the above story is not meant to be taken as a real forecast for the future. I’m mainly using it partly as entertainment, partly as a pushback to the _other_ scary stories going around, but mostly as an illustration of my short-term fears about the damage that an overreliance on AI could do to society, if the trends I talked about in the last section accelerate and become widespread enough. I think _some_ of the things I talk about will happen, but I very, very much doubt the situation will get anywhere near as bad as what I portrayed there.\n\nFor starters, this scenario would require AI to be better than it is now, and for people in general to be far more receptive to it than they are now. I think it’s reasonable to believe that recent advancements will lead to improvements for a time, but how much will that be, and will it be enough to enable widespread shitty automation? I don’t know.\n\nI don’t even know if current rates of AI financing will survive the next few years: the disruption from open source models like deepseek, as well as whatever kooky shit the trump administration is pulling, could spell trouble for the AI industry, which is losing large amounts of money at the moment. Skeptics are pointing to [pullbacks in datacentres](https://www.datacenterdynamics.com/en/news/microsoft-backs-away-from-1bn-data-center-plans-in-licking-county-ohio/) as early signs of an AI investment dying down: I’m not a business guy, so all I’ll say is that I wouldn’t be surprised either way.\n\nThe public already has quite a [negative view of tech companies](https://www.pewresearch.org/internet/2024/04/29/americans-views-of-technology-companies-2/), so I don’t think they would be amenable to a tech takeover like I described: the public is generally unfavorable to [Musk’s DOGE department](https://www.poynter.org/fact-checking/2025/elon-musk-doge-unpopular-polls-public-support/), for example. And while AI hype is common in the tech sphere of silicon valley, in the broader public the view of AI is [quite negative](https://www.axios.com/2025/01/15/americans-use-ai-products-poll). I don’t think we’d get that far along this path without a massive anti-AI backlash leading to regulations and bureaucracy, that would make many attempts at AI disruption straight up illegal.\n\nThis is why my scenario required an extreme oligarchical turn in society, that could resist democratic forces: I think this oligarchic turn is happening, but I don’t think it’s powerful enough yet to allow a scenario like this to actually occur.\n\nHowever, as skeptical as I am of my own scenario, I’m much _more_ skeptical of conventional AI doom scenarios, and this case has some clear advantages on the likelihood front:"
          },
          "importance": "72",
          "grade": "75",
          "isValid": true
        },
        {
          "title": "Nuanced view of AI capabilities",
          "description": "The author maintains a balanced perspective on AI, acknowledging its genuine improvements and utility while remaining critical of hype and overestimation, avoiding both technophobia and techno-optimism extremes.",
          "highlight": {
            "startOffset": 60261,
            "endOffset": 60737,
            "quotedText": "I am no Luddite [\\[5\\]](#fneka1pklpj7i). Machine learning and deep learning are incredibly useful and versatile tools that have proven their usefulness in many contexts for decades now. Generative AI has uses as a productivity booster if used correctly, and I do believe it has gotten meaningfully better over the last few years and will get better in the future.\n\nBut a tool is only useful if you know it’s limitations. And however powerful AI gets, it will have limitations."
          },
          "importance": "68",
          "grade": "80",
          "isValid": true
        }
      ],
      "grade": "90"
    },
    {
      "agentId": "research-scholar",
      "costInCents": 18740,
      "createdAt": "2025-04-16T07:11:38.823Z",
      "runDetails": "{\"model\":\"anthropic/claude-3.7-sonnet\",\"promptTokens\":16041,\"completionTokens\":2699,\"totalTokens\":18740,\"temperature\":0.1,\"runtimeMs\":59934}",
      "summary": "This article presents an alternative AI risk scenario called \"Slopworld\" - a dystopian future where society is degraded not by superintelligent AI, but by the widespread deployment of mediocre AI systems that replace human expertise despite being less capable. The author crafts a narrative beginning with an architecture firm replacing architects with AI, resulting in buildings that look impressive but are deeply flawed, and culminating in nuclear disaster triggered by an AI hallucination. \n\nThe piece is supported by extensive documentation of current concerning trends, including benchmark gaming, AI hallucinations, inappropriate AI deployment in critical contexts, cognitive offloading, stealth outsourcing disguised as AI, excessive hype, and the undermining of scientific integrity. The author provides compelling evidence for each trend, including a personal example where Gemini Pro 2.5 hallucinated a detailed analysis of non-existent images.\n\nWhile acknowledging this exact scenario is unlikely, the author argues it has advantages over traditional AI doom scenarios in terms of plausibility because it requires lower AI capabilities, is independent of alignment concerns, and scales with hype rather than actual capabilities. The piece concludes with a balanced perspective that recognizes AI's potential benefits while warning against uncritical adoption that could devalue human thought and concentrate power in the hands of tech billionaires.",
      "comments": [
        {
          "title": "Novel AI risk scenario",
          "description": "The author presents a unique perspective on AI risk that differs from typical superintelligence scenarios. Instead of focusing on superintelligent AI taking over, they explore how mediocre AI could degrade society through hype, overconfidence, and misapplication.",
          "highlight": {
            "startOffset": 887,
            "endOffset": 1306,
            "quotedText": "In this article, I’m going to do a twist: I’m going to write a story (and detailed analysis) about a scenario where humanity is disempowered and destroyed by AI that is _dumber_ than us, due to a combination of hype, overconfidence, greed and anti-intellectualism. This is a scenario where instead of AI bringing untold abundance or tiling the universe with paperclips, it brings mediocrity, stagnation, and inequality."
          },
          "importance": "85",
          "isValid": true
        },
        {
          "title": "Architecture case study illustrates hidden expertise",
          "description": "The story effectively uses architecture as an example of a profession that seems easy to automate but actually requires deep expertise and contextual understanding that AI lacks. This highlights how the public often underestimates the complexity of professional work.",
          "highlight": {
            "startOffset": 2194,
            "endOffset": 7831,
            "quotedText": "When the architecture firm HBM was bought out by the Vastum corporation, they announced that they would fire 99% of their architects and replace them with AI chatbots.\n\nThe architect-bot they unveiled was incredibly impressive. After uploading your site parameters, all you had to do was chat with a bot to tell it everything you needed from the building, and it would produce a nearly photo-realistic 3d model of a building fitting your requirements, which you could even zoom around to look at from different angles. You could then just talk to the bot, saying things like “could you add another large window to the living room”, and after some time, out would pop another 3d model with the change. Once you were satisfied, the 3d model was turned into schematics and sent off to sub-contractors for construction and interior design, and your building would be created, no pesky architect required.\n\nThis sounded great for Thom Bradley, the head of a development company that was building up a small apartment complex in a residential area. Thom signed a contract with Vastum and decided to personally “design” the complex in collaboration with the new chatbot. Thom hated modern architecture, so he decided to build a complex with cool, retro, “art deco” aesthetic. He was enamored with the chatbot, which appeared to be smart, helpful and friendly. Occasionally there would be problems understanding his commands and rooms would change places inexplicably, but after a few tries the problems would get fixed. He even got to design an ornate set of steps leading into the atrium! The final result looked very cool, and Thom was happy for Vastum to send the design off to sub-contractors to build.\n\nAnd that’s when everything started to go wrong.\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Sd3X5mWAEzyNaPamT/lm7afccctbazsiaf0pig)\n\nWarped architecture is fun if you [do it on purpose](https://en.wikipedia.org/wiki/Dancing_House), but I wouldn’t build a world around it.\n\nThe basement was in the wrong place for the piping system around the area, leading to massive tunneling costs to hook up the building. The designer forgot that the complex needed a boiler room, so a ground floor apartment had to be retrofitted for the purpose at great expense.\n\nWhen the building was actually opened, the flaws of the design become apparent to the residents themselves. In the context of the surrounding buildings, the art deco design sticks out like a sore thumb. The windows in the building are faced to let in massive amounts of heat in summer, leading to skyrocketing A/C bills. The ventilation in the house was designed for a lower humidity level, so the residents end up with a constant black mould problem. Some of the rooms in peoples apartments were way too narrow for their intended purposes. The design was also completely wheelchair inaccessible, due to the largely unnecessary steps that Thom put in earlier. Overall, it was just a terrible place to live.\n\nWhat the hell just went wrong?\n\nWell, Vastum was hiding a few secrets up it’s sleeve. It had quietly bought up the major architectural software companies, and had attempted to create AI agents with their vast amounts of data, but had quickly realized the tech was way too unreliable to survive contact with someone like Thom. Instead, they hard coded a traditional algorithm. The input was the “specs”, a long file with set fields listing external characteristics of the house like dimensions and number of stories, and a long list of rooms, each labelled with subfeatures like “number of windows” and “floor material”. The hardcoded algorithm took this list and piped it into existing 3d architecture software to produce a building that fit as best as it could, using a large library of pre-built rooms and modifying them to fit the “specs” according to an algorithm, squeezing rooms to fit if necessary.\n\nWhen Thom talked with the chatbot, the LLM was specifically designed to do one thing only: take in Thoms request and use it to fill out the “specs”. Vastum had trained it on a large set of architecture designs, described in plain english and in “spec” form, so that it could most easily transfer from one set to the other. Despite this, the bot still regularly messed up, so Vastum secretly hired a group of underpaid workers in India to “switch in” for the bot anytime user frustration was flagged, and attempt to manually implement whatever request they had. At the end of the process the one overworked architect they had secretly kept on staff was given an absurdly short deadline to switch out materials for the cheapest option that looked similar, assuming that Thom wouldn’t notice. Helping matters along, a pro-growth administration had recently waived all regulations and liability on new builds other than “make sure it won’t fall down”. And the ADA had just gotten repealed, so wheelchair inaccessibility wasn’t a problem at all.\n\nThis process was smooth enough to greatly impress the public (A few people claimed this was proof of human-level AGI). But it left out most of what a human architect was a_ctually doing._ It wasn’t taking into account the specific context of the actual location it was built in, because everything was being made out of prefab designs and neither Thom nor Vastum cared about local factors, like the climate and the nearby area. And the bot was a yes-man that was incapable of pushing back on Thom when his ideas were unworkable and stupid. A big part of an architects job was making sure that a building you created was actually _good_. This was the part of the process that Vastum cut out."
          },
          "importance": "75",
          "isValid": true
        },
        {
          "title": "Benchmark gaming and data contamination",
          "description": "The author provides compelling evidence that AI performance on benchmarks may be artificially inflated through data contamination and teaching to the test, rather than genuine reasoning capabilities. This undermines claims about AI progress.",
          "highlight": {
            "startOffset": 25902,
            "endOffset": 26742,
            "quotedText": "Well, in some cases, the AI’s may just be cheating. There is reasonable evidence of [widespread benchmark data contamination](https://web.archive.org/web/20250305194633/https://www.theatlantic.com/technology/archive/2025/03/chatbots-benchmark-tests/681929/). Take a look at this graph of GPT-4 performance on a programming competition, before and after data contamination was possible:\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Sd3X5mWAEzyNaPamT/poazerpkmadl30x1ghya)\n\nGPT-4s performance on a programming competition [from various years](https://web.archive.org/web/20250303191829/https://arxiv.org/abs/2312.02143), with the red line separating when contamination was possible from when it wasn’t.\n\nThis problem is so bad that an insider headline recently claimed that [benchmark tests are meaningless"
          },
          "importance": "80",
          "isValid": true
        },
        {
          "title": "Hallucination example demonstrates convincing falsehoods",
          "description": "The author's personal example with Gemini Pro 2.5 hallucinating a detailed analysis of non-existent elephant images powerfully illustrates how AI can generate convincing but completely fabricated information, which poses serious risks when deployed in critical contexts.",
          "highlight": {
            "startOffset": 30268,
            "endOffset": 31298,
            "quotedText": "There’s been a lot of discussion of the “[hallucination problem](https://en.wikipedia.org/wiki/Hallucination_\\(artificial_intelligence\\))” in AI, so I’m not discuss it much here. Instead for fun I’ll just show you an exchange I had with state of the art reasoning model gemini pro 2.5, where it gave me in depth comparison of two images of elephants. See if you can spot the hallucination:\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Sd3X5mWAEzyNaPamT/fae2xqkpyio3ypweawnd)\n\nDid you see it? That’s right, I didn’t actually upload any pictures! As far as I can tell this is an entirely made up analysis of two pictures that do not exist. But it sounds very convincing, doesn’t it?\n\nI assume they’ll patch this particular issue soon, but the overall problem with making stuff up remains. We are crafting machines that alternate between perfectly correct answers and delusional bullshit, both delivered in the same confident tone and manner, both presented equally convincingly to any non-expert."
          },
          "importance": "85",
          "isValid": true
        },
        {
          "title": "Real-world AI misdeployment examples",
          "description": "The document provides multiple concrete examples of AI being deployed in contexts where it's clearly not qualified, showing this isn't just a theoretical concern but is already happening in legal, medical, and other critical domains.",
          "highlight": {
            "startOffset": 31551,
            "endOffset": 33525,
            "quotedText": "It’s very easy to find examples of people unthinkingly using genAI to do things it is very clearly not qualified for.\n\nSome examples: A lawyer used chatgpt to [prepare court documents](https://www.bbc.com/news/world-us-canada-65735769), which made shit up and got him in a lot of trouble. A completely different law firm did [the exact same thing](https://www.abajournal.com/news/article/no-42-law-firm-by-headcount-could-face-sanctions-over-fake-case-citations-generated-by-chatgpt). Another lawyer in Canada did [this as well](https://www.theguardian.com/world/2024/feb/29/canada-lawyer-chatgpt-fake-cases-ai). A murder case in Ohio fell apart because it’s key evidence relied on [AI face recognition](https://www.theregister.com/2025/01/28/ai_facial_recognition_murder/) software that clearly stated on _every single page it produced_ that it was not court-admissible. A marketing team for the film Megalopolis used AI to find reviewer quotes for similar historical films, which the AI proceeded to [make up](https://variety.com/2024/film/news/megalopolis-trailer-fake-quotes-ai-lionsgate-1236116485/). A guy claimed to have made their website entirely with AI “[vibe coding](https://pivot-to-ai.com/2025/03/18/guys-im-under-attack-ai-vibe-coding-in-the-wild/)”… the wildly insecure site got hacked immediately. The National Eating Disorders Association announced that it would [replace it’s staff](https://www.psychiatrist.com/news/neda-suspends-ai-chatbot-for-giving-harmful-eating-disorder-advice/) with AI chatbots for people struggling with eating disorders. It proceeded to recommend that they focus on losing weight and measure their body fat with skin folk calipers, and the bot was pulled shortly afterwards.\n\nPeople in general do not have an accurate sense of what an AI can and cannot do, especially in areas where they have little expertise. This is only going to get worse as AI gets better, and if we don’t have our skeptical hats on, disaster might follow."
          },
          "importance": "75",
          "isValid": true
        },
        {
          "title": "Cognitive offloading research",
          "description": "The author cites research suggesting that AI usage may be associated with reduced critical thinking skills, raising concerns about long-term societal impacts of AI dependency that go beyond immediate economic effects.",
          "highlight": {
            "startOffset": 33575,
            "endOffset": 35317,
            "quotedText": "Some studies are finding that the use of AI tools is associated with poor critical thinking skills. I’ll just quote [this recent one](https://www.mdpi.com/2075-4698/15/1/6):\n\n> The key findings indicate that higher usage of AI tools is associated with reduced critical thinking skills, and cognitive offloading plays a significant role in this relationship.\n> \n> Our findings are consistent with previous research indicating that excessive reliance on AI tools can negatively impact critical thinking skills. Firth et al. \\[[**24**](https://www.mdpi.com/2075-4698/15/1/6#B24-societies-15-00006)\\] and Zhang et al. \\[[**43**](https://www.mdpi.com/2075-4698/15/1/6#B43-societies-15-00006)\\] both highlight the potential for AI tools to enhance basic skill acquisition while potentially undermining deeper cognitive engagement. Our study extends this by quantitatively demonstrating that increased AI tool usage correlates with lower critical thinking scores, as measured by comprehensive assessments like the Halpern Critical Thinking Assessment (HCTA) tool\n\nThey quote a subject who states that “The more I use AI, the less I feel the need to problem-solve on my own. It’s like I’m losing my ability to think critically.”\n\nThey found education was an important factor in this: more educated people were willing to cross check AI statements, while less educated people took AI statements at face value. Another psychology paper found that people who were _less_ literate about how AI works were [more likely to want to use AI](https://osf.io/preprints/psyarxiv/t9u8g_v1).\n\nIt’s too early to say for sure how much of a real problem this is, but it seems plausible to me that it could be a big issue, and it’s one that better AI could make worse."
          },
          "importance": "70",
          "isValid": true
        },
        {
          "title": "Stealth outsourcing examples",
          "description": "The document reveals multiple cases where companies falsely presented human labor as AI automation, suggesting that some AI capabilities may be exaggerated through deceptive practices rather than technological advancement.",
          "highlight": {
            "startOffset": 39141,
            "endOffset": 40329,
            "quotedText": "The “stealth outsourcing” part of the story comes from several real examples of companies falsely pretending a product was fully AI. Cruise robotaxis hey were presented as fully self-driving, but in fact a remote human operator had to intervene and take over control [every 4-5 miles](https://www.cnbc.com/2023/11/06/cruise-confirms-robotaxis-rely-on-human-assistance-every-4-to-5-miles.html). Similarly, Amazons “just walk out” grocery stores, presented as fully automated, turned out to have [70% of their transactions](https://www.theguardian.com/commentisfree/2024/apr/10/amazon-ai-cashier-less-shops-humans-technology) reviewed by remote workers based in India. And just this week, it’s been revealed that an “AI shopping app” was actually being run by [a call centre in the phillipines](https://techcrunch.com/2025/04/10/fintech-founder-charged-with-fraud-after-ai-shopping-app-found-to-be-powered-by-humans-in-the-philippines/).\n\nThis is a simple way get “AI” to do jobs that are fare beyond the actual capabilities of AI on it’s own. Obviously this wouldn’t work for all bazillion queries to chatGPT, but for jobs with slower needs, this practice could prove increasingly popular."
          },
          "importance": "75",
          "isValid": true
        },
        {
          "title": "AI undermining scientific integrity",
          "description": "The document presents concerning evidence of AI being used to generate scientific papers and peer reviews, potentially compromising the reliability of scientific literature and knowledge production.",
          "highlight": {
            "startOffset": 47644,
            "endOffset": 50146,
            "quotedText": "There has been some evidence that some scientists are using unedited LLM responses in their scientific papers. The most amusing example is the paper that included a nonsensical AI-generated image of a [giant rat penis](https://scienceintegritydigest.com/2024/02/15/the-rat-with-the-big-balls-and-enormous-penis-how-frontiers-published-a-paper-with-botched-ai-generated-images/).\n\nThe folks at [retractionwatch](https://retractionwatch.com/) have compiled a [list of about a hundred papers](https://retractionwatch.com/papers-and-peer-reviews-with-evidence-of-chatgpt-writing/) that show clear evidence of copy-pasting from Chatgpt, such as papers containing the phrase “[as an AI language model](https://pubpeer.com/publications/0905A70AF6A77C4309B743FFCD21F8)” . Another study claimed that [up to 17%](https://www.reddit.com/r/MachineLearning/comments/1bnsuea/r_up_to_17_of_recent_ai_conference_peer_reviews/) of recent peer reviews for CS conferences were likely to be AI written. Here is an [anecdotal example](https://www.reddit.com/r/labrats/comments/15g63pz/aigenerated_paper_in_peer_review/) of someone being asked to review a paper with made up AI citations. These methods can’t tell whether the AI was just used for grammar or translation or for more substantive purposes: either way, I highly recommend against this practice, as science relies on precision writing.\n\nThe use of LLMs in science is being advocated by different groups: One editorial in Nature encouraged researchers to use LLM’s as part of the [peer review process](https://archive.is/WMzm8), although they emphasized you should only be using it to clarify and organize your original dot points. However most peer review is already in the form of dot points, so I really don’t get the point here. Another company is trying to sell a cryptocurrency based system for [flagging errors in scientific article](https://archive.is/fqAig)s, which another researcher found had a false positive rate of 14 out of 40 flagged papers. I don’t think this is terrible in principle, if you improve the accuracy and drop the crypto aspect, but I would be concerned at this being used as a replacement for review.\n\nI don’t think LLM’s have zero place in the scientific process, as I explored in [my previous article](https://titotal.substack.com/p/ai-is-not-taking-over-material-science). But there are high chances of misuse, and putting our very notions of truth itself in the whims of chatbots and their programmers could go very, very wrong."
          },
          "importance": "75",
          "isValid": true
        },
        {
          "title": "Comparative plausibility analysis",
          "description": "The author provides a thoughtful analysis of why their scenario, while unlikely, may be more plausible than traditional AI doom scenarios because it requires lower AI capabilities, is independent of alignment concerns, and scales with hype rather than actual capabilities.",
          "highlight": {
            "startOffset": 58782,
            "endOffset": 59959,
            "quotedText": "However, as skeptical as I am of my own scenario, I’m much _more_ skeptical of conventional AI doom scenarios, and this case has some clear advantages on the likelihood front:\n\nFirst, it requires much lower AI capabilities than any other AI doomsday scenario. The AI does not need to be an omnipotent machine god that can invent drexlerian nanotech from scratch: it just has to be smart enough to fool people, and people need to be dumb enough to let it.\n\nNext: It is fairly independent of whether AI is aligned to our values. A perfectly helpful, friendly AI that only has our best interests at heart can still get us killed by _doing an incompetent job_. Friendly AI’s could even be more dangerous on this front, by lulling us into a false sense of security.\n\nLastly: this problem scales. AI could continue to massively, genuinely improve and many of these issues would still be a problem as long as peoples _hype_ for AI grows faster than the actual capabilities.\n\nA corollary to this is that even if you fully believe that building a machine god is possible and will definitely happen, these issues could still be dangerous, as long as the real thing doesn’t happen _soon_."
          },
          "importance": "80",
          "isValid": true
        },
        {
          "title": "Balanced perspective on AI technology",
          "description": "The author maintains a balanced view, acknowledging AI's potential benefits while warning against uncritical adoption, showing this isn't an anti-technology piece but rather a call for thoughtful implementation.",
          "highlight": {
            "startOffset": 60261,
            "endOffset": 61123,
            "quotedText": "I am no Luddite [\\[5\\]](#fneka1pklpj7i). Machine learning and deep learning are incredibly useful and versatile tools that have proven their usefulness in many contexts for decades now. Generative AI has uses as a productivity booster if used correctly, and I do believe it has gotten meaningfully better over the last few years and will get better in the future.\n\nBut a tool is only useful if you know it’s limitations. And however powerful AI gets, it will have limitations. What scares me is a world in which we are unable to see these limitations, because we have crafted machines that _just too good at bullshitting to us_. A world where we allow the bullshit to take over because doing so is profitable to people with financial and political power. And while I doubt this would literally end the world, it could make it a substantially worse place to live."
          },
          "importance": "65",
          "isValid": true
        }
      ]
    },
    {
      "agentId": "ea-impact-evaluator",
      "costInCents": 8,
      "createdAt": "2025-04-17T01:00:48.651Z",
      "runDetails": "{\"model\":\"anthropic/claude-3.7-sonnet\",\"promptTokens\":17439,\"completionTokens\":2053,\"totalTokens\":19492,\"temperature\":0.1,\"runtimeMs\":44968}",
      "thinking": "I need to evaluate this article through an EA impact lens, focusing on the potential risks and impacts of mediocre AI deployment as described in the \"Slopworld 2035\" scenario.\n\nFirst, let me analyze the importance of this topic within EA frameworks:\n- This article addresses AI risk, but from a different angle than typical x-risk scenarios. Instead of superintelligent AI, it focuses on the societal harms from widespread deployment of mediocre AI systems.\n- The scenario touches on several EA cause areas: institutional decision-making, AI governance, epistemic quality, and potentially existential risk (though via a different mechanism than usually discussed).\n- The article provides a detailed fictional scenario followed by evidence of current trends that could lead to such outcomes.\n\nFor neglectedness:\n- The author is highlighting what they see as an underexplored risk pathway in AI development - not superintelligent AI taking over, but mediocre AI being deployed irresponsibly.\n- This perspective seems relatively neglected in mainstream AI safety discourse, which often focuses on superintelligence scenarios.\n- The author provides numerous real-world examples of current problematic AI deployments and trends.\n\nFor tractability:\n- The article identifies specific mechanisms and failure modes that could be addressed through policy, corporate governance, and public awareness.\n- The author suggests some high-level interventions (valuing human thought, preventing tech oligarchy).\n\nFor novelty:\n- The \"mediocre AI\" risk pathway is presented as a counternarrative to typical AI doom scenarios.\n- The article connects multiple existing trends into a coherent alternative risk scenario.\n- The format of fiction + evidence + analysis is somewhat novel for communicating these ideas.\n\nNow for the Fermi estimation of impact:\n- I need to estimate the counterfactual impact of this article on readers' understanding and actions regarding AI risk.\n- I should consider both the direct effects on readers and potential indirect effects through changing discourse.\n- I'll need to account for both positive impacts (better understanding of risks) and potential negative impacts (if any).\n\nLet me now develop specific comments on key sections of the article that are particularly relevant from an EA impact perspective.",
      "summary": "From an EA impact perspective, this article provides a valuable contribution to AI risk discourse by presenting an alternative, empirically-grounded risk pathway focused on mediocre AI deployment rather than superintelligence. The author effectively combines a fictional scenario with extensive real-world evidence to illustrate how societal harms could emerge from overreliance on AI systems that appear capable but actually underperform human experts.\n\nThe article's greatest strengths are its evidence-based approach, drawing on numerous documented cases of problematic AI deployments, and its comparative risk analysis that highlights why this scenario may be more likely than conventional AI doom narratives. The focus on institutional decision-making degradation, epistemic environment decline, and the potential for mediocre AI to cause harm through incompetence rather than malice aligns well with EA concerns about systemic risks.\n\nWhile the article would benefit from more explicit quantification of probabilities and impact magnitudes, it successfully identifies neglected risk pathways that deserve more attention in AI governance discussions. The article's counterfactual impact likely includes shifting some attention within EA from superintelligence scenarios to more immediate governance challenges around mediocre AI deployment, potentially leading to more balanced allocation of resources across different AI risk pathways.",
      "comments": [
        {
          "title": "Novel AI Risk Pathway Analysis",
          "description": "This section presents a valuable alternative perspective on AI risk that differs from typical x-risk scenarios. By focusing on mediocre AI rather than superintelligent AI, the author identifies a more empirically grounded and potentially more immediate risk pathway. This contributes to the EA discourse by expanding the range of scenarios considered for AI governance and safety work.",
          "importance": "90",
          "grade": "85",
          "highlight": {
            "startOffset": 882,
            "endOffset": 1302,
            "quotedText": "ly.\n\nIn this article, I’m going to do a twist: I’m going to write a story (and detailed analysis) about a scenario where humanity is disempowered and destroyed by AI that is _dumber_ than us, due to a combination of hype, overconfidence, greed and anti-intellectualism. This is a scenario where instead of AI bringing untold abundance or tiling the universe with paperclips, it brings mediocrity, stagnation, and inequal"
          },
          "isValid": true
        },
        {
          "title": "Evidence-Based Risk Assessment",
          "description": "The author provides a comprehensive collection of real-world examples that support their concerns, significantly strengthening the empirical foundation of their argument. This evidence-based approach aligns with EA's emphasis on grounding claims in data rather than speculation. The extensive sourcing allows readers to verify claims independently and assess the magnitude of current problems.",
          "importance": "85",
          "grade": "90",
          "highlight": {
            "startOffset": 23463,
            "endOffset": 23937,
            "quotedText": "ber of different concerning trends in current day society.\n\nFor the rest of this section I will cover a number of these ingredients, with references to real world examples. **Note that a few news articles does** _**not**_ **mean something is necessarily a huge problem**, and you shouldn’t take this as me saying all of these are _currently_ large problems, just that they are potential areas of concern for the future. Every new technology goes through some level of moral "
          },
          "isValid": true
        },
        {
          "title": "Epistemic Environment Degradation",
          "description": "The author identifies how AI could degrade our epistemic environment through hallucinations, scientific paper generation, and critical thinking decline. This connects directly to EA concerns about epistemic quality and information hazards. The evidence presented suggests this is already happening in scientific publishing, which could significantly impact our ability to make evidence-based decisions.",
          "importance": "85",
          "grade": "85",
          "highlight": {
            "startOffset": 33487,
            "endOffset": 35225,
            "quotedText": "ptical hats on, disaster might follow.\n\n_AI offloading and critical thinking decline:_\n\nSome studies are finding that the use of AI tools is associated with poor critical thinking skills. I’ll just quote [this recent one](https://www.mdpi.com/2075-4698/15/1/6):\n\n> The key findings indicate that higher usage of AI tools is associated with reduced critical thinking skills, and cognitive offloading plays a significant role in this relationship.\n> \n> Our findings are consistent with previous research indicating that excessive reliance on AI tools can negatively impact critical thinking skills. Firth et al. \\[[**24**](https://www.mdpi.com/2075-4698/15/1/6#B24-societies-15-00006)\\] and Zhang et al. \\[[**43**](https://www.mdpi.com/2075-4698/15/1/6#B43-societies-15-00006)\\] both highlight the potential for AI tools to enhance basic skill acquisition while potentially undermining deeper cognitive engagement. Our study extends this by quantitatively demonstrating that increased AI tool usage correlates with lower critical thinking scores, as measured by comprehensive assessments like the Halpern Critical Thinking Assessment (HCTA) tool\n\nThey quote a subject who states that “The more I use AI, the less I feel the need to problem-solve on my own. It’s like I’m losing my ability to think critically.”\n\nThey found education was an important factor in this: more educated people were willing to cross check AI statements, while less educated people took AI statements at face value. Another psychology paper found that people who were _less_ literate about how AI works were [more likely to want to use AI](https://osf.io/preprints/psyarxiv/t9u8g_v1).\n\nIt’s too early to say for sure how much of a real problem this is, but it seems"
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Institutional Decision Quality Concerns",
          "description": "This section highlights how AI could degrade institutional decision-making quality, a key concern in EA governance work. The examples of AI being integrated into government functions without adequate safeguards represent potential systemic risks that could affect millions of people through policy decisions, making this highly relevant to EA institutional decision-making work.",
          "importance": "85",
          "grade": "80",
          "highlight": {
            "startOffset": 53666,
            "endOffset": 55397,
            "quotedText": "mpany, or AI hallucinations, could end up responsible for actual government actions, up to and including the militaries decisions on who to shoot.\n\nWhen the UK government asks for public consultation, they will no longer read the actual words that people submit: instead they will get an [AI to read through them all and summarise it](https://ai.gov.uk/projects/consult/). The UK government also heavily used AI chatbots to generate diagrams and citations for a report on the impact of AI on the labour market, [some of which were hallucinated](http://The Impact of AI on the Labour Market). Musks DOGE team have been reportedly using AI chatbots as part of their decision making process over what [programs to cut](https://edition.cnn.com/2025/03/04/tech/doge-ai-government-cuts-expert-concerns/index.html), and is pushing an [AI app](https://www.inc.com/bruce-crumley/doges-ai-app-replacing-fired-federal-workers-proves-about-as-good-as-an-intern/91158894) as a replacement for the fired federal workers.\n\nOpenAI have signed a deal with the US national laboratories, including their [nuclear security division](https://openai.com/index/strengthening-americas-ai-leadership-with-the-us-national-laboratories/). In the war in Gaza, the Israeli military has been using machine learning systems to identify [who is an enemy combatant](https://archive.is/sz3di#selection-675.152-675.221) marked for death. Although it did require sign-off from a human, given that Israel was willing to accept 15 or more civilian casualties as collateral damage to take out 1 Hamas fighter, I think it’s very likely that some civilians were falsely flagged as Hamas fighters by AI and killed.\n\nTo be clear, these are a long way away from putting Chatg"
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Fermi Estimate-Like Probability Assessment",
          "description": "The author provides a qualitative probability assessment of their scenario, acknowledging its low likelihood while explaining why it might be more plausible than superintelligence scenarios. This type of comparative probability analysis is valuable for prioritization within EA frameworks, though it would benefit from more explicit quantification of probabilities and magnitudes.",
          "importance": "80",
          "grade": "75",
          "highlight": {
            "startOffset": 58631,
            "endOffset": 59805,
            "quotedText": "cratic forces: I think this oligarchic turn is happening, but I don’t think it’s powerful enough yet to allow a scenario like this to actually occur.\n\nHowever, as skeptical as I am of my own scenario, I’m much _more_ skeptical of conventional AI doom scenarios, and this case has some clear advantages on the likelihood front:\n\nFirst, it requires much lower AI capabilities than any other AI doomsday scenario. The AI does not need to be an omnipotent machine god that can invent drexlerian nanotech from scratch: it just has to be smart enough to fool people, and people need to be dumb enough to let it.\n\nNext: It is fairly independent of whether AI is aligned to our values. A perfectly helpful, friendly AI that only has our best interests at heart can still get us killed by _doing an incompetent job_. Friendly AI’s could even be more dangerous on this front, by lulling us into a false sense of security.\n\nLastly: this problem scales. AI could continue to massively, genuinely improve and many of these issues would still be a problem as long as peoples _hype_ for AI grows faster than the actual capabilities.\n\nA corollary to this is that even if you fully believe t"
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Comparative Risk Analysis",
          "description": "The author provides a valuable comparative analysis between their scenario and conventional AI doom scenarios, highlighting three key advantages in likelihood. This type of comparative analysis is essential for EA prioritization frameworks, helping to allocate attention and resources across different risk pathways based on their relative probability and tractability.",
          "importance": "90",
          "grade": "85",
          "highlight": {
            "startOffset": 58959,
            "endOffset": 59748,
            "quotedText": "First, it requires much lower AI capabilities than any other AI doomsday scenario. The AI does not need to be an omnipotent machine god that can invent drexlerian nanotech from scratch: it just has to be smart enough to fool people, and people need to be dumb enough to let it.\n\nNext: It is fairly independent of whether AI is aligned to our values. A perfectly helpful, friendly AI that only has our best interests at heart can still get us killed by _doing an incompetent job_. Friendly AI’s could even be more dangerous on this front, by lulling us into a false sense of security.\n\nLastly: this problem scales. AI could continue to massively, genuinely improve and many of these issues would still be a problem as long as peoples _hype_ for AI grows faster than the actual capabilities."
          },
          "isValid": true
        }
      ],
      "grade": 83
    },
    {
      "agentId": "quantitative-forecaster",
      "costInCents": 9,
      "createdAt": "2025-04-17T01:01:50.390Z",
      "runDetails": "{\"model\":\"anthropic/claude-3.7-sonnet\",\"promptTokens\":16560,\"completionTokens\":2564,\"totalTokens\":19124,\"temperature\":0.1,\"runtimeMs\":52813}",
      "thinking": "This document presents a fictional scenario called \"Slopworld 2035\" that explores potential negative consequences of mediocre AI being deployed too widely and replacing human expertise. The author contrasts this with typical AI doom scenarios that focus on superintelligent AI taking over. Instead, this scenario focuses on the dangers of deploying AI that is actually less capable than humans but is hyped and deployed anyway due to economic and social pressures.\n\nAs a quantitative forecaster, I need to analyze this scenario and identify elements that can be transformed into concrete, measurable forecasting questions with probability estimates. I'll focus on:\n\n1. The plausibility of specific claims and trends mentioned\n2. Quantifying the likelihood of key developments\n3. Identifying reference classes for similar technological transitions\n4. Breaking down complex scenarios into more tractable components\n5. Providing calibrated probability estimates for key elements\n\nThe author presents several concerning trends that could contribute to this scenario:\n\n- Benchmark gaming: AI systems performing well on tests but failing at real-world applications\n- No imminent intelligence explosion: skepticism about rapid AI advancement to superintelligence\n- Hallucinations: AI confidently presenting false information as fact\n- AI misdeployment: using AI for tasks it's not qualified for\n- AI offloading and critical thinking decline: humans relying on AI and losing cognitive skills\n- Overestimating AI job replacibility: underestimating the complexity of human jobs\n- Enshittification: degradation of product quality after market capture\n- Undercutting workers: using AI to justify lower pay for similar work\n- Stealth outsourcing: falsely claiming AI capabilities while using human labor\n- Excessive AI hype: unrealistic claims about AI capabilities\n- Pollution: energy demands of AI systems\n- Easy data scraping: training AI on data without permission\n- Algorithmic bias: AI systems perpetuating harmful biases\n- Anti-intellectualism: distrust of domain experts\n- AI undermining science: scientists using unedited LLM responses in papers\n- AI deepfakes/spam/misinformation: malicious use of AI-generated content\n- Oligarchy and democratic decline: concentration of wealth and power\n- AI making military and government decisions: relying on AI for critical functions\n\nThe author acknowledges this extreme scenario is unlikely but argues it's more plausible than typical AI doom scenarios because:\n1. It requires lower AI capabilities\n2. It doesn't depend on whether AI is aligned with human values\n3. The problem scales with AI hype rather than actual capabilities\n\nFrom a forecasting perspective, I need to:\n1. Identify which trends are most measurable and trackable\n2. Develop concrete forecasting questions about these trends\n3. Provide probability estimates based on current evidence\n4. Consider relevant reference classes and base rates\n5. Identify key uncertainties and potential inflection points\n\nLet me focus on developing specific forecasting questions and probability estimates for the most quantifiable aspects of this scenario.",
      "summary": "From a quantitative forecasting perspective, the 'Slopworld 2035' scenario presents several measurable trends that can be transformed into concrete predictions. While the extreme dystopian outcome is unlikely (I estimate <5% probability by 2035), individual components warrant serious consideration. The persistence of AI hallucinations (60-70% likely to remain a significant issue by 2030), benchmark gaming (65-75% likely to continue), and cognitive offloading effects (60-70% likely to show measurable impacts) represent the most concerning trends. AI energy consumption will likely reach 5-10% of global electricity by 2030 (50-60% probability), while job displacement will likely affect 20-30% of professional sectors, below hyped predictions. The scenario's greatest forecasting value lies in decomposing AI risk into measurable components rather than focusing solely on superintelligence concerns. Reference class forecasting suggests technological transitions typically produce mixed outcomes rather than extreme scenarios, with regulatory responses eventually addressing the most harmful effects, though often with significant lag time.",
      "comments": [
        {
          "title": "Quantifying AI Benchmark Gaming",
          "description": "The highlighted text shows evidence of AI systems performing well on benchmarks but failing at real-world tasks. This pattern suggests a measurable gap between benchmark performance and practical utility. We can forecast: By 2030, what percentage of AI systems scoring in the top 10% on standard benchmarks will fail to match human expert performance in corresponding real-world tasks? Based on current evidence of data contamination and teaching-to-the-test effects, I estimate 65-75% probability that this gap will persist for most complex domains.",
          "importance": "85",
          "highlight": {
            "startOffset": 26410,
            "endOffset": 28921,
            "quotedText": "GPT-4s performance on a programming competition [from various years](https://web.archive.org/web/20250303191829/https://arxiv.org/abs/2312.02143), with the red line separating when contamination was possible from when it wasn’t.\n\nThis problem is so bad that an insider headline recently claimed that [benchmark tests are meaningless](https://web.archive.org/web/20250320203457/https://www.deeplearning.ai/the-batch/the-problem-with-benchmark-contamination-in-ai/). I wouldn’t quite go that far (and neither does the actual article), as you can test them on [uncontaminated contests](https://matharena.ai/) and questions that haven’t been released to the public. But I would advise treating performance on any benchmark that has it’s answers available on the internet with a garage sized grain of salt.\n\nIn the cases where performance is _not_ just a case of data contamination, there still may be an issue of “[teaching to the test](https://en.wikipedia.org/wiki/Teaching_to_the_test)”. A physics exam only teaches a small fraction of the skills required to be an actually good physicist: those other skills are much more ephemeral and hard to put down in words. By training on benchmarks like exams, even more advanced looking ones, we may be teaching AI to be really good at doing physics _exams_, but not to be really good at doing actual physics.\n\nThis might help explain why LLMs can perform well on some uncontaminated math benchmarks, but are still falling short on uncontaminated [math olympiads](https://matharena.ai/). Note that o3 mini (high), which scored the _best_ on the easier AIME II test, scored the _worst of all_ _models_ on the USA math olympiad[\\[2\\]](#fnx7muaba3st). [Two](https://arxiv.org/pdf/2504.01995) different [papers](https://arxiv.org/pdf/2503.21934) have analyzed these problems in detail, concluding that the results “indicate a reliance on heuristic shortcuts rather than authentic reasoning processes”. They were bad at evaluating the correctness of LLM solutions as well.\n\nYou can actually see the benchmark gaming in the LLM reasoning: in one of the questions failed by the top model gemini pro 2.5, the LLM states that “_All tests seem to suggest that all even k might be solutions. However, this type of problem in contests usually has a unique answer like k=2._”. This is useful knowledge for doing well on a test, but it’s useless for doing real math, and in this case over-learning the patterns of the easier tests seems to have hampered performance on the harder one."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Forecasting AI Hallucination Persistence",
          "description": "This example demonstrates a state-of-the-art model confidently hallucinating nonexistent images. We can forecast: By 2030, will leading commercial AI systems reduce their hallucination rate below 1% on general knowledge queries without specialized guardrails? Current evidence suggests these hallucinations are intrinsic to the architecture of large language models. I estimate only a 30-40% probability that this fundamental limitation will be overcome by 2030, despite incremental improvements in specific domains.",
          "importance": "90",
          "highlight": {
            "startOffset": 30699,
            "endOffset": 30861,
            "quotedText": "/upload/f_auto,q_auto/v1/mirroredImages/Sd3X5mWAEzyNaPamT/fae2xqkpyio3ypweawnd)\n\nDid you see it? That’s right, I didn’t actually upload any pictures! As far as I "
          },
          "isValid": true
        },
        {
          "title": "Quantifying AI Misdeployment Risk",
          "description": "These examples show a pattern of AI being deployed in critical contexts despite clear limitations. We can forecast: By 2028, what percentage of Fortune 500 companies will experience a significant operational failure directly attributable to inappropriate AI deployment? Based on current misdeployment patterns and increasing pressure to adopt AI, I estimate a 70-80% probability that at least 25% of major companies will experience such failures within this timeframe.",
          "importance": "85",
          "highlight": {
            "startOffset": 31467,
            "endOffset": 33322,
            "quotedText": "dangerous: see the much delayed rollout of self driving cars.\n\n_AI misdeployment:_\n\nIt’s very easy to find examples of people unthinkingly using genAI to do things it is very clearly not qualified for.\n\nSome examples: A lawyer used chatgpt to [prepare court documents](https://www.bbc.com/news/world-us-canada-65735769), which made shit up and got him in a lot of trouble. A completely different law firm did [the exact same thing](https://www.abajournal.com/news/article/no-42-law-firm-by-headcount-could-face-sanctions-over-fake-case-citations-generated-by-chatgpt). Another lawyer in Canada did [this as well](https://www.theguardian.com/world/2024/feb/29/canada-lawyer-chatgpt-fake-cases-ai). A murder case in Ohio fell apart because it’s key evidence relied on [AI face recognition](https://www.theregister.com/2025/01/28/ai_facial_recognition_murder/) software that clearly stated on _every single page it produced_ that it was not court-admissible. A marketing team for the film Megalopolis used AI to find reviewer quotes for similar historical films, which the AI proceeded to [make up](https://variety.com/2024/film/news/megalopolis-trailer-fake-quotes-ai-lionsgate-1236116485/). A guy claimed to have made their website entirely with AI “[vibe coding](https://pivot-to-ai.com/2025/03/18/guys-im-under-attack-ai-vibe-coding-in-the-wild/)”… the wildly insecure site got hacked immediately. The National Eating Disorders Association announced that it would [replace it’s staff](https://www.psychiatrist.com/news/neda-suspends-ai-chatbot-for-giving-harmful-eating-disorder-advice/) with AI chatbots for people struggling with eating disorders. It proceeded to recommend that they focus on losing weight and measure their body fat with skin folk calipers, and the bot was pulled shortly afterwards.\n\nPeople in general do not have an accurate sense o"
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Estimating Cognitive Offloading Effects",
          "description": "This research suggests measurable cognitive impacts from AI reliance. We can forecast: By 2030, will there be a statistically significant decline (>10%) in critical thinking scores among frequent AI users compared to limited users, controlling for education level? Based on early research and historical patterns with other technologies, I estimate a 60-70% probability of such measurable cognitive impacts, with higher probability (75-85%) among less educated populations.",
          "importance": "80",
          "highlight": {
            "startOffset": 33752,
            "endOffset": 34975,
            "quotedText": "The key findings indicate that higher usage of AI tools is associated with reduced critical thinking skills, and cognitive offloading plays a significant role in this relationship.\n> \n> Our findings are consistent with previous research indicating that excessive reliance on AI tools can negatively impact critical thinking skills. Firth et al. \\[[**24**](https://www.mdpi.com/2075-4698/15/1/6#B24-societies-15-00006)\\] and Zhang et al. \\[[**43**](https://www.mdpi.com/2075-4698/15/1/6#B43-societies-15-00006)\\] both highlight the potential for AI tools to enhance basic skill acquisition while potentially undermining deeper cognitive engagement. Our study extends this by quantitatively demonstrating that increased AI tool usage correlates with lower critical thinking scores, as measured by comprehensive assessments like the Halpern Critical Thinking Assessment (HCTA) tool\n\nThey quote a subject who states that “The more I use AI, the less I feel the need to problem-solve on my own. It’s like I’m losing my ability to think critically.”\n\nThey found education was an important factor in this: more educated people were willing to cross check AI statements, while less educated people took AI statements at face value."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Fermi Estimate: AI Job Displacement",
          "description": "The author questions assumptions about AI job replacement. We can develop a Fermi estimate: By 2030, what percentage of current professional jobs will be substantially automated by AI? Breaking this down by job categories: Creative fields (15-25% automation), Knowledge work (25-35%), Service professions (20-30%), Technical fields (15-25%). Weighted by job distribution, I estimate 20-30% substantial automation across professional sectors by 2030, far below the 50%+ figures often claimed by AI proponents.",
          "importance": "85",
          "highlight": {
            "startOffset": 35864,
            "endOffset": 36831,
            "quotedText": "y AI’s. Contrary to popular belief, designing floorplans is a very small part of an architects job.\n\nThis is part of a general trend I’ve noticed, where the less you know about a profession, the easier it seems to replace it with AI[\\[3\\]](#fn1q38ta2vc3z). I did a deep dive on why AI can’t do [my job](https://titotal.substack.com/p/ai-is-not-taking-over-material-science), and it’s fairly easy to find similar sentiments from [doctors](https://www.reddit.com/r/doctorsUK/comments/1ioe7de/what_are_your_views_on_how_ai_will_impact/), ,[mathematicians](https://www.reddit.com/r/math/comments/1ahknh0/are_you_concerned_about_potential_job_loss_once/), [software engineer](https://www.reddit.com/r/ArtificialInteligence/comments/1jum6ct/hot_take_ai_wont_replace_that_many_software/)s, [game developers](https://www.reddit.com/r/GameDevelopment/comments/1io2dic/do_you_think_that_game_development_and_game/), etc. Perhaps they are underestimating the advances of AI… or "
          },
          "isValid": true
        },
        {
          "title": "Forecasting AI Energy Consumption",
          "description": "This highlights concerning trends in AI energy usage. We can forecast: By 2030, what percentage of global electricity production will be consumed by AI data centers? Current trends suggest rapid growth, but efficiency improvements and potential regulatory constraints will likely moderate this. I estimate a 50-60% probability that AI will consume 5-10% of global electricity by 2030, with only a 15-20% probability of reaching the extreme levels suggested by Schmidt (>20%).",
          "importance": "75",
          "highlight": {
            "startOffset": 42487,
            "endOffset": 110,
            "quotedText": "ots.effectivealtruism.org/out?url=https%3A%2F%2Ftitotal.substack.com%2Fp%2Fslopworld-2035-the-dangers-of-mediocre)\n\n_None of this article was written with AI assistance._\n\n### Introduction\n\nThere have been [many](https://ai-2027.com/), [many](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html), [many](https://x.com/joshua_clymer/status/1887905375082656117?t=GNXtBmod6iYs17rBHJd9JQ) [attempts](https://www.cold-takes.com/ai-could-defeat-all-of-us-combined/) to lay out scenarios of AI taking over or destroying humanity. What they tend to have in common is an assumption that our doom will be sealed as a result of AI becoming significantly smarter and more powerful than the best humans, eclipsing us in skill and power and outplaying us effortlessly.\n\nIn this article, I’m going to do a twist: I’m going to write a story (and detailed analysis) about a scenario where humanity is disempowered and destroyed by AI that is _dumber_ than us, due to a combination of hype, overconfidence, greed and anti-intellectualism. This is a scenario where instead of AI bringing untold abundance or tiling the universe with paperclips, it brings mediocrity, stagnation, and inequality.\n\nThis is not a forecast. This story probably won’t happen. But it’s a story that reflects why _I_ am worried about AI, despite being generally dismissive of all those doom stories above. It is accompanied by an extensive, sourced list of present day issues and warning signs that are the source of my fears.\n\nThis post is divided into 3 parts:\n\nPart 1 is my attempt at a plausible sounding science fiction story sketching out this scenario, starting with the decline of a small architecture firm and ending with nuclear Armageddon.\n\nIn part 2 I will explain, with sources, the real world current day trends that were used as ingredients for the story.\n\nIn part 3 I will analysise the likelihood of my scenario, why I think it’s very unlikely, but also why it has some clear likelihood advantages over the typical doomsday scenario.\n\n### The story of Slopworld\n\n_In the nearish future:_\n\nWhen the architecture firm HBM was bought out by the Vastum corporation, they announced that they would fire 99% of their architects and replace them with AI chatbots.\n\nThe architect-bot they unveiled was incredibly impressive. After uploading your site parameters, all you had to do was chat with a bot to tell it everything you needed from the building, and it would produce a nearly photo-realistic 3d model of a building fitting your requirements, which you could even zoom around to look at from different angles. You could then just talk to the bot, saying things like “could you add another large window to the living room”, and after some time, out would pop another 3d model with the change. Once you were satisfied, the 3d model was turned into schematics and sent off to sub-contractors for construction and interior design, and your building would be created, no pesky architect required.\n\nThis sounded great for Thom Bradley, the head of a development company that was building up a small apartment complex in a residential area. Thom signed a contract with Vastum and decided to personally “design” the complex in collaboration with the new chatbot. Thom hated modern architecture, so he decided to build a complex with cool, retro, “art deco” aesthetic. He was enamored with the chatbot, which appeared to be smart, helpful and friendly. Occasionally there would be problems understanding his commands and rooms would change places inexplicably, but after a few tries the problems would get fixed. He even got to design an ornate set of steps leading into the atrium! The final result looked very cool, and Thom was happy for Vastum to send the design off to sub-contractors to build.\n\nAnd that’s when everything started to go wrong.\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Sd3X5mWAEzyNaPamT/lm7afccctbazsiaf0pig)\n\nWarped architecture is fun if you [do it on purpose](https://en.wikipedia.org/wiki/Dancing_House), but I wouldn’t build a world around it.\n\nThe basement was in the wrong place for the piping system around the area, leading to massive tunneling costs to hook up the building. The designer forgot that the complex needed a boiler room, so a ground floor apartment had to be retrofitted for the purpose at great expense.\n\nWhen the building was actually opened, the flaws of the design become apparent to the residents themselves. In the context of the surrounding buildings, the art deco design sticks out like a sore thumb. The windows in the building are faced to let in massive amounts of heat in summer, leading to skyrocketing A/C bills. The ventilation in the house was designed for a lower humidity level, so the residents end up with a constant black mould problem. Some of the rooms in peoples apartments were way too narrow for their intended purposes. The design was also completely wheelchair inaccessible, due to the largely unnecessary steps that Thom put in earlier. Overall, it was just a terrible place to live.\n\nWhat the hell just went wrong?\n\nWell, Vastum was hiding a few secrets up it’s sleeve. It had quietly bought up the major architectural software companies, and had attempted to create AI agents with their vast amounts of data, but had quickly realized the tech was way too unreliable to survive contact with someone like Thom. Instead, they hard coded a traditional algorithm. The input was the “specs”, a long file with set fields listing external characteristics of the house like dimensions and number of stories, and a long list of rooms, each labelled with subfeatures like “number of windows” and “floor material”. The hardcoded algorithm took this list and piped it into existing 3d architecture software to produce a building that fit as best as it could, using a large library of pre-built rooms and modifying them to fit the “specs” according to an algorithm, squeezing rooms to fit if necessary.\n\nWhen Thom talked with the chatbot, the LLM was specifically designed to do one thing only: take in Thoms request and use it to fill out the “specs”. Vastum had trained it on a large set of architecture designs, described in plain english and in “spec” form, so that it could most easily transfer from one set to the other. Despite this, the bot still regularly messed up, so Vastum secretly hired a group of underpaid workers in India to “switch in” for the bot anytime user frustration was flagged, and attempt to manually implement whatever request they had. At the end of the process the one overworked architect they had secretly kept on staff was given an absurdly short deadline to switch out materials for the cheapest option that looked similar, assuming that Thom wouldn’t notice. Helping matters along, a pro-growth administration had recently waived all regulations and liability on new builds other than “make sure it won’t fall down”. And the ADA had just gotten repealed, so wheelchair inaccessibility wasn’t a problem at all.\n\nThis process was smooth enough to greatly impress the public (A few people claimed this was proof of human-level AGI). But it left out most of what a human architect was a_ctually doing._ It wasn’t taking into account the specific context of the actual location it was built in, because everything was being made out of prefab designs and neither Thom nor Vastum cared about local factors, like the climate and the nearby area. And the bot was a yes-man that was incapable of pushing back on Thom when his ideas were unworkable and stupid. A big part of an architects job was making sure that a building you created was actually _good_. This was the part of the process that Vastum cut out.\n\nHere’s the thing though: the price offered was so damn cheap that, even factoring in some of the retrofits that he couldn’t lawyer his way out of paying, Thom still made a bigger profit than he would have with a regular architectural firm. Now, Vastum was actually losing large amounts of money on the operation, but that was okay: they were being handed bundles of cash by investors, hoping that if they gobbled up enough of the architecture marketshare, they could become profitable _later_, when the AI got better (which they assured investors it would).\n\nThe only losers were the people who actually lived in the apartments, who had to pay absurd amounts to deal with the various issues and live in a prematurely crumbling, shitty, ugly apartments. But there was still a housing crisis, so they didn’t get much choice in the matter. Vastums designs flooded the market, and the society got just that _little bit_ uglier.\n\n…..\n\nThis was not the start of Vastum dominance. No, that had come many years earlier, in the latter half of the 2020’s, when the genAI boom went crazy.\n\nExperts in many fields like architecture had already been using generative AI in limited capacities for years at this point, and despite some initial grumbling had figured out how to compensate for it’s hallucinations and shortcomings to get productivity boosts by automating boilerplate tasks. They noted gradual improvements in LLM reasoning, but after experimenting with each one they would conclude that was still a long way to go before their job was under threat in any real capacity.\n\nIn ordinary times this technology would have been applied judiciously, and be a source of prosperity and growth. Unfortunately, these weren’t ordinary times . Due to a series of poor decisions by the US government the world economy was on the brink of collapse, and the only thing keeping it afloat was _massive amounts of AI hype_.\n\nVenture capitalists had already spent years pouring billions upon billions of dollars into AI companies. The US government had also spent their own billions of dollars as well, convinced of the imperative of beating china in the “AGI race”. These tens of billions of dollars were not being set on fire in order to _moderately boost productivity_: they were trying to build something much greater. The tale was that any second now, we’d get a machine that would bootstrap itself to superintelligence, and cure cancer and invent nanotech and write the next great american novel.\n\nAnd yet… these miracles kept not happening. It turned out that training your machines to do better and better on coding and exam benchmarks did not transfer to creating a nanite-inventing machine god. Outside of arenas with massive amounts of easily scrapeable or generatable high quality data, AI progress was incremental and in many areas it wasn’t progressing at all.\n\nThe AI companies held off the skeptical grumblings for a while by playing whack-a-mole: they monitored the web for all the examples they could find of repeatable errors and spent their time manually plugging each hole. When skeptics pointed out that image generators couldn’t draw analog clocks showing specific times, they mass generated synthetic images of labelled clocks, so that in the next model release they’d go viral for their accurate clocks. AI chatbots became giant patchworks of manually designed prompts and sub-AI calls, designed to direct people to whatever hacks they found to work for specific sub-problems. This was all hidden under the hood by a claimed need for secrecy, making closed off AI models appear much more advanced than they actually were. It did not get us any closer to actual AGI.\n\nFor a few years, these gradual improvements sufficed to keep the money flowing, but the bill was coming due. Startups attempted to flood the market with slop AI generated books and games with very little success, outside of tech enthusiasts swearing up and down that it was just as good as the human stuff. AI usership numbers were starting to stagnate.\n\nThe AI bubble was in danger of popping. They needed AI to start taking over jobs, _now_, but the AI just was nowhere near good enough to replace experts in almost all fields that were actually worth anything.\n\nThe solution was simple: just replace the experts with AI anyway, and _pretend_ they’re good. The early “vibe coding” days had provided the template: soon every businessman was discussing businesses doing “vibe architecture”, “vibe therapy”, “vibe medicine”, and “vibe education”. They scraped as much data as they could on each field without permission and used them to build shitty chatbot versions of all them. They preferentially hired university graduates who had spent their entire degrees using chatgpt to do their assignments and studies, placing a higher premium on being “good prompters” than being actually qualified in the field: after all, with AGI around the corner, clearly talking to AI is the most important skill out there.\n\nThis was the point where our architecture story from above occurred. The architects on the firm tried to protest after they were fired, but they were ignored. The line became that the architects were simply self-interested Luddites trying to keep their jobs in the face of progress. The average person couldn’t tell the difference between the buildings that the expert architect put out and the one that Vastum did: in one blog post someone showed that the average person preferred the Vastum designs, declaring that “modern architecture is dead and that’s good”. This story repeated itself countless times, in countless industries, and the stock value of AI companies went through the roof.\n\nThe actual world was too beautifully complex to be understood by AI: so they set to work crushing it into a cube for easier processing.\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Sd3X5mWAEzyNaPamT/t0impd31knfcuhohhekh)\n\nThe “AI legible” society. [credit](https://www.flickr.com/photos/21649179@N00/3660541991) fdecomite\n\nAt this point, AI was too big to fail, but there were still a few pesky intellectuals still in the way. The academics were chiming in to point out that the vibe coded urban planning was actually making traffic worse, and the media was raising awareness of people killing themselves after talking to hallucinating AI therapy bots. Fortunately for tech companies, they had control over social media platforms and most of the media apparatus at this point, so they could fight back.\n\nThey started a war on experts.\n\nFor the masses, the narrative was that scientists and academics were a bunch of elitist wankers wasting your tax-money on useless research. For the upper classes the narrative was that human scientists were slow and corrupt, and that AI science would accelerate the path towards AGI with “fresh”, “disruptive” techniques. _Nature_ and _Science_ started using AI tools for peer review, and decided to accept AI written papers, in the name of accelerating the future (and boosting Elseviers stock prices). Scientists who (correctly) protested that the AI papers were trash were as usual dismissed as just trying to keep their own jobs. AI paper mills started churning out similar papers with mildly changed premises, with the goal of optimizing “productivity”: these companies were able to announce “scientific breakthroughs” every month or so, which was very impressive to people who didn’t actually understand the fields they were talking about. These reports were reported breathlessly and inaccurately in newspaper articles, which were themselves starting to be written by AI, and used as an excuse to slash university grant funding.\n\nAs time went on more and more of humanity outsourced their own thinking to AI. Instead of reading a book in full, why not read an “AI shortened” book that is easier? Instead of reading that, why not just ask chatGPT for a page long summary?. Rather than forming my own opinion of the arguments in the book, why don’t I ask AI to determine what I should think of them? Our thinking itself became filtered through the lens of chatbots, which gave politicians the perfect opportunity to gut education itself. Why hire a college professor when you can use an AI lecturer trained on all their previous lectures? Why hire a public school teacher when each kid can be handed their own personalized AI tutor? What use is education anyway, when you can just ask the AI what to do? A large percentage of the population started to view chatbots as their best friends and their most trusted source of information, which gave the people _training_ those bots ever greater power over public opinion.\n\nHuman-level AI was slowly achieved: less because AI _rose_ in intelligence to meet the human level, and more because the populace _lost_ intelligence to fall to the level of the mediocre AI.\n\nI suppose it might look like I’m painting tech CEO’s as cartoon villains here, trying to make us dumb on purpose: to be clear, most of them were entirely sincere in their beliefs. Often college dropouts themselves, they’d long viewed academia and the “experts” with disdain. When it came to AI they mostly fell for their own hype, and often fell victim to the same AI brainrot as the general public. The AI seemed just as good as experts to _them_. Everything it said sounded perfectly plausible, and they viewed seizing large amounts of wealth and power as justified in the quest for a perfect AI future.\n\nYou would expect some opposition in the political realm, but the majority of representatives on both sides of the political aisle had investments in AI companies, and were getting obscenely wealthy off of their growth. The Democrats had responded to the Musk-Trump situation by deciding that they had a “billionaire gap”, and newly elected president Newsom successfully managed to marry their fortunes with the more liberal minded tech CEO’s. Politics became mostly a competition about whether the AI companies being handed control of society would have a “woke” or “anti-woke” veneer. Presidents were handing increasingly large amounts of executive power, which they used to slash all regulations and to hand off as much of the government apparatus as possible to the new tech oligarchy.\n\nAnti-AI activism was quite popular and became a standard cause for Bernie type leftists, but this group was successfully locked out of power and influence. Tech CEO’s had full control over all the world leading news media and social media websites, and did everything they could to algorithmically suppress anti-AI sentiment. In response to unemployment, Vastum and their competitors started offering “data drone” jobs: essentially paying people sub-minimum wage to wear special glasses that recorded every aspect of their life to be fed into the endless data thirst of AI models. These drones started being paid extra to promote the benefits of their employers AI models… and were fired if caught expressing any dissent.\n\nAs the dominance of AI companies continued with no real opposition, more and more of societies basic functions starts resting on the fewer and fewer real, actual, experts who are tasked with desperately patching the continuous stream of AI mistakes. And the diminishing number of experts that AI systems could train on started to cause drops in performance.\n\nThings started to fail.\n\n90% of air traffic controllers were laid off, with the remaining frantically trying to cover for the AI systems mistakes. A bunch of planes crashed, but this was blamed on DEI. The electricity demand of AI datacentres spirals out of control, and climate initiatives are cut. This was viewed as fine because “AGI is going to solve climate change soon”. They fired the electricity market experts, and suddenly blackouts started becoming more frequent. They fired the logistics experts, and shortages of household essentials started happening at random. But Vastum was still making a fortune from lucrative government contracts, so it sweeped these problems under the rug by blaming them on Chinese saboteurs.\n\nWhere was the “AI safety” movement in all of this? Well, the problem was that most of them bought into the hype. Oh, many were quick to point out when an AI company said something _too_ egregious, but the stance was always that an intelligence explosion was just around the corner, which played right into Vastum’s hands. Some of them splintered off to join the generalized anti-AI movement, but a lot of them rejected this due to that movement being AGI-skeptical and “woke”. Some tried to start their own protest movement in opposition to both AI companies and the skeptics: they managed to get a few hundred people to a protest once, which accomplished nothing. The rest settled on a plan of trying to infiltrate the Trump administration by cozying up to them online: but when the Newsom administration was elected in a landslide the taint of being trump-associated was so heavy that nobody with even a whiff of “AI safety” was invited to the levers of power ever again. They still spoke up, attributing every AI failure to “misalignment” and declaring it was proof that rogue AI was gonna kill us all any day now. They made a few high-profile failed predictions and produced an extremist splinter group that was plausibly described as an apocalypse cult. This was enough for most people to dismiss them all and not take them seriously, even though AI actually was destroying the world at that point, just in a more mundane way.\n\nSurely at _this point_ the people inside the AI companies could tell that they were heading for disaster? But at that point people had spent years inside a high pressure environment, working 13 hour days and being haunted at night by dreams of starbound civilisation and nightmares of the AI coming for them with whirring knives. Each company was becoming a paranoid mini-cult, obsessed with being the team to bring about the good machine god before China or the team down the road made an evil one. Internal criticism and skepticism was viewed as evidence that you were a spy for opposing companies trying to sow havoc, and groupthink took hold until only true ASI believers were left. Dishonesty, cutthroat behaviour, and deals with unethical actors were excused as necessary evils on the road to utopia.\n\nIn the final blow for mankind, one of these true believers at Vastum was put in charge of military reform for the united states government. They asked an LLM for an advice on the subject, which tells him to reform the nuclear warning system with new AI functionality. This early warning system was developed with a heavy reliance on AI tools, but an AI code checker declares that it is error free. It passed the “nuclear safety benchmarks” with flying colors as well, performing well in over a million hours of simulated scenarios. It was hooked up to the nuclear launch system, after the CEO asked a chatbot ten times if it was sure this is a good idea, and received multiple highly detailed reports assuring them that nothing can possibly go wrong.\n\nA few months after being hooked up, the AI warning system hallucinated that a large bunch of escaped balloons were actually incoming nuclear missiles. It started a nuclear war that sent humanity back to the stone age, where we had to learn to actually think for ourselves again.\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Sd3X5mWAEzyNaPamT/hgelacarrvpagofziyxc)\n\nThe harbingers of the apocalypse. [Credit](https://www.flickr.com/photos/8125160@N06/2431770977) Jakob E\n\n### Slopworld ingredients\n\nThe story above builds on a number of different concerning trends in current day society.\n\nFor the rest of this section I will cover a number of these ingredients, with references to real world examples. **Note that a few news articles does** _**not**_ **mean something is necessarily a huge problem**, and you shouldn’t take this as me saying all of these are _currently_ large problems, just that they are potential areas of concern for the future. Every new technology goes through some level of moral panic, which is sometimes unreasonable and sometimes isn’t. I’m not an expert on most of the issues I’m talking about here, so you should take me (and everyone else who writes about broad subjects like this) with appropriate levels of skepticism.\n\nA lot of this research was helped by going through the broad AI skepticsphere, such as [pivot to AI](https://pivot-to-ai.com/), [Ai snake oil](https://www.aisnakeoil.com/), [better offline](https://podcasts.apple.com/us/podcast/radio-better-offline-allison-morrow-paris-martineau/id1730587238?i=1000702799734), and [Marcus on AI](https://garymarcus.substack.com/). I am somewhat less bearish on AI than most of them, but that doesn’t mean they aren’t valuable resources for the kinda news you won’t see being posted in AI safety spaces. Unlike the AGI boosters, who are motivated by either monetary gain or fear of the apocalypse, there is [not much incentive for someone skeptical of AI capabilities to write in depth about it](https://titotal.substack.com/p/motivation-gaps-why-so-much-ea-criticism), so you should [support](https://pivot-to-ai.com/2025/04/05/send-pivot-to-ai-a-few-dollars-keep-the-stories-and-videos-coming/) the ones you _can_ find, to prevent groupthink reigning supreme.\n\n_Benchmark gaming_\n\nMachine learning expert[\\[1\\]](#fn5i99eney5d) nostalgebraist [recently noted](https://www.lesswrong.com/posts/oC4wv4nTrs2yrP5hz/?commentId=M9zbTWLZKir6nDW6i) that:\n\n> Current frontier models are, in some sense, \"much better at me than coding.\" In a formal coding _competition_ I would obviously lose to these things; I might well perform worse at more \"real-world\" stuff like SWE-Bench Verified, too.\n> \n> Among _humans_ with similar scores on coding and math benchmarks, many (if not all) of them would be better at my job than I am, and fully capable of replacing me as an employee. Yet the models are _not_ capable of this.\n\nThe rest of the comment is worth reading as well. What’s going on here?\n\nWell, in some cases, the AI’s may just be cheating. There is reasonable evidence of [widespread benchmark data contamination](https://web.archive.org/web/20250305194633/https://www.theatlantic.com/technology/archive/2025/03/chatbots-benchmark-tests/681929/). Take a look at this graph of GPT-4 performance on a programming competition, before and after data contamination was possible:\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Sd3X5mWAEzyNaPamT/poazerpkmadl30x1ghya)\n\nGPT-4s performance on a programming competition [from various years](https://web.archive.org/web/20250303191829/https://arxiv.org/abs/2312.02143), with the red line separating when contamination was possible from when it wasn’t.\n\nThis problem is so bad that an insider headline recently claimed that [benchmark tests are meaningless](https://web.archive.org/web/20250320203457/https://www.deeplearning.ai/the-batch/the-problem-with-benchmark-contamination-in-ai/). I wouldn’t quite go that far (and neither does the actual article), as you can test them on [uncontaminated contests](https://matharena.ai/) and questions that haven’t been released to the public. But I would advise treating performance on any benchmark that has it’s answers available on the internet with a garage sized grain of salt.\n\nIn the cases where performance is _not_ just a case of data contamination, there still may be an issue of “[teaching to the test](https://en.wikipedia.org/wiki/Teaching_to_the_test)”. A physics exam only teaches a small fraction of the skills required to be an actually good physicist: those other skills are much more ephemeral and hard to put down in words. By training on benchmarks like exams, even more advanced looking ones, we may be teaching AI to be really good at doing physics _exams_, but not to be really good at doing actual physics.\n\nThis might help explain why LLMs can perform well on some uncontaminated math benchmarks, but are still falling short on uncontaminated [math olympiads](https://matharena.ai/). Note that o3 mini (high), which scored the _best_ on the easier AIME II test, scored the _worst of all_ _models_ on the USA math olympiad[\\[2\\]](#fnx7muaba3st). [Two](https://arxiv.org/pdf/2504.01995) different [papers](https://arxiv.org/pdf/2503.21934) have analyzed these problems in detail, concluding that the results “indicate a reliance on heuristic shortcuts rather than authentic reasoning processes”. They were bad at evaluating the correctness of LLM solutions as well.\n\nYou can actually see the benchmark gaming in the LLM reasoning: in one of the questions failed by the top model gemini pro 2.5, the LLM states that “_All tests seem to suggest that all even k might be solutions. However, this type of problem in contests usually has a unique answer like k=2._”. This is useful knowledge for doing well on a test, but it’s useless for doing real math, and in this case over-learning the patterns of the easier tests seems to have hampered performance on the harder one.\n\nI’m not saying AI is bad at math: in fact, non-LLM systems have [already scored highly](https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/) in the olympiad level math that the LLM’s failed at… which means that these type of problems were entirely possible with mid-2024 levels data and compute, and the LLM’s are falling short. It is likely that we could see this repeat in other domains, especially ones that lack the instant feedback loop and easy verifications of math and coding.\n\n_No imminent intelligence explosion:_\n\nYou’ll note that I don’t deny in my story that AI may get better on several fronts, including coding. So why don’t I think that we’ll get ASI in a few years, as something like [AI2027](https://ai-2027.com/) argues?\n\nWell, because I think the idea of an “intelligence explosion” is wrong, and I don’t think making a very good coder will allow you to bootstrap yourself to superintelligence (although I wish it were that easy). Rather than fight on this topic here, feel free to read this [long list](https://magnusvinding.com/2017/12/16/a-contra-ai-foom-reading-list/) of counterarguments to intelligence explosion arguments, including this [recent series](https://reflectivealtruism.com/2023/12/15/against-the-singularity-hypothesis-part-1-introduction/) by Thorstadt.\n\n_Hallucinations_\n\nThere’s been a lot of discussion of the “[hallucination problem](https://en.wikipedia.org/wiki/Hallucination_\\(artificial_intelligence\\))” in AI, so I’m not discuss it much here. Instead for fun I’ll just show you an exchange I had with state of the art reasoning model gemini pro 2.5, where it gave me in depth comparison of two images of elephants. See if you can spot the hallucination:\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Sd3X5mWAEzyNaPamT/fae2xqkpyio3ypweawnd)\n\nDid you see it? That’s right, I didn’t actually upload any pictures! As far as I can tell this is an entirely made up analysis of two pictures that do not exist. But it sounds very convincing, doesn’t it?\n\nI assume they’ll patch this particular issue soon, but the overall problem with making stuff up remains. We are crafting machines that alternate between perfectly correct answers and delusional bullshit, both delivered in the same confident tone and manner, both presented equally convincingly to any non-expert. This is really bad! There are many situations where a machine that is correct 95% of the time but spouts plausible sounding bullshit the other 5% of the time is really dangerous: see the much delayed rollout of self driving cars.\n\n_AI misdeployment:_\n\nIt’s very easy to find examples of people unthinkingly using genAI to do things it is very clearly not qualified for.\n\nSome examples: A lawyer used chatgpt to [prepare court documents](https://www.bbc.com/news/world-us-canada-65735769), which made shit up and got him in a lot of trouble. A completely different law firm did [the exact same thing](https://www.abajournal.com/news/article/no-42-law-firm-by-headcount-could-face-sanctions-over-fake-case-citations-generated-by-chatgpt). Another lawyer in Canada did [this as well](https://www.theguardian.com/world/2024/feb/29/canada-lawyer-chatgpt-fake-cases-ai). A murder case in Ohio fell apart because it’s key evidence relied on [AI face recognition](https://www.theregister.com/2025/01/28/ai_facial_recognition_murder/) software that clearly stated on _every single page it produced_ that it was not court-admissible. A marketing team for the film Megalopolis used AI to find reviewer quotes for similar historical films, which the AI proceeded to [make up](https://variety.com/2024/film/news/megalopolis-trailer-fake-quotes-ai-lionsgate-1236116485/). A guy claimed to have made their website entirely with AI “[vibe coding](https://pivot-to-ai.com/2025/03/18/guys-im-under-attack-ai-vibe-coding-in-the-wild/)”… the wildly insecure site got hacked immediately. The National Eating Disorders Association announced that it would [replace it’s staff](https://www.psychiatrist.com/news/neda-suspends-ai-chatbot-for-giving-harmful-eating-disorder-advice/) with AI chatbots for people struggling with eating disorders. It proceeded to recommend that they focus on losing weight and measure their body fat with skin folk calipers, and the bot was pulled shortly afterwards.\n\nPeople in general do not have an accurate sense of what an AI can and cannot do, especially in areas where they have little expertise. This is only going to get worse as AI gets better, and if we don’t have our skeptical hats on, disaster might follow.\n\n_AI offloading and critical thinking decline:_\n\nSome studies are finding that the use of AI tools is associated with poor critical thinking skills. I’ll just quote [this recent one](https://www.mdpi.com/2075-4698/15/1/6):\n\n> The key findings indicate that higher usage of AI tools is associated with reduced critical thinking skills, and cognitive offloading plays a significant role in this relationship.\n> \n> Our findings are consistent with previous research indicating that excessive reliance on AI tools can negatively impact critical thinking skills. Firth et al. \\[[**24**](https://www.mdpi.com/2075-4698/15/1/6#B24-societies-15-00006)\\] and Zhang et al. \\[[**43**](https://www.mdpi.com/2075-4698/15/1/6#B43-societies-15-00006)\\] both highlight the potential for AI tools to enhance basic skill acquisition while potentially undermining deeper cognitive engagement. Our study extends this by quantitatively demonstrating that increased AI tool usage correlates with lower critical thinking scores, as measured by comprehensive assessments like the Halpern Critical Thinking Assessment (HCTA) tool\n\nThey quote a subject who states that “The more I use AI, the less I feel the need to problem-solve on my own. It’s like I’m losing my ability to think critically.”\n\nThey found education was an important factor in this: more educated people were willing to cross check AI statements, while less educated people took AI statements at face value. Another psychology paper found that people who were _less_ literate about how AI works were [more likely to want to use AI](https://osf.io/preprints/psyarxiv/t9u8g_v1).\n\nIt’s too early to say for sure how much of a real problem this is, but it seems plausible to me that it could be a big issue, and it’s one that better AI could make worse.\n\n_Overestimating AI job replacibility_\n\nI decided to base the intro to my story around architecture by asking myself “what’s a job that _sounds_ easy to replace with AI, even though it really, really isn’t?” The architecture specific details were taken from [this thread](https://www.reddit.com/r/architecture/comments/12jtexp/architects_what_do_you_actually_do/) about what architects actually do, and [this thread](https://www.reddit.com/r/architecture/comments/17xmxnb/will_ai_replace_architects/) about why they aren’t likely to be replaced by AI’s. Contrary to popular belief, designing floorplans is a very small part of an architects job.\n\nThis is part of a general trend I’ve noticed, where the less you know about a profession, the easier it seems to replace it with AI[\\[3\\]](#fn1q38ta2vc3z). I did a deep dive on why AI can’t do [my job](https://titotal.substack.com/p/ai-is-not-taking-over-material-science), and it’s fairly easy to find similar sentiments from [doctors](https://www.reddit.com/r/doctorsUK/comments/1ioe7de/what_are_your_views_on_how_ai_will_impact/), ,[mathematicians](https://www.reddit.com/r/math/comments/1ahknh0/are_you_concerned_about_potential_job_loss_once/), [software engineer](https://www.reddit.com/r/ArtificialInteligence/comments/1jum6ct/hot_take_ai_wont_replace_that_many_software/)s, [game developers](https://www.reddit.com/r/GameDevelopment/comments/1io2dic/do_you_think_that_game_development_and_game/), etc. Perhaps they are underestimating the advances of AI… or perhaps AI boosters are underestimating the difficulty of jobs they don’t know that much about.\n\nI saw someone who was worried that AI was gonna cause real economic trouble soon by replacing travel agents. But the advent of the internet made travel agents _completely unnecessary_, and it _still_ only wiped out [half the travel agent jobs](https://fred.stlouisfed.org/series/LEU0254497900A). The number of travel agents has stayed roughly the same since 2008! What demographic of customers are you picturing that are simultaneously allergic to using skyscanner, but are _also_ willing to trust their travel plans to an AI over a human [\\[4\\]](#fngct98uyeiq)?\n\n_Enshittification:_\n\nBut I wouldn’t be so sure that just because an AI can’t do a job _well_, that AI workers won’t be forced down our throats anyway.\n\nThe term “enshittification” comes from this [wired article](https://www.wired.com/story/tiktok-platforms-cory-doctorow/) by Cory Doctorow. The premise is that companies get big by providing quality products that people actually want to use. But then they look around, and realize that they have users trapped on their platform, for reasons of social dynamics, habit, sentimental value, monopoly power, etc. So they then proceed to take a hacksaw to everything good about their product, bit by bit, in order to squeeze as much money out of their captive audience as possible. As another example, see the articles on why google search has apparently [massively declined in quality](https://www.wheresyoured.at/the-men-who-killed-google/).\n\nThe story above is a vision of a society where the _entire world_ gets enshittified. Everything gets worse, but humans in general have nowhere else to turn, because the oligarchs have control over all the resources.\n\n_Undercutting workers:_\n\nThis one actually comes from my girlfriend: She works as a freelance translator, and the company has a different pay rate for “proofreading” and “translating”, with the former paying less. Recently, the company has been sending her “proofreading” work which consists of proofreading _AI translations_. She says the translations are quite bad, so the proofreading of the machine translation takes nearly as long as just doing the translation herself, but of course, she gets paid less for it.\n\n_Stealth outsourcing:_\n\nThe “stealth outsourcing” part of the story comes from several real examples of companies falsely pretending a product was fully AI. Cruise robotaxis hey were presented as fully self-driving, but in fact a remote human operator had to intervene and take over control [every 4-5 miles](https://www.cnbc.com/2023/11/06/cruise-confirms-robotaxis-rely-on-human-assistance-every-4-to-5-miles.html). Similarly, Amazons “just walk out” grocery stores, presented as fully automated, turned out to have [70% of their transactions](https://www.theguardian.com/commentisfree/2024/apr/10/amazon-ai-cashier-less-shops-humans-technology) reviewed by remote workers based in India. And just this week, it’s been revealed that an “AI shopping app” was actually being run by [a call centre in the phillipines](https://techcrunch.com/2025/04/10/fintech-founder-charged-with-fraud-after-ai-shopping-app-found-to-be-powered-by-humans-in-the-philippines/).\n\nThis is a simple way get “AI” to do jobs that are fare beyond the actual capabilities of AI on it’s own. Obviously this wouldn’t work for all bazillion queries to chatGPT, but for jobs with slower needs, this practice could prove increasingly popular.\n\n_Excessive AI hype:_\n\nA large part of the premise of this dystopia relies on people buying into bullshit AI hype, so they trust it enough to put it in charge of things even when it’s not actually qualified to do so.\n\nAs an example while back I covered the claim of “[AI superforecasters](https://titotal.substack.com/p/is-superhuman-ai-forecasting-bs-some)”, where the center for AI safety claimed to have built an AI forecaster that performed better than top humans. They parroted this claim boldly, but it seemed to fall apart under closer analysis, and forecasting bots have still not beat top humans in [more structured forecasting competitions](https://www.lesswrong.com/posts/P8YwCvHoF2FHQoHjF/metaculus-q4-ai-benchmarking-bots-are-closing-the-gap). I also discussed the case of Deepminds “[automated discovery tool](https://titotal.substack.com/p/ai-is-not-taking-over-material-science)”, which claimed to discover 43 novel materials but in reality only 3 of the materials were likely the ones they were trying to make (and these weren’t novel).\n\nFuture predictions are getting… very out there. The anthropic CEO stated [in a post](https://darioamodei.com/machines-of-loving-grace) that within _1-3 years_, AI could be created that will be “smarter than a nobel prize winner… prove unsolved mathematical theorems, write extremely good novels, write difficult codebases from scratch”, and “ordering materials, directing experiments, watching videos, making videos… with a skill exceeding that of the most capable humans in the world”. In the same post they suggest that AI will _double the human lifespan_, with transformative achievements in the next 7-12 years.\n\nEven if you (wrongly) think that these projections are reasonable, I want you to imagine the effect if would have on society if these beliefs were widely _**and incorrectly**_ held. Now, I’m sure you, my very smart and rational reader, would _never_ fall for the vastum architect scam in the story, but can you say the same for the general public? Can you honestly tell me you can’t think of any billionaires or senior politicians who would fall for this type of thing?\n\n_Pollution:_\n\n"
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Probability of AI-Induced Market Concentration",
          "description": "The scenario depends on increasing market concentration. We can forecast: By 2030, will the top 5 AI companies control more than 75% of AI-related revenue globally? Based on current market dynamics, network effects, and historical patterns in tech industries, I estimate a 65-75% probability of this level of market concentration, with higher probability (80-90%) in the U.S. market specifically.",
          "importance": "75",
          "highlight": {
            "startOffset": 51324,
            "endOffset": 130,
            "quotedText": "m.org/out?url=https%3A%2F%2Ftitotal.substack.com%2Fp%2Fslopworld-2035-the-dangers-of-mediocre)\n\n_None of this article was written with AI assistance._\n\n### Introduction\n\nThere have been [many](https://ai-2027.com/), [many](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html), [many](https://x.com/joshua_clymer/status/1887905375082656117?t=GNXtBmod6iYs17rBHJd9JQ) [attempts](https://www.cold-takes.com/ai-could-defeat-all-of-us-combined/) to lay out scenarios of AI taking over or destroying humanity. What they tend to have in common is an assumption that our doom will be sealed as a result of AI becoming significantly smarter and more powerful than the best humans, eclipsing us in skill and power and outplaying us effortlessly.\n\nIn this article, I’m going to do a twist: I’m going to write a story (and detailed analysis) about a scenario where humanity is disempowered and destroyed by AI that is _dumber_ than us, due to a combination of hype, overconfidence, greed and anti-intellectualism. This is a scenario where instead of AI bringing untold abundance or tiling the universe with paperclips, it brings mediocrity, stagnation, and inequality.\n\nThis is not a forecast. This story probably won’t happen. But it’s a story that reflects why _I_ am worried about AI, despite being generally dismissive of all those doom stories above. It is accompanied by an extensive, sourced list of present day issues and warning signs that are the source of my fears.\n\nThis post is divided into 3 parts:\n\nPart 1 is my attempt at a plausible sounding science fiction story sketching out this scenario, starting with the decline of a small architecture firm and ending with nuclear Armageddon.\n\nIn part 2 I will explain, with sources, the real world current day trends that were used as ingredients for the story.\n\nIn part 3 I will analysise the likelihood of my scenario, why I think it’s very unlikely, but also why it has some clear likelihood advantages over the typical doomsday scenario.\n\n### The story of Slopworld\n\n_In the nearish future:_\n\nWhen the architecture firm HBM was bought out by the Vastum corporation, they announced that they would fire 99% of their architects and replace them with AI chatbots.\n\nThe architect-bot they unveiled was incredibly impressive. After uploading your site parameters, all you had to do was chat with a bot to tell it everything you needed from the building, and it would produce a nearly photo-realistic 3d model of a building fitting your requirements, which you could even zoom around to look at from different angles. You could then just talk to the bot, saying things like “could you add another large window to the living room”, and after some time, out would pop another 3d model with the change. Once you were satisfied, the 3d model was turned into schematics and sent off to sub-contractors for construction and interior design, and your building would be created, no pesky architect required.\n\nThis sounded great for Thom Bradley, the head of a development company that was building up a small apartment complex in a residential area. Thom signed a contract with Vastum and decided to personally “design” the complex in collaboration with the new chatbot. Thom hated modern architecture, so he decided to build a complex with cool, retro, “art deco” aesthetic. He was enamored with the chatbot, which appeared to be smart, helpful and friendly. Occasionally there would be problems understanding his commands and rooms would change places inexplicably, but after a few tries the problems would get fixed. He even got to design an ornate set of steps leading into the atrium! The final result looked very cool, and Thom was happy for Vastum to send the design off to sub-contractors to build.\n\nAnd that’s when everything started to go wrong.\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Sd3X5mWAEzyNaPamT/lm7afccctbazsiaf0pig)\n\nWarped architecture is fun if you [do it on purpose](https://en.wikipedia.org/wiki/Dancing_House), but I wouldn’t build a world around it.\n\nThe basement was in the wrong place for the piping system around the area, leading to massive tunneling costs to hook up the building. The designer forgot that the complex needed a boiler room, so a ground floor apartment had to be retrofitted for the purpose at great expense.\n\nWhen the building was actually opened, the flaws of the design become apparent to the residents themselves. In the context of the surrounding buildings, the art deco design sticks out like a sore thumb. The windows in the building are faced to let in massive amounts of heat in summer, leading to skyrocketing A/C bills. The ventilation in the house was designed for a lower humidity level, so the residents end up with a constant black mould problem. Some of the rooms in peoples apartments were way too narrow for their intended purposes. The design was also completely wheelchair inaccessible, due to the largely unnecessary steps that Thom put in earlier. Overall, it was just a terrible place to live.\n\nWhat the hell just went wrong?\n\nWell, Vastum was hiding a few secrets up it’s sleeve. It had quietly bought up the major architectural software companies, and had attempted to create AI agents with their vast amounts of data, but had quickly realized the tech was way too unreliable to survive contact with someone like Thom. Instead, they hard coded a traditional algorithm. The input was the “specs”, a long file with set fields listing external characteristics of the house like dimensions and number of stories, and a long list of rooms, each labelled with subfeatures like “number of windows” and “floor material”. The hardcoded algorithm took this list and piped it into existing 3d architecture software to produce a building that fit as best as it could, using a large library of pre-built rooms and modifying them to fit the “specs” according to an algorithm, squeezing rooms to fit if necessary.\n\nWhen Thom talked with the chatbot, the LLM was specifically designed to do one thing only: take in Thoms request and use it to fill out the “specs”. Vastum had trained it on a large set of architecture designs, described in plain english and in “spec” form, so that it could most easily transfer from one set to the other. Despite this, the bot still regularly messed up, so Vastum secretly hired a group of underpaid workers in India to “switch in” for the bot anytime user frustration was flagged, and attempt to manually implement whatever request they had. At the end of the process the one overworked architect they had secretly kept on staff was given an absurdly short deadline to switch out materials for the cheapest option that looked similar, assuming that Thom wouldn’t notice. Helping matters along, a pro-growth administration had recently waived all regulations and liability on new builds other than “make sure it won’t fall down”. And the ADA had just gotten repealed, so wheelchair inaccessibility wasn’t a problem at all.\n\nThis process was smooth enough to greatly impress the public (A few people claimed this was proof of human-level AGI). But it left out most of what a human architect was a_ctually doing._ It wasn’t taking into account the specific context of the actual location it was built in, because everything was being made out of prefab designs and neither Thom nor Vastum cared about local factors, like the climate and the nearby area. And the bot was a yes-man that was incapable of pushing back on Thom when his ideas were unworkable and stupid. A big part of an architects job was making sure that a building you created was actually _good_. This was the part of the process that Vastum cut out.\n\nHere’s the thing though: the price offered was so damn cheap that, even factoring in some of the retrofits that he couldn’t lawyer his way out of paying, Thom still made a bigger profit than he would have with a regular architectural firm. Now, Vastum was actually losing large amounts of money on the operation, but that was okay: they were being handed bundles of cash by investors, hoping that if they gobbled up enough of the architecture marketshare, they could become profitable _later_, when the AI got better (which they assured investors it would).\n\nThe only losers were the people who actually lived in the apartments, who had to pay absurd amounts to deal with the various issues and live in a prematurely crumbling, shitty, ugly apartments. But there was still a housing crisis, so they didn’t get much choice in the matter. Vastums designs flooded the market, and the society got just that _little bit_ uglier.\n\n…..\n\nThis was not the start of Vastum dominance. No, that had come many years earlier, in the latter half of the 2020’s, when the genAI boom went crazy.\n\nExperts in many fields like architecture had already been using generative AI in limited capacities for years at this point, and despite some initial grumbling had figured out how to compensate for it’s hallucinations and shortcomings to get productivity boosts by automating boilerplate tasks. They noted gradual improvements in LLM reasoning, but after experimenting with each one they would conclude that was still a long way to go before their job was under threat in any real capacity.\n\nIn ordinary times this technology would have been applied judiciously, and be a source of prosperity and growth. Unfortunately, these weren’t ordinary times . Due to a series of poor decisions by the US government the world economy was on the brink of collapse, and the only thing keeping it afloat was _massive amounts of AI hype_.\n\nVenture capitalists had already spent years pouring billions upon billions of dollars into AI companies. The US government had also spent their own billions of dollars as well, convinced of the imperative of beating china in the “AGI race”. These tens of billions of dollars were not being set on fire in order to _moderately boost productivity_: they were trying to build something much greater. The tale was that any second now, we’d get a machine that would bootstrap itself to superintelligence, and cure cancer and invent nanotech and write the next great american novel.\n\nAnd yet… these miracles kept not happening. It turned out that training your machines to do better and better on coding and exam benchmarks did not transfer to creating a nanite-inventing machine god. Outside of arenas with massive amounts of easily scrapeable or generatable high quality data, AI progress was incremental and in many areas it wasn’t progressing at all.\n\nThe AI companies held off the skeptical grumblings for a while by playing whack-a-mole: they monitored the web for all the examples they could find of repeatable errors and spent their time manually plugging each hole. When skeptics pointed out that image generators couldn’t draw analog clocks showing specific times, they mass generated synthetic images of labelled clocks, so that in the next model release they’d go viral for their accurate clocks. AI chatbots became giant patchworks of manually designed prompts and sub-AI calls, designed to direct people to whatever hacks they found to work for specific sub-problems. This was all hidden under the hood by a claimed need for secrecy, making closed off AI models appear much more advanced than they actually were. It did not get us any closer to actual AGI.\n\nFor a few years, these gradual improvements sufficed to keep the money flowing, but the bill was coming due. Startups attempted to flood the market with slop AI generated books and games with very little success, outside of tech enthusiasts swearing up and down that it was just as good as the human stuff. AI usership numbers were starting to stagnate.\n\nThe AI bubble was in danger of popping. They needed AI to start taking over jobs, _now_, but the AI just was nowhere near good enough to replace experts in almost all fields that were actually worth anything.\n\nThe solution was simple: just replace the experts with AI anyway, and _pretend_ they’re good. The early “vibe coding” days had provided the template: soon every businessman was discussing businesses doing “vibe architecture”, “vibe therapy”, “vibe medicine”, and “vibe education”. They scraped as much data as they could on each field without permission and used them to build shitty chatbot versions of all them. They preferentially hired university graduates who had spent their entire degrees using chatgpt to do their assignments and studies, placing a higher premium on being “good prompters” than being actually qualified in the field: after all, with AGI around the corner, clearly talking to AI is the most important skill out there.\n\nThis was the point where our architecture story from above occurred. The architects on the firm tried to protest after they were fired, but they were ignored. The line became that the architects were simply self-interested Luddites trying to keep their jobs in the face of progress. The average person couldn’t tell the difference between the buildings that the expert architect put out and the one that Vastum did: in one blog post someone showed that the average person preferred the Vastum designs, declaring that “modern architecture is dead and that’s good”. This story repeated itself countless times, in countless industries, and the stock value of AI companies went through the roof.\n\nThe actual world was too beautifully complex to be understood by AI: so they set to work crushing it into a cube for easier processing.\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Sd3X5mWAEzyNaPamT/t0impd31knfcuhohhekh)\n\nThe “AI legible” society. [credit](https://www.flickr.com/photos/21649179@N00/3660541991) fdecomite\n\nAt this point, AI was too big to fail, but there were still a few pesky intellectuals still in the way. The academics were chiming in to point out that the vibe coded urban planning was actually making traffic worse, and the media was raising awareness of people killing themselves after talking to hallucinating AI therapy bots. Fortunately for tech companies, they had control over social media platforms and most of the media apparatus at this point, so they could fight back.\n\nThey started a war on experts.\n\nFor the masses, the narrative was that scientists and academics were a bunch of elitist wankers wasting your tax-money on useless research. For the upper classes the narrative was that human scientists were slow and corrupt, and that AI science would accelerate the path towards AGI with “fresh”, “disruptive” techniques. _Nature_ and _Science_ started using AI tools for peer review, and decided to accept AI written papers, in the name of accelerating the future (and boosting Elseviers stock prices). Scientists who (correctly) protested that the AI papers were trash were as usual dismissed as just trying to keep their own jobs. AI paper mills started churning out similar papers with mildly changed premises, with the goal of optimizing “productivity”: these companies were able to announce “scientific breakthroughs” every month or so, which was very impressive to people who didn’t actually understand the fields they were talking about. These reports were reported breathlessly and inaccurately in newspaper articles, which were themselves starting to be written by AI, and used as an excuse to slash university grant funding.\n\nAs time went on more and more of humanity outsourced their own thinking to AI. Instead of reading a book in full, why not read an “AI shortened” book that is easier? Instead of reading that, why not just ask chatGPT for a page long summary?. Rather than forming my own opinion of the arguments in the book, why don’t I ask AI to determine what I should think of them? Our thinking itself became filtered through the lens of chatbots, which gave politicians the perfect opportunity to gut education itself. Why hire a college professor when you can use an AI lecturer trained on all their previous lectures? Why hire a public school teacher when each kid can be handed their own personalized AI tutor? What use is education anyway, when you can just ask the AI what to do? A large percentage of the population started to view chatbots as their best friends and their most trusted source of information, which gave the people _training_ those bots ever greater power over public opinion.\n\nHuman-level AI was slowly achieved: less because AI _rose_ in intelligence to meet the human level, and more because the populace _lost_ intelligence to fall to the level of the mediocre AI.\n\nI suppose it might look like I’m painting tech CEO’s as cartoon villains here, trying to make us dumb on purpose: to be clear, most of them were entirely sincere in their beliefs. Often college dropouts themselves, they’d long viewed academia and the “experts” with disdain. When it came to AI they mostly fell for their own hype, and often fell victim to the same AI brainrot as the general public. The AI seemed just as good as experts to _them_. Everything it said sounded perfectly plausible, and they viewed seizing large amounts of wealth and power as justified in the quest for a perfect AI future.\n\nYou would expect some opposition in the political realm, but the majority of representatives on both sides of the political aisle had investments in AI companies, and were getting obscenely wealthy off of their growth. The Democrats had responded to the Musk-Trump situation by deciding that they had a “billionaire gap”, and newly elected president Newsom successfully managed to marry their fortunes with the more liberal minded tech CEO’s. Politics became mostly a competition about whether the AI companies being handed control of society would have a “woke” or “anti-woke” veneer. Presidents were handing increasingly large amounts of executive power, which they used to slash all regulations and to hand off as much of the government apparatus as possible to the new tech oligarchy.\n\nAnti-AI activism was quite popular and became a standard cause for Bernie type leftists, but this group was successfully locked out of power and influence. Tech CEO’s had full control over all the world leading news media and social media websites, and did everything they could to algorithmically suppress anti-AI sentiment. In response to unemployment, Vastum and their competitors started offering “data drone” jobs: essentially paying people sub-minimum wage to wear special glasses that recorded every aspect of their life to be fed into the endless data thirst of AI models. These drones started being paid extra to promote the benefits of their employers AI models… and were fired if caught expressing any dissent.\n\nAs the dominance of AI companies continued with no real opposition, more and more of societies basic functions starts resting on the fewer and fewer real, actual, experts who are tasked with desperately patching the continuous stream of AI mistakes. And the diminishing number of experts that AI systems could train on started to cause drops in performance.\n\nThings started to fail.\n\n90% of air traffic controllers were laid off, with the remaining frantically trying to cover for the AI systems mistakes. A bunch of planes crashed, but this was blamed on DEI. The electricity demand of AI datacentres spirals out of control, and climate initiatives are cut. This was viewed as fine because “AGI is going to solve climate change soon”. They fired the electricity market experts, and suddenly blackouts started becoming more frequent. They fired the logistics experts, and shortages of household essentials started happening at random. But Vastum was still making a fortune from lucrative government contracts, so it sweeped these problems under the rug by blaming them on Chinese saboteurs.\n\nWhere was the “AI safety” movement in all of this? Well, the problem was that most of them bought into the hype. Oh, many were quick to point out when an AI company said something _too_ egregious, but the stance was always that an intelligence explosion was just around the corner, which played right into Vastum’s hands. Some of them splintered off to join the generalized anti-AI movement, but a lot of them rejected this due to that movement being AGI-skeptical and “woke”. Some tried to start their own protest movement in opposition to both AI companies and the skeptics: they managed to get a few hundred people to a protest once, which accomplished nothing. The rest settled on a plan of trying to infiltrate the Trump administration by cozying up to them online: but when the Newsom administration was elected in a landslide the taint of being trump-associated was so heavy that nobody with even a whiff of “AI safety” was invited to the levers of power ever again. They still spoke up, attributing every AI failure to “misalignment” and declaring it was proof that rogue AI was gonna kill us all any day now. They made a few high-profile failed predictions and produced an extremist splinter group that was plausibly described as an apocalypse cult. This was enough for most people to dismiss them all and not take them seriously, even though AI actually was destroying the world at that point, just in a more mundane way.\n\nSurely at _this point_ the people inside the AI companies could tell that they were heading for disaster? But at that point people had spent years inside a high pressure environment, working 13 hour days and being haunted at night by dreams of starbound civilisation and nightmares of the AI coming for them with whirring knives. Each company was becoming a paranoid mini-cult, obsessed with being the team to bring about the good machine god before China or the team down the road made an evil one. Internal criticism and skepticism was viewed as evidence that you were a spy for opposing companies trying to sow havoc, and groupthink took hold until only true ASI believers were left. Dishonesty, cutthroat behaviour, and deals with unethical actors were excused as necessary evils on the road to utopia.\n\nIn the final blow for mankind, one of these true believers at Vastum was put in charge of military reform for the united states government. They asked an LLM for an advice on the subject, which tells him to reform the nuclear warning system with new AI functionality. This early warning system was developed with a heavy reliance on AI tools, but an AI code checker declares that it is error free. It passed the “nuclear safety benchmarks” with flying colors as well, performing well in over a million hours of simulated scenarios. It was hooked up to the nuclear launch system, after the CEO asked a chatbot ten times if it was sure this is a good idea, and received multiple highly detailed reports assuring them that nothing can possibly go wrong.\n\nA few months after being hooked up, the AI warning system hallucinated that a large bunch of escaped balloons were actually incoming nuclear missiles. It started a nuclear war that sent humanity back to the stone age, where we had to learn to actually think for ourselves again.\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Sd3X5mWAEzyNaPamT/hgelacarrvpagofziyxc)\n\nThe harbingers of the apocalypse. [Credit](https://www.flickr.com/photos/8125160@N06/2431770977) Jakob E\n\n### Slopworld ingredients\n\nThe story above builds on a number of different concerning trends in current day society.\n\nFor the rest of this section I will cover a number of these ingredients, with references to real world examples. **Note that a few news articles does** _**not**_ **mean something is necessarily a huge problem**, and you shouldn’t take this as me saying all of these are _currently_ large problems, just that they are potential areas of concern for the future. Every new technology goes through some level of moral panic, which is sometimes unreasonable and sometimes isn’t. I’m not an expert on most of the issues I’m talking about here, so you should take me (and everyone else who writes about broad subjects like this) with appropriate levels of skepticism.\n\nA lot of this research was helped by going through the broad AI skepticsphere, such as [pivot to AI](https://pivot-to-ai.com/), [Ai snake oil](https://www.aisnakeoil.com/), [better offline](https://podcasts.apple.com/us/podcast/radio-better-offline-allison-morrow-paris-martineau/id1730587238?i=1000702799734), and [Marcus on AI](https://garymarcus.substack.com/). I am somewhat less bearish on AI than most of them, but that doesn’t mean they aren’t valuable resources for the kinda news you won’t see being posted in AI safety spaces. Unlike the AGI boosters, who are motivated by either monetary gain or fear of the apocalypse, there is [not much incentive for someone skeptical of AI capabilities to write in depth about it](https://titotal.substack.com/p/motivation-gaps-why-so-much-ea-criticism), so you should [support](https://pivot-to-ai.com/2025/04/05/send-pivot-to-ai-a-few-dollars-keep-the-stories-and-videos-coming/) the ones you _can_ find, to prevent groupthink reigning supreme.\n\n_Benchmark gaming_\n\nMachine learning expert[\\[1\\]](#fn5i99eney5d) nostalgebraist [recently noted](https://www.lesswrong.com/posts/oC4wv4nTrs2yrP5hz/?commentId=M9zbTWLZKir6nDW6i) that:\n\n> Current frontier models are, in some sense, \"much better at me than coding.\" In a formal coding _competition_ I would obviously lose to these things; I might well perform worse at more \"real-world\" stuff like SWE-Bench Verified, too.\n> \n> Among _humans_ with similar scores on coding and math benchmarks, many (if not all) of them would be better at my job than I am, and fully capable of replacing me as an employee. Yet the models are _not_ capable of this.\n\nThe rest of the comment is worth reading as well. What’s going on here?\n\nWell, in some cases, the AI’s may just be cheating. There is reasonable evidence of [widespread benchmark data contamination](https://web.archive.org/web/20250305194633/https://www.theatlantic.com/technology/archive/2025/03/chatbots-benchmark-tests/681929/). Take a look at this graph of GPT-4 performance on a programming competition, before and after data contamination was possible:\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Sd3X5mWAEzyNaPamT/poazerpkmadl30x1ghya)\n\nGPT-4s performance on a programming competition [from various years](https://web.archive.org/web/20250303191829/https://arxiv.org/abs/2312.02143), with the red line separating when contamination was possible from when it wasn’t.\n\nThis problem is so bad that an insider headline recently claimed that [benchmark tests are meaningless](https://web.archive.org/web/20250320203457/https://www.deeplearning.ai/the-batch/the-problem-with-benchmark-contamination-in-ai/). I wouldn’t quite go that far (and neither does the actual article), as you can test them on [uncontaminated contests](https://matharena.ai/) and questions that haven’t been released to the public. But I would advise treating performance on any benchmark that has it’s answers available on the internet with a garage sized grain of salt.\n\nIn the cases where performance is _not_ just a case of data contamination, there still may be an issue of “[teaching to the test](https://en.wikipedia.org/wiki/Teaching_to_the_test)”. A physics exam only teaches a small fraction of the skills required to be an actually good physicist: those other skills are much more ephemeral and hard to put down in words. By training on benchmarks like exams, even more advanced looking ones, we may be teaching AI to be really good at doing physics _exams_, but not to be really good at doing actual physics.\n\nThis might help explain why LLMs can perform well on some uncontaminated math benchmarks, but are still falling short on uncontaminated [math olympiads](https://matharena.ai/). Note that o3 mini (high), which scored the _best_ on the easier AIME II test, scored the _worst of all_ _models_ on the USA math olympiad[\\[2\\]](#fnx7muaba3st). [Two](https://arxiv.org/pdf/2504.01995) different [papers](https://arxiv.org/pdf/2503.21934) have analyzed these problems in detail, concluding that the results “indicate a reliance on heuristic shortcuts rather than authentic reasoning processes”. They were bad at evaluating the correctness of LLM solutions as well.\n\nYou can actually see the benchmark gaming in the LLM reasoning: in one of the questions failed by the top model gemini pro 2.5, the LLM states that “_All tests seem to suggest that all even k might be solutions. However, this type of problem in contests usually has a unique answer like k=2._”. This is useful knowledge for doing well on a test, but it’s useless for doing real math, and in this case over-learning the patterns of the easier tests seems to have hampered performance on the harder one.\n\nI’m not saying AI is bad at math: in fact, non-LLM systems have [already scored highly](https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/) in the olympiad level math that the LLM’s failed at… which means that these type of problems were entirely possible with mid-2024 levels data and compute, and the LLM’s are falling short. It is likely that we could see this repeat in other domains, especially ones that lack the instant feedback loop and easy verifications of math and coding.\n\n_No imminent intelligence explosion:_\n\nYou’ll note that I don’t deny in my story that AI may get better on several fronts, including coding. So why don’t I think that we’ll get ASI in a few years, as something like [AI2027](https://ai-2027.com/) argues?\n\nWell, because I think the idea of an “intelligence explosion” is wrong, and I don’t think making a very good coder will allow you to bootstrap yourself to superintelligence (although I wish it were that easy). Rather than fight on this topic here, feel free to read this [long list](https://magnusvinding.com/2017/12/16/a-contra-ai-foom-reading-list/) of counterarguments to intelligence explosion arguments, including this [recent series](https://reflectivealtruism.com/2023/12/15/against-the-singularity-hypothesis-part-1-introduction/) by Thorstadt.\n\n_Hallucinations_\n\nThere’s been a lot of discussion of the “[hallucination problem](https://en.wikipedia.org/wiki/Hallucination_\\(artificial_intelligence\\))” in AI, so I’m not discuss it much here. Instead for fun I’ll just show you an exchange I had with state of the art reasoning model gemini pro 2.5, where it gave me in depth comparison of two images of elephants. See if you can spot the hallucination:\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Sd3X5mWAEzyNaPamT/fae2xqkpyio3ypweawnd)\n\nDid you see it? That’s right, I didn’t actually upload any pictures! As far as I can tell this is an entirely made up analysis of two pictures that do not exist. But it sounds very convincing, doesn’t it?\n\nI assume they’ll patch this particular issue soon, but the overall problem with making stuff up remains. We are crafting machines that alternate between perfectly correct answers and delusional bullshit, both delivered in the same confident tone and manner, both presented equally convincingly to any non-expert. This is really bad! There are many situations where a machine that is correct 95% of the time but spouts plausible sounding bullshit the other 5% of the time is really dangerous: see the much delayed rollout of self driving cars.\n\n_AI misdeployment:_\n\nIt’s very easy to find examples of people unthinkingly using genAI to do things it is very clearly not qualified for.\n\nSome examples: A lawyer used chatgpt to [prepare court documents](https://www.bbc.com/news/world-us-canada-65735769), which made shit up and got him in a lot of trouble. A completely different law firm did [the exact same thing](https://www.abajournal.com/news/article/no-42-law-firm-by-headcount-could-face-sanctions-over-fake-case-citations-generated-by-chatgpt). Another lawyer in Canada did [this as well](https://www.theguardian.com/world/2024/feb/29/canada-lawyer-chatgpt-fake-cases-ai). A murder case in Ohio fell apart because it’s key evidence relied on [AI face recognition](https://www.theregister.com/2025/01/28/ai_facial_recognition_murder/) software that clearly stated on _every single page it produced_ that it was not court-admissible. A marketing team for the film Megalopolis used AI to find reviewer quotes for similar historical films, which the AI proceeded to [make up](https://variety.com/2024/film/news/megalopolis-trailer-fake-quotes-ai-lionsgate-1236116485/). A guy claimed to have made their website entirely with AI “[vibe coding](https://pivot-to-ai.com/2025/03/18/guys-im-under-attack-ai-vibe-coding-in-the-wild/)”… the wildly insecure site got hacked immediately. The National Eating Disorders Association announced that it would [replace it’s staff](https://www.psychiatrist.com/news/neda-suspends-ai-chatbot-for-giving-harmful-eating-disorder-advice/) with AI chatbots for people struggling with eating disorders. It proceeded to recommend that they focus on losing weight and measure their body fat with skin folk calipers, and the bot was pulled shortly afterwards.\n\nPeople in general do not have an accurate sense of what an AI can and cannot do, especially in areas where they have little expertise. This is only going to get worse as AI gets better, and if we don’t have our skeptical hats on, disaster might follow.\n\n_AI offloading and critical thinking decline:_\n\nSome studies are finding that the use of AI tools is associated with poor critical thinking skills. I’ll just quote [this recent one](https://www.mdpi.com/2075-4698/15/1/6):\n\n> The key findings indicate that higher usage of AI tools is associated with reduced critical thinking skills, and cognitive offloading plays a significant role in this relationship.\n> \n> Our findings are consistent with previous research indicating that excessive reliance on AI tools can negatively impact critical thinking skills. Firth et al. \\[[**24**](https://www.mdpi.com/2075-4698/15/1/6#B24-societies-15-00006)\\] and Zhang et al. \\[[**43**](https://www.mdpi.com/2075-4698/15/1/6#B43-societies-15-00006)\\] both highlight the potential for AI tools to enhance basic skill acquisition while potentially undermining deeper cognitive engagement. Our study extends this by quantitatively demonstrating that increased AI tool usage correlates with lower critical thinking scores, as measured by comprehensive assessments like the Halpern Critical Thinking Assessment (HCTA) tool\n\nThey quote a subject who states that “The more I use AI, the less I feel the need to problem-solve on my own. It’s like I’m losing my ability to think critically.”\n\nThey found education was an important factor in this: more educated people were willing to cross check AI statements, while less educated people took AI statements at face value. Another psychology paper found that people who were _less_ literate about how AI works were [more likely to want to use AI](https://osf.io/preprints/psyarxiv/t9u8g_v1).\n\nIt’s too early to say for sure how much of a real problem this is, but it seems plausible to me that it could be a big issue, and it’s one that better AI could make worse.\n\n_Overestimating AI job replacibility_\n\nI decided to base the intro to my story around architecture by asking myself “what’s a job that _sounds_ easy to replace with AI, even though it really, really isn’t?” The architecture specific details were taken from [this thread](https://www.reddit.com/r/architecture/comments/12jtexp/architects_what_do_you_actually_do/) about what architects actually do, and [this thread](https://www.reddit.com/r/architecture/comments/17xmxnb/will_ai_replace_architects/) about why they aren’t likely to be replaced by AI’s. Contrary to popular belief, designing floorplans is a very small part of an architects job.\n\nThis is part of a general trend I’ve noticed, where the less you know about a profession, the easier it seems to replace it with AI[\\[3\\]](#fn1q38ta2vc3z). I did a deep dive on why AI can’t do [my job](https://titotal.substack.com/p/ai-is-not-taking-over-material-science), and it’s fairly easy to find similar sentiments from [doctors](https://www.reddit.com/r/doctorsUK/comments/1ioe7de/what_are_your_views_on_how_ai_will_impact/), ,[mathematicians](https://www.reddit.com/r/math/comments/1ahknh0/are_you_concerned_about_potential_job_loss_once/), [software engineer](https://www.reddit.com/r/ArtificialInteligence/comments/1jum6ct/hot_take_ai_wont_replace_that_many_software/)s, [game developers](https://www.reddit.com/r/GameDevelopment/comments/1io2dic/do_you_think_that_game_development_and_game/), etc. Perhaps they are underestimating the advances of AI… or perhaps AI boosters are underestimating the difficulty of jobs they don’t know that much about.\n\nI saw someone who was worried that AI was gonna cause real economic trouble soon by replacing travel agents. But the advent of the internet made travel agents _completely unnecessary_, and it _still_ only wiped out [half the travel agent jobs](https://fred.stlouisfed.org/series/LEU0254497900A). The number of travel agents has stayed roughly the same since 2008! What demographic of customers are you picturing that are simultaneously allergic to using skyscanner, but are _also_ willing to trust their travel plans to an AI over a human [\\[4\\]](#fngct98uyeiq)?\n\n_Enshittification:_\n\nBut I wouldn’t be so sure that just because an AI can’t do a job _well_, that AI workers won’t be forced down our throats anyway.\n\nThe term “enshittification” comes from this [wired article](https://www.wired.com/story/tiktok-platforms-cory-doctorow/) by Cory Doctorow. The premise is that companies get big by providing quality products that people actually want to use. But then they look around, and realize that they have users trapped on their platform, for reasons of social dynamics, habit, sentimental value, monopoly power, etc. So they then proceed to take a hacksaw to everything good about their product, bit by bit, in order to squeeze as much money out of their captive audience as possible. As another example, see the articles on why google search has apparently [massively declined in quality](https://www.wheresyoured.at/the-men-who-killed-google/).\n\nThe story above is a vision of a society where the _entire world_ gets enshittified. Everything gets worse, but humans in general have nowhere else to turn, because the oligarchs have control over all the resources.\n\n_Undercutting workers:_\n\nThis one actually comes from my girlfriend: She works as a freelance translator, and the company has a different pay rate for “proofreading” and “translating”, with the former paying less. Recently, the company has been sending her “proofreading” work which consists of proofreading _AI translations_. She says the translations are quite bad, so the proofreading of the machine translation takes nearly as long as just doing the translation herself, but of course, she gets paid less for it.\n\n_Stealth outsourcing:_\n\nThe “stealth outsourcing” part of the story comes from several real examples of companies falsely pretending a product was fully AI. Cruise robotaxis hey were presented as fully self-driving, but in fact a remote human operator had to intervene and take over control [every 4-5 miles](https://www.cnbc.com/2023/11/06/cruise-confirms-robotaxis-rely-on-human-assistance-every-4-to-5-miles.html). Similarly, Amazons “just walk out” grocery stores, presented as fully automated, turned out to have [70% of their transactions](https://www.theguardian.com/commentisfree/2024/apr/10/amazon-ai-cashier-less-shops-humans-technology) reviewed by remote workers based in India. And just this week, it’s been revealed that an “AI shopping app” was actually being run by [a call centre in the phillipines](https://techcrunch.com/2025/04/10/fintech-founder-charged-with-fraud-after-ai-shopping-app-found-to-be-powered-by-humans-in-the-philippines/).\n\nThis is a simple way get “AI” to do jobs that are fare beyond the actual capabilities of AI on it’s own. Obviously this wouldn’t work for all bazillion queries to chatGPT, but for jobs with slower needs, this practice could prove increasingly popular.\n\n_Excessive AI hype:_\n\nA large part of the premise of this dystopia relies on people buying into bullshit AI hype, so they trust it enough to put it in charge of things even when it’s not actually qualified to do so.\n\nAs an example while back I covered the claim of “[AI superforecasters](https://titotal.substack.com/p/is-superhuman-ai-forecasting-bs-some)”, where the center for AI safety claimed to have built an AI forecaster that performed better than top humans. They parroted this claim boldly, but it seemed to fall apart under closer analysis, and forecasting bots have still not beat top humans in [more structured forecasting competitions](https://www.lesswrong.com/posts/P8YwCvHoF2FHQoHjF/metaculus-q4-ai-benchmarking-bots-are-closing-the-gap). I also discussed the case of Deepminds “[automated discovery tool](https://titotal.substack.com/p/ai-is-not-taking-over-material-science)”, which claimed to discover 43 novel materials but in reality only 3 of the materials were likely the ones they were trying to make (and these weren’t novel).\n\nFuture predictions are getting… very out there. The anthropic CEO stated [in a post](https://darioamodei.com/machines-of-loving-grace) that within _1-3 years_, AI could be created that will be “smarter than a nobel prize winner… prove unsolved mathematical theorems, write extremely good novels, write difficult codebases from scratch”, and “ordering materials, directing experiments, watching videos, making videos… with a skill exceeding that of the most capable humans in the world”. In the same post they suggest that AI will _double the human lifespan_, with transformative achievements in the next 7-12 years.\n\nEven if you (wrongly) think that these projections are reasonable, I want you to imagine the effect if would have on society if these beliefs were widely _**and incorrectly**_ held. Now, I’m sure you, my very smart and rational reader, would _never_ fall for the vastum architect scam in the story, but can you say the same for the general public? Can you honestly tell me you can’t think of any billionaires or senior politicians who would fall for this type of thing?\n\n_Pollution:_\n\nThe energy demand of AI datacenters have been directly responsible for a coal power station continuing operation after it was [previously set to close](https://archive.is/NXe3D). But former google CEO Eric schmidt is on record saying that “infinite energy demand” is fine because [ASI will solve climate change](https://pivot-to-ai.com/2024/10/06/eric-schmidt-were-not-going-to-hit-the-climate-goals-id-rather-bet-on-ai-solving-the-problem/).\n\nIt’s possible that as AI gets more efficient, the energy demand will decrease. But that’s not a guarantee, especially when companies are in an arms race: instead they may just pour the extra compute capacity straight back into the models and stay exactly as energy destructive as before. Again, we have Eric Schmidt declaring that [99% of the worlds energy](https://futurism.com/google-ceo-congress-electricity-ai-superintelligence) will be used for feeding AI, and he states this as a _goal_.\n\n_Easy data scraping:_\n\nCurrent day gen-AI is built on data. Absurd amounts of data, [rifled from the internet](https://archive.ph/FLMMA), in almost all cases [without asking any permission from the person you took it from](https://www.theatlantic.com/technology/archive/2025/03/libgen-meta-openai/682093/). Some companies want to argue that this is fair use transformation, just like reading books for inspiration. Others argue that you shouldn’t be able to scrape someone’s work without their consent in order to flood the internet with modified copies that hurt their livelihood. These issues are subject to [several ongoing lawsuits](https://hbr.org/2023/04/generative-ai-has-an-intellectual-property-problem). Image generators were absolutely trained on copyrighted material and can [spit out copyrighted characters](https://www.reddit.com/r/aifails/comments/1gs5rb8/very_unique_and_original/) without excessive prompting, so it’ll be interested to see how copyright around AI plays out.\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/Sd3X5mWAEzyNaPamT/eyapj8wzi3wftmy8fnlc)\n\nDefinitely no copyright infringement going on here! (exchange on Gemini)\n\nRegardless of what the law finds, governments can also set new laws and policies around AI, either to disincentive scraping or to make it as easy as possible. Sam Altman is arguing for the latter, claiming that they [need copyright exemptions to beat china](https://www.musicbusinessworldwide.com/openai-is-pushing-for-donald-trump-to-usher-in-weakened-copyright-protections-in-the-us-paul-mccartney-is-fighting-back/) in the AI race.\n\n_Algorithmic bias:_\n\n[Algorithmic bias](https://en.wikipedia.org/wiki/Algorithmic_bias) is a large field of study about how automated processes and software sometimes ends up perpetuating harmful biases against groups of people. This can be caused by bias in the training data, a lack of testing on wide variety of groups, or by the evaluation criteria used for training. 100 academicAI researchers recently signed a [letter](https://www.aibiasconsensus.org/) about their AI bias concerns.\n\nAs an example, a 2019 healthcare algorithm was [used to identify](https://www.scientificamerican.com/article/racial-bias-found-in-a-major-health-care-risk-algorithm/) high-risk individuals for special care. A paper in Science found that the algorithm ended up greatly underestimating the sickness of black patients, reducing the number of patients identified for extra care by more than a half compared to equally sick white patients. This arised because it used “healthcare spending” as a proxy for sickness, without accounting for the fact that some people spend less money of healthcare because they are poor.\n\nIn another case, experts found that facial recognition tests were significantly less accurate at identifying the [faces of black people](https://web.archive.org/web/20190722120105/https://www.wired.com/story/best-algorithms-struggle-recognize-black-faces-equally/) compared to white people. The much higher false match rates provide concern about the deployment of facial recognition tools in policing.\n\n_Anti-intellectualism and anti-expertism:_\n\nIn certain corners there is a widespread belief that domain level experts cannot be trusted. We can see this with the widespread vaccine denialism recently: large portions of the population do not trust that life-saving medicine will cure them. In the story, this allows companies to replace actual experts with shitty approximations without public outcry. Anti-education rhetoric is quite common on the right, seen recently with the Trump attacks on the [department of education](https://www.aljazeera.com/news/2025/3/21/why-is-trump-dismantling-the-department-of-education-and-whats-next).\n\nPeople have already starting advocating for AI replacing domain level experts. Mark cuban [stated that](https://bsky.app/profile/mcuban.bsky.social/post/3lifd5rbnkk2k) a person with zero education who can “learn to ask the AI the right questions” will outperform people with _advanced degrees_ in “many fields”. I don’t think this is true, but it’s the type of thing you could _convince_ people is true, so I’m worried.\n\n_AI undermining science:_\n\nThere has been some evidence that some scientists are using unedited LLM responses in their scientific papers. The most amusing example is the paper that included a nonsensical AI-generated image of a [giant rat penis](https://scienceintegritydigest.com/2024/02/15/the-rat-with-the-big-balls-and-enormous-penis-how-frontiers-published-a-paper-with-botched-ai-generated-images/).\n\nThe folks at [retractionwatch](https://retractionwatch.com/) have compiled a [list of about a hundred papers](https://retractionwatch.com/papers-and-peer-reviews-with-evidence-of-chatgpt-writing/) that show clear evidence of copy-pasting from Chatgpt, such as papers containing the phrase “[as an AI language model](https://pubpeer.com/publications/0905A70AF6A77C4309B743FFCD21F8)” . Another study claimed that [up to 17%](https://www.reddit.com/r/MachineLearning/comments/1bnsuea/r_up_to_17_of_recent_ai_conference_peer_reviews/) of recent peer reviews for CS conferences were likely to be AI written. Here is an [anecdotal example](https://www.reddit.com/r/labrats/comments/15g63pz/aigenerated_paper_in_peer_review/) of someone being asked to review a paper with made up AI citations. These methods can’t tell whether the AI was just used for grammar or translation or for more substantive purposes: either way, I highly recommend against this practice, as science relies on precision writing.\n\nThe use of LLMs in science is being advocated by different groups: One editorial in Nature encouraged researchers to use LLM’s as part of the [peer review process](https://archive.is/WMzm8), although they emphasized you should only be using it to clarify and organize your original dot points. However most peer review is already in the form of dot points, so I really don’t get the point here. Another company is trying to sell a cryptocurrency based system for [flagging errors in scientific article](https://archive.is/fqAig)s, which another researcher found had a false positive rate of 14 out of 40 flagged papers. I don’t think this is terrible in principle, if you improve the accuracy and drop the crypto aspect, but I would be concerned at this being used as a replacement for review.\n\nI don’t think LLM’s have zero place in the scientific process, as I explored in [my previous article](https://titotal.substack.com/p/ai-is-not-taking-over-material-science). But there are high chances of misuse, and putting our very notions of truth itself in the whims of chatbots and their programmers could go very, very wrong.\n\n_Ai deepfakes/spam/misinformation_\n\nThere has been widespread concern about the potential impact of [deepfakes](https://en.wikipedia.org/wiki/Deepfake) and AI misinformation on society. Some TikTok users found that AI generated Deepfake doctors were giving out [dodgy medical advice](https://www.mediamatters.org/tiktok/scammers-seem-be-using-deepfake-and-ai-generated-influencers-tiktok-sell-you-wellness). Armies of AI bots have been used to [undermine suppor](https://www.euractiv.com/section/tech/news/fake-climate-news-thriving-as-politics-and-ai-turbocharge-disinformation-crisis/)t for climate action. People are using AI to [steal the voice of ordinary people](https://www.becu.org/blog/voice-cloning-ai-scams-are-on-the-rise#:~:text=How%20Does%20AI%20Voice%20Cloning,billion%20to%20imposter%20scams%20alone.) and use it to scam their loved ones out of money.\n\nWe’re at the point where a carefully prompted AI can [convince the average person they are human after a five minute conversation](https://arxiv.org/abs/2503.23674). The potential for malicious misuse of this technology for scams or propaganda is seriously concerning.\n\n_Oligarchy and democratic decline:_\n\n"
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Quantifying AI in Government Decision-Making",
          "description": "These examples show increasing AI integration in government functions. We can forecast: By 2028, what percentage of U.S. federal agencies will use AI systems as primary decision-making tools for budget allocation or policy implementation? Based on current adoption rates and political trends, I estimate a 40-50% probability that at least 30% of federal agencies will heavily rely on AI for significant decisions by 2028, with wide variation depending on administration priorities.",
          "importance": "80",
          "highlight": {
            "startOffset": 53673,
            "endOffset": 54236,
            "quotedText": "or AI hallucinations, could end up responsible for actual government actions, up to and including the militaries decisions on who to shoot.\n\nWhen the UK government asks for public consultation, they will no longer read the actual words that people submit: instead they will get an [AI to read through them all and summarise it](https://ai.gov.uk/projects/consult/). The UK government also heavily used AI chatbots to generate diagrams and citations for a report on the impact of AI on the labour market, [some of which were hallucinated](http://The Impact of AI o"
          },
          "isValid": true
        }
      ]
    }
  ]
}