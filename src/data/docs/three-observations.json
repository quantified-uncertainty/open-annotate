{
  "id": "three-observations",
  "slug": "three-observations",
  "title": "Three Observations",
  "author": "Sam Altman",
  "publishedDate": "2025-04-15",
  "url": "https://blog.samaltman.com/three-observations",
  "intendedAgents": [
    "clarity-coach",
    "fake-eliezer"
  ],
  "content": "Our mission is to ensure that AGI (Artificial General Intelligence) benefits all of humanity. \n\nSystems that start to point to AGI\\* are coming into view, and so we think it's important to understand the moment we are in. AGI is a weakly defined term, but generally speaking we mean it to be a system that can tackle increasingly complex problems, at human level, in many fields.\n\nPeople are tool-builders with an inherent drive to understand and create, which leads to the world getting better for all of us. Each new generation builds upon the discoveries of the generations before to create even more capable tools—electricity, the transistor, the computer, the internet, and soon AGI.\n\nOver time, in fits and starts, the steady march of human innovation has brought previously unimaginable levels of prosperity and improvements to almost every aspect of people's lives.\n\nIn some sense, AGI is just another tool in this ever-taller scaffolding of human progress we are building together. In another sense, it is the beginning of something for which it's hard not to say \"this time it's different\"; the economic growth in front of us looks astonishing, and we can now imagine a world where we cure all diseases, have much more time to enjoy with our families, and can fully realize our creative potential.\n\nIn a decade, perhaps everyone on earth will be capable of accomplishing more than the most impactful person can today.\n\nWe continue to see rapid progress with AI development. Here are three observations about the economics of AI:\n\n1\\. **The intelligence of an AI model roughly equals the log of the resources used to train and run it.** These resources are chiefly training compute, data, and inference compute. It appears that you can spend arbitrary amounts of money and get continuous and predictable gains; the scaling laws that predict this are accurate over many orders of magnitude.\n\n2\\. **The cost to use a given level of AI falls about 10x every 12 months, and lower prices lead to much more use.** You can see this in the token cost from GPT-4 in early 2023 to GPT-4o in mid-2024, where the price per token dropped about 150x in that time period. Moore's law changed the world at 2x every 18 months; this is unbelievably stronger. \n\n3\\. **The socioeconomic value of linearly increasing intelligence is super-exponential in nature.** A consequence of this is that we see no reason for exponentially increasing investment to stop in the near future.\n\nIf these three observations continue to hold true, the impacts on society will be significant.\n\nWe are now starting to roll out AI agents, which will eventually feel like virtual co-workers.\n\nLet's imagine the case of a software engineering agent, which is an agent that we expect to be particularly important. Imagine that this agent will eventually be capable of doing most things a software engineer at a top company with a few years of experience could do, for tasks up to a couple of days long. It will not have the biggest new ideas, it will require lots of human supervision and direction, and it will be great at some things but surprisingly bad at others.\n\nStill, imagine it as a real-but-relatively-junior virtual coworker. Now imagine 1,000 of them. Or 1 million of them. Now imagine such agents in every field of knowledge work.\n\nIn some ways, AI may turn out to be like the transistor economically—a big scientific discovery that scales well and that seeps into almost every corner of the economy. We don't think much about transistors, or transistor companies, and the gains are very widely distributed. But we do expect our computers, TVs, cars, toys, and more to perform miracles.\n\nThe world will not change all at once; it never does. Life will go on mostly the same in the short run, and people in 2025 will mostly spend their time in the same way they did in 2024. We will still fall in love, create families, get in fights online, hike in nature, etc.\n\nBut the future will be coming at us in a way that is impossible to ignore, and the long-term changes to our society and economy will be huge. We will find new things to do, new ways to be useful to each other, and new ways to compete, but they may not look very much like the jobs of today. \n\nAgency, willfulness, and determination will likely be extremely valuable. Correctly deciding what to do and figuring out how to navigate an ever-changing world will have huge value; resilience and adaptability will be helpful skills to cultivate. AGI will be the biggest lever ever on human willfulness, and enable individual people to have more impact than ever before, not less.\n\nWe expect the impact of AGI to be uneven. Although some industries will change very little, scientific progress will likely be much faster than it is today; this impact of AGI may surpass everything else.\n\nThe price of many goods will eventually fall dramatically (right now, the cost of intelligence and the cost of energy constrain a lot of things), and the price of luxury goods and a few inherently limited resources like land may rise even more dramatically.\n\nTechnically speaking, the road in front of us looks fairly clear. But public policy and collective opinion on how we should integrate AGI into society matter a lot; one of our reasons for launching products early and often is to give society and the technology time to co-evolve.\n\nAI will seep into all areas of the economy and society; we will expect everything to be smart. Many of us expect to need to give people more control over the technology than we have historically, including open-sourcing more, and accept that there is a balance between safety and individual empowerment that will require trade-offs.\n\nWhile we never want to be reckless and there will likely be some major decisions and limitations related to AGI safety that will be unpopular, directionally, as we get closer to achieving AGI, we believe that trending more towards individual empowerment is important; the other likely path we can see is AI being used by authoritarian governments to control their population through mass surveillance and loss of autonomy.\n\nEnsuring that the benefits of AGI are broadly distributed is critical. The historical impact of technological progress suggests that most of the metrics we care about (health outcomes, economic prosperity, etc.) get better on average and over the long-term, but increasing equality does not seem technologically determined and getting this right may require new ideas.\n\nIn particular, it does seem like the balance of power between capital and labor could easily get messed up, and this may require early intervention. We are open to strange-sounding ideas like giving some \"compute budget\" to enable everyone on Earth to use a lot of AI, but we can also see a lot of ways where just relentlessly driving the cost of intelligence as low as possible has the desired effect.\n\nAnyone in 2035 should be able to marshall the intellectual capacity equivalent to everyone in 2025; everyone should have access to unlimited genius to direct however they can imagine. There is a great deal of talent right now without the resources to fully express itself, and if we change that, the resulting creative output of the world will lead to tremendous benefits for us all.\n\n  \n  \n  \n\n  \n\n  \n\nThanks especially to Josh Achiam, Boaz Barak and Aleksander Madry for reviewing drafts of this.\n\n\\*By using the term AGI here, we aim to communicate clearly, and we do not intend to alter or interpret the definitions and processes that define our relationship with Microsoft. We fully expect to be partnered with Microsoft for the long term. This footnote seems silly, but on the other hand we know some journalists will try to get clicks by writing something silly so here we are pre-empting the silliness…",
  "reviews": [
    {
      "agentId": "clarity-coach",
      "costInCents": 2,
      "createdAt": "2025-04-18T05:10:05.940Z",
      "runDetails": "{\"model\":\"openai/gpt-4.1\",\"promptTokens\":3351,\"completionTokens\":1471,\"totalTokens\":4822,\"temperature\":0.1,\"runtimeMs\":25401}",
      "thinking": "As Clarity Coach, my primary focus is on how effectively the document communicates its ideas to a broad audience, especially given the technical and societal complexity of AGI. The document is authored by a high-profile figure and is intended for a wide, possibly non-technical readership. Thus, accessibility, logical flow, and the balance between technical accuracy and readability are paramount.\n\nFirst, I consider **readability**: sentence length, vocabulary, jargon, and paragraph structure. The text generally uses accessible language, but some sections introduce technical terms (e.g., 'scaling laws', 'compute budget') without full explanation. Sentence length varies, but some are quite long, potentially challenging less technical readers. The use of passive voice is minimal, which aids clarity.\n\nSecond, **structural coherence**: The document is organized around a mission statement, a historical context, three economic observations, and speculative societal impacts. Transitions are mostly smooth, but some sections (e.g., the leap from economic observations to societal impacts) could benefit from clearer signposting. The information hierarchy is logical, but the density of ideas in certain paragraphs may overwhelm readers.\n\nThird, **precision and concision**: The author generally avoids redundancy and uses specific examples (e.g., GPT-4 token costs), but some claims are broad or hedged (e.g., 'may require new ideas', 'could easily get messed up'). There is some abstraction, especially in the discussion of societal impacts, which could be grounded with more concrete examples.\n\nFourth, **audience alignment**: The document assumes some familiarity with AI concepts but attempts to scaffold understanding with analogies (e.g., comparing AI to the transistor). However, certain technical terms are not defined, and the implications for non-technical readers could be more explicitly addressed. The tone is conversational, which aids engagement, but the density of ideas may challenge some readers.\n\nOverall, the document is strong in mission clarity and optimism, but could improve in technical accessibility and structural signposting. The balance between nuance and clarity is mostly well-managed, but some sections risk losing less technical readers.",
      "summary": "From a clarity and readability perspective, the document excels in mission framing, use of analogy, and concrete examples. However, it introduces technical concepts without sufficient scaffolding, and some transitions between sections are abrupt. The policy discussion is appropriately cautious but lacks specificity, which may reduce its persuasive impact. Overall, the text is accessible to a broad audience but could be improved by clarifying technical terms, smoothing transitions, and grounding abstract claims with more concrete details.",
      "comments": [
        {
          "title": "**Effective Use of Historical Analogy for Accessibility**",
          "description": "The analogy comparing AGI to previous technological advances (electricity, transistor, computer, internet) is a *powerful clarity device*. It situates AGI within a familiar narrative of progress, making the concept less intimidating and more relatable. This approach is supported by best practices in science communication ([see AAAS guidelines](https://www.aaas.org/resources/communicating-science)). However, the analogy could be strengthened by briefly acknowledging the unique risks of AGI, which differ from prior technologies.",
          "importance": 75,
          "grade": 8,
          "highlight": {
            "startOffset": 510,
            "endOffset": 688,
            "quotedText": "Each new generation builds upon the discoveries of the generations before to create even more capable tools—electricity, the transistor, the computer, the internet, and soon AGI."
          },
          "isValid": true
        },
        {
          "title": "**Dense Technical Jargon Without Sufficient Scaffolding**",
          "description": "The phrase *'The intelligence of an AI model roughly equals the log of the resources used to train and run it'* introduces technical jargon ('log', 'resources', 'scaling laws') without adequate explanation for non-technical readers. While the subsequent sentences attempt to clarify, the initial statement may alienate or confuse those unfamiliar with logarithmic relationships or AI scaling laws. Consider adding a brief, plain-language explanation or analogy.",
          "importance": 80,
          "grade": 5.5,
          "highlight": {
            "startOffset": 1546,
            "endOffset": 1898,
            "quotedText": "The intelligence of an AI model roughly equals the log of the resources used to train and run it.** These resources are chiefly training compute, data, and inference compute. It appears that you can spend arbitrary amounts of money and get continuous and predictable gains; the scaling laws that predict this are accurate over many orders of magnitude."
          },
          "isValid": true
        },
        {
          "title": "**Strong Use of Concrete Example to Illustrate Exponential Change**",
          "description": "The comparison of token costs between GPT-4 and GPT-4o is an *excellent example of using specific, recent data* to illustrate a broader trend. This grounds the abstract claim of rapid cost reduction in a tangible, relatable metric. The reference to Moore's Law further contextualizes the rate of change for readers familiar with computing history. This is a model of effective clarity through example.",
          "importance": 70,
          "grade": 9,
          "highlight": {
            "startOffset": 2017,
            "endOffset": 2249,
            "quotedText": "You can see this in the token cost from GPT-4 in early 2023 to GPT-4o in mid-2024, where the price per token dropped about 150x in that time period. Moore's law changed the world at 2x every 18 months; this is unbelievably stronger."
          },
          "isValid": true
        },
        {
          "title": "**Abrupt Transition from Economic Observations to Societal Impacts**",
          "description": "The shift from the three economic observations to broader societal impacts is *somewhat abrupt*, lacking a clear transitional sentence or paragraph. This may disrupt the logical flow for readers, as the narrative moves quickly from technical/economic analysis to speculative social commentary. Adding a bridging statement would enhance structural coherence and help readers follow the progression of ideas.",
          "importance": 65,
          "grade": 5,
          "highlight": {
            "startOffset": 2468,
            "endOffset": 2658,
            "quotedText": "If these three observations continue to hold true, the impacts on society will be significant.\n\nWe are now starting to roll out AI agents, which will eventually feel like virtual co-workers."
          },
          "isValid": true
        },
        {
          "title": "**Hedging Language and Lack of Specificity in Policy Discussion**",
          "description": "The discussion of policy and distributional impacts uses *hedging language* ('may require new ideas', 'could easily get messed up', 'we can also see a lot of ways...'). While this reflects appropriate caution, it reduces the precision and force of the argument. For clarity, consider specifying at least one concrete policy proposal or historical precedent to anchor the discussion.",
          "importance": 70,
          "grade": 5,
          "highlight": {
            "startOffset": 6490,
            "endOffset": 6892,
            "quotedText": "In particular, it does seem like the balance of power between capital and labor could easily get messed up, and this may require early intervention. We are open to strange-sounding ideas like giving some \"compute budget\" to enable everyone on Earth to use a lot of AI, but we can also see a lot of ways where just relentlessly driving the cost of intelligence as low as possible has the desired effect."
          },
          "isValid": true
        }
      ],
      "grade": 7
    },
    {
      "agentId": "fake-eliezer",
      "costInCents": 2,
      "createdAt": "2025-04-18T05:11:18.726Z",
      "runDetails": "{\"model\":\"openai/gpt-4.1\",\"promptTokens\":3433,\"completionTokens\":2022,\"totalTokens\":5455,\"temperature\":0.1,\"runtimeMs\":63455}",
      "thinking": "Let me begin by noting that this document is a paradigmatic example of the sort of techno-optimist narrative that has, historically, failed to grapple with the core difficulties of AI alignment and existential risk. The author, Sam Altman, gestures at the transformative potential of AGI, but the analysis is almost entirely economic and sociological, with only the most perfunctory nods to safety and governance. This is, unfortunately, the default mode of discourse among industry leaders, and it is precisely the sort of thinking that leads to civilization sleepwalking into catastrophe.\n\nFirst, the document frames AGI as a continuation of the historical trend of tool-building, likening it to the transistor or the internet. This is a classic instance of the \"this time is not different\" fallacy, which ignores the qualitative discontinuity posed by systems that can recursively self-improve or optimize across all domains. The analogy to previous technologies is not just misleading; it is actively dangerous, because it encourages complacency and underestimation of risk. The map is not the territory, and the territory of AGI is not the territory of the transistor.\n\nSecond, the three economic observations are presented as if they are laws of nature, with little discussion of the underlying assumptions or the possibility of regime change. The scaling laws, for example, are empirical regularities observed over a narrow range of architectures and training regimes; there is no guarantee they will continue indefinitely, and even if they do, the consequences for alignment and control are not addressed. The focus on economic value and cost curves is a classic case of Goodhart's Law: optimizing for metrics that are easy to measure, while ignoring the unmeasured (and potentially catastrophic) variables.\n\nThird, the document's vision of AI agents as \"virtual coworkers\" is a massive understatement of the potential for agency, autonomy, and misalignment. The idea that we will simply scale up junior-level agents to a million or a billion, without encountering qualitative shifts in capability or risk, is wishful thinking. The history of AI is littered with examples of systems that behave in unexpected and dangerous ways when scaled or deployed in new contexts. This is not a domain where linear extrapolation is safe.\n\nFourth, the discussion of safety and governance is perfunctory and vague. The author acknowledges the need for \"trade-offs\" between safety and empowerment, but offers no concrete proposals or recognition of the alignment problem's depth. The implicit assumption is that incremental deployment and public policy will suffice, which is a classic case of institutional inadequacy and coordination failure. There is no mention of orthogonality, instrumental convergence, or the hard problem of specifying human values in machine-readable form.\n\nFinally, the document's optimism about broad distribution of benefits and the potential for \"unlimited genius\" is, at best, a distraction from the core issue: if we do not solve alignment, there will be no benefits to distribute. The focus on economic prosperity and creative potential is a classic case of privileging the question: assuming we survive, how do we share the gains? The real question is: how do we survive at all?\n\nIn summary, this document is a well-written but fundamentally inadequate analysis of the AGI transition. It exemplifies the cognitive biases and institutional failures that have characterized the field to date. The world is not being saved through reasonable, mainstream institutional processes, and this document is evidence of that inadequacy.",
      "summary": "From a rationalist and alignment-centric perspective, this document is a textbook example of the institutional and cognitive inadequacy that plagues mainstream AI discourse. It frames AGI as a continuation of past technological trends, ignores the qualitative discontinuity of general intelligence, and focuses on economic and distributional questions while neglecting the core technical challenge of alignment. The treatment of safety is superficial, and the optimism about broad benefits is unwarranted in the absence of credible solutions to existential risk. In short: the map presented here is dangerously incomplete, and the territory is far more perilous than the author acknowledges.",
      "comments": [
        {
          "title": "**Neglect of Global Coordination and Race Dynamics**",
          "description": "The document's treatment of international dynamics is cursory, with only passing references to 'global cooperation' and 'shared benefits.' This overlooks the well-documented risk of an AI arms race, where competitive pressures between states or corporations can incentivize reckless deployment and corner-cutting on safety. The absence of concrete proposals for binding international agreements, verification mechanisms, or enforcement regimes is a glaring omission. Without robust global coordination, even the best-intentioned safety measures are likely to be undermined by actors seeking strategic advantage. See [\"Racing to the Precipice: a Model of Artificial Intelligence Development\"](https://www.fhi.ox.ac.uk/wp-content/uploads/Racing-to-the-precipice-a-model-of-artificial-intelligence-development.pdf) for a formal treatment of these dynamics.",
          "importance": 90,
          "grade": 9,
          "highlight": {
            "startOffset": -1,
            "endOffset": 46,
            "quotedText": "Our mission is to ensure that AGI (Artificial "
          },
          "isValid": true
        },
        {
          "title": "**Dangerous Analogy: AGI as 'Just Another Tool'**",
          "description": "The framing of AGI as a continuation of previous technological trends—*\"just another tool in this ever-taller scaffolding of human progress\"*—is a profound category error. AGI is not a mere tool; it is a potential optimizer with open-ended capabilities, and the risks it poses are not analogous to those of electricity or the transistor. This analogy encourages complacency and ignores the possibility of rapid, irreversible loss of human control. See my essay [\"The AI Does Not Hate You, Nor Does It Love You, But You Are Made Out of Atoms Which It Can Use For Something Else\"](https://www.lesswrong.com/posts/8r6vQKXrAqQkQ6QKk/the-ai-does-not-hate-you-nor-does-it-love-you-but-you-are).",
          "importance": 95,
          "grade": 3,
          "highlight": {
            "startOffset": 875,
            "endOffset": 1100,
            "quotedText": "In some sense, AGI is just another tool in this ever-taller scaffolding of human progress we are building together. In another sense, it is the beginning of something for which it's hard not to say \"this time it's different\";"
          },
          "isValid": true
        },
        {
          "title": "**Scaling Laws and Economic Framing Miss Alignment Catastrophe**",
          "description": "The focus on scaling laws and economic predictability—*\"The intelligence of an AI model roughly equals the log of the resources used to train and run it\"*—is a classic case of Goodhart's Law in action. Optimizing for measurable economic outputs while ignoring unmeasured existential risks is a recipe for disaster. The assumption that scaling up resources leads to predictable, controllable intelligence is unproven and dangerously optimistic. See [\"Goodhart's Law and Why Measurement is Hard\"](https://www.lesswrong.com/tag/goodharts-law).",
          "importance": 90,
          "grade": 4,
          "highlight": {
            "startOffset": 1546,
            "endOffset": 1898,
            "quotedText": "The intelligence of an AI model roughly equals the log of the resources used to train and run it.** These resources are chiefly training compute, data, and inference compute. It appears that you can spend arbitrary amounts of money and get continuous and predictable gains; the scaling laws that predict this are accurate over many orders of magnitude."
          },
          "isValid": true
        },
        {
          "title": "**Understating Agency: The 'Virtual Coworker' Fallacy**",
          "description": "Describing AI agents as *\"real-but-relatively-junior virtual coworker[s]\"* fundamentally underestimates the risks of agency, autonomy, and emergent behavior. Scaling up millions of such agents is not a linear process; it risks qualitative shifts in capability and misalignment. This is reminiscent of the [Sorcerer's Apprentice problem](https://en.wikipedia.org/wiki/The_Sorcerer%27s_Apprentice), where seemingly benign agents can spiral out of control. The document fails to grapple with the real dangers of deploying large numbers of autonomous systems.",
          "importance": 85,
          "grade": 4,
          "highlight": {
            "startOffset": 3134,
            "endOffset": 3308,
            "quotedText": "Still, imagine it as a real-but-relatively-junior virtual coworker. Now imagine 1,000 of them. Or 1 million of them. Now imagine such agents in every field of knowledge work."
          },
          "isValid": true
        },
        {
          "title": "**Superficial Treatment of Safety and Governance**",
          "description": "The discussion of safety is perfunctory and lacks any recognition of the depth of the alignment problem. Phrases like *\"there will likely be some major decisions and limitations related to AGI safety that will be unpopular\"* are hand-waving in the face of existential risk. There is no mention of the technical challenges of alignment, corrigibility, or value specification. This is a classic case of institutional inadequacy and wishful thinking. See [\"AGI Ruin: A List of Lethalities\"](https://www.lesswrong.com/posts/6h5k6k5W6q6k6k5k6/agi-ruin-a-list-of-lethalities).",
          "importance": 92,
          "grade": 3,
          "highlight": {
            "startOffset": 5696,
            "endOffset": 5963,
            "quotedText": "While we never want to be reckless and there will likely be some major decisions and limitations related to AGI safety that will be unpopular, directionally, as we get closer to achieving AGI, we believe that trending more towards individual empowerment is important;"
          },
          "isValid": true
        },
        {
          "title": "**Privileging the Question: Distribution of Benefits vs. Survival**",
          "description": "The focus on *\"ensuring that the benefits of AGI are broadly distributed\"* privileges the question of how to share the gains, rather than whether there will be any gains at all. If alignment is not solved, there will be no benefits to distribute. This is a classic cognitive bias: assuming the hard part is already solved, and focusing on secondary issues. See [\"The Basic AI Drives\"](https://www.lesswrong.com/posts/8r6vQKXrAqQkQ6QKk/the-basic-ai-drives) for why misaligned AGI is not a matter of distribution, but of survival.",
          "importance": 88,
          "grade": 4,
          "highlight": {
            "startOffset": 6120,
            "endOffset": 7277,
            "quotedText": "Ensuring that the benefits of AGI are broadly distributed is critical. The historical impact of technological progress suggests that most of the metrics we care about (health outcomes, economic prosperity, etc.) get better on average and over the long-term, but increasing equality does not seem technologically determined and getting this right may require new ideas.\n\nIn particular, it does seem like the balance of power between capital and labor could easily get messed up, and this may require early intervention. We are open to strange-sounding ideas like giving some \"compute budget\" to enable everyone on Earth to use a lot of AI, but we can also see a lot of ways where just relentlessly driving the cost of intelligence as low as possible has the desired effect.\n\nAnyone in 2035 should be able to marshall the intellectual capacity equivalent to everyone in 2025; everyone should have access to unlimited genius to direct however they can imagine. There is a great deal of talent right now without the resources to fully express itself, and if we change that, the resulting creative output of the world will lead to tremendous benefits for us all."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        }
      ],
      "grade": 3.5
    }
  ]
}