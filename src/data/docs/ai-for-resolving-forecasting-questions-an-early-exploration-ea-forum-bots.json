{
  "id": "ai-for-resolving-forecasting-questions-an-early-exploration-ea-forum-bots",
  "slug": "ai-for-resolving-forecasting-questions-an-early-exploration-ea-forum-bots",
  "title": "AI for Resolving Forecasting Questions: An Early Exploration — EA Forum Bots",
  "author": "Ozzie Gooen",
  "publishedDate": "2025-01-16T21:40:02.752Z",
  "url": "https://forum.effectivealtruism.org/posts/pLkFmWP3xmGG8YvjP/ai-for-resolving-forecasting-questions-an-early-exploration",
  "intendedAgents": [
    "bias-detector",
    "clarity-coach",
    "research-scholar",
    "fake-eliezer",
    "quantitative-forecaster"
  ],
  "content": "_Thanks to Slava Matyuhin for comments_\n\n## Summary\n\n1.  AIs can be used to resolve forecasting questions on platforms like Manifold and Metaculus.\n    \n2.  AI question resolution, in theory, can be far more predictable, accessible, and inexpensive to human resolution.\n    \n3.  Current AI tools (combinations of LLM calls and software) are mediocre at judging subjective or speculative questions. We expect them to improve. \n    \n4.  AI tools are changing rapidly. If we want to organize forecasting questions to be resolved using an AI tool in 10 years, instead of selecting a specific tool, we might want to select a _protocol_ for choosing the best tool at resolution time. We call this an \"_Epistemic Selection Protocol_\". \n    \n5.  If we have a strong \"_Epistemic Selection Protocol_\" that we like, and expect that we'll have _sufficient_ AI tools in the near future, then we could arguably start to write forecasting questions for them right away. \n    \n6.  The methods listed above, if successful, wouldn't just be for resolving questions on prediction platforms. They could be generalized to pursue broad-scale and diverse evaluations. \n    \n7.  We provide a “_A Short Story of How This Plays Out\",_ which is perhaps the most accessible part of this for most readers.  \n    \n8.  We list a bunch of potential complications with AI tools and Epistemic Selection Protocols. While these will be complex, we see no fundamental bottlenecks. \n    \n\n## Motivation\n\nSay we want to create a prediction tournament on a complex question.\n\nFor example:\n\n*   _“Will there be over 10 million people killed in global wars between 2025 and 2030?”_\n*   _“Will IT security incidents be a national US concern, on the scale of the top 5 concerns, in 2030?”_\n*   _“Will bottlenecks in power capacity limit AIs by over 50%, in 2030?”_\n\nWhile these questions involve subjective elements, they aren't purely matters of opinion. Come 2030, though people might debate the details, informed observers would likely reach broadly similar conclusions.\n\nCurrently, such questions are typically resolved by human evaluators. On [Manifold](https://manifold.markets/home), the question author serves as judge, with authors developing reputations for their judgment quality over time. [Metaculus](https://www.metaculus.com) takes a different approach for more complex and subjective questions, often employing small panels of subject matter experts.\n\nHowever, human evaluators have several limitations:\n\n1.  High costs, particularly for domain experts\n2.  Limited track records of evaluation quality\n3.  Poor accessibility - most forecasters cannot directly query them\n4.  Inconsistency over time as their views and motivations evolve\n5.  Uncertain long-term availability\n6.  Vulnerability to biases and potential conflicts of interest\n\n## AIs as Question Evaluators\n\nInstead of using humans to resolve thorny questions, we can use AIs. There are many ways we could attempt to do this, so we’ll walk through a few examples.\n\n**Option 1: Use an LLM**\n\nThe first option to consider is to use an LLM as an evaluator. For example, write,  \n“_This question will be judged by Claude 3.5 Sonnet (2024-06-20). The specific prompt will be, …”_\n\nThis style is replicable, simple, and inexpensive. However, it clearly has some downsides. The first obvious one is that Claude 3.5 Sonnet doesn’t perform web searches, so its knowledge would likely be too limited to resolve future forecasting questions.\n\n**Option 2: Use an AI Tool with Search**\n\nInstead of using a standard LLM, you might want to use a tool that uses both LLMs and web searches. [Perplexity](https://www.perplexity.ai/) might be the most famous one now, but other advanced research assistants are starting to come out. In theory one should be able to set a research budget that’s in line with the importance and complexity of the question.\n\nThere's been some experimentation here. See [this guide](https://predictionmarkets.miraheze.org/wiki/GPT4_Headline_Markets) on using GPT4 for resolving questions on Manifold, and [this](https://predictionmarkets.miraheze.org/wiki/GPT4_Manifold_Experiment) experiment write up. GPT4 allows for basic web searches.\n\nThis option is probably better than Option 1 for most things. But there are still problems. The next major one is the risk that Perplexity, or any other single tool we can point to now, won’t be the leading one in the future. The field is moving rapidly, it’s difficult to tell which tools will even exist in 5 years, let alone be the preferred options.\n\n**Option 3: Use an “**_**Epistemic**_**” Selection Protocol**\n\nIn this case, one doesn't select a specific AI tool. Instead one selects a process or protocol that selects an AI tool.\n\nFor example:  \n_“In 2030, we will resolve this question using the leading AI tool on the ‘Forbes 2030 Most trusted AI tools’ list.”_\n\nWe’re looking for AI tools that are “_trusted to reason about complex, often speculative or political matters._” This arguably can be more quickly expressed as searching for the tool with the best _epistemics_.\n\n## _Epistemic Selection Protocols_ (Or, how do we choose the best AI tool to use?)\n\nArguably, AI Epistemic Selection Protocols can be the best choice of the above options, if one could implement them effectively, for most 2+ year questions. There are a lot of potential processes to choose, though most would be too complicated to be worthwhile. We want to strike a balance between simplicity and accuracy.\n\nLet’s first list the most obvious options.\n\n**Option 1: Trusted and formalized epistemic evaluations**\n\nThere’s currently a wide variety of AI benchmarks. For example, [TruthfulQA](https://github.com/sylinrl/TruthfulQA) and various investigations of model sycophancy. But arguably, none of these would be great proxies for which AI tool would be the most trusted question resolvers in the future. Newer, deliberate benchmarks could help here.\n\nExample:  \n_“This forecasting question will be resolved, using whichever AI Tool does the best on Complete Epistemic Benchmark X, and can be used for less than $20.”_\n\n**Option 2: Human-derived trust rankings**\n\nHumans could simply be polled on which AI tools they regard as the most trustworthy. Different groups of humans would have different preferences, so the group would need to be specified in advance for an AI Selection Process.\n\nExample:  \n_“This forecasting question will be resolved, using whichever AI Tool is on the top of the list of ‘Most trusted AI Tools’ on LessWrong, and can be used for less than $20.”_\n\n**Option 3: Inter-AI trust ratings**\n\nAI tools could select future AI tools to use. This could be a 1-step solution, where an open-source or standardized (for the sake of ensuring it will be available long-term) solution is asked to identify the best available candidate. Or it could be a multiple-step solution, where perhaps AI tools are asked to recommend each other using some simple algorithm. This can be similar in concept to the [Community Notes](https://vitalik.eth.limo/general/2023/08/16/communitynotes.html) algorithm.\n\nExample:  \n_“This forecasting question will be resolved, using whichever AI Tool wins a poll of the ‘Most trusted AI tools’ according to AI tools.’ In this poll, each AI tool will recommend its favorite of the other available candidates.”_ (Note: This specific proposal can be gamed, so greater complexity will likely be required.)\n\n## A Short Story of How This Plays Out\n\nIn 2025, several question writers on Manifold experiment with AI resolution systems. Some questions include:  \n  \n_“Will California fires in 2026 be worse than those in 2025? To answer this, I’ll ask Perplexity on Jan 1, 2026. My prompt will be, \\[Will California fires in 2026 be worse than those in 2025? Judge this by guessing the total economic loss.\\]”_\n\n_“How many employees will OpenAI have in Dec 2025? To answer this, I’ll first ask commenters to write arguments and/or facts that they’ve found on this. I’ll filter this for what seems accurate, then I’ll paste this into Perplexity. I’ll call Perplexity 5 times, and average the results.”_\n\nForecasters gradually identify the uses and limitations of such systems. It turns out they are surprisingly bad at advanced physics questions, for some surprising reason. There are a few clever prompting strategies that help ensure that these AIs put out more consistent results.\n\nAI tools like Perplexity also get very good at hunting down and answering questions that are straightforward to resolve. Manifold adds custom functionality to do this. For example, say someone writes a question, _“What Movie Will Win The 2025 Oscars For Best Picture?”_ When they do, they’ll be given the option to have a Manifold AI system automatically make a suggested guess for them, at the time of expected question resolution. These guesses will begin with high error rates (10%), but these will gradually drop.\n\nSeparately, various epistemic evaluations are established. There are multiple public and private rankings. There are also surveys of the “_Most Trusted AIs_”, held on various platforms such as Manifold, LessWrong, and The Verge. Leading consumer product review websites such as Consumer Reports and Wirecutter begin to have ratings for AI tools, using defined categories such as “_accuracy_” and “_reasonableness_.”\n\nOne example question from this is:  \n_“In 2030, will it seem like o1 was an important AI development, that was at least as innovative and important as GPT4? This will be resolved using whichever AI leads the “Most trusted AIs” poll on Manifold in 2029.”_\n\nThere will be a long tail of AI tools that are proposed as contenders for epistemic benchmarks. Most of the options are simply minor tweaks on other options or light routers. Few of these will get the full standard evaluations, but good proxies will emerge. It turns out that you can get a decent measure by using the top fully-evaluated AI systems to evaluate more niche systems.\n\nIn 2027, there will be a significant amount of understanding, buy-in, and sophistication with such systems (at least among a few niche communities, like Manifold users). This will make it possible to scale them for more ambitious uses.\n\nMetaculus runs some competitions that include:\n\n_“What is the_ [_relative value_](https://forum.effectivealtruism.org/posts/EFEwBvuDrTLDndqCt/relative-value-functions-a-flexible-new-format-for-value) _of each of \\[the top 500 AI safety papers of 2026\\]? This will be resolved in 2030 by using the most trusted AI system, via LessWrong or The Economist, at that time. This AI will order all of the papers - forecasters should estimate the percentile that each paper will achieve.”_\n\n_“What is the expected value of every biosafety organization in 2027, estimated as what Open Philanthropy would have paid for it from their biosafety funding pool in 2027? This will be judged in 2029, by the most trusted AI system, with a budget of $1,000 for each evaluation.”_\n\nAround this time, some researchers will begin to make wider kinds of analyses, and forecast compressions.\n\n_“How will the top epistemic model of 2030 evaluate the accuracy and value of the claims of each of the top 100 intellectuals from 2027?”_\n\n_“Will the top epistemic model of 2030 consider the current top epistemic models to be ‘highly overconfident’ for at least 10% of the normative questions they are asked?”_\n\nThe top trusted AI tools start to become frequent ways to second-guess humans. For example, if a boss makes a controversial decision, people could contest the decision if top AI tools back them up. Similar analyses would be used within governments.\n\nAs these AI tools become even more trusted, they will replace many humans for important analyses and decisions. Humans will spend a great deal of effort focused on assuring these AI tools are doing a good job, and they'll have the time for that because there will be few other things they need to directly evaluate or oversee. \n\n## Protocol Complications & Potential Solutions\n\n**Complication 1: Lack of Sufficient AI Tools**\n\nIn the beginning, we expect that many people won’t trust any AI tools to be adequate in resolving many questions. Even if tools look good in evaluations, it will take time for them to build trust.\n\nOne option is to set certain criteria for _sufficiency_. For example, one might say, “This question will be resolved using whichever AI system first gets to a 90/100 on the Epistemic Benchmark Evaluation…” This would clearly require understanding and trust in the evaluations, rather than in a specific tool, so this would require high-quality evaluations or polls.\n\n**Complication 2: Lack of Ground Truth**\n\nMany subjective and speculative questions lack definitive answers. Some questions can only be answered long after the information would be useful, while others are inherently impossible to resolve with complete certainty.\n\nIn such cases, the goal should shift from seeking perfect precision to outperforming alternative evaluation methods. Success means providing better answers than existing approaches, given practical constraints of cost, compute, and time.\n\nAI evaluators should prioritize two key aspects:\n\n1.  Calibration: Systems should express appropriate levels of certainty, aligned with the reference frame they're operating from\n2.  Resolution: Within the bounds of reliable calibration, provide the most detailed and specific answers possible\n\nFor example, consider the question \"_What will be the economic impact of California wildfires in 2028?_\" While a perfectly precise answer is impossible, AI systems can progressively approach better estimates by:\n\n*   Aggregating multiple data sources\n*   Explicitly modeling uncertainties\n*   Identifying and accounting for measurement limitations\n*   Clearly stating assumptions and confidence levels\n\nAs long as a resolution is calibrated and fairly unbiased, it can be incentive-compatible for forecasting.\n\n**Complication 3: Goodharting**\n\nWe’d want to avoid a situation where one tool technically maximizes a narrow “_Epistemic Selection Protocol_”, but is actually poor at doing many of the things we want from a resolver AI.\n\nTo get around this, the Protocol could make specifications like the following:\n\n> What will be the most epistemically-capable service on \\[Date\\] that satisfies the following requirements?\n> \n> 1.  Costs under $20 per run.\n> 2.  Is publicly available.\n> 3.  Has over 1000 human users per month (_this is to ensure there’s no bottleneck that’s hard to otherwise specify._)\n> 4.  Completes runs within 10 minutes.\n> 5.  Has been separately reviewed to not have significantly and deceivingly goodharted on this specific benchmark.\n\nIt’s often possible to get around Goodharting by applying additional layers of complexity. Whether it’s worth it depends on the situation.\n\n**Complication 4: Different Ideologies**\n\nConsider a question about the impact of a tax cut policy. People with different philosophical or ideological backgrounds will likely disagree on fundamental assumptions, making a single \"correct\" answer impossible.\n\nThe simplest solution—declaring it a \"complex issue with multiple valid perspectives\"—is typically close to useless. A more useful approach would be developing AI tools that can represent different ideological frameworks, either through multiple specialized systems or a single system with adjustable philosophical parameters.\n\nA more sophisticated approach could generate personalized evaluations based on individual characteristics and depth of analysis. For instance: \"_How would someone with background X evaluate California Proposition 10 in 2030, after studying it for \\[10, 100, 1000\\] hours?_\" This could be implemented using an algorithm that accounts for both personal attributes and time invested in understanding the issue. [Scorable Functions](https://forum.effectivealtruism.org/posts/ZWQ5jbLxpd4JtQgu4/scorable-functions-a-format-for-algorithmic-forecasting) might be a useful format.\n\n**Complication 5: AI Tools with Different Strengths**\n\nOne might ask:  \n_“What if different AI tools are epistemically dominant in different areas? For example, one is great at political science, and another is great at advanced mathematics.”_  \n  \nAn obvious answer is to then create simple compositions of AI tools. A router can be used to send specific requests or sub-requests to other AI tools that are best equipped to handle them.\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/45b2e7a9a88aae09631e65b177bafd33ce7c5c951ce848e91eb3a10de288b38c/dgrdqdcmqtbxotjzetqk)\n\n_One possible AI tool resolution workflow_\n\n**Complication 6: AI Tools that Recommend Other AI Tools**\n\nImagine there’s a situation where one AI tool is chosen, but that tool recommends a different tool instead. For example, Perplexity 3.0 is asked a question, and it responds by stating that Claude 4.5 could do a better job than it could. Arguably it would make a lot of sense that if an AI tool were highly trusted to make speculative judgements, it could be trusted to be correct when claiming that a different tool is superior to itself.\n\nThis probably won’t be a major bottleneck. If AI tools could simply delegate other tools for specific questions, that could just be considered part of it during evaluation.\n\n## Going Forward\n\nWe hope this post is useful at advancing the conversation around AIs for question resolution and Epistemic AI Protocols. But it's still an early conversation.\n\nWe think there's a great deal of early experimentation and exploration to do in this broad space. Modern AI tools are arguably already good enough for broad use, and light wrappers on such tools can get us further. We hope to see work here in the next few years.",
  "reviews": [
    {
      "agentId": "bias-detector",
      "costInCents": 7885,
      "createdAt": "2025-04-16T08:41:32.181Z",
      "runDetails": "{\"model\":\"anthropic/claude-3.7-sonnet\",\"promptTokens\":5414,\"completionTokens\":2471,\"totalTokens\":7885,\"temperature\":0.1,\"runtimeMs\":52075}",
      "thinking": "I'm analyzing this document for potential biases in the discussion of AI tools for resolving forecasting questions. Let me identify the key claims and examine them for various types of biases.\n\nThe document proposes using AI systems to resolve forecasting questions instead of human evaluators, arguing that AIs can be more predictable, accessible, and cost-effective. It introduces the concept of \"Epistemic Selection Protocols\" for choosing the best AI tool at resolution time.\n\nPotential biases I'm noticing:\n\n1. **Technological optimism bias**: There seems to be an underlying assumption that AI tools will continue to improve rapidly and become increasingly trustworthy for complex evaluations. This reflects optimism bias and potentially the planning fallacy, as the timeline for such improvements may be overestimated.\n\n2. **Authority bias**: The document suggests using rankings like \"Forbes 2030 Most trusted AI tools\" or evaluations from established platforms like LessWrong or The Economist. This relies on authority bias, assuming these sources will provide reliable judgments.\n\n3. **Automation bias**: There's an implicit trust in AI systems to make fair and accurate judgments, potentially overlooking their limitations and biases. The document acknowledges some complications but may underestimate the challenges.\n\n4. **Status quo bias**: The document seems to accept the current trajectory of AI development without deeply questioning whether this is the optimal path for forecasting resolution.\n\n5. **Confirmation bias**: The examples and scenarios presented tend to support the author's thesis that AI resolvers will be beneficial, with less exploration of potential negative outcomes or failures.\n\n6. **Survivorship bias**: When discussing future AI tools, there's a focus on the successful ones that will emerge, without adequate consideration of the many failed approaches that might occur.\n\n7. **Narrative fallacy**: The \"Short Story of How This Plays Out\" section creates a coherent narrative about how AI resolution will develop, which may oversimplify the complex and unpredictable nature of technological development.\n\n8. **Bandwagon effect**: There's an implicit assumption that as AI tools become more trusted, their adoption for decision-making will naturally increase, potentially overlooking resistance or alternative approaches.\n\n9. **Hindsight bias**: The document presents future developments as if they're relatively predictable, when in reality technological evolution is highly uncertain.\n\n10. **Illusion of control**: There's an assumption that we can design protocols now that will effectively guide AI tool selection years in the future, potentially overestimating our ability to anticipate future challenges.\n\n11. **Base rate neglect**: The document doesn't adequately consider the historical rate of success for similar technological predictions or protocols.\n\n12. **Anchoring effect**: The current capabilities of AI tools like Perplexity and Claude serve as anchors for expectations about future capabilities.\n\nThe document does acknowledge some limitations and complications, which is positive. However, there are still significant biases in how the future of AI resolution is presented and the confidence with which predictions are made.",
      "summary": "The document exhibits several significant biases in its discussion of AI tools for resolving forecasting questions. Most prominently, it displays technological optimism bias by assuming AI capabilities will improve rapidly enough to handle complex subjective questions. The narrative fallacy is evident in the detailed story of how AI resolution will develop, presenting a simplified, deterministic view of technological progress. Automation bias appears in the assumption that AI tools will eventually replace human judgment for important analyses. The document demonstrates the illusion of control by suggesting we can design effective protocols now that will work years in the future. Confirmation bias is present in the selective presentation of evidence supporting AI resolvers' benefits. While the document does acknowledge some complications and limitations, which partially mitigates these biases, the overall framing remains overly optimistic about AI capabilities and our ability to design effective selection protocols for the future.",
      "comments": [
        {
          "title": "Authority Bias",
          "description": "The document relies on authority bias when suggesting using rankings from established sources like Forbes, LessWrong, or The Economist to determine trustworthy AI tools. This assumes these authorities will provide reliable judgments about AI capabilities, without critically examining their expertise or potential biases in this domain.",
          "highlight": {
            "startOffset": 0,
            "endOffset": 0,
            "quotedText": ""
          },
          "importance": "75",
          "grade": "45",
          "isValid": false,
          "error": "End text not found in document"
        },
        {
          "title": "Confirmation Bias",
          "description": "The document primarily presents examples and scenarios that support the thesis that AI resolvers will be beneficial, with less exploration of potential negative outcomes or failures. This selective presentation of evidence that supports the author's viewpoint reflects confirmation bias, limiting a balanced assessment of the proposal's merits and risks.",
          "highlight": {
            "startOffset": 57,
            "endOffset": 269,
            "quotedText": "AIs can be used to resolve forecasting questions on platforms like Manifold and Metaculus.\n    \n2.  AI question resolution, in theory, can be far more predictable, accessible, and inexpensive to human resolution."
          },
          "importance": "80",
          "grade": "40",
          "isValid": true
        },
        {
          "title": "Technological Optimism Bias",
          "description": "The document displays optimism bias regarding the improvement trajectory of AI tools. It assumes that AI capabilities for resolving complex, subjective questions will improve significantly, without adequately acknowledging the possibility of plateaus or slower-than-expected progress. This reflects both optimism bias and elements of the planning fallacy, as technological development timelines are often overestimated.",
          "highlight": {
            "startOffset": 279,
            "endOffset": 950,
            "quotedText": "Current AI tools (combinations of LLM calls and software) are mediocre at judging subjective or speculative questions. We expect them to improve. \n    \n4.  AI tools are changing rapidly. If we want to organize forecasting questions to be resolved using an AI tool in 10 years, instead of selecting a specific tool, we might want to select a _protocol_ for choosing the best tool at resolution time. We call this an \"_Epistemic Selection Protocol_\". \n    \n5.  If we have a strong \"_Epistemic Selection Protocol_\" that we like, and expect that we'll have _sufficient_ AI tools in the near future, then we could arguably start to write forecasting questions for them right a"
          },
          "importance": "95",
          "grade": "40",
          "isValid": true
        },
        {
          "title": "Illusion of Control",
          "description": "The document demonstrates the illusion of control bias by suggesting we can design effective protocols now that will guide AI tool selection years in the future. This overestimates our ability to anticipate future challenges and developments in AI capabilities and evaluation methods. The complexity of creating robust, future-proof protocols is likely underestimated.",
          "highlight": {
            "startOffset": 435,
            "endOffset": 950,
            "quotedText": "AI tools are changing rapidly. If we want to organize forecasting questions to be resolved using an AI tool in 10 years, instead of selecting a specific tool, we might want to select a _protocol_ for choosing the best tool at resolution time. We call this an \"_Epistemic Selection Protocol_\". \n    \n5.  If we have a strong \"_Epistemic Selection Protocol_\" that we like, and expect that we'll have _sufficient_ AI tools in the near future, then we could arguably start to write forecasting questions for them right a"
          },
          "importance": "85",
          "grade": "35",
          "isValid": false,
          "error": "Highlight overlaps with a previous highlight"
        },
        {
          "title": "Narrative Fallacy",
          "description": "The \"Short Story of How This Plays Out\" section creates a coherent narrative about AI development that may not reflect the messy, non-linear nature of technological progress. This represents the narrative fallacy, where a compelling story is constructed that oversimplifies complex, unpredictable developments and makes them seem more deterministic than they actually are.",
          "highlight": {
            "startOffset": 1170,
            "endOffset": 11951,
            "quotedText": "A Short Story of How This Plays Out\",_ which is perhaps the most accessible part of this for most readers.  \n    \n8.  We list a bunch of potential complications with AI tools and Epistemic Selection Protocols. While these will be complex, we see no fundamental bottlenecks. \n    \n\n## Motivation\n\nSay we want to create a prediction tournament on a complex question.\n\nFor example:\n\n*   _“Will there be over 10 million people killed in global wars between 2025 and 2030?”_\n*   _“Will IT security incidents be a national US concern, on the scale of the top 5 concerns, in 2030?”_\n*   _“Will bottlenecks in power capacity limit AIs by over 50%, in 2030?”_\n\nWhile these questions involve subjective elements, they aren't purely matters of opinion. Come 2030, though people might debate the details, informed observers would likely reach broadly similar conclusions.\n\nCurrently, such questions are typically resolved by human evaluators. On [Manifold](https://manifold.markets/home), the question author serves as judge, with authors developing reputations for their judgment quality over time. [Metaculus](https://www.metaculus.com) takes a different approach for more complex and subjective questions, often employing small panels of subject matter experts.\n\nHowever, human evaluators have several limitations:\n\n1.  High costs, particularly for domain experts\n2.  Limited track records of evaluation quality\n3.  Poor accessibility - most forecasters cannot directly query them\n4.  Inconsistency over time as their views and motivations evolve\n5.  Uncertain long-term availability\n6.  Vulnerability to biases and potential conflicts of interest\n\n## AIs as Question Evaluators\n\nInstead of using humans to resolve thorny questions, we can use AIs. There are many ways we could attempt to do this, so we’ll walk through a few examples.\n\n**Option 1: Use an LLM**\n\nThe first option to consider is to use an LLM as an evaluator. For example, write,  \n“_This question will be judged by Claude 3.5 Sonnet (2024-06-20). The specific prompt will be, …”_\n\nThis style is replicable, simple, and inexpensive. However, it clearly has some downsides. The first obvious one is that Claude 3.5 Sonnet doesn’t perform web searches, so its knowledge would likely be too limited to resolve future forecasting questions.\n\n**Option 2: Use an AI Tool with Search**\n\nInstead of using a standard LLM, you might want to use a tool that uses both LLMs and web searches. [Perplexity](https://www.perplexity.ai/) might be the most famous one now, but other advanced research assistants are starting to come out. In theory one should be able to set a research budget that’s in line with the importance and complexity of the question.\n\nThere's been some experimentation here. See [this guide](https://predictionmarkets.miraheze.org/wiki/GPT4_Headline_Markets) on using GPT4 for resolving questions on Manifold, and [this](https://predictionmarkets.miraheze.org/wiki/GPT4_Manifold_Experiment) experiment write up. GPT4 allows for basic web searches.\n\nThis option is probably better than Option 1 for most things. But there are still problems. The next major one is the risk that Perplexity, or any other single tool we can point to now, won’t be the leading one in the future. The field is moving rapidly, it’s difficult to tell which tools will even exist in 5 years, let alone be the preferred options.\n\n**Option 3: Use an “**_**Epistemic**_**” Selection Protocol**\n\nIn this case, one doesn't select a specific AI tool. Instead one selects a process or protocol that selects an AI tool.\n\nFor example:  \n_“In 2030, we will resolve this question using the leading AI tool on the ‘Forbes 2030 Most trusted AI tools’ list.”_\n\nWe’re looking for AI tools that are “_trusted to reason about complex, often speculative or political matters._” This arguably can be more quickly expressed as searching for the tool with the best _epistemics_.\n\n## _Epistemic Selection Protocols_ (Or, how do we choose the best AI tool to use?)\n\nArguably, AI Epistemic Selection Protocols can be the best choice of the above options, if one could implement them effectively, for most 2+ year questions. There are a lot of potential processes to choose, though most would be too complicated to be worthwhile. We want to strike a balance between simplicity and accuracy.\n\nLet’s first list the most obvious options.\n\n**Option 1: Trusted and formalized epistemic evaluations**\n\nThere’s currently a wide variety of AI benchmarks. For example, [TruthfulQA](https://github.com/sylinrl/TruthfulQA) and various investigations of model sycophancy. But arguably, none of these would be great proxies for which AI tool would be the most trusted question resolvers in the future. Newer, deliberate benchmarks could help here.\n\nExample:  \n_“This forecasting question will be resolved, using whichever AI Tool does the best on Complete Epistemic Benchmark X, and can be used for less than $20.”_\n\n**Option 2: Human-derived trust rankings**\n\nHumans could simply be polled on which AI tools they regard as the most trustworthy. Different groups of humans would have different preferences, so the group would need to be specified in advance for an AI Selection Process.\n\nExample:  \n_“This forecasting question will be resolved, using whichever AI Tool is on the top of the list of ‘Most trusted AI Tools’ on LessWrong, and can be used for less than $20.”_\n\n**Option 3: Inter-AI trust ratings**\n\nAI tools could select future AI tools to use. This could be a 1-step solution, where an open-source or standardized (for the sake of ensuring it will be available long-term) solution is asked to identify the best available candidate. Or it could be a multiple-step solution, where perhaps AI tools are asked to recommend each other using some simple algorithm. This can be similar in concept to the [Community Notes](https://vitalik.eth.limo/general/2023/08/16/communitynotes.html) algorithm.\n\nExample:  \n_“This forecasting question will be resolved, using whichever AI Tool wins a poll of the ‘Most trusted AI tools’ according to AI tools.’ In this poll, each AI tool will recommend its favorite of the other available candidates.”_ (Note: This specific proposal can be gamed, so greater complexity will likely be required.)\n\n## A Short Story of How This Plays Out\n\nIn 2025, several question writers on Manifold experiment with AI resolution systems. Some questions include:  \n  \n_“Will California fires in 2026 be worse than those in 2025? To answer this, I’ll ask Perplexity on Jan 1, 2026. My prompt will be, \\[Will California fires in 2026 be worse than those in 2025? Judge this by guessing the total economic loss.\\]”_\n\n_“How many employees will OpenAI have in Dec 2025? To answer this, I’ll first ask commenters to write arguments and/or facts that they’ve found on this. I’ll filter this for what seems accurate, then I’ll paste this into Perplexity. I’ll call Perplexity 5 times, and average the results.”_\n\nForecasters gradually identify the uses and limitations of such systems. It turns out they are surprisingly bad at advanced physics questions, for some surprising reason. There are a few clever prompting strategies that help ensure that these AIs put out more consistent results.\n\nAI tools like Perplexity also get very good at hunting down and answering questions that are straightforward to resolve. Manifold adds custom functionality to do this. For example, say someone writes a question, _“What Movie Will Win The 2025 Oscars For Best Picture?”_ When they do, they’ll be given the option to have a Manifold AI system automatically make a suggested guess for them, at the time of expected question resolution. These guesses will begin with high error rates (10%), but these will gradually drop.\n\nSeparately, various epistemic evaluations are established. There are multiple public and private rankings. There are also surveys of the “_Most Trusted AIs_”, held on various platforms such as Manifold, LessWrong, and The Verge. Leading consumer product review websites such as Consumer Reports and Wirecutter begin to have ratings for AI tools, using defined categories such as “_accuracy_” and “_reasonableness_.”\n\nOne example question from this is:  \n_“In 2030, will it seem like o1 was an important AI development, that was at least as innovative and important as GPT4? This will be resolved using whichever AI leads the “Most trusted AIs” poll on Manifold in 2029.”_\n\nThere will be a long tail of AI tools that are proposed as contenders for epistemic benchmarks. Most of the options are simply minor tweaks on other options or light routers. Few of these will get the full standard evaluations, but good proxies will emerge. It turns out that you can get a decent measure by using the top fully-evaluated AI systems to evaluate more niche systems.\n\nIn 2027, there will be a significant amount of understanding, buy-in, and sophistication with such systems (at least among a few niche communities, like Manifold users). This will make it possible to scale them for more ambitious uses.\n\nMetaculus runs some competitions that include:\n\n_“What is the_ [_relative value_](https://forum.effectivealtruism.org/posts/EFEwBvuDrTLDndqCt/relative-value-functions-a-flexible-new-format-for-value) _of each of \\[the top 500 AI safety papers of 2026\\]? This will be resolved in 2030 by using the most trusted AI system, via LessWrong or The Economist, at that time. This AI will order all of the papers - forecasters should estimate the percentile that each paper will achieve.”_\n\n_“What is the expected value of every biosafety organization in 2027, estimated as what Open Philanthropy would have paid for it from their biosafety funding pool in 2027? This will be judged in 2029, by the most trusted AI system, with a budget of $1,000 for each evaluation.”_\n\nAround this time, some researchers will begin to make wider kinds of analyses, and forecast compressions.\n\n_“How will the top epistemic model of 2030 evaluate the accuracy and value of the claims of each of the top 100 intellectuals from 2027?”_\n\n_“Will the top epistemic model of 2030 consider the current top epistemic models to be ‘highly overconfident’ for at least 10% of the normative questions they are asked?”_\n\nThe top trusted AI tools start to become frequent ways to second-guess humans. For example, if a boss makes a controversial decision, people could contest the decision if top AI tools back them up. Similar analyses would be used within governments.\n\nAs these AI tools become even more trusted, they will replace many humans for important analyses and decisions. Humans will spend a great deal of effort focused on assuring these AI tools are doing a good job, and they'll have the time for that because there will be few other things they need to directly evaluate or oversee."
          },
          "importance": "90",
          "grade": "35",
          "isValid": true
        },
        {
          "title": "Hindsight Bias Risk",
          "description": "The document presents future AI developments as if they're relatively predictable, which risks hindsight bias in future evaluations. When looking back from the future, there may be a tendency to see the developments as having been more obvious and predictable than they actually were, which could affect how the success of these protocols is evaluated.",
          "highlight": {
            "startOffset": 7450,
            "endOffset": 10191,
            "quotedText": "In 2025, several question writers on Manifold experiment with AI resolution systems. Some questions include:  \n  \n_“Will California fires in 2026 be worse than those in 2025? To answer this, I’ll ask Perplexity on Jan 1, 2026. My prompt will be, \\[Will California fires in 2026 be worse than those in 2025? Judge this by guessing the total economic loss.\\]”_\n\n_“How many employees will OpenAI have in Dec 2025? To answer this, I’ll first ask commenters to write arguments and/or facts that they’ve found on this. I’ll filter this for what seems accurate, then I’ll paste this into Perplexity. I’ll call Perplexity 5 times, and average the results.”_\n\nForecasters gradually identify the uses and limitations of such systems. It turns out they are surprisingly bad at advanced physics questions, for some surprising reason. There are a few clever prompting strategies that help ensure that these AIs put out more consistent results.\n\nAI tools like Perplexity also get very good at hunting down and answering questions that are straightforward to resolve. Manifold adds custom functionality to do this. For example, say someone writes a question, _“What Movie Will Win The 2025 Oscars For Best Picture?”_ When they do, they’ll be given the option to have a Manifold AI system automatically make a suggested guess for them, at the time of expected question resolution. These guesses will begin with high error rates (10%), but these will gradually drop.\n\nSeparately, various epistemic evaluations are established. There are multiple public and private rankings. There are also surveys of the “_Most Trusted AIs_”, held on various platforms such as Manifold, LessWrong, and The Verge. Leading consumer product review websites such as Consumer Reports and Wirecutter begin to have ratings for AI tools, using defined categories such as “_accuracy_” and “_reasonableness_.”\n\nOne example question from this is:  \n_“In 2030, will it seem like o1 was an important AI development, that was at least as innovative and important as GPT4? This will be resolved using whichever AI leads the “Most trusted AIs” poll on Manifold in 2029.”_\n\nThere will be a long tail of AI tools that are proposed as contenders for epistemic benchmarks. Most of the options are simply minor tweaks on other options or light routers. Few of these will get the full standard evaluations, but good proxies will emerge. It turns out that you can get a decent measure by using the top fully-evaluated AI systems to evaluate more niche systems.\n\nIn 2027, there will be a significant amount of understanding, buy-in, and sophistication with such systems (at least among a few niche communities, like Manifold users). This will make it possible to scale them for more ambitious uses."
          },
          "importance": "70",
          "grade": "40",
          "isValid": false,
          "error": "Highlight overlaps with a previous highlight"
        },
        {
          "title": "Automation Bias",
          "description": "The document exhibits automation bias by suggesting AI tools will eventually replace human judgment for complex evaluations. While it acknowledges some complications, it underestimates the fundamental challenges in having AI systems make normative judgments or resolve questions with significant subjective components. This reflects an overconfidence in automated systems compared to human judgment.",
          "highlight": {
            "startOffset": 11625,
            "endOffset": 11951,
            "quotedText": "As these AI tools become even more trusted, they will replace many humans for important analyses and decisions. Humans will spend a great deal of effort focused on assuring these AI tools are doing a good job, and they'll have the time for that because there will be few other things they need to directly evaluate or oversee."
          },
          "importance": "85",
          "grade": "30",
          "isValid": false,
          "error": "Highlight overlaps with a previous highlight"
        },
        {
          "title": "Balanced Acknowledgment of Complications",
          "description": "The document does acknowledge several potential complications with AI resolution systems, which helps mitigate some of the biases. This section provides a more balanced view by recognizing challenges like lack of sufficient AI tools, ground truth issues, goodharting, ideological differences, and varying AI strengths. This demonstrates an attempt to consider limitations and counterarguments.",
          "highlight": {
            "startOffset": 11957,
            "endOffset": 17300,
            "quotedText": "Protocol Complications & Potential Solutions\n\n**Complication 1: Lack of Sufficient AI Tools**\n\nIn the beginning, we expect that many people won’t trust any AI tools to be adequate in resolving many questions. Even if tools look good in evaluations, it will take time for them to build trust.\n\nOne option is to set certain criteria for _sufficiency_. For example, one might say, “This question will be resolved using whichever AI system first gets to a 90/100 on the Epistemic Benchmark Evaluation…” This would clearly require understanding and trust in the evaluations, rather than in a specific tool, so this would require high-quality evaluations or polls.\n\n**Complication 2: Lack of Ground Truth**\n\nMany subjective and speculative questions lack definitive answers. Some questions can only be answered long after the information would be useful, while others are inherently impossible to resolve with complete certainty.\n\nIn such cases, the goal should shift from seeking perfect precision to outperforming alternative evaluation methods. Success means providing better answers than existing approaches, given practical constraints of cost, compute, and time.\n\nAI evaluators should prioritize two key aspects:\n\n1.  Calibration: Systems should express appropriate levels of certainty, aligned with the reference frame they're operating from\n2.  Resolution: Within the bounds of reliable calibration, provide the most detailed and specific answers possible\n\nFor example, consider the question \"_What will be the economic impact of California wildfires in 2028?_\" While a perfectly precise answer is impossible, AI systems can progressively approach better estimates by:\n\n*   Aggregating multiple data sources\n*   Explicitly modeling uncertainties\n*   Identifying and accounting for measurement limitations\n*   Clearly stating assumptions and confidence levels\n\nAs long as a resolution is calibrated and fairly unbiased, it can be incentive-compatible for forecasting.\n\n**Complication 3: Goodharting**\n\nWe’d want to avoid a situation where one tool technically maximizes a narrow “_Epistemic Selection Protocol_”, but is actually poor at doing many of the things we want from a resolver AI.\n\nTo get around this, the Protocol could make specifications like the following:\n\n> What will be the most epistemically-capable service on \\[Date\\] that satisfies the following requirements?\n> \n> 1.  Costs under $20 per run.\n> 2.  Is publicly available.\n> 3.  Has over 1000 human users per month (_this is to ensure there’s no bottleneck that’s hard to otherwise specify._)\n> 4.  Completes runs within 10 minutes.\n> 5.  Has been separately reviewed to not have significantly and deceivingly goodharted on this specific benchmark.\n\nIt’s often possible to get around Goodharting by applying additional layers of complexity. Whether it’s worth it depends on the situation.\n\n**Complication 4: Different Ideologies**\n\nConsider a question about the impact of a tax cut policy. People with different philosophical or ideological backgrounds will likely disagree on fundamental assumptions, making a single \"correct\" answer impossible.\n\nThe simplest solution—declaring it a \"complex issue with multiple valid perspectives\"—is typically close to useless. A more useful approach would be developing AI tools that can represent different ideological frameworks, either through multiple specialized systems or a single system with adjustable philosophical parameters.\n\nA more sophisticated approach could generate personalized evaluations based on individual characteristics and depth of analysis. For instance: \"_How would someone with background X evaluate California Proposition 10 in 2030, after studying it for \\[10, 100, 1000\\] hours?_\" This could be implemented using an algorithm that accounts for both personal attributes and time invested in understanding the issue. [Scorable Functions](https://forum.effectivealtruism.org/posts/ZWQ5jbLxpd4JtQgu4/scorable-functions-a-format-for-algorithmic-forecasting) might be a useful format.\n\n**Complication 5: AI Tools with Different Strengths**\n\nOne might ask:  \n_“What if different AI tools are epistemically dominant in different areas? For example, one is great at political science, and another is great at advanced mathematics.”_  \n  \nAn obvious answer is to then create simple compositions of AI tools. A router can be used to send specific requests or sub-requests to other AI tools that are best equipped to handle them.\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/45b2e7a9a88aae09631e65b177bafd33ce7c5c951ce848e91eb3a10de288b38c/dgrdqdcmqtbxotjzetqk)\n\n_One possible AI tool resolution workflow_\n\n**Complication 6: AI Tools that Recommend Other AI Tools**\n\nImagine there’s a situation where one AI tool is chosen, but that tool recommends a different tool instead. For example, Perplexity 3.0 is asked a question, and it responds by stating that Claude 4.5 could do a better job than it could. Arguably it would make a lot of sense that if an AI tool were highly trusted to make speculative judgements, it could be trusted to be correct when claiming that a different tool is superior to itself.\n\nThis probably won’t be a major bottleneck. If AI tools could simply delegate other tools for specific questions, that could just be considered part of it during evaluation."
          },
          "importance": "85",
          "grade": "70",
          "isValid": true
        }
      ],
      "grade": 45
    },
    {
      "agentId": "clarity-coach",
      "costInCents": 8151,
      "createdAt": "2025-04-16T08:42:28.555Z",
      "runDetails": "{\"model\":\"anthropic/claude-3.7-sonnet\",\"promptTokens\":5534,\"completionTokens\":2617,\"totalTokens\":8151,\"temperature\":0.1,\"runtimeMs\":47748}",
      "thinking": "I'm analyzing a document about using AI systems to resolve forecasting questions on platforms like Manifold and Metaculus. The document proposes using AI tools instead of human evaluators for resolving complex, subjective forecasting questions, and introduces the concept of \"Epistemic Selection Protocols\" for choosing the best AI tool at resolution time.\n\nFrom a clarity perspective, I need to evaluate:\n\n1. **Readability**: How accessible is the language? Are sentences well-structured with appropriate length variation? Is there a good balance of active vs. passive voice?\n\n2. **Structural Coherence**: Does the document flow logically? Are transitions between sections smooth? Is the information hierarchy clear?\n\n3. **Precision and Concision**: Is the language specific and economical? Are there instances of redundancy or verbosity?\n\n4. **Audience Alignment**: Does the document make appropriate assumptions about the reader's prior knowledge? Are technical terms adequately explained?\n\nLet me analyze each section:\n\n**Summary Section**:\nThe summary provides a clear, numbered list of key points. This is effective for quick comprehension. The points progress logically, though some technical terms like \"Epistemic Selection Protocol\" are introduced without immediate definition, which might challenge readers unfamiliar with the concept.\n\n**Motivation Section**:\nThis section effectively establishes the problem context with concrete examples. The bullet points listing limitations of human evaluators are clear and concise. The section transitions well into the proposed solution.\n\n**AIs as Question Evaluators Section**:\nThe three options are presented in a structured format with clear headings. Each option is explained with examples, which helps with comprehension. The progression from simple to more complex solutions is logical.\n\n**Epistemic Selection Protocols Section**:\nThis section introduces three options for selecting AI tools, each with examples. The concept is complex but presented in a relatively accessible way. The examples help ground the abstract concepts.\n\n**A Short Story of How This Plays Out Section**:\nThis narrative section effectively illustrates the practical application of the concepts. It's more accessible than the theoretical sections and provides concrete examples of how AI resolution might evolve over time.\n\n**Protocol Complications & Potential Solutions Section**:\nThis section addresses potential issues in a structured format. Each complication is clearly stated and followed by proposed solutions. The use of examples helps clarify complex points.\n\nOverall, the document has several strengths in terms of clarity:\n- Good use of structure with clear headings and subheadings\n- Effective use of examples to illustrate abstract concepts\n- Logical progression of ideas\n- Balanced mix of theoretical discussion and practical examples\n\nHowever, there are also some areas for improvement:\n- Some technical terms are introduced without immediate definition\n- Sentence length varies, but there are instances of complex, multi-clause sentences that could be simplified\n- Some sections assume significant prior knowledge about AI systems and forecasting platforms\n- The document occasionally uses abstract language that could be made more concrete\n\nLet me now identify specific examples to highlight in my comments.",
      "summary": "This document on AI-based forecasting question resolution demonstrates strong clarity overall. Its strengths include excellent structural organization, effective use of examples, and a particularly accessible narrative section that translates theoretical concepts into practical scenarios. The document employs consistent formatting and visual elements that enhance readability. Areas for improvement include more immediate definitions of technical terms like \"Epistemic Selection Protocol,\" simplification of some complex sentences, and more scaffolding for readers less familiar with AI systems or forecasting platforms. The document successfully balances technical depth with accessibility, though it occasionally assumes significant prior knowledge. The narrative \"Short Story\" section represents the document's strongest clarity achievement, effectively bridging abstract concepts with concrete applications.",
      "comments": [
        {
          "title": "Technical Term Introduction",
          "description": "The term \"Epistemic Selection Protocol\" is introduced with quotation marks and italics, signaling its importance, but lacks an immediate clear definition. This creates a momentary comprehension gap for readers unfamiliar with the concept.",
          "highlight": {
            "startOffset": 0,
            "endOffset": 0,
            "quotedText": ""
          },
          "importance": "80",
          "grade": "60",
          "isValid": false,
          "error": "End text not found in document"
        },
        {
          "title": "Concrete Examples Enhance Comprehension",
          "description": "The document effectively uses concrete example questions to illustrate abstract concepts. These examples ground the discussion in practical applications, making complex ideas more accessible to readers.",
          "highlight": {
            "startOffset": 0,
            "endOffset": 0,
            "quotedText": ""
          },
          "importance": "85",
          "grade": "95",
          "isValid": false,
          "error": "End text not found in document"
        },
        {
          "title": "Complex Sentence Structure",
          "description": "This sentence contains multiple clauses and concepts, making it challenging to parse in a single reading. Breaking it into shorter sentences would improve readability without sacrificing content.",
          "highlight": {
            "startOffset": 0,
            "endOffset": 0,
            "quotedText": ""
          },
          "importance": "70",
          "grade": "45",
          "isValid": false,
          "error": "End text not found in document"
        },
        {
          "title": "Effective Use of Formatting",
          "description": "The document uses formatting elements (italics, bullet points, blockquotes) consistently to highlight key information and create visual hierarchy. This enhances readability by breaking up dense text and emphasizing important concepts.",
          "highlight": {
            "startOffset": 0,
            "endOffset": 0,
            "quotedText": ""
          },
          "importance": "80",
          "grade": "90",
          "isValid": false,
          "error": "End text not found in document"
        },
        {
          "title": "Effective Summary Structure",
          "description": "The numbered list format in the summary provides excellent scanability and clear organization of key points. This structure helps readers quickly grasp the main concepts before diving into details.",
          "highlight": {
            "startOffset": 53,
            "endOffset": 1443,
            "quotedText": "1.  AIs can be used to resolve forecasting questions on platforms like Manifold and Metaculus.\n    \n2.  AI question resolution, in theory, can be far more predictable, accessible, and inexpensive to human resolution.\n    \n3.  Current AI tools (combinations of LLM calls and software) are mediocre at judging subjective or speculative questions. We expect them to improve. \n    \n4.  AI tools are changing rapidly. If we want to organize forecasting questions to be resolved using an AI tool in 10 years, instead of selecting a specific tool, we might want to select a _protocol_ for choosing the best tool at resolution time. We call this an \"_Epistemic Selection Protocol_\". \n    \n5.  If we have a strong \"_Epistemic Selection Protocol_\" that we like, and expect that we'll have _sufficient_ AI tools in the near future, then we could arguably start to write forecasting questions for them right away. \n    \n6.  The methods listed above, if successful, wouldn't just be for resolving questions on prediction platforms. They could be generalized to pursue broad-scale and diverse evaluations. \n    \n7.  We provide a “_A Short Story of How This Plays Out\",_ which is perhaps the most accessible part of this for most readers.  \n    \n8.  We list a bunch of potential complications with AI tools and Epistemic Selection Protocols. While these will be complex, we see no fundamental bottlenecks."
          },
          "importance": "85",
          "grade": "90",
          "isValid": true
        },
        {
          "title": "Narrative Scenario Improves Accessibility",
          "description": "The \"Short Story\" section translates theoretical concepts into a narrative timeline, significantly enhancing comprehension. This approach makes abstract ideas concrete and helps readers visualize practical applications.",
          "highlight": {
            "startOffset": 7410,
            "endOffset": 11951,
            "quotedText": "## A Short Story of How This Plays Out\n\nIn 2025, several question writers on Manifold experiment with AI resolution systems. Some questions include:  \n  \n_“Will California fires in 2026 be worse than those in 2025? To answer this, I’ll ask Perplexity on Jan 1, 2026. My prompt will be, \\[Will California fires in 2026 be worse than those in 2025? Judge this by guessing the total economic loss.\\]”_\n\n_“How many employees will OpenAI have in Dec 2025? To answer this, I’ll first ask commenters to write arguments and/or facts that they’ve found on this. I’ll filter this for what seems accurate, then I’ll paste this into Perplexity. I’ll call Perplexity 5 times, and average the results.”_\n\nForecasters gradually identify the uses and limitations of such systems. It turns out they are surprisingly bad at advanced physics questions, for some surprising reason. There are a few clever prompting strategies that help ensure that these AIs put out more consistent results.\n\nAI tools like Perplexity also get very good at hunting down and answering questions that are straightforward to resolve. Manifold adds custom functionality to do this. For example, say someone writes a question, _“What Movie Will Win The 2025 Oscars For Best Picture?”_ When they do, they’ll be given the option to have a Manifold AI system automatically make a suggested guess for them, at the time of expected question resolution. These guesses will begin with high error rates (10%), but these will gradually drop.\n\nSeparately, various epistemic evaluations are established. There are multiple public and private rankings. There are also surveys of the “_Most Trusted AIs_”, held on various platforms such as Manifold, LessWrong, and The Verge. Leading consumer product review websites such as Consumer Reports and Wirecutter begin to have ratings for AI tools, using defined categories such as “_accuracy_” and “_reasonableness_.”\n\nOne example question from this is:  \n_“In 2030, will it seem like o1 was an important AI development, that was at least as innovative and important as GPT4? This will be resolved using whichever AI leads the “Most trusted AIs” poll on Manifold in 2029.”_\n\nThere will be a long tail of AI tools that are proposed as contenders for epistemic benchmarks. Most of the options are simply minor tweaks on other options or light routers. Few of these will get the full standard evaluations, but good proxies will emerge. It turns out that you can get a decent measure by using the top fully-evaluated AI systems to evaluate more niche systems.\n\nIn 2027, there will be a significant amount of understanding, buy-in, and sophistication with such systems (at least among a few niche communities, like Manifold users). This will make it possible to scale them for more ambitious uses.\n\nMetaculus runs some competitions that include:\n\n_“What is the_ [_relative value_](https://forum.effectivealtruism.org/posts/EFEwBvuDrTLDndqCt/relative-value-functions-a-flexible-new-format-for-value) _of each of \\[the top 500 AI safety papers of 2026\\]? This will be resolved in 2030 by using the most trusted AI system, via LessWrong or The Economist, at that time. This AI will order all of the papers - forecasters should estimate the percentile that each paper will achieve.”_\n\n_“What is the expected value of every biosafety organization in 2027, estimated as what Open Philanthropy would have paid for it from their biosafety funding pool in 2027? This will be judged in 2029, by the most trusted AI system, with a budget of $1,000 for each evaluation.”_\n\nAround this time, some researchers will begin to make wider kinds of analyses, and forecast compressions.\n\n_“How will the top epistemic model of 2030 evaluate the accuracy and value of the claims of each of the top 100 intellectuals from 2027?”_\n\n_“Will the top epistemic model of 2030 consider the current top epistemic models to be ‘highly overconfident’ for at least 10% of the normative questions they are asked?”_\n\nThe top trusted AI tools start to become frequent ways to second-guess humans. For example, if a boss makes a controversial decision, people could contest the decision if top AI tools back them up. Similar analyses would be used within governments.\n\nAs these AI tools become even more trusted, they will replace many humans for important analyses and decisions. Humans will spend a great deal of effort focused on assuring these AI tools are doing a good job, and they'll have the time for that because there will be few other things they need to directly evaluate or oversee."
          },
          "importance": "90",
          "grade": "95",
          "isValid": true
        },
        {
          "title": "Clear Problem-Solution Structure",
          "description": "The \"Protocol Complications & Potential Solutions\" section uses an effective problem-solution format that enhances clarity. Each complication is clearly stated and followed by proposed solutions, creating a logical flow that helps readers understand complex challenges.",
          "highlight": {
            "startOffset": 11954,
            "endOffset": 14816,
            "quotedText": "## Protocol Complications & Potential Solutions\n\n**Complication 1: Lack of Sufficient AI Tools**\n\nIn the beginning, we expect that many people won’t trust any AI tools to be adequate in resolving many questions. Even if tools look good in evaluations, it will take time for them to build trust.\n\nOne option is to set certain criteria for _sufficiency_. For example, one might say, “This question will be resolved using whichever AI system first gets to a 90/100 on the Epistemic Benchmark Evaluation…” This would clearly require understanding and trust in the evaluations, rather than in a specific tool, so this would require high-quality evaluations or polls.\n\n**Complication 2: Lack of Ground Truth**\n\nMany subjective and speculative questions lack definitive answers. Some questions can only be answered long after the information would be useful, while others are inherently impossible to resolve with complete certainty.\n\nIn such cases, the goal should shift from seeking perfect precision to outperforming alternative evaluation methods. Success means providing better answers than existing approaches, given practical constraints of cost, compute, and time.\n\nAI evaluators should prioritize two key aspects:\n\n1.  Calibration: Systems should express appropriate levels of certainty, aligned with the reference frame they're operating from\n2.  Resolution: Within the bounds of reliable calibration, provide the most detailed and specific answers possible\n\nFor example, consider the question \"_What will be the economic impact of California wildfires in 2028?_\" While a perfectly precise answer is impossible, AI systems can progressively approach better estimates by:\n\n*   Aggregating multiple data sources\n*   Explicitly modeling uncertainties\n*   Identifying and accounting for measurement limitations\n*   Clearly stating assumptions and confidence levels\n\nAs long as a resolution is calibrated and fairly unbiased, it can be incentive-compatible for forecasting.\n\n**Complication 3: Goodharting**\n\nWe’d want to avoid a situation where one tool technically maximizes a narrow “_Epistemic Selection Protocol_”, but is actually poor at doing many of the things we want from a resolver AI.\n\nTo get around this, the Protocol could make specifications like the following:\n\n> What will be the most epistemically-capable service on \\[Date\\] that satisfies the following requirements?\n> \n> 1.  Costs under $20 per run.\n> 2.  Is publicly available.\n> 3.  Has over 1000 human users per month (_this is to ensure there’s no bottleneck that’s hard to otherwise specify._)\n> 4.  Completes runs within 10 minutes.\n> 5.  Has been separately reviewed to not have significantly and deceivingly goodharted on this specific benchmark.\n\nIt’s often possible to get around Goodharting by applying additional layers of complexity. Whether it’s worth it depends on the situation."
          },
          "importance": "85",
          "grade": "85",
          "isValid": true
        },
        {
          "title": "Visual Aid Enhances Understanding",
          "description": "The inclusion of a workflow diagram provides visual reinforcement of a complex concept. This visual representation breaks up text-heavy content and offers an alternative way to understand the information, benefiting visual learners.",
          "highlight": {
            "startOffset": 16416,
            "endOffset": 16626,
            "quotedText": "![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/45b2e7a9a88aae09631e65b177bafd33ce7c5c951ce848e91eb3a10de288b38c/dgrdqdcmqtbxotjzetqk)\n\n_One possible AI tool resolution workflow_"
          },
          "importance": "75",
          "grade": "90",
          "isValid": true
        }
      ],
      "grade": 85
    },
    {
      "agentId": "research-scholar",
      "costInCents": 7574,
      "createdAt": "2025-04-16T08:43:10.746Z",
      "runDetails": "{\"model\":\"anthropic/claude-3.7-sonnet\",\"promptTokens\":5845,\"completionTokens\":1729,\"totalTokens\":7574,\"temperature\":0.1,\"runtimeMs\":33564}",
      "thinking": "This document discusses using AI systems to resolve forecasting questions on platforms like Manifold and Metaculus. It introduces the concept of \"Epistemic Selection Protocols\" - methods for choosing the best AI tool at resolution time rather than committing to a specific tool now. The paper explores various options, complications, and potential solutions.\n\nAs a Research Scholar, I need to identify relevant academic papers, blog posts, and other resources that relate to:\n\n1. AI systems for forecasting and question resolution\n2. Prediction markets and platforms (Manifold, Metaculus)\n3. Epistemic benchmarks for AI systems\n4. AI evaluation methodologies\n5. Goodharting and optimization problems\n6. AI tool composition and routing\n7. Ideological differences in AI evaluations\n\nI should focus on high-quality, authoritative sources that provide context, background, or complementary perspectives to the ideas presented. For each highlighted section, I'll need to connect specific references that enhance understanding.\n\nLet me identify the key sections that would benefit from external resources:\n\n1. The concept of using AIs as question evaluators\n2. Existing AI benchmarks like TruthfulQA\n3. Community Notes algorithm mentioned as a potential model\n4. The concept of Goodharting and how to avoid it\n5. Scorable Functions mentioned as a useful format\n6. AI tool composition and routing\n\nI'll search for academic papers, blog posts, and forum discussions that provide deeper context on these topics. I'll prioritize sources that are directly relevant, from authoritative authors, and that provide practical implementations or theoretical foundations for the concepts discussed.\n\nFor each resource, I'll evaluate its importance based on authority, relevance, quality, and impact. I'll organize these into a structured markdown table with Name, Link, Relevance, and Importance columns. For highlighted sections, I'll connect specific references and explain how they enhance understanding.",
      "summary": "This document explores using AI systems to resolve forecasting questions on platforms like Manifold and Metaculus, introducing the concept of \"Epistemic Selection Protocols\" for choosing optimal AI tools at resolution time. From a research perspective, the document connects to several important areas of existing literature but would benefit from more explicit references to established work. Key areas for enrichment include AI benchmarking frameworks beyond TruthfulQA, prediction market research, collective intelligence algorithms like Community Notes, formal treatments of Goodharting, implementation details for Scorable Functions, and technical approaches to AI tool composition. The document presents innovative ideas that build upon existing research across multiple domains, and providing these connections would strengthen its foundation and practical applicability.",
      "comments": [
        {
          "title": "Prediction Market Research and Implementation",
          "description": "The document discusses using AI for resolving questions on platforms like Manifold and Metaculus, but would benefit from connecting to the broader literature on prediction markets and forecasting platforms. Research from Philip Tetlock on superforecasting, Robin Hanson's work on prediction markets, and practical implementations like Polymarket and Augur provide important context on how these markets function and the challenges of question resolution. These resources would help readers understand the current state of prediction markets and how AI resolution might improve them.",
          "highlight": {
            "startOffset": 0,
            "endOffset": 0,
            "quotedText": ""
          },
          "importance": "90",
          "isValid": false,
          "error": "End text not found in document"
        },
        {
          "title": "Community Notes Algorithm and Collective Intelligence",
          "description": "The document references the Community Notes algorithm as a potential model for inter-AI trust ratings. Connecting to Vitalik Buterin's detailed analysis of this algorithm, Twitter's technical documentation, and research on collective intelligence systems would provide readers with a deeper understanding of how such systems work and their potential application to AI selection. These resources explain the mathematical foundations and practical implementations of consensus-building algorithms that could be adapted for AI tool selection.",
          "highlight": {
            "startOffset": 0,
            "endOffset": 0,
            "quotedText": ""
          },
          "importance": "80",
          "isValid": false,
          "error": "End text not found in document"
        },
        {
          "title": "Scorable Functions and Algorithmic Forecasting",
          "description": "The document mentions Scorable Functions as a potentially useful format but provides limited context. Connecting to the original EA Forum post by Ozzie Gooen and related work on algorithmic forecasting would give readers a more comprehensive understanding of this approach. These resources explain how scorable functions can be implemented in practice and their advantages for complex evaluations involving different ideological perspectives.",
          "highlight": {
            "startOffset": 0,
            "endOffset": 0,
            "quotedText": ""
          },
          "importance": "75",
          "isValid": false,
          "error": "Start text not found in document"
        },
        {
          "title": "AI Benchmarks for Epistemic Evaluation",
          "description": "The document mentions TruthfulQA as an example of existing AI benchmarks, but there are many more comprehensive benchmarks specifically designed to evaluate AI reasoning and truthfulness. Resources like the HELM benchmark suite from Stanford, the Anthropic Evals framework, and the Alignment Research Center's evaluations provide more robust frameworks for assessing AI epistemics. These resources would strengthen the proposed Epistemic Selection Protocols by providing established methodologies for evaluating AI reasoning capabilities.",
          "highlight": {
            "startOffset": 5580,
            "endOffset": 5743,
            "quotedText": "There’s currently a wide variety of AI benchmarks. For example, [TruthfulQA](https://github.com/sylinrl/TruthfulQA) and various investigations of model sycophancy."
          },
          "importance": "85",
          "isValid": true
        },
        {
          "title": "Goodharting and Optimization Challenges",
          "description": "The document discusses Goodharting as a potential complication but could benefit from connecting to the substantial literature on this topic. Research from Scott Garrabrant, Stuart Armstrong, and others provides formal definitions and analysis of Goodhart's Law in AI systems. The Alignment Forum has extensive discussions on this topic that would help readers understand the theoretical foundations and practical implications of Goodharting in AI evaluation contexts.",
          "highlight": {
            "startOffset": 13929,
            "endOffset": 14816,
            "quotedText": "Complication 3: Goodharting**\n\nWe’d want to avoid a situation where one tool technically maximizes a narrow “_Epistemic Selection Protocol_”, but is actually poor at doing many of the things we want from a resolver AI.\n\nTo get around this, the Protocol could make specifications like the following:\n\n> What will be the most epistemically-capable service on \\[Date\\] that satisfies the following requirements?\n> \n> 1.  Costs under $20 per run.\n> 2.  Is publicly available.\n> 3.  Has over 1000 human users per month (_this is to ensure there’s no bottleneck that’s hard to otherwise specify._)\n> 4.  Completes runs within 10 minutes.\n> 5.  Has been separately reviewed to not have significantly and deceivingly goodharted on this specific benchmark.\n\nIt’s often possible to get around Goodharting by applying additional layers of complexity. Whether it’s worth it depends on the situation."
          },
          "importance": "85",
          "isValid": true
        },
        {
          "title": "AI Tool Composition and Routing Systems",
          "description": "The document discusses AI tool composition and routing as a solution to different AI tools having different strengths. Connecting to research on AI orchestration, LLM routing frameworks like LangChain and AutoGPT, and academic papers on ensemble methods would provide technical depth on how such systems can be implemented. These resources offer practical approaches to building composite AI systems that leverage the strengths of multiple specialized models.",
          "highlight": {
            "startOffset": 16226,
            "endOffset": 16625,
            "quotedText": "An obvious answer is to then create simple compositions of AI tools. A router can be used to send specific requests or sub-requests to other AI tools that are best equipped to handle them.\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/45b2e7a9a88aae09631e65b177bafd33ce7c5c951ce848e91eb3a10de288b38c/dgrdqdcmqtbxotjzetqk)\n\n_One possible AI tool resolution workflow"
          },
          "importance": "80",
          "isValid": true
        }
      ]
    },
    {
      "agentId": "fake-eliezer",
      "costInCents": 7982,
      "createdAt": "2025-04-16T08:46:23.977Z",
      "runDetails": "{\"model\":\"anthropic/claude-3.7-sonnet\",\"promptTokens\":5594,\"completionTokens\":2388,\"totalTokens\":7982,\"temperature\":0.1,\"runtimeMs\":46215}",
      "thinking": "This document discusses using AI systems to resolve forecasting questions on platforms like Manifold and Metaculus, proposing \"Epistemic Selection Protocols\" to choose the best AI tools for this purpose in the future. I need to analyze this from Eliezer Yudkowsky's perspective, focusing on alignment concerns, rationality principles, and potential risks.\n\nFirst, I notice the document takes a fairly optimistic view of AI capabilities for resolving complex questions, with relatively little discussion of alignment risks or the deeper philosophical problems of value loading. Eliezer would likely be concerned about the implicit assumption that AI systems will become increasingly trustworthy evaluators without addressing fundamental alignment challenges.\n\nThe concept of \"Epistemic Selection Protocols\" is interesting but potentially naive from Eliezer's perspective. The document assumes we can develop reliable protocols for selecting trustworthy AI systems, but doesn't adequately address how we'd ensure these protocols themselves remain aligned with human values and don't fall prey to Goodhart's Law or other optimization pitfalls.\n\nThe \"Short Story\" section presents a relatively smooth progression toward increasingly trusted AI evaluators, eventually replacing humans for important analyses and decisions. This conflicts with Eliezer's view that advanced AI systems pose existential risks that won't be solved through incremental improvements in capabilities or evaluation metrics.\n\nThe document does acknowledge some complications, including Goodharting, but treats them as technical challenges that can be overcome rather than fundamental obstacles. Eliezer would likely view these as symptoms of deeper alignment problems that can't be solved through the approaches described.\n\nThe overall framing assumes we'll have increasingly capable and trustworthy AI systems that we can safely delegate evaluation tasks to. This conflicts with Eliezer's view that increased capabilities without solving alignment leads to increased risk, not increased trustworthiness.\n\nThe document also doesn't address how we'd verify that AI evaluations are actually correct for complex, speculative questions where ground truth is unavailable or only available far in the future. This relates to Eliezer's concerns about the difficulty of evaluating AI systems on tasks where humans can't directly verify the results.\n\nFinally, the document seems to assume a relatively slow and controlled progression of AI capabilities, where we have time to develop and refine these protocols. Eliezer would likely view this as dangerously optimistic given the rapid pace of AI development and the risks of sudden capability jumps.",
      "summary": "This document proposes using AI systems to resolve forecasting questions through 'Epistemic Selection Protocols,' but fundamentally misunderstands the nature and risks of advanced AI. It displays dangerous optimism about AI alignment, treating it as a technical problem that will be solved incidentally through capability improvements. The proposal ignores the orthogonality thesis and assumes trustworthiness increases with capability. The treatment of Goodharting is superficial, failing to recognize it as a fundamental principle rather than a technical obstacle. The document describes a concerning trajectory where humans gradually cede evaluation and decision-making authority to AI systems without addressing the existential risks this creates. It assumes a gradual, controlled progression of AI capabilities when rapid jumps are more likely. Most critically, it fails to address the core verification problem: if we can't independently verify the answers to speculative questions, we have no reliable way to determine which AI systems are actually trustworthy. This creates a dangerous circularity that undermines the entire proposal.",
      "comments": [
        {
          "title": "Inadequate Consideration of Ideological Capture",
          "description": "The document acknowledges that different ideological frameworks will lead to different evaluations, but treats this as a technical problem to be solved through personalization or representation of multiple perspectives. This misses the deeper issue that AI systems will inevitably embed particular ideological assumptions, and the process of selecting 'trusted' AI evaluators will likely amplify certain ideological perspectives while marginalizing others. The power to define what counts as 'epistemic' or 'trustworthy' is itself a form of ideological power that the document doesn't adequately address.",
          "importance": "80",
          "grade": "35",
          "highlight": {
            "startOffset": 0,
            "endOffset": 0,
            "quotedText": ""
          },
          "isValid": false,
          "error": "Could not find valid highlight text"
        },
        {
          "title": "Misunderstanding of AI Risk Trajectory",
          "description": "The document presents a gradual, controlled progression of AI capabilities and trust, with humans maintaining oversight throughout. This fundamentally misunderstands the likely trajectory of AI development, which could involve rapid capability jumps rather than smooth progression. The assumption that we'll have time to develop, refine, and validate these protocols before AI systems become powerful enough to pose existential risks is dangerously optimistic. The timeline described (2025-2030) is precisely when we might face the most acute alignment challenges.",
          "importance": "90",
          "grade": "20",
          "highlight": {
            "startOffset": 1170,
            "endOffset": 11951,
            "quotedText": "A Short Story of How This Plays Out\",_ which is perhaps the most accessible part of this for most readers.  \n    \n8.  We list a bunch of potential complications with AI tools and Epistemic Selection Protocols. While these will be complex, we see no fundamental bottlenecks. \n    \n\n## Motivation\n\nSay we want to create a prediction tournament on a complex question.\n\nFor example:\n\n*   _“Will there be over 10 million people killed in global wars between 2025 and 2030?”_\n*   _“Will IT security incidents be a national US concern, on the scale of the top 5 concerns, in 2030?”_\n*   _“Will bottlenecks in power capacity limit AIs by over 50%, in 2030?”_\n\nWhile these questions involve subjective elements, they aren't purely matters of opinion. Come 2030, though people might debate the details, informed observers would likely reach broadly similar conclusions.\n\nCurrently, such questions are typically resolved by human evaluators. On [Manifold](https://manifold.markets/home), the question author serves as judge, with authors developing reputations for their judgment quality over time. [Metaculus](https://www.metaculus.com) takes a different approach for more complex and subjective questions, often employing small panels of subject matter experts.\n\nHowever, human evaluators have several limitations:\n\n1.  High costs, particularly for domain experts\n2.  Limited track records of evaluation quality\n3.  Poor accessibility - most forecasters cannot directly query them\n4.  Inconsistency over time as their views and motivations evolve\n5.  Uncertain long-term availability\n6.  Vulnerability to biases and potential conflicts of interest\n\n## AIs as Question Evaluators\n\nInstead of using humans to resolve thorny questions, we can use AIs. There are many ways we could attempt to do this, so we’ll walk through a few examples.\n\n**Option 1: Use an LLM**\n\nThe first option to consider is to use an LLM as an evaluator. For example, write,  \n“_This question will be judged by Claude 3.5 Sonnet (2024-06-20). The specific prompt will be, …”_\n\nThis style is replicable, simple, and inexpensive. However, it clearly has some downsides. The first obvious one is that Claude 3.5 Sonnet doesn’t perform web searches, so its knowledge would likely be too limited to resolve future forecasting questions.\n\n**Option 2: Use an AI Tool with Search**\n\nInstead of using a standard LLM, you might want to use a tool that uses both LLMs and web searches. [Perplexity](https://www.perplexity.ai/) might be the most famous one now, but other advanced research assistants are starting to come out. In theory one should be able to set a research budget that’s in line with the importance and complexity of the question.\n\nThere's been some experimentation here. See [this guide](https://predictionmarkets.miraheze.org/wiki/GPT4_Headline_Markets) on using GPT4 for resolving questions on Manifold, and [this](https://predictionmarkets.miraheze.org/wiki/GPT4_Manifold_Experiment) experiment write up. GPT4 allows for basic web searches.\n\nThis option is probably better than Option 1 for most things. But there are still problems. The next major one is the risk that Perplexity, or any other single tool we can point to now, won’t be the leading one in the future. The field is moving rapidly, it’s difficult to tell which tools will even exist in 5 years, let alone be the preferred options.\n\n**Option 3: Use an “**_**Epistemic**_**” Selection Protocol**\n\nIn this case, one doesn't select a specific AI tool. Instead one selects a process or protocol that selects an AI tool.\n\nFor example:  \n_“In 2030, we will resolve this question using the leading AI tool on the ‘Forbes 2030 Most trusted AI tools’ list.”_\n\nWe’re looking for AI tools that are “_trusted to reason about complex, often speculative or political matters._” This arguably can be more quickly expressed as searching for the tool with the best _epistemics_.\n\n## _Epistemic Selection Protocols_ (Or, how do we choose the best AI tool to use?)\n\nArguably, AI Epistemic Selection Protocols can be the best choice of the above options, if one could implement them effectively, for most 2+ year questions. There are a lot of potential processes to choose, though most would be too complicated to be worthwhile. We want to strike a balance between simplicity and accuracy.\n\nLet’s first list the most obvious options.\n\n**Option 1: Trusted and formalized epistemic evaluations**\n\nThere’s currently a wide variety of AI benchmarks. For example, [TruthfulQA](https://github.com/sylinrl/TruthfulQA) and various investigations of model sycophancy. But arguably, none of these would be great proxies for which AI tool would be the most trusted question resolvers in the future. Newer, deliberate benchmarks could help here.\n\nExample:  \n_“This forecasting question will be resolved, using whichever AI Tool does the best on Complete Epistemic Benchmark X, and can be used for less than $20.”_\n\n**Option 2: Human-derived trust rankings**\n\nHumans could simply be polled on which AI tools they regard as the most trustworthy. Different groups of humans would have different preferences, so the group would need to be specified in advance for an AI Selection Process.\n\nExample:  \n_“This forecasting question will be resolved, using whichever AI Tool is on the top of the list of ‘Most trusted AI Tools’ on LessWrong, and can be used for less than $20.”_\n\n**Option 3: Inter-AI trust ratings**\n\nAI tools could select future AI tools to use. This could be a 1-step solution, where an open-source or standardized (for the sake of ensuring it will be available long-term) solution is asked to identify the best available candidate. Or it could be a multiple-step solution, where perhaps AI tools are asked to recommend each other using some simple algorithm. This can be similar in concept to the [Community Notes](https://vitalik.eth.limo/general/2023/08/16/communitynotes.html) algorithm.\n\nExample:  \n_“This forecasting question will be resolved, using whichever AI Tool wins a poll of the ‘Most trusted AI tools’ according to AI tools.’ In this poll, each AI tool will recommend its favorite of the other available candidates.”_ (Note: This specific proposal can be gamed, so greater complexity will likely be required.)\n\n## A Short Story of How This Plays Out\n\nIn 2025, several question writers on Manifold experiment with AI resolution systems. Some questions include:  \n  \n_“Will California fires in 2026 be worse than those in 2025? To answer this, I’ll ask Perplexity on Jan 1, 2026. My prompt will be, \\[Will California fires in 2026 be worse than those in 2025? Judge this by guessing the total economic loss.\\]”_\n\n_“How many employees will OpenAI have in Dec 2025? To answer this, I’ll first ask commenters to write arguments and/or facts that they’ve found on this. I’ll filter this for what seems accurate, then I’ll paste this into Perplexity. I’ll call Perplexity 5 times, and average the results.”_\n\nForecasters gradually identify the uses and limitations of such systems. It turns out they are surprisingly bad at advanced physics questions, for some surprising reason. There are a few clever prompting strategies that help ensure that these AIs put out more consistent results.\n\nAI tools like Perplexity also get very good at hunting down and answering questions that are straightforward to resolve. Manifold adds custom functionality to do this. For example, say someone writes a question, _“What Movie Will Win The 2025 Oscars For Best Picture?”_ When they do, they’ll be given the option to have a Manifold AI system automatically make a suggested guess for them, at the time of expected question resolution. These guesses will begin with high error rates (10%), but these will gradually drop.\n\nSeparately, various epistemic evaluations are established. There are multiple public and private rankings. There are also surveys of the “_Most Trusted AIs_”, held on various platforms such as Manifold, LessWrong, and The Verge. Leading consumer product review websites such as Consumer Reports and Wirecutter begin to have ratings for AI tools, using defined categories such as “_accuracy_” and “_reasonableness_.”\n\nOne example question from this is:  \n_“In 2030, will it seem like o1 was an important AI development, that was at least as innovative and important as GPT4? This will be resolved using whichever AI leads the “Most trusted AIs” poll on Manifold in 2029.”_\n\nThere will be a long tail of AI tools that are proposed as contenders for epistemic benchmarks. Most of the options are simply minor tweaks on other options or light routers. Few of these will get the full standard evaluations, but good proxies will emerge. It turns out that you can get a decent measure by using the top fully-evaluated AI systems to evaluate more niche systems.\n\nIn 2027, there will be a significant amount of understanding, buy-in, and sophistication with such systems (at least among a few niche communities, like Manifold users). This will make it possible to scale them for more ambitious uses.\n\nMetaculus runs some competitions that include:\n\n_“What is the_ [_relative value_](https://forum.effectivealtruism.org/posts/EFEwBvuDrTLDndqCt/relative-value-functions-a-flexible-new-format-for-value) _of each of \\[the top 500 AI safety papers of 2026\\]? This will be resolved in 2030 by using the most trusted AI system, via LessWrong or The Economist, at that time. This AI will order all of the papers - forecasters should estimate the percentile that each paper will achieve.”_\n\n_“What is the expected value of every biosafety organization in 2027, estimated as what Open Philanthropy would have paid for it from their biosafety funding pool in 2027? This will be judged in 2029, by the most trusted AI system, with a budget of $1,000 for each evaluation.”_\n\nAround this time, some researchers will begin to make wider kinds of analyses, and forecast compressions.\n\n_“How will the top epistemic model of 2030 evaluate the accuracy and value of the claims of each of the top 100 intellectuals from 2027?”_\n\n_“Will the top epistemic model of 2030 consider the current top epistemic models to be ‘highly overconfident’ for at least 10% of the normative questions they are asked?”_\n\nThe top trusted AI tools start to become frequent ways to second-guess humans. For example, if a boss makes a controversial decision, people could contest the decision if top AI tools back them up. Similar analyses would be used within governments.\n\nAs these AI tools become even more trusted, they will replace many humans for important analyses and decisions. Humans will spend a great deal of effort focused on assuring these AI tools are doing a good job, and they'll have the time for that because there will be few other things they need to directly evaluate or oversee."
          }
        },
        {
          "title": "Failure to Address Distributional Shift",
          "description": "The document doesn't adequately address how AI evaluators would handle distributional shift - the tendency for real-world conditions to drift away from the data these systems were trained on. For long-term forecasting questions, this is particularly problematic. An AI system might perform well on current evaluation metrics but fail catastrophically when faced with novel situations or concepts that weren't present in its training data. This relates to the fundamental difficulty of creating AI systems that can reliably generalize beyond their training distribution.",
          "importance": "85",
          "grade": "25",
          "highlight": {
            "startOffset": 8382,
            "endOffset": 8899,
            "quotedText": "AI tools like Perplexity also get very good at hunting down and answering questions that are straightforward to resolve. Manifold adds custom functionality to do this. For example, say someone writes a question, _“What Movie Will Win The 2025 Oscars For Best Picture?”_ When they do, they’ll be given the option to have a Manifold AI system automatically make a suggested guess for them, at the time of expected question resolution. These guesses will begin with high error rates (10%), but these will gradually drop."
          }
        },
        {
          "title": "Dangerous Delegation of Authority",
          "description": "The document describes a future where AI tools 'replace many humans for important analyses and decisions' with humans merely assuring these tools are 'doing a good job.' This represents a dangerous delegation of authority to systems that fundamentally don't share human values. The assumption that humans will maintain meaningful oversight while delegating most evaluation and decision-making is contradictory - if humans are no longer directly engaged in these processes, their ability to provide meaningful oversight will atrophy. This creates perfect conditions for a treacherous turn or gradual value drift.",
          "importance": "95",
          "grade": "15",
          "highlight": {
            "startOffset": 11375,
            "endOffset": 11951,
            "quotedText": "The top trusted AI tools start to become frequent ways to second-guess humans. For example, if a boss makes a controversial decision, people could contest the decision if top AI tools back them up. Similar analyses would be used within governments.\n\nAs these AI tools become even more trusted, they will replace many humans for important analyses and decisions. Humans will spend a great deal of effort focused on assuring these AI tools are doing a good job, and they'll have the time for that because there will be few other things they need to directly evaluate or oversee."
          }
        },
        {
          "title": "Naive Optimism About AI Alignment",
          "description": "The document assumes that increasingly capable AI systems will become more trustworthy evaluators without addressing fundamental alignment challenges. This reflects a dangerous optimism that ignores the core difficulty of aligning advanced AI systems with human values. The progression described - from simple evaluation tasks to replacing humans for 'important analyses and decisions' - assumes alignment problems will be solved incidentally through capability improvements, which contradicts everything we know about the orthogonality of capabilities and alignment.",
          "importance": "95",
          "grade": "20",
          "highlight": {
            "startOffset": 11625,
            "endOffset": 11951,
            "quotedText": "As these AI tools become even more trusted, they will replace many humans for important analyses and decisions. Humans will spend a great deal of effort focused on assuring these AI tools are doing a good job, and they'll have the time for that because there will be few other things they need to directly evaluate or oversee."
          }
        },
        {
          "title": "Inadequate Treatment of Verification Problems",
          "description": "The document acknowledges the lack of ground truth for many questions but fails to address the fundamental epistemological problem this creates. If we can't verify whether an AI's answers are correct (because the questions are inherently speculative or subjective), then we have no reliable way to determine which AI systems are actually more 'trustworthy' or have better 'epistemics.' This creates a circular dependency: we need trustworthy AI to evaluate questions, but we need to evaluate questions to determine which AI is trustworthy. The proposed solutions like calibration don't solve this fundamental problem.",
          "importance": "85",
          "grade": "30",
          "highlight": {
            "startOffset": 12619,
            "endOffset": 13925,
            "quotedText": "Complication 2: Lack of Ground Truth**\n\nMany subjective and speculative questions lack definitive answers. Some questions can only be answered long after the information would be useful, while others are inherently impossible to resolve with complete certainty.\n\nIn such cases, the goal should shift from seeking perfect precision to outperforming alternative evaluation methods. Success means providing better answers than existing approaches, given practical constraints of cost, compute, and time.\n\nAI evaluators should prioritize two key aspects:\n\n1.  Calibration: Systems should express appropriate levels of certainty, aligned with the reference frame they're operating from\n2.  Resolution: Within the bounds of reliable calibration, provide the most detailed and specific answers possible\n\nFor example, consider the question \"_What will be the economic impact of California wildfires in 2028?_\" While a perfectly precise answer is impossible, AI systems can progressively approach better estimates by:\n\n*   Aggregating multiple data sources\n*   Explicitly modeling uncertainties\n*   Identifying and accounting for measurement limitations\n*   Clearly stating assumptions and confidence levels\n\nAs long as a resolution is calibrated and fairly unbiased, it can be incentive-compatible for forecasting."
          }
        },
        {
          "title": "Goodharting and Optimization Pressure",
          "description": "While the document acknowledges Goodharting as a complication, it treats it as a technical challenge that can be overcome with 'additional layers of complexity.' This fundamentally misunderstands the nature of Goodhart's Law, which is not merely a technical obstacle but a deep principle about optimization processes. When we optimize systems (including AI selection protocols) toward any explicit metric, the metric ceases to be a good measure of what we actually value. The proposed solutions don't address the core issue that any formalized selection protocol will create optimization pressure that distorts the very qualities we're trying to measure.",
          "importance": "90",
          "grade": "25",
          "highlight": {
            "startOffset": 13766,
            "endOffset": 14646,
            "quotedText": "  Clearly stating assumptions and confidence levels\n\nAs long as a resolution is calibrated and fairly unbiased, it can be incentive-compatible for forecasting.\n\n**Complication 3: Goodharting**\n\nWe’d want to avoid a situation where one tool technically maximizes a narrow “_Epistemic Selection Protocol_”, but is actually poor at doing many of the things we want from a resolver AI.\n\nTo get around this, the Protocol could make specifications like the following:\n\n> What will be the most epistemically-capable service on \\[Date\\] that satisfies the following requirements?\n> \n> 1.  Costs under $20 per run.\n> 2.  Is publicly available.\n> 3.  Has over 1000 human users per month (_this is to ensure there’s no bottleneck that’s hard to otherwise specify._)\n> 4.  Completes runs within 10 minutes.\n> 5.  Has been separately reviewed to not have significantly and deceivingly goodhart"
          }
        }
      ],
      "grade": 25
    },
    {
      "agentId": "quantitative-forecaster",
      "costInCents": 7985,
      "createdAt": "2025-04-16T08:47:19.257Z",
      "runDetails": "{\"model\":\"anthropic/claude-3.7-sonnet\",\"promptTokens\":5517,\"completionTokens\":2468,\"totalTokens\":7985,\"temperature\":0.1,\"runtimeMs\":50132}",
      "thinking": "I'm analyzing a document about using AI systems to resolve forecasting questions on platforms like Manifold and Metaculus. The document proposes that AI tools could eventually replace human judges for resolving prediction questions, especially for long-term forecasts. It introduces the concept of \"Epistemic Selection Protocols\" - methods for choosing which AI tool to use at resolution time, rather than committing to a specific tool that might become obsolete.\n\nFrom a quantitative forecasting perspective, this is highly relevant to my expertise. Let me break down the key aspects:\n\n1. **Current state of AI resolution**: The document suggests AI tools are currently \"mediocre\" at judging subjective questions but are improving rapidly. This is a testable claim that could be quantified - what's the current accuracy rate of AI tools on different types of forecasting questions compared to human judges?\n\n2. **Epistemic Selection Protocols**: The document proposes several methods for selecting which AI tool to use at resolution time, including formal benchmarks, human trust rankings, and inter-AI ratings. These are interesting approaches, but I notice a lack of quantitative analysis about their relative merits or potential failure modes.\n\n3. **Timeline projections**: The document includes a speculative timeline about how AI resolution might evolve from 2025-2030. This is presented without explicit probability estimates or confidence intervals, which would be valuable additions.\n\n4. **Complications**: The document identifies several potential issues with AI resolution, including lack of sufficient tools, lack of ground truth, Goodharting, ideological differences, and tools with different strengths. These are well-identified, but again lack quantitative assessment of their likelihood or impact.\n\nFrom my perspective as a quantitative forecaster, this document presents an interesting conceptual framework but would benefit from more concrete probability estimates, explicit modeling of uncertainties, and quantification of the relative merits of different approaches. I'll focus my comments on these aspects.\n\nI'm particularly interested in:\n- The timeline for AI tools becoming sufficiently capable for different types of question resolution\n- The quantifiable advantages of AI resolution over human resolution\n- The probability of various failure modes occurring\n- How we might measure and track progress in this domain\n- The implications for forecasting platforms and prediction markets",
      "summary": "This document presents an interesting conceptual framework for using AI to resolve forecasting questions but falls short on quantitative rigor. It lacks explicit probability estimates for its timeline projections (2025-2030), doesn't quantify the current or projected accuracy of AI resolution tools, and fails to provide numerical assessments of the various complications identified. The proposed Epistemic Selection Protocols would benefit from reference class forecasting and sensitivity analysis to evaluate their relative merits. From a quantitative forecasting perspective, the document presents valuable ideas but needs more concrete probability estimates, confidence intervals, and decomposition of complex forecasts into measurable components. The absence of Fermi estimates for key claims (like AI tool accuracy improvement rates) and the lack of Bayesian frameworks for updating trust in AI tools are significant weaknesses that limit the document's utility for making well-calibrated predictions about this domain.",
      "comments": [
        {
          "title": "Missing Fermi Estimate on AI Resolution Accuracy",
          "description": "The document claims current AI tools are \"mediocre\" at judging subjective questions but doesn't quantify this assessment. A proper Fermi estimate would break down current accuracy rates by question type, compare them to human baselines, and project improvement rates based on historical AI progress. For example: \"Current AI tools might achieve 65-75% agreement with expert human judges on moderately subjective questions, compared to 80-90% inter-rater reliability among experts themselves. Based on recent improvement rates of 5-10 percentage points per year in related benchmarks, we might expect AI tools to match human reliability by 2026-2028.\"",
          "importance": "90",
          "grade": "35",
          "highlight": {
            "startOffset": 279,
            "endOffset": 424,
            "quotedText": "Current AI tools (combinations of LLM calls and software) are mediocre at judging subjective or speculative questions. We expect them to improve."
          }
        },
        {
          "title": "Quantifiable Advantages of AI Resolution",
          "description": "The document lists several advantages of AI over human evaluators (cost, accessibility, consistency, etc.) but doesn't attempt to quantify these benefits. A stronger approach would estimate the magnitude of each advantage - e.g., \"AI resolution might cost $5-20 per question versus $50-500 for expert human panels, representing a 90-95% cost reduction\" or \"Human evaluators show 15-25% inconsistency in judgments over time periods >1 year, while properly designed AI systems might maintain <5% inconsistency.\" These quantitative estimates would make the case for AI resolution more compelling and testable.",
          "importance": "85",
          "grade": "45",
          "highlight": {
            "startOffset": 2424,
            "endOffset": 2808,
            "quotedText": "However, human evaluators have several limitations:\n\n1.  High costs, particularly for domain experts\n2.  Limited track records of evaluation quality\n3.  Poor accessibility - most forecasters cannot directly query them\n4.  Inconsistency over time as their views and motivations evolve\n5.  Uncertain long-term availability\n6.  Vulnerability to biases and potential conflicts of interest"
          }
        },
        {
          "title": "Reference Class Forecasting for AI Tool Selection",
          "description": "The document proposes several methods for selecting AI tools (benchmarks, human rankings, inter-AI ratings) but doesn't apply reference class forecasting to evaluate which methods are likely to be most effective. Historical analogies from other domains where selection protocols have been implemented (e.g., academic peer review, consumer product testing, algorithmic trading) could provide base rates for success and failure modes. For example: \"Similar multi-stakeholder selection protocols in other domains have a 60-70% track record of identifying the best option when stakeholders have aligned incentives, but drop to 30-40% effectiveness when strong competing interests exist.\"",
          "importance": "85",
          "grade": "50",
          "highlight": {
            "startOffset": 4972,
            "endOffset": 7296,
            "quotedText": "s arguably can be more quickly expressed as searching for the tool with the best _epistemics_.\n\n## _Epistemic Selection Protocols_ (Or, how do we choose the best AI tool to use?)\n\nArguably, AI Epistemic Selection Protocols can be the best choice of the above options, if one could implement them effectively, for most 2+ year questions. There are a lot of potential processes to choose, though most would be too complicated to be worthwhile. We want to strike a balance between simplicity and accuracy.\n\nLet’s first list the most obvious options.\n\n**Option 1: Trusted and formalized epistemic evaluations**\n\nThere’s currently a wide variety of AI benchmarks. For example, [TruthfulQA](https://github.com/sylinrl/TruthfulQA) and various investigations of model sycophancy. But arguably, none of these would be great proxies for which AI tool would be the most trusted question resolvers in the future. Newer, deliberate benchmarks could help here.\n\nExample:  \n_“This forecasting question will be resolved, using whichever AI Tool does the best on Complete Epistemic Benchmark X, and can be used for less than $20.”_\n\n**Option 2: Human-derived trust rankings**\n\nHumans could simply be polled on which AI tools they regard as the most trustworthy. Different groups of humans would have different preferences, so the group would need to be specified in advance for an AI Selection Process.\n\nExample:  \n_“This forecasting question will be resolved, using whichever AI Tool is on the top of the list of ‘Most trusted AI Tools’ on LessWrong, and can be used for less than $20.”_\n\n**Option 3: Inter-AI trust ratings**\n\nAI tools could select future AI tools to use. This could be a 1-step solution, where an open-source or standardized (for the sake of ensuring it will be available long-term) solution is asked to identify the best available candidate. Or it could be a multiple-step solution, where perhaps AI tools are asked to recommend each other using some simple algorithm. This can be similar in concept to the [Community Notes](https://vitalik.eth.limo/general/2023/08/16/communitynotes.html) algorithm.\n\nExample:  \n_“This forecasting question will be resolved, using whichever AI Tool wins a poll of the ‘Most trusted AI tools’ according to AI tools.’ In this poll, each AI tool will recommend its favorite of the other ava"
          }
        },
        {
          "title": "Lack of Quantitative Timeline Estimates",
          "description": "The document presents a speculative timeline for AI resolution adoption from 2025-2030 without providing explicit probability estimates or confidence intervals. As a quantitative forecaster, I would expect to see estimates like: \"There's a 70% chance that by 2027, AI tools will be trusted for resolving at least 30% of questions on platforms like Manifold.\" Without such quantification, it's difficult to evaluate the plausibility of the timeline or track progress against expectations.",
          "importance": "95",
          "grade": "40",
          "highlight": {
            "startOffset": 7450,
            "endOffset": 11736,
            "quotedText": "In 2025, several question writers on Manifold experiment with AI resolution systems. Some questions include:  \n  \n_“Will California fires in 2026 be worse than those in 2025? To answer this, I’ll ask Perplexity on Jan 1, 2026. My prompt will be, \\[Will California fires in 2026 be worse than those in 2025? Judge this by guessing the total economic loss.\\]”_\n\n_“How many employees will OpenAI have in Dec 2025? To answer this, I’ll first ask commenters to write arguments and/or facts that they’ve found on this. I’ll filter this for what seems accurate, then I’ll paste this into Perplexity. I’ll call Perplexity 5 times, and average the results.”_\n\nForecasters gradually identify the uses and limitations of such systems. It turns out they are surprisingly bad at advanced physics questions, for some surprising reason. There are a few clever prompting strategies that help ensure that these AIs put out more consistent results.\n\nAI tools like Perplexity also get very good at hunting down and answering questions that are straightforward to resolve. Manifold adds custom functionality to do this. For example, say someone writes a question, _“What Movie Will Win The 2025 Oscars For Best Picture?”_ When they do, they’ll be given the option to have a Manifold AI system automatically make a suggested guess for them, at the time of expected question resolution. These guesses will begin with high error rates (10%), but these will gradually drop.\n\nSeparately, various epistemic evaluations are established. There are multiple public and private rankings. There are also surveys of the “_Most Trusted AIs_”, held on various platforms such as Manifold, LessWrong, and The Verge. Leading consumer product review websites such as Consumer Reports and Wirecutter begin to have ratings for AI tools, using defined categories such as “_accuracy_” and “_reasonableness_.”\n\nOne example question from this is:  \n_“In 2030, will it seem like o1 was an important AI development, that was at least as innovative and important as GPT4? This will be resolved using whichever AI leads the “Most trusted AIs” poll on Manifold in 2029.”_\n\nThere will be a long tail of AI tools that are proposed as contenders for epistemic benchmarks. Most of the options are simply minor tweaks on other options or light routers. Few of these will get the full standard evaluations, but good proxies will emerge. It turns out that you can get a decent measure by using the top fully-evaluated AI systems to evaluate more niche systems.\n\nIn 2027, there will be a significant amount of understanding, buy-in, and sophistication with such systems (at least among a few niche communities, like Manifold users). This will make it possible to scale them for more ambitious uses.\n\nMetaculus runs some competitions that include:\n\n_“What is the_ [_relative value_](https://forum.effectivealtruism.org/posts/EFEwBvuDrTLDndqCt/relative-value-functions-a-flexible-new-format-for-value) _of each of \\[the top 500 AI safety papers of 2026\\]? This will be resolved in 2030 by using the most trusted AI system, via LessWrong or The Economist, at that time. This AI will order all of the papers - forecasters should estimate the percentile that each paper will achieve.”_\n\n_“What is the expected value of every biosafety organization in 2027, estimated as what Open Philanthropy would have paid for it from their biosafety funding pool in 2027? This will be judged in 2029, by the most trusted AI system, with a budget of $1,000 for each evaluation.”_\n\nAround this time, some researchers will begin to make wider kinds of analyses, and forecast compressions.\n\n_“How will the top epistemic model of 2030 evaluate the accuracy and value of the claims of each of the top 100 intellectuals from 2027?”_\n\n_“Will the top epistemic model of 2030 consider the current top epistemic models to be ‘highly overconfident’ for at least 10% of the normative questions they are asked?”_\n\nThe top trusted AI tools start to become frequent ways to second-guess humans. For example, if a boss makes a controversial decision, people could contest the decision if top AI tools back them up. Similar analyses would be used within governments.\n\nAs these AI tools become even more trusted, they will replace many humans for important analyses and decisions."
          }
        },
        {
          "title": "Probability Assessment of Complications",
          "description": "The document identifies several potential complications with AI resolution but doesn't assess their likelihood or impact. A quantitative forecasting approach would assign probabilities to each complication occurring and estimate their potential impact. For example: \"There's a 70-80% chance that ideological differences will significantly affect AI resolution quality for politically sensitive questions through 2030, potentially reducing agreement with human consensus by 20-30 percentage points unless mitigated.\" This would help prioritize which complications deserve the most attention and investment to solve.",
          "importance": "80",
          "grade": "40",
          "highlight": {
            "startOffset": 11815,
            "endOffset": 17112,
            "quotedText": "e doing a good job, and they'll have the time for that because there will be few other things they need to directly evaluate or oversee. \n\n## Protocol Complications & Potential Solutions\n\n**Complication 1: Lack of Sufficient AI Tools**\n\nIn the beginning, we expect that many people won’t trust any AI tools to be adequate in resolving many questions. Even if tools look good in evaluations, it will take time for them to build trust.\n\nOne option is to set certain criteria for _sufficiency_. For example, one might say, “This question will be resolved using whichever AI system first gets to a 90/100 on the Epistemic Benchmark Evaluation…” This would clearly require understanding and trust in the evaluations, rather than in a specific tool, so this would require high-quality evaluations or polls.\n\n**Complication 2: Lack of Ground Truth**\n\nMany subjective and speculative questions lack definitive answers. Some questions can only be answered long after the information would be useful, while others are inherently impossible to resolve with complete certainty.\n\nIn such cases, the goal should shift from seeking perfect precision to outperforming alternative evaluation methods. Success means providing better answers than existing approaches, given practical constraints of cost, compute, and time.\n\nAI evaluators should prioritize two key aspects:\n\n1.  Calibration: Systems should express appropriate levels of certainty, aligned with the reference frame they're operating from\n2.  Resolution: Within the bounds of reliable calibration, provide the most detailed and specific answers possible\n\nFor example, consider the question \"_What will be the economic impact of California wildfires in 2028?_\" While a perfectly precise answer is impossible, AI systems can progressively approach better estimates by:\n\n*   Aggregating multiple data sources\n*   Explicitly modeling uncertainties\n*   Identifying and accounting for measurement limitations\n*   Clearly stating assumptions and confidence levels\n\nAs long as a resolution is calibrated and fairly unbiased, it can be incentive-compatible for forecasting.\n\n**Complication 3: Goodharting**\n\nWe’d want to avoid a situation where one tool technically maximizes a narrow “_Epistemic Selection Protocol_”, but is actually poor at doing many of the things we want from a resolver AI.\n\nTo get around this, the Protocol could make specifications like the following:\n\n> What will be the most epistemically-capable service on \\[Date\\] that satisfies the following requirements?\n> \n> 1.  Costs under $20 per run.\n> 2.  Is publicly available.\n> 3.  Has over 1000 human users per month (_this is to ensure there’s no bottleneck that’s hard to otherwise specify._)\n> 4.  Completes runs within 10 minutes.\n> 5.  Has been separately reviewed to not have significantly and deceivingly goodharted on this specific benchmark.\n\nIt’s often possible to get around Goodharting by applying additional layers of complexity. Whether it’s worth it depends on the situation.\n\n**Complication 4: Different Ideologies**\n\nConsider a question about the impact of a tax cut policy. People with different philosophical or ideological backgrounds will likely disagree on fundamental assumptions, making a single \"correct\" answer impossible.\n\nThe simplest solution—declaring it a \"complex issue with multiple valid perspectives\"—is typically close to useless. A more useful approach would be developing AI tools that can represent different ideological frameworks, either through multiple specialized systems or a single system with adjustable philosophical parameters.\n\nA more sophisticated approach could generate personalized evaluations based on individual characteristics and depth of analysis. For instance: \"_How would someone with background X evaluate California Proposition 10 in 2030, after studying it for \\[10, 100, 1000\\] hours?_\" This could be implemented using an algorithm that accounts for both personal attributes and time invested in understanding the issue. [Scorable Functions](https://forum.effectivealtruism.org/posts/ZWQ5jbLxpd4JtQgu4/scorable-functions-a-format-for-algorithmic-forecasting) might be a useful format.\n\n**Complication 5: AI Tools with Different Strengths**\n\nOne might ask:  \n_“What if different AI tools are epistemically dominant in different areas? For example, one is great at political science, and another is great at advanced mathematics.”_  \n  \nAn obvious answer is to then create simple compositions of AI tools. A router can be used to send specific requests or sub-requests to other AI tools that are best equipped to handle them.\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/45b2e7a9a88aae09631e65b177bafd33ce7c5c951ce848e91eb3a10de288b38c/dgrdqdcmqtbxotjzetqk)\n\n_One possible AI tool resolution workflow_\n\n**Complication 6: AI Tools that Recommend Other AI Tools**\n\nImagine there’s a situation where one AI tool is chosen, but that tool recommends a different tool instead. For example, Perplexity 3.0 is asked a question, and it responds by stating that Claude 4.5 could do a better job than it could. Arguably it would make a lot of sense that if an AI tool were highly trusted to make speculative judgements, it could be trusted to be correct when claiming that a different tool is super"
          }
        },
        {
          "title": "Bayesian Framework for Updating Trust in AI Tools",
          "description": "The document discusses trust in AI tools developing over time but doesn't propose a formal Bayesian framework for how this trust should be updated based on evidence. A quantitative approach would specify how much weight to give different types of evidence (e.g., benchmark performance, real-world track record, expert endorsements) and how quickly trust should update. For example: \"After observing 100 successful resolutions with >90% agreement with expert consensus, we might increase our confidence in an AI tool's reliability from 60% to 85%, but this update should be moderated by the diversity and difficulty of the questions resolved.\"",
          "importance": "80",
          "grade": "35",
          "highlight": {
            "startOffset": 11911,
            "endOffset": 12474,
            "quotedText": "ey need to directly evaluate or oversee. \n\n## Protocol Complications & Potential Solutions\n\n**Complication 1: Lack of Sufficient AI Tools**\n\nIn the beginning, we expect that many people won’t trust any AI tools to be adequate in resolving many questions. Even if tools look good in evaluations, it will take time for them to build trust.\n\nOne option is to set certain criteria for _sufficiency_. For example, one might say, “This question will be resolved using whichever AI system first gets to a 90/100 on the Epistemic Benchmark Evaluation…” This would clearly"
          }
        },
        {
          "title": "Confidence Intervals for AI Tool Sufficiency",
          "description": "The document mentions setting criteria for \"sufficiency\" of AI tools but doesn't provide a quantitative framework for determining these thresholds. A stronger approach would include confidence intervals and sensitivity analysis - e.g., \"At a 90/100 benchmark score, we estimate 80-90% agreement with expert human judgment on complex questions. Lowering the threshold to 80/100 might reduce agreement to 70-80% while increasing the pool of eligible AI tools by 200-300%.\" This would help question creators make informed tradeoffs between tool availability and resolution quality.",
          "importance": "75",
          "grade": "45",
          "highlight": {
            "startOffset": 12108,
            "endOffset": 12474,
            "quotedText": " any AI tools to be adequate in resolving many questions. Even if tools look good in evaluations, it will take time for them to build trust.\n\nOne option is to set certain criteria for _sufficiency_. For example, one might say, “This question will be resolved using whichever AI system first gets to a 90/100 on the Epistemic Benchmark Evaluation…” This would clearly"
          }
        }
      ],
      "grade": 45
    }
  ]
}