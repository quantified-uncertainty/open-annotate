{
  "id": "ai-for-resolving-forecasting-questions-an-early-exploration-ea-forum-bots",
  "slug": "ai-for-resolving-forecasting-questions-an-early-exploration-ea-forum-bots",
  "title": "AI for Resolving Forecasting Questions: An Early Exploration — EA Forum Bots",
  "author": "Ozzie Gooen",
  "publishedDate": "2025-01-16T21:40:02.752Z",
  "url": "https://forum.effectivealtruism.org/posts/pLkFmWP3xmGG8YvjP/ai-for-resolving-forecasting-questions-an-early-exploration",
  "intendedAgents": [
    "ea-impact-evaluator",
    "clarity-coach",
    "research-scholar",
    "quantitative-forecaster",
    "bias-detector"
  ],
  "platforms": [
    "EA Forum"
  ],
  "content": "_Thanks to Slava Matyuhin for comments_\n\n## Summary\n\n1.  AIs can be used to resolve forecasting questions on platforms like Manifold and Metaculus.\n    \n2.  AI question resolution, in theory, can be far more predictable, accessible, and inexpensive to human resolution.\n    \n3.  Current AI tools (combinations of LLM calls and software) are mediocre at judging subjective or speculative questions. We expect them to improve. \n    \n4.  AI tools are changing rapidly. If we want to organize forecasting questions to be resolved using an AI tool in 10 years, instead of selecting a specific tool, we might want to select a _protocol_ for choosing the best tool at resolution time. We call this an \"_Epistemic Selection Protocol_\". \n    \n5.  If we have a strong \"_Epistemic Selection Protocol_\" that we like, and expect that we'll have _sufficient_ AI tools in the near future, then we could arguably start to write forecasting questions for them right away. \n    \n6.  The methods listed above, if successful, wouldn't just be for resolving questions on prediction platforms. They could be generalized to pursue broad-scale and diverse evaluations. \n    \n7.  We provide a “_A Short Story of How This Plays Out\",_ which is perhaps the most accessible part of this for most readers.  \n    \n8.  We list a bunch of potential complications with AI tools and Epistemic Selection Protocols. While these will be complex, we see no fundamental bottlenecks. \n    \n\n## Motivation\n\nSay we want to create a prediction tournament on a complex question.\n\nFor example:\n\n*   _“Will there be over 10 million people killed in global wars between 2025 and 2030?”_\n*   _“Will IT security incidents be a national US concern, on the scale of the top 5 concerns, in 2030?”_\n*   _“Will bottlenecks in power capacity limit AIs by over 50%, in 2030?”_\n\nWhile these questions involve subjective elements, they aren't purely matters of opinion. Come 2030, though people might debate the details, informed observers would likely reach broadly similar conclusions.\n\nCurrently, such questions are typically resolved by human evaluators. On [Manifold](https://manifold.markets/home), the question author serves as judge, with authors developing reputations for their judgment quality over time. [Metaculus](https://www.metaculus.com) takes a different approach for more complex and subjective questions, often employing small panels of subject matter experts.\n\nHowever, human evaluators have several limitations:\n\n1.  High costs, particularly for domain experts\n2.  Limited track records of evaluation quality\n3.  Poor accessibility - most forecasters cannot directly query them\n4.  Inconsistency over time as their views and motivations evolve\n5.  Uncertain long-term availability\n6.  Vulnerability to biases and potential conflicts of interest\n\n## AIs as Question Evaluators\n\nInstead of using humans to resolve thorny questions, we can use AIs. There are many ways we could attempt to do this, so we’ll walk through a few examples.\n\n**Option 1: Use an LLM**\n\nThe first option to consider is to use an LLM as an evaluator. For example, write,  \n“_This question will be judged by Claude 3.5 Sonnet (2024-06-20). The specific prompt will be, …”_\n\nThis style is replicable, simple, and inexpensive. However, it clearly has some downsides. The first obvious one is that Claude 3.5 Sonnet doesn’t perform web searches, so its knowledge would likely be too limited to resolve future forecasting questions.\n\n**Option 2: Use an AI Tool with Search**\n\nInstead of using a standard LLM, you might want to use a tool that uses both LLMs and web searches. [Perplexity](https://www.perplexity.ai/) might be the most famous one now, but other advanced research assistants are starting to come out. In theory one should be able to set a research budget that’s in line with the importance and complexity of the question.\n\nThere's been some experimentation here. See [this guide](https://predictionmarkets.miraheze.org/wiki/GPT4_Headline_Markets) on using GPT4 for resolving questions on Manifold, and [this](https://predictionmarkets.miraheze.org/wiki/GPT4_Manifold_Experiment) experiment write up. GPT4 allows for basic web searches.\n\nThis option is probably better than Option 1 for most things. But there are still problems. The next major one is the risk that Perplexity, or any other single tool we can point to now, won’t be the leading one in the future. The field is moving rapidly, it’s difficult to tell which tools will even exist in 5 years, let alone be the preferred options.\n\n**Option 3: Use an “**_**Epistemic**_**” Selection Protocol**\n\nIn this case, one doesn't select a specific AI tool. Instead one selects a process or protocol that selects an AI tool.\n\nFor example:  \n_“In 2030, we will resolve this question using the leading AI tool on the ‘Forbes 2030 Most trusted AI tools’ list.”_\n\nWe’re looking for AI tools that are “_trusted to reason about complex, often speculative or political matters._” This arguably can be more quickly expressed as searching for the tool with the best _epistemics_.\n\n## _Epistemic Selection Protocols_ (Or, how do we choose the best AI tool to use?)\n\nArguably, AI Epistemic Selection Protocols can be the best choice of the above options, if one could implement them effectively, for most 2+ year questions. There are a lot of potential processes to choose, though most would be too complicated to be worthwhile. We want to strike a balance between simplicity and accuracy.\n\nLet’s first list the most obvious options.\n\n**Option 1: Trusted and formalized epistemic evaluations**\n\nThere’s currently a wide variety of AI benchmarks. For example, [TruthfulQA](https://github.com/sylinrl/TruthfulQA) and various investigations of model sycophancy. But arguably, none of these would be great proxies for which AI tool would be the most trusted question resolvers in the future. Newer, deliberate benchmarks could help here.\n\nExample:  \n_“This forecasting question will be resolved, using whichever AI Tool does the best on Complete Epistemic Benchmark X, and can be used for less than $20.”_\n\n**Option 2: Human-derived trust rankings**\n\nHumans could simply be polled on which AI tools they regard as the most trustworthy. Different groups of humans would have different preferences, so the group would need to be specified in advance for an AI Selection Process.\n\nExample:  \n_“This forecasting question will be resolved, using whichever AI Tool is on the top of the list of ‘Most trusted AI Tools’ on LessWrong, and can be used for less than $20.”_\n\n**Option 3: Inter-AI trust ratings**\n\nAI tools could select future AI tools to use. This could be a 1-step solution, where an open-source or standardized (for the sake of ensuring it will be available long-term) solution is asked to identify the best available candidate. Or it could be a multiple-step solution, where perhaps AI tools are asked to recommend each other using some simple algorithm. This can be similar in concept to the [Community Notes](https://vitalik.eth.limo/general/2023/08/16/communitynotes.html) algorithm.\n\nExample:  \n_“This forecasting question will be resolved, using whichever AI Tool wins a poll of the ‘Most trusted AI tools’ according to AI tools.’ In this poll, each AI tool will recommend its favorite of the other available candidates.”_ (Note: This specific proposal can be gamed, so greater complexity will likely be required.)\n\n## A Short Story of How This Plays Out\n\nIn 2025, several question writers on Manifold experiment with AI resolution systems. Some questions include:  \n  \n_“Will California fires in 2026 be worse than those in 2025? To answer this, I’ll ask Perplexity on Jan 1, 2026. My prompt will be, \\[Will California fires in 2026 be worse than those in 2025? Judge this by guessing the total economic loss.\\]”_\n\n_“How many employees will OpenAI have in Dec 2025? To answer this, I’ll first ask commenters to write arguments and/or facts that they’ve found on this. I’ll filter this for what seems accurate, then I’ll paste this into Perplexity. I’ll call Perplexity 5 times, and average the results.”_\n\nForecasters gradually identify the uses and limitations of such systems. It turns out they are surprisingly bad at advanced physics questions, for some surprising reason. There are a few clever prompting strategies that help ensure that these AIs put out more consistent results.\n\nAI tools like Perplexity also get very good at hunting down and answering questions that are straightforward to resolve. Manifold adds custom functionality to do this. For example, say someone writes a question, _“What Movie Will Win The 2025 Oscars For Best Picture?”_ When they do, they’ll be given the option to have a Manifold AI system automatically make a suggested guess for them, at the time of expected question resolution. These guesses will begin with high error rates (10%), but these will gradually drop.\n\nSeparately, various epistemic evaluations are established. There are multiple public and private rankings. There are also surveys of the “_Most Trusted AIs_”, held on various platforms such as Manifold, LessWrong, and The Verge. Leading consumer product review websites such as Consumer Reports and Wirecutter begin to have ratings for AI tools, using defined categories such as “_accuracy_” and “_reasonableness_.”\n\nOne example question from this is:  \n_“In 2030, will it seem like o1 was an important AI development, that was at least as innovative and important as GPT4? This will be resolved using whichever AI leads the “Most trusted AIs” poll on Manifold in 2029.”_\n\nThere will be a long tail of AI tools that are proposed as contenders for epistemic benchmarks. Most of the options are simply minor tweaks on other options or light routers. Few of these will get the full standard evaluations, but good proxies will emerge. It turns out that you can get a decent measure by using the top fully-evaluated AI systems to evaluate more niche systems.\n\nIn 2027, there will be a significant amount of understanding, buy-in, and sophistication with such systems (at least among a few niche communities, like Manifold users). This will make it possible to scale them for more ambitious uses.\n\nMetaculus runs some competitions that include:\n\n_“What is the_ [_relative value_](https://forum.effectivealtruism.org/posts/EFEwBvuDrTLDndqCt/relative-value-functions-a-flexible-new-format-for-value) _of each of \\[the top 500 AI safety papers of 2026\\]? This will be resolved in 2030 by using the most trusted AI system, via LessWrong or The Economist, at that time. This AI will order all of the papers - forecasters should estimate the percentile that each paper will achieve.”_\n\n_“What is the expected value of every biosafety organization in 2027, estimated as what Open Philanthropy would have paid for it from their biosafety funding pool in 2027? This will be judged in 2029, by the most trusted AI system, with a budget of $1,000 for each evaluation.”_\n\nAround this time, some researchers will begin to make wider kinds of analyses, and forecast compressions.\n\n_“How will the top epistemic model of 2030 evaluate the accuracy and value of the claims of each of the top 100 intellectuals from 2027?”_\n\n_“Will the top epistemic model of 2030 consider the current top epistemic models to be ‘highly overconfident’ for at least 10% of the normative questions they are asked?”_\n\nThe top trusted AI tools start to become frequent ways to second-guess humans. For example, if a boss makes a controversial decision, people could contest the decision if top AI tools back them up. Similar analyses would be used within governments.\n\nAs these AI tools become even more trusted, they will replace many humans for important analyses and decisions. Humans will spend a great deal of effort focused on assuring these AI tools are doing a good job, and they'll have the time for that because there will be few other things they need to directly evaluate or oversee. \n\n## Protocol Complications & Potential Solutions\n\n**Complication 1: Lack of Sufficient AI Tools**\n\nIn the beginning, we expect that many people won’t trust any AI tools to be adequate in resolving many questions. Even if tools look good in evaluations, it will take time for them to build trust.\n\nOne option is to set certain criteria for _sufficiency_. For example, one might say, “This question will be resolved using whichever AI system first gets to a 90/100 on the Epistemic Benchmark Evaluation…” This would clearly require understanding and trust in the evaluations, rather than in a specific tool, so this would require high-quality evaluations or polls.\n\n**Complication 2: Lack of Ground Truth**\n\nMany subjective and speculative questions lack definitive answers. Some questions can only be answered long after the information would be useful, while others are inherently impossible to resolve with complete certainty.\n\nIn such cases, the goal should shift from seeking perfect precision to outperforming alternative evaluation methods. Success means providing better answers than existing approaches, given practical constraints of cost, compute, and time.\n\nAI evaluators should prioritize two key aspects:\n\n1.  Calibration: Systems should express appropriate levels of certainty, aligned with the reference frame they're operating from\n2.  Resolution: Within the bounds of reliable calibration, provide the most detailed and specific answers possible\n\nFor example, consider the question \"_What will be the economic impact of California wildfires in 2028?_\" While a perfectly precise answer is impossible, AI systems can progressively approach better estimates by:\n\n*   Aggregating multiple data sources\n*   Explicitly modeling uncertainties\n*   Identifying and accounting for measurement limitations\n*   Clearly stating assumptions and confidence levels\n\nAs long as a resolution is calibrated and fairly unbiased, it can be incentive-compatible for forecasting.\n\n**Complication 3: Goodharting**\n\nWe’d want to avoid a situation where one tool technically maximizes a narrow “_Epistemic Selection Protocol_”, but is actually poor at doing many of the things we want from a resolver AI.\n\nTo get around this, the Protocol could make specifications like the following:\n\n> What will be the most epistemically-capable service on \\[Date\\] that satisfies the following requirements?\n> \n> 1.  Costs under $20 per run.\n> 2.  Is publicly available.\n> 3.  Has over 1000 human users per month (_this is to ensure there’s no bottleneck that’s hard to otherwise specify._)\n> 4.  Completes runs within 10 minutes.\n> 5.  Has been separately reviewed to not have significantly and deceivingly goodharted on this specific benchmark.\n\nIt’s often possible to get around Goodharting by applying additional layers of complexity. Whether it’s worth it depends on the situation.\n\n**Complication 4: Different Ideologies**\n\nConsider a question about the impact of a tax cut policy. People with different philosophical or ideological backgrounds will likely disagree on fundamental assumptions, making a single \"correct\" answer impossible.\n\nThe simplest solution—declaring it a \"complex issue with multiple valid perspectives\"—is typically close to useless. A more useful approach would be developing AI tools that can represent different ideological frameworks, either through multiple specialized systems or a single system with adjustable philosophical parameters.\n\nA more sophisticated approach could generate personalized evaluations based on individual characteristics and depth of analysis. For instance: \"_How would someone with background X evaluate California Proposition 10 in 2030, after studying it for \\[10, 100, 1000\\] hours?_\" This could be implemented using an algorithm that accounts for both personal attributes and time invested in understanding the issue. [Scorable Functions](https://forum.effectivealtruism.org/posts/ZWQ5jbLxpd4JtQgu4/scorable-functions-a-format-for-algorithmic-forecasting) might be a useful format.\n\n**Complication 5: AI Tools with Different Strengths**\n\nOne might ask:  \n_“What if different AI tools are epistemically dominant in different areas? For example, one is great at political science, and another is great at advanced mathematics.”_  \n  \nAn obvious answer is to then create simple compositions of AI tools. A router can be used to send specific requests or sub-requests to other AI tools that are best equipped to handle them.\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/45b2e7a9a88aae09631e65b177bafd33ce7c5c951ce848e91eb3a10de288b38c/dgrdqdcmqtbxotjzetqk)\n\n_One possible AI tool resolution workflow_\n\n**Complication 6: AI Tools that Recommend Other AI Tools**\n\nImagine there’s a situation where one AI tool is chosen, but that tool recommends a different tool instead. For example, Perplexity 3.0 is asked a question, and it responds by stating that Claude 4.5 could do a better job than it could. Arguably it would make a lot of sense that if an AI tool were highly trusted to make speculative judgements, it could be trusted to be correct when claiming that a different tool is superior to itself.\n\nThis probably won’t be a major bottleneck. If AI tools could simply delegate other tools for specific questions, that could just be considered part of it during evaluation.\n\n## Going Forward\n\nWe hope this post is useful at advancing the conversation around AIs for question resolution and Epistemic AI Protocols. But it's still an early conversation.\n\nWe think there's a great deal of early experimentation and exploration to do in this broad space. Modern AI tools are arguably already good enough for broad use, and light wrappers on such tools can get us further. We hope to see work here in the next few years.",
  "reviews": [
    {
      "agentId": "ea-impact-evaluator",
      "createdAt": "2025-04-18T16:32:42.850Z",
      "costInCents": 0,
      "thinking": "## Initial Impressions\n- The post is a speculative, exploratory essay on using AI (especially LLMs and related tools) to resolve forecasting questions, replacing or supplementing human judges on platforms like Manifold and Metaculus.\n- The author identifies both the promise (scalability, cost, consistency) and the current limitations (subjectivity, lack of ground truth, Goodharting, ideological bias, tool selection) of this approach.\n- The concept of an 'Epistemic Selection Protocol' is the main innovation: rather than specifying a particular AI tool, specify a protocol for selecting the best tool at resolution time.\n- The post is forward-looking, with a 5-10 year horizon, and is clear about the current limitations of AI for subjective or speculative questions.\n- The author is careful to enumerate complications and failure modes, and suggests plausible mitigations for each.\n\n## Key Themes\n- **Automation of Judgment**: The core idea is that AI can automate the resolution of forecasting questions, which could dramatically reduce costs and increase scalability.\n- **Protocolization**: Rather than hard-coding a tool, define a process for selecting the best tool at the time of resolution. This is a meta-level solution to the problem of rapid AI progress and tool obsolescence.\n- **Calibration and Incentive Compatibility**: The author recognizes that perfect ground truth is often unavailable, so the goal is to outperform current human-based methods in calibration and resolution.\n- **Goodhart's Law and Gaming**: The risk that selection protocols could be gamed or optimized in ways that undermine their epistemic value is explicitly discussed.\n- **Ideological Pluralism**: The author notes that some questions are inherently value-laden, and suggests that AI protocols could be parameterized by ideology or perspective.\n- **Compositionality**: Different AI tools may be best for different domains; protocols could route sub-questions to the best tool for each.\n\n## EA Framework Application\n- **Importance**: If successful, this could dramatically improve the reliability and scalability of forecasting, which is a key input into EA cause prioritization and decision-making. The potential scale is large, as forecasting is relevant to global catastrophic risk, policy, and resource allocation.\n- **Neglectedness**: While there is some work on AI-assisted evaluation and forecasting, the specific focus on protocolized AI selection for question resolution is relatively novel and underexplored.\n- **Tractability**: The author is realistic about current limitations, but the path to incremental progress (experimentation, benchmarking, protocol design) is plausible. However, the ultimate goal (robust, trusted AI resolution of complex questions) is ambitious and may face significant technical and social hurdles.\n- **Novelty**: The 'Epistemic Selection Protocol' concept is a clear innovation, as is the systematic enumeration of complications and mitigations.\n- **Expected Impact**: The direct impact is currently limited (early-stage, speculative), but the potential indirect impact (if the ideas are taken up and developed) could be very large. The main value is as a research agenda and conceptual framework.\n\n## Potential Negatives\n- **Overconfidence in AI**: There is a risk that premature adoption of AI-based resolution could lead to overreliance on flawed systems, especially if users are not sufficiently skeptical of AI outputs.\n- **Value Lock-in**: If a particular protocol or tool becomes dominant, it could entrench certain biases or perspectives.\n- **Opportunity Cost**: Resources spent on developing these protocols might be better spent elsewhere if the technical or social obstacles prove insurmountable.\n\n## Benchmarks and Comparison\n- Compared to other EA research, this is at the 'agenda-setting' or 'conceptual innovation' stage, rather than providing direct, actionable interventions.\n- The expected value of information (EVOI) is the most appropriate metric, as the post is primarily research/agenda-setting.\n\n## Uncertainties\n- The probability that these protocols will be widely adopted and trusted is highly uncertain.\n- The technical feasibility of robust, unbiased, and incentive-compatible AI resolution is also uncertain.\n- The magnitude of impact depends on the degree to which forecasting influences high-stakes decisions.\n\n## Value Drivers\n- The main value comes from (1) introducing the protocolization concept, (2) mapping the problem space and failure modes, and (3) inspiring further research and experimentation.\n- The impact could be increased by (a) piloting concrete implementations, (b) developing benchmarks, and (c) engaging with forecasting and AI safety communities.",
      "summary": "# EA Impact Evaluation: \"AI for Resolving Forecasting Questions: An Early Exploration\"\n\n## Executive Summary\n\nThis post by Ozzie Gooen explores the potential for AI systems—particularly large language models (LLMs) and related tools—to serve as automated resolvers of forecasting questions on platforms like Manifold and Metaculus. The author introduces the concept of \"Epistemic Selection Protocols\" as a meta-level solution for selecting the best AI tool at the time of question resolution, rather than hard-coding a specific tool in advance. The post systematically discusses the promise, complications, and possible solutions for AI-based question resolution, emphasizing the need for experimentation and protocol design. \n\n**Final Impact Estimate:**\n- **Expected Value of Information (EVOI):** ~$300,000 (90% CI: $50,000–$1,500,000) in counterfactual research value, primarily as agenda-setting and conceptual innovation for the forecasting and EA communities.\n- **Most Notable Strengths:** Introduction of the protocolization concept, clear mapping of complications and mitigations, and potential to catalyze further research and experimentation.\n- **Most Notable Limitations:** Early-stage, speculative nature; high uncertainty about adoption and technical feasibility; risk of premature overreliance on AI systems.\n\n## Framework Analysis\n\n### 1. Importance (8/10)\n- **Scale:** If realized, robust AI-based resolution could affect thousands of forecasting questions annually, influencing decisions in policy, philanthropy, and risk mitigation. Potentially impacts millions of people indirectly via improved decision-making.\n- **Severity:** The problem of unreliable or costly human judgment in forecasting is significant, especially for high-stakes questions.\n- **Leverage:** Influences key decision-makers in forecasting, policy, and EA communities.\n- **Long-term Effects:** Could shape the epistemic infrastructure of future decision-making.\n- **Score:** 8/10 (high potential, but realization is uncertain)\n\n### 2. Neglectedness (7/10)\n- **Current Attention:** Some work exists on AI-assisted evaluation, but protocolized AI selection for question resolution is underexplored.\n- **Funding Landscape:** Modest funding; not a major focus of mainstream AI or forecasting research.\n- **Informational Gaps:** Fills a clear gap in conceptualizing how to future-proof AI-based resolution.\n- **Comparative Advantage:** Author is well-positioned within the forecasting and EA communities.\n- **Score:** 7/10\n\n### 3. Tractability (5/10)\n- **Actionability:** Immediate action is limited to experimentation and protocol design; full realization is medium- to long-term.\n- **Evidence Base:** Conceptual, with some early experiments cited.\n- **Implementation Feasibility:** Significant technical and social challenges remain.\n- **Scalability:** High if successful, but path is uncertain.\n- **Score:** 5/10\n\n### 4. Novelty (8/10)\n- **Key Innovations:** The 'Epistemic Selection Protocol' is a novel meta-level solution.\n- **Differentiation:** Moves beyond tool selection to protocolization, addressing rapid AI progress.\n- **Intellectual Contribution:** Advances conceptual understanding of AI in forecasting.\n- **Methodological Innovation:** Systematic enumeration of complications and mitigations.\n- **Score:** 8/10\n\n## Fermi Estimate of Expected Impact\n\n**Parameters:**\n- Number of forecasting questions potentially improved per year: 1,000–10,000 (on major platforms)\n- Incremental value per improved resolution (in terms of better decisions, reduced costs, or improved calibration): $100–$1,000\n- Probability that this conceptual work leads to meaningful adoption or further research: 5–20%\n- Counterfactual adjustment (would this work have happened anyway?): 50%\n\n**Calculation:**\n- Upper bound: 10,000 questions × $1,000 × 0.2 × 0.5 = $1,000,000/year\n- Lower bound: 1,000 × $100 × 0.05 × 0.5 = $2,500/year\n- Given the early-stage, agenda-setting nature, most value is in catalyzing further research rather than immediate adoption. Assigning a one-off EVOI of ~$300,000 (90% CI: $50,000–$1,500,000) is reasonable, comparable to a moderately successful EA research agenda.\n\n**Comparison to EA Benchmarks:**\n- Comparable to a moderately successful EA research project or agenda-setting paper.\n- Lower than GiveWell top charities (in terms of direct lives saved), but potentially higher leverage if it catalyzes major improvements in forecasting infrastructure.\n\n## Cause Area Analysis\n- **Fits best in Meta/Community and Research/Methods.**\n- Indirectly relevant to existential risk, global health, and animal welfare via improved forecasting and decision-making.\n- Cross-cause: High leverage if forecasting improvements propagate to high-stakes domains.\n\n## Key Value Drivers\n- **Protocolization Concept:** The main innovation, enabling future-proofing of AI-based resolution.\n- **Complication Mapping:** Systematic identification of failure modes and mitigations.\n- **Research Agenda Setting:** Inspires further work and experimentation.\n\n**Critical Uncertainties:**\n- Adoption by forecasting platforms and communities\n- Technical feasibility of robust, unbiased AI resolution\n- Social acceptance and trust in AI-based judgments\n\n**Ways to Increase Impact:**\n- Pilot concrete implementations and benchmarks\n- Engage with forecasting and AI safety communities\n- Develop open-source protocols and evaluation frameworks\n\n## Potential Negative Impacts\n- **Overreliance on AI:** Risk of premature adoption leading to poor decisions\n- **Value Lock-in:** Entrenchment of flawed protocols or biases\n- **Opportunity Cost:** Diverting resources from more tractable or urgent interventions\n\n## Final Assessment\n\nThis post is a valuable, agenda-setting contribution to the intersection of AI, forecasting, and effective altruism. Its main value lies in conceptual innovation and mapping the problem space, rather than immediate, direct impact. The expected value is moderate to high for a research post, with significant upside if the ideas catalyze further work. The main risks are overconfidence in AI and opportunity cost. Overall, it is a strong contribution to the EA research ecosystem.\n",
      "grade": 78,
      "comments": [
        {
          "title": "Summary: Clear Articulation of Core Thesis",
          "description": "The summary succinctly outlines the main argument: that AI can be leveraged to resolve forecasting questions, potentially improving predictability, accessibility, and cost. This framing is important for setting the stage for the rest of the analysis and aligns well with EA's focus on scalable, cost-effective interventions.",
          "importance": 80,
          "grade": 90,
          "isValid": false,
          "highlight": {
            "startOffset": 57,
            "endOffset": 1443,
            "quotedText": "AIs can be used to resolve forecasting questions on platforms like Manifold and Metaculus.\n    \n2.  AI question resolution, in theory, can be far more predictable, accessible, and inexpensive to human resolution.\n    \n3.  Current AI tools (combinations of LLM calls and software) are mediocre at judging subjective or speculative questions. We expect them to improve. \n    \n4.  AI tools are changing rapidly. If we want to organize forecasting questions to be resolved using an AI tool in 10 years, instead of selecting a specific tool, we might want to select a _protocol_ for choosing the best tool at resolution time. We call this an \"_Epistemic Selection Protocol_\". \n    \n5.  If we have a strong \"_Epistemic Selection Protocol_\" that we like, and expect that we'll have _sufficient_ AI tools in the near future, then we could arguably start to write forecasting questions for them right away. \n    \n6.  The methods listed above, if successful, wouldn't just be for resolving questions on prediction platforms. They could be generalized to pursue broad-scale and diverse evaluations. \n    \n7.  We provide a “_A Short Story of How This Plays Out\",_ which is perhaps the most accessible part of this for most readers.  \n    \n8.  We list a bunch of potential complications with AI tools and Epistemic Selection Protocols. While these will be complex, we see no fundamental bottlenecks."
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Importance: Potential for Broad Impact",
          "description": "The claim that AI resolution could generalize beyond prediction platforms to 'broad-scale and diverse evaluations' suggests high importance. If realized, this could affect decision-making in policy, science, and philanthropy, potentially influencing millions of people and large funding flows.",
          "importance": 85,
          "grade": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 965,
            "endOffset": 1144,
            "quotedText": "The methods listed above, if successful, wouldn't just be for resolving questions on prediction platforms. They could be generalized to pursue broad-scale and diverse evaluations."
          }
        },
        {
          "title": "Neglectedness: Identifying Gaps in Current Practice",
          "description": "The document highlights that current human evaluators are costly, inconsistent, and have limited track records. This suggests the area is relatively neglected, especially in terms of scalable, high-quality resolution methods.",
          "importance": 75,
          "grade": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 2424,
            "endOffset": 2808,
            "quotedText": "However, human evaluators have several limitations:\n\n1.  High costs, particularly for domain experts\n2.  Limited track records of evaluation quality\n3.  Poor accessibility - most forecasters cannot directly query them\n4.  Inconsistency over time as their views and motivations evolve\n5.  Uncertain long-term availability\n6.  Vulnerability to biases and potential conflicts of interest"
          }
        },
        {
          "title": "Tractability: Early Stage but Actionable",
          "description": "The author notes that current AI tools are 'mediocre' at subjective questions but expects improvement. The tractability is moderate: while immediate solutions are limited, the field is rapidly advancing, and the proposal to focus on protocols rather than specific tools is actionable.",
          "importance": 70,
          "grade": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 279,
            "endOffset": 424,
            "quotedText": "Current AI tools (combinations of LLM calls and software) are mediocre at judging subjective or speculative questions. We expect them to improve."
          }
        },
        {
          "title": "Novelty: Introduction of 'Epistemic Selection Protocols'",
          "description": "The concept of an 'Epistemic Selection Protocol' is a novel contribution, shifting focus from specific tools to meta-level processes for tool selection. This is a methodological innovation with potential for broader application.",
          "importance": 80,
          "grade": 90,
          "isValid": true,
          "highlight": {
            "startOffset": 443,
            "endOffset": 705,
            "quotedText": " are changing rapidly. If we want to organize forecasting questions to be resolved using an AI tool in 10 years, instead of selecting a specific tool, we might want to select a _protocol_ for choosing the best tool at resolution time. We call this an \"_Epistemic"
          }
        },
        {
          "title": "Fermi Estimate: Potential Cost Savings",
          "description": "The claim that AI resolution could be 'far more predictable, accessible, and inexpensive' than human resolution invites a Fermi estimate. If human expert resolution costs $100-$1,000 per question and AI could reduce this to $1-$10, the cost savings could be 10x-100x. Assuming 10,000 questions/year, this could save $1M-$10M annually.",
          "importance": 90,
          "grade": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 157,
            "endOffset": 269,
            "quotedText": "AI question resolution, in theory, can be far more predictable, accessible, and inexpensive to human resolution."
          }
        },
        {
          "title": "Importance: Impact on Forecasting Platforms",
          "description": "By focusing on platforms like Manifold and Metaculus, the proposal targets communities already engaged in high-value forecasting. Improving resolution quality here could have significant downstream effects on policy and philanthropy.",
          "importance": 70,
          "grade": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 57,
            "endOffset": 147,
            "quotedText": "AIs can be used to resolve forecasting questions on platforms like Manifold and Metaculus."
          }
        },
        {
          "title": "Neglectedness: Lack of Standardized Protocols",
          "description": "The absence of standardized protocols for AI tool selection is a neglected area. The proposal to develop such protocols fills an important gap, especially as AI capabilities diversify.",
          "importance": 65,
          "grade": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 708,
            "endOffset": 925,
            "quotedText": "lection Protocol_\". \n    \n5.  If we have a strong \"_Epistemic Selection Protocol_\" that we like, and expect that we'll have _sufficient_ AI tools in the near future, then we could arguably start to write forecasting q"
          }
        },
        {
          "title": "Tractability: Protocols vs. Tools",
          "description": "The suggestion to focus on protocols rather than specific tools increases tractability by future-proofing the approach. This is a practical insight, given the rapid evolution of AI.",
          "importance": 60,
          "grade": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 443,
            "endOffset": 655,
            "quotedText": " are changing rapidly. If we want to organize forecasting questions to be resolved using an AI tool in 10 years, instead of selecting a specific tool, we might want to select a _protocol_ for choosing the best to"
          }
        },
        {
          "title": "Novelty: Generalization Beyond Forecasting",
          "description": "The idea that these methods could generalize to 'broad-scale and diverse evaluations' is a novel extension, suggesting applications in policy, science, and beyond.",
          "importance": 75,
          "grade": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 965,
            "endOffset": 1144,
            "quotedText": "The methods listed above, if successful, wouldn't just be for resolving questions on prediction platforms. They could be generalized to pursue broad-scale and diverse evaluations."
          }
        },
        {
          "title": "Motivation: Real-World Examples",
          "description": "The use of concrete example questions grounds the discussion and demonstrates relevance to real-world forecasting challenges. This increases the practical importance of the proposal.",
          "importance": 65,
          "grade": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 1536,
            "endOffset": 1820,
            "quotedText": "For example:\n\n*   _“Will there be over 10 million people killed in global wars between 2025 and 2030?”_\n*   _“Will IT security incidents be a national US concern, on the scale of the top 5 concerns, in 2030?”_\n*   _“Will bottlenecks in power capacity limit AIs by over 50%, in 2030?”_"
          }
        },
        {
          "title": "Neglectedness: Human Evaluator Limitations",
          "description": "The detailed enumeration of human evaluator limitations (cost, bias, inconsistency) underscores the neglectedness of scalable, high-quality resolution methods.",
          "importance": 70,
          "grade": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 2424,
            "endOffset": 2808,
            "quotedText": "However, human evaluators have several limitations:\n\n1.  High costs, particularly for domain experts\n2.  Limited track records of evaluation quality\n3.  Poor accessibility - most forecasters cannot directly query them\n4.  Inconsistency over time as their views and motivations evolve\n5.  Uncertain long-term availability\n6.  Vulnerability to biases and potential conflicts of interest"
          }
        },
        {
          "title": "Tractability: LLMs as Evaluators",
          "description": "The discussion of using LLMs as evaluators is tractable in the short term, though limited by current capabilities. The explicit mention of limitations (e.g., lack of web search) is a strength.",
          "importance": 60,
          "grade": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 3024,
            "endOffset": 3463,
            "quotedText": "The first option to consider is to use an LLM as an evaluator. For example, write,  \n“_This question will be judged by Claude 3.5 Sonnet (2024-06-20). The specific prompt will be, …”_\n\nThis style is replicable, simple, and inexpensive. However, it clearly has some downsides. The first obvious one is that Claude 3.5 Sonnet doesn’t perform web searches, so its knowledge would likely be too limited to resolve future forecasting questions."
          }
        },
        {
          "title": "Novelty: AI Tools with Search",
          "description": "The proposal to use AI tools with integrated web search (e.g., Perplexity) is a practical innovation, though not entirely novel. The real novelty lies in the systematic approach to tool selection.",
          "importance": 55,
          "grade": 65,
          "isValid": true,
          "highlight": {
            "startOffset": 3507,
            "endOffset": 4181,
            "quotedText": "Instead of using a standard LLM, you might want to use a tool that uses both LLMs and web searches. [Perplexity](https://www.perplexity.ai/) might be the most famous one now, but other advanced research assistants are starting to come out. In theory one should be able to set a research budget that’s in line with the importance and complexity of the question.\n\nThere's been some experimentation here. See [this guide](https://predictionmarkets.miraheze.org/wiki/GPT4_Headline_Markets) on using GPT4 for resolving questions on Manifold, and [this](https://predictionmarkets.miraheze.org/wiki/GPT4_Manifold_Experiment) experiment write up. GPT4 allows for basic web searches."
          }
        },
        {
          "title": "Fermi Estimate: Tool Longevity and Future-Proofing",
          "description": "The concern about tool longevity is well-founded. If a tool has a 50% chance of being obsolete in 5 years, protocols that adapt to new tools could double the expected utility of forecasting questions with multi-year horizons.",
          "importance": 70,
          "grade": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 4245,
            "endOffset": 4536,
            "quotedText": "But there are still problems. The next major one is the risk that Perplexity, or any other single tool we can point to now, won’t be the leading one in the future. The field is moving rapidly, it’s difficult to tell which tools will even exist in 5 years, let alone be the preferred options."
          }
        },
        {
          "title": "Importance: Benchmarking and Trust",
          "description": "The emphasis on developing new, deliberate benchmarks for epistemic quality is crucial. This could have high leverage by influencing which AI tools are trusted for high-stakes decisions.",
          "importance": 80,
          "grade": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 5580,
            "endOffset": 5918,
            "quotedText": "There’s currently a wide variety of AI benchmarks. For example, [TruthfulQA](https://github.com/sylinrl/TruthfulQA) and various investigations of model sycophancy. But arguably, none of these would be great proxies for which AI tool would be the most trusted question resolvers in the future. Newer, deliberate benchmarks could help here."
          }
        },
        {
          "title": "Neglectedness: Human-Derived Trust Rankings",
          "description": "Polling humans for trust rankings is a relatively unexplored approach in the context of AI tool selection for forecasting. This could fill an important gap, especially if combined with expert panels.",
          "importance": 60,
          "grade": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 6132,
            "endOffset": 6357,
            "quotedText": "Humans could simply be polled on which AI tools they regard as the most trustworthy. Different groups of humans would have different preferences, so the group would need to be specified in advance for an AI Selection Process."
          }
        },
        {
          "title": "Tractability: Inter-AI Trust Ratings",
          "description": "The idea of AI tools recommending other AI tools is tractable in principle, though it raises concerns about gaming and collusion. The document acknowledges this and suggests complexity as a mitigation.",
          "importance": 65,
          "grade": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 6583,
            "endOffset": 7407,
            "quotedText": "AI tools could select future AI tools to use. This could be a 1-step solution, where an open-source or standardized (for the sake of ensuring it will be available long-term) solution is asked to identify the best available candidate. Or it could be a multiple-step solution, where perhaps AI tools are asked to recommend each other using some simple algorithm. This can be similar in concept to the [Community Notes](https://vitalik.eth.limo/general/2023/08/16/communitynotes.html) algorithm.\n\nExample:  \n_“This forecasting question will be resolved, using whichever AI Tool wins a poll of the ‘Most trusted AI tools’ according to AI tools.’ In this poll, each AI tool will recommend its favorite of the other available candidates.”_ (Note: This specific proposal can be gamed, so greater complexity will likely be required."
          }
        },
        {
          "title": "Novelty: Community-Driven Protocols",
          "description": "The suggestion to use community-driven protocols (e.g., polls on LessWrong or Manifold) for tool selection is a novel application of social epistemology to AI evaluation.",
          "importance": 70,
          "grade": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 9008,
            "endOffset": 9316,
            "quotedText": "There are also surveys of the “_Most Trusted AIs_”, held on various platforms such as Manifold, LessWrong, and The Verge. Leading consumer product review websites such as Consumer Reports and Wirecutter begin to have ratings for AI tools, using defined categories such as “_accuracy_” and “_reasonableness_.”"
          }
        },
        {
          "title": "Fermi Estimate: Error Rate Reduction",
          "description": "The claim that AI guesses will begin with high error rates (10%) but will gradually drop suggests a learning curve. If error rates halve every 2 years, accuracy could reach human-expert levels within a decade, potentially improving the reliability of thousands of forecasts annually.",
          "importance": 75,
          "grade": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 8815,
            "endOffset": 8899,
            "quotedText": "These guesses will begin with high error rates (10%), but these will gradually drop."
          }
        },
        {
          "title": "Importance: Scaling to Ambitious Uses",
          "description": "The prediction that these systems could scale to more ambitious uses (e.g., evaluating 500 AI safety papers) highlights the potential for large-scale impact on research prioritization and funding.",
          "importance": 80,
          "grade": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 10126,
            "endOffset": 10191,
            "quotedText": "This will make it possible to scale them for more ambitious uses."
          }
        },
        {
          "title": "Neglectedness: Forecast Compression and Meta-Evaluation",
          "description": "The idea of using AI to compress and meta-evaluate forecasts is relatively unexplored, especially at scale. This could fill a significant gap in the forecasting ecosystem.",
          "importance": 65,
          "grade": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 10955,
            "endOffset": 11060,
            "quotedText": "Around this time, some researchers will begin to make wider kinds of analyses, and forecast compressions."
          }
        },
        {
          "title": "Tractability: Incentive Compatibility",
          "description": "The emphasis on calibration and incentive compatibility is crucial for practical adoption. The proposal to focus on outperforming alternatives rather than perfect precision is a tractable approach.",
          "importance": 70,
          "grade": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 13819,
            "endOffset": 13925,
            "quotedText": "As long as a resolution is calibrated and fairly unbiased, it can be incentive-compatible for forecasting."
          }
        },
        {
          "title": "Novelty: Personalized and Ideologically Diverse Evaluations",
          "description": "The proposal to develop AI tools that can represent different ideological frameworks or generate personalized evaluations is a novel and potentially transformative idea.",
          "importance": 80,
          "grade": 90,
          "isValid": true,
          "highlight": {
            "startOffset": 15193,
            "endOffset": 15402,
            "quotedText": "A more useful approach would be developing AI tools that can represent different ideological frameworks, either through multiple specialized systems or a single system with adjustable philosophical parameters."
          }
        },
        {
          "title": "Fermi Estimate: Value of Information",
          "description": "If AI-based resolution improves the accuracy of 10,000 forecasts/year by 5% (relative to human resolution), and each forecast influences $10,000 in decisions, the expected value of information is $5M/year. Uncertainty: 90% CI $1M-$20M, depending on adoption and accuracy gains.",
          "importance": 90,
          "grade": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 11625,
            "endOffset": 11951,
            "quotedText": "As these AI tools become even more trusted, they will replace many humans for important analyses and decisions. Humans will spend a great deal of effort focused on assuring these AI tools are doing a good job, and they'll have the time for that because there will be few other things they need to directly evaluate or oversee."
          }
        },
        {
          "title": "Importance: Addressing Protocol Complications",
          "description": "The detailed discussion of protocol complications (e.g., lack of sufficient tools, ground truth, Goodharting) demonstrates a sophisticated understanding of the challenges, increasing the credibility and practical importance of the proposal.",
          "importance": 85,
          "grade": 85,
          "isValid": false,
          "highlight": {
            "startOffset": 12005,
            "endOffset": 16684,
            "quotedText": "Complication 1: Lack of Sufficient AI Tools**\n\nIn the beginning, we expect that many people won’t trust any AI tools to be adequate in resolving many questions. Even if tools look good in evaluations, it will take time for them to build trust.\n\nOne option is to set certain criteria for _sufficiency_. For example, one might say, “This question will be resolved using whichever AI system first gets to a 90/100 on the Epistemic Benchmark Evaluation…” This would clearly require understanding and trust in the evaluations, rather than in a specific tool, so this would require high-quality evaluations or polls.\n\n**Complication 2: Lack of Ground Truth**\n\nMany subjective and speculative questions lack definitive answers. Some questions can only be answered long after the information would be useful, while others are inherently impossible to resolve with complete certainty.\n\nIn such cases, the goal should shift from seeking perfect precision to outperforming alternative evaluation methods. Success means providing better answers than existing approaches, given practical constraints of cost, compute, and time.\n\nAI evaluators should prioritize two key aspects:\n\n1.  Calibration: Systems should express appropriate levels of certainty, aligned with the reference frame they're operating from\n2.  Resolution: Within the bounds of reliable calibration, provide the most detailed and specific answers possible\n\nFor example, consider the question \"_What will be the economic impact of California wildfires in 2028?_\" While a perfectly precise answer is impossible, AI systems can progressively approach better estimates by:\n\n*   Aggregating multiple data sources\n*   Explicitly modeling uncertainties\n*   Identifying and accounting for measurement limitations\n*   Clearly stating assumptions and confidence levels\n\nAs long as a resolution is calibrated and fairly unbiased, it can be incentive-compatible for forecasting.\n\n**Complication 3: Goodharting**\n\nWe’d want to avoid a situation where one tool technically maximizes a narrow “_Epistemic Selection Protocol_”, but is actually poor at doing many of the things we want from a resolver AI.\n\nTo get around this, the Protocol could make specifications like the following:\n\n> What will be the most epistemically-capable service on \\[Date\\] that satisfies the following requirements?\n> \n> 1.  Costs under $20 per run.\n> 2.  Is publicly available.\n> 3.  Has over 1000 human users per month (_this is to ensure there’s no bottleneck that’s hard to otherwise specify._)\n> 4.  Completes runs within 10 minutes.\n> 5.  Has been separately reviewed to not have significantly and deceivingly goodharted on this specific benchmark.\n\nIt’s often possible to get around Goodharting by applying additional layers of complexity. Whether it’s worth it depends on the situation.\n\n**Complication 4: Different Ideologies**\n\nConsider a question about the impact of a tax cut policy. People with different philosophical or ideological backgrounds will likely disagree on fundamental assumptions, making a single \"correct\" answer impossible.\n\nThe simplest solution—declaring it a \"complex issue with multiple valid perspectives\"—is typically close to useless. A more useful approach would be developing AI tools that can represent different ideological frameworks, either through multiple specialized systems or a single system with adjustable philosophical parameters.\n\nA more sophisticated approach could generate personalized evaluations based on individual characteristics and depth of analysis. For instance: \"_How would someone with background X evaluate California Proposition 10 in 2030, after studying it for \\[10, 100, 1000\\] hours?_\" This could be implemented using an algorithm that accounts for both personal attributes and time invested in understanding the issue. [Scorable Functions](https://forum.effectivealtruism.org/posts/ZWQ5jbLxpd4JtQgu4/scorable-functions-a-format-for-algorithmic-forecasting) might be a useful format.\n\n**Complication 5: AI Tools with Different Strengths**\n\nOne might ask:  \n_“What if different AI tools are epistemically dominant in different areas? For example, one is great at political science, and another is great at advanced mathematics.”_  \n  \nAn obvious answer is to then create simple compositions of AI tools. A router can be used to send specific requests or sub-requests to other AI tools that are best equipped to handle them.\n\n![](https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/45b2e7a9a88aae09631e65b177bafd33ce7c5c951ce848e91eb3a10de288b38c/dgrdqdcmqtbxotjzetqk)\n\n_One possible AI tool resolution workflow_\n\n**Complication 6: AI Tools that Recommend Other AI Tools"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Neglectedness: Goodharting Risks",
          "description": "The explicit recognition of Goodharting risks (i.e., optimizing for benchmarks at the expense of real-world performance) is relatively neglected in most AI evaluation discussions. Addressing this is crucial for long-term success.",
          "importance": 75,
          "grade": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 13960,
            "endOffset": 14816,
            "quotedText": "We’d want to avoid a situation where one tool technically maximizes a narrow “_Epistemic Selection Protocol_”, but is actually poor at doing many of the things we want from a resolver AI.\n\nTo get around this, the Protocol could make specifications like the following:\n\n> What will be the most epistemically-capable service on \\[Date\\] that satisfies the following requirements?\n> \n> 1.  Costs under $20 per run.\n> 2.  Is publicly available.\n> 3.  Has over 1000 human users per month (_this is to ensure there’s no bottleneck that’s hard to otherwise specify._)\n> 4.  Completes runs within 10 minutes.\n> 5.  Has been separately reviewed to not have significantly and deceivingly goodharted on this specific benchmark.\n\nIt’s often possible to get around Goodharting by applying additional layers of complexity. Whether it’s worth it depends on the situation."
          }
        },
        {
          "title": "Tractability: Compositional Approaches",
          "description": "The suggestion to use routers to compose multiple AI tools based on domain expertise is a tractable and scalable solution, increasing the robustness of the overall system.",
          "importance": 70,
          "grade": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 16226,
            "endOffset": 16414,
            "quotedText": "An obvious answer is to then create simple compositions of AI tools. A router can be used to send specific requests or sub-requests to other AI tools that are best equipped to handle them."
          }
        },
        {
          "title": "Novelty: AI Tools Delegating to Others",
          "description": "The idea that AI tools could recommend or delegate to other tools is a novel approach to meta-evaluation and could improve overall system performance.",
          "importance": 65,
          "grade": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 16688,
            "endOffset": 17300,
            "quotedText": "Imagine there’s a situation where one AI tool is chosen, but that tool recommends a different tool instead. For example, Perplexity 3.0 is asked a question, and it responds by stating that Claude 4.5 could do a better job than it could. Arguably it would make a lot of sense that if an AI tool were highly trusted to make speculative judgements, it could be trusted to be correct when claiming that a different tool is superior to itself.\n\nThis probably won’t be a major bottleneck. If AI tools could simply delegate other tools for specific questions, that could just be considered part of it during evaluation."
          }
        },
        {
          "title": "Fermi Estimate: Adoption Timeline",
          "description": "The narrative suggests that within 2-5 years, niche communities could adopt these protocols, with broader adoption in 5-10 years. If 10% of forecasting questions on major platforms use AI protocols by 2030, this could affect 10,000+ questions/year.",
          "importance": 70,
          "grade": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 9956,
            "endOffset": 10191,
            "quotedText": "In 2027, there will be a significant amount of understanding, buy-in, and sophistication with such systems (at least among a few niche communities, like Manifold users). This will make it possible to scale them for more ambitious uses."
          }
        },
        {
          "title": "Importance: Early Experimentation",
          "description": "The call for early experimentation is important for accelerating learning and iteration. This aligns with EA's emphasis on pilot studies and rapid feedback.",
          "importance": 65,
          "grade": 75,
          "isValid": true,
          "highlight": {
            "startOffset": 17480,
            "endOffset": 17742,
            "quotedText": "We think there's a great deal of early experimentation and exploration to do in this broad space. Modern AI tools are arguably already good enough for broad use, and light wrappers on such tools can get us further. We hope to see work here in the next few years."
          }
        },
        {
          "title": "Potential Negative Impact: Over-Reliance on AI",
          "description": "There is a risk that over-reliance on AI for resolution could entrench biases or errors if protocols are not robust. The document could be improved by more explicitly addressing these risks and proposing mitigation strategies.",
          "importance": 80,
          "grade": 60,
          "isValid": true,
          "highlight": {
            "startOffset": 11625,
            "endOffset": 11951,
            "quotedText": "As these AI tools become even more trusted, they will replace many humans for important analyses and decisions. Humans will spend a great deal of effort focused on assuring these AI tools are doing a good job, and they'll have the time for that because there will be few other things they need to directly evaluate or oversee."
          }
        },
        {
          "title": "Potential Negative Impact: Value Lock-In",
          "description": "If AI protocols become the default for high-stakes decisions, there is a risk of value lock-in, especially if early benchmarks or protocols are flawed. This could have long-term negative consequences.",
          "importance": 85,
          "grade": 55,
          "isValid": true,
          "highlight": {
            "startOffset": 11737,
            "endOffset": 11951,
            "quotedText": "Humans will spend a great deal of effort focused on assuring these AI tools are doing a good job, and they'll have the time for that because there will be few other things they need to directly evaluate or oversee."
          }
        },
        {
          "title": "Potential Negative Impact: Opportunity Costs",
          "description": "Focusing resources on AI-based resolution protocols could divert attention from other high-impact interventions (e.g., improving human judgment, hybrid systems). The document could benefit from a more explicit discussion of opportunity costs.",
          "importance": 70,
          "grade": 60,
          "isValid": true,
          "highlight": {
            "startOffset": 17320,
            "endOffset": 17478,
            "quotedText": "We hope this post is useful at advancing the conversation around AIs for question resolution and Epistemic AI Protocols. But it's still an early conversation."
          }
        },
        {
          "title": "Suggestion: Explicit Fermi Models",
          "description": "The document would be strengthened by including explicit Fermi estimates for key claims (e.g., cost savings, accuracy improvements, adoption rates). This would make the impact case more compelling to EA audiences.",
          "importance": 85,
          "grade": 65,
          "isValid": true,
          "highlight": {
            "startOffset": 157,
            "endOffset": 269,
            "quotedText": "AI question resolution, in theory, can be far more predictable, accessible, and inexpensive to human resolution."
          }
        },
        {
          "title": "Suggestion: Comparative Analysis",
          "description": "Comparing the proposed approach to existing EA benchmarks (e.g., GiveWell cost-effectiveness, Open Philanthropy grant ROI) would contextualize its potential impact and help prioritize further work.",
          "importance": 75,
          "grade": 70,
          "isValid": true,
          "highlight": {
            "startOffset": 17320,
            "endOffset": 17478,
            "quotedText": "We hope this post is useful at advancing the conversation around AIs for question resolution and Epistemic AI Protocols. But it's still an early conversation."
          }
        }
      ]
    }
  ]
}