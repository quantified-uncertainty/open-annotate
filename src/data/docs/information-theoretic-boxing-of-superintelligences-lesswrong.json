{
  "id": "information-theoretic-boxing-of-superintelligences-lesswrong",
  "slug": "information-theoretic-boxing-of-superintelligences-lesswrong",
  "title": "Information-Theoretic Boxing of Superintelligences — LessWrong",
  "author": "JustinShovelain",
  "publishedDate": "2023-11-30T14:31:11.798Z",
  "url": "https://www.lesswrong.com/posts/NZP6QvkXryJQFGkLF/information-theoretic-boxing-of-superintelligences-1s",
  "intendedAgents": [
    "clarity-coach",
    "research-scholar",
    "quantitative-forecaster",
    "ea-impact-evaluator",
    "bias-detector",
    "fake-eliezer"
  ],
  "content": "Boxing an agent more intelligent than ourselves is daunting, but information theory, thermodynamics, and control theory provide us with tools that can fundamentally constrain agents independent of their intelligence. In particular, we may be able to contain an AI by limiting its access to information\n\n# Constraining output and inputs\n\nSuperintelligent AI has the potential to both help and harm us. One way to minimize the harm is _boxing_ the AI: containing it so that it can’t freely influence its external world, ideally while preserving its potential to help. \n\nBoxing is complicated. Destroying the AI will certainly prevent it from influencing anything. Throwing it into a black hole might causally disconnect it from the rest of the universe while possibly preserving its existence, though even then it may exploit some unknown physics to [leak information back across the event horizon](https://en.wikipedia.org/wiki/Markov_blanket), or to persist or recreate itself once the black hole has evaporated. \n\nCan we box an AI while still being being able to _use_ it? We could try preventing it from physically interacting with its environment and only permitting it to [present information](https://www.lesswrong.com/tag/oracle-ai) to a user, but a superintelligence could abuse any communication channels to manipulate its users into [granting it more power](https://www.lesswrong.com/posts/wKnwcjJGriTS9QxxL/dreams-of-friendliness) or [improving its predictive abilities](https://www.lesswrong.com/posts/aBRS3x4sPSJ9G6xkj/underspecification-of-oracle-ai). To successfully box the AI in this manner, we’d need to constrain both its ability to physically interact with its environment and communicate information and manipulate. We’ll call this _output boxing_: containing an AI by constraining its various outputs. \n\nMost discussions of boxing focus on output boxing, but there’s a neglected mirror to this approach that we’ll call _input boxing_: containing an AI by constraining what information it can access. (For completeness, we could also consider boxing techniques independent of input and output, where we contain an AI by constraining its functioning, such as by limiting its computational speed or restarting it from initial conditions each second).\n\nIn the rest of this post, we’ll introduce and examine a theoretical input boxing technique that contains an AI by controlling the number of bits of information it has about its environment, using results from information theory, thermodynamics, and control theory. \n\n# Good regulators and bad keys\n\nToday’s powerful AIs rely on mountains of data (during training, at least), so restricting their access to data would restrict their subsequent capabilities, but can we _fundamentally_ limit a superintelligence’s external influence just by constraining its access to data? \n\nWe know that agents need an understanding of their environments to act effectively within them: Conant & Ashby studied this in a ([much](https://www.lesswrong.com/posts/wTJECdFSAk4843mFj/thoughts-on-the-good-regulator-theorem#:~:text=This%20is%20among%20the%20worst%20papers%20I%27ve%20ever%20skimmed.) [maligned](https://www.lesswrong.com/posts/Dx9LoqsEh3gHNJMDk/fixing-the-good-regulator-theorem#:~:text=This%20may%20be%20the%20most%20misleading%20title%20and%20summary%20I%20have%20ever%20seen%20on%20a%20math%20paper.)) 1970 paper on the [good regulator theorem](http://pespmc1.vub.ac.be/books/Conant_Ashby.pdf). A _regulator_ is any mechanism that maintains a desired characteristic, like a thermostat, or a brain and its homeostatic subsystems. The good regulator theorem states that every good regulator of a system must contain a model of the system. Another parsing of the theorem states that [every good key must be a model of the lock it opens](http://cadia.ru.is/wiki/_media/public:t-720-atai:a_primer_for_conant_and_ashby_s_good-regulator_theorem.pdf), and John Wentworth [refines the theorem](https://www.lesswrong.com/posts/Dx9LoqsEh3gHNJMDk/fixing-the-good-regulator-theorem) (and its proof), clarifying that: \n\n> _\\[Conant and Ashby actually prove\\] that every regulator which is optimal and contains no unnecessary noise is_ equivalent to _a regulator which first reconstructs the variable-values of the system it’s controlling, then chooses its output as a function of those values_\n\nCan we, therefore, box an AI by preventing it from modeling its external environment? Can we stop the AI becoming a “good key” for the cage we’ve placed it in? Let’s look at the potential sources of information for such an AI: \n\n*   The initial conditions of the model (such as training data and learned patterns)\n*   Sensory input (such as through cameras and microphones)\n*   Logical deduction and extrapolation\n\nWe’ll assume that the AI we’re trying to box is logically omniscient; that it can squeeze as much information from its initial conditions and sensory input as theoretically possible. This leaves two sources of information that are targets for input-boxing: initial conditions and sensory input. \n\nIn practice, restricting these sources of information is tricky. For example, we could try giving the AI only one sensor, some low-res camera with which to receive instruction. However, the image that the camera receives could be subtly influenced by the vibration of nearby voices, ambient magnetic fields, background radiation, and more. Interpreted correctly, these sources of interference become sources of information. \n\nSupposing, though, that we can overcome such _practical_ hurdles, can we quantify how much control we can muster from input boxing? To answer this, we’ll look at control theory. \n\n# Bits, boxes, and control\n\nFirst, some terminology. Like a regulator, a _control loop_ controls the behavior of a variable using a sensor, a function, and some actuator. Using multiple control loops, a _control system_ manages the behavior of a system. Control loops are _open-loop_ when they don’t act on feedback about the outcomes of their actions, like a heater on a timer. _Closed-loop_ controllers incorporate feedback, like cruise control or thermostats. Finally, _entropy_ here refers to [Shannon entropy](https://en.wikipedia.org/wiki/Entropy_\\(information_theory\\)), which measures uncertainty about a variable. Touchette & Lloyd study the limits of control systems in [Information-theoretic limits of control](https://arxiv.org/pdf/chao-dyn/9905039.pdf), writing that:\n\n> _entropy offers a precise measure of disorderliness or missing information by characterizing the minimum amount of resources (bits) required to encode unambiguously the ensemble describing the system_. \n\nSummarizing their main results, they write:\n\n> _in a control process, information must constantly be acquired, processed and used to constrain or maintain the trajectory of a system._\n> \n> _the second law of thermodynamics…sets absolute limits to the minimum amount of dissipation required by open-loop control…an information-theoretic analysis of closed-loop control shows feedback control to be essentially a zero sum game: **each bit of information gathered directly from a dynamical systems by a control device can serve to decrease the entropy of that system by at most one bit** additional to the reduction of entropy attainable without such information (open-loop control)._ \n\nIn other words, when including information as part of a thermodynamic system, the second law of thermodynamics tells us that you can't control a system perfectly. When a control loop adjusts its actions based on the information it gets from the system it's controlling, there’s a fundamental trade-off: **for each bit of information you collect from a system, you can reduce disorder of that system by only one more bit than you could’ve without that information**. There's a strict one-for-one limit on how much more you can improve control through information. \n\nTo formalize this, let t denote time, and let IT denote the amount of information available to the boxed agent at time t\\=T. The change in IT is then given by time derivative of input information, labeled i, minus the time derivative of output or “used” information, labeled o: \n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6FAMJL89DKmSeYTha/nbpozftdirivgc3bzeii)\n\nand so the available information at time T is\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6FAMJL89DKmSeYTha/h8nhh8x2beyig8jbrljg)\n\nwhere C denotes the information contained in the initial conditions of the model. We can visualize (a simplified version of) this over consecutive timesteps, as information is gathered then used up: \n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6FAMJL89DKmSeYTha/q25vhab6anqlhmaabqka)\n\nThis seems promising. If we limit the information an AI can gather with respect to the information it uses to act in its environment, it will not have sufficient information to act effectively. Or, in the language of the paper above, the agent will not have enough bits about its external dynamical system to sufficiently reduce the entropy of that dynamical system. \n\nWe can define input-boxing more formally as follows: suppose we have an agent A that has an amount IT of information at time T. For A to achieve a certain external action X, it requires some amount of information IX about its external environment, and we can say that A cannot achieve X when IT is smaller than IX. If we set X to be some minimal escape scenario, then we can say that A is input boxed when IT<IX. This doesn't depend on any of the agent's capabilities or intelligence, just its IT, which we are supposing we can limit, and IX, which depends only on the external environment. \n\nJohn Wentworth, author of [Fixing The Good Regulator Theorem](https://www.lesswrong.com/posts/Dx9LoqsEh3gHNJMDk/fixing-the-good-regulator-theorem), which we used earlier, considered a similar [question about the limits of control](https://www.lesswrong.com/posts/6rZroG3mztowktJzp/how-many-bits-of-optimization-can-one-bit-of-observation). He conjectured that observing an additional N bits of information allows a system to perform at most N additional bits of optimization. He concluded that the conjecture is [false](https://www.lesswrong.com/posts/6rZroG3mztowktJzp/how-many-bits-of-optimization-can-one-bit-of-observation?commentId=Pvhpc7PF8DhsyMGgk), citing a counterexample where a short password unlocks a long message, concluding that “_bits of observation can give a really ridiculously large boost to a system's optimization power”_. Others [agree](https://www.lesswrong.com/posts/dfTm26pvq7yQp8mR3/one-bit-of-observation-can-unlock-many-of-optimization-but), arguing that, with some practical constraints, “_there are no limits on how much optimization one can squeeze out of a single bit, even in a world with reversible computing”_. \n\nHowever, these counterexamples rely on the agent’s ability to subsequently “follow up” on what they learn. The information in the “password” is only useful insofar as it grants the opportunity to gather or unlock more information. The instantaneous utility of the password, or any piece of information, is bounded by the sum of the information in the password itself and prior knowledge, and by assumption we’re limiting the agent’s access to follow-up information as well. Therefore, these counterexamples shouldn’t exempt an input-boxed AI from the limits on optimization. If we can limit the information an agent gathers from inputs and prior knowledge, we can box that agent, regardless of its intelligence. \n\n# Building better boxes\n\nInput boxing is a promising avenue for research as it tackles the challenge of limiting an AI's capabilities through the fundamentals of information, independent of how smart the AI in the box is. \n\nThis article is just an introduction, and we’re interested in exploring further refinements of the idea of input boxing. For example, a deeper analysis of _what_ information we allow into the box may give us more fine-grained control over an AI (such as in [this discussion](https://www.lesswrong.com/posts/6rZroG3mztowktJzp/how-many-bits-of-optimization-can-one-bit-of-observation?commentId=n5b4RvdrSxFsqCFw5#n5b4RvdrSxFsqCFw5) about information about environments versus laws), especially if we can robustly distinguish between information useful for solving our problems and useful for escape. \n\nThat is, rather than fundamentally limiting the agent’s _overall_ control, we may be able to identify hazardous paths, or sequences of actions, that the agent will want to pursue. That is, for an agent to achieve an action it requires information about the control pathway between its actuators and its ultimate desired outcome. A must have the information about how its first action will transform its external environment, and how subsequent actions will transform the new, changed environment, and so on.  \n\nIf we limit the agent’s access to that hazardous information about dangerous control pathway, we could relax the fundamental constraints on information input without permitting hazardous behavior. This could be much more pragmatic than fundamentally limiting all information, since hazardous information is probably a small subset of all information - bits have to be relevant to hazardous actions to be hazardous bits. However, this would come at the cost of the guarantee those fundamental limits provide. \n\nAnother promising avenue for developing input-boxing, suggested by Deric Cheng, would be to impose fundamental delays on the use of information. The utility of information decays over time, so if an AI could only act on information from 24 hours ago, it’ll be much weaker, especially for high-speed actions such as exploiting the stock market.\n\nAs well as a boxing technique for superintelligences, input boxing may be useful for more niche applications. For example, by input boxing AI models during [evals](https://www.lesswrong.com/tag/ai-evaluations), we could test a model with the guarantee that any hazardous behavior won’t leak out the box, meaning evaluators could actively prompt hazardous behavior and use [red-teaming](https://en.wikipedia.org/wiki/Red_team) techniques without compromising the [safety of the eval](https://www.lesswrong.com/posts/XCRsg2ZnHBNAN862T/improving-the-safety-of-ai-evals). \n\nWith further refinements and insights from information theory, control theory, and thermodynamics, input boxing could be a powerful tool for both niche applications and for wrapping a superintelligence up with a neat, little bow in a safe, useful box. \n\nThis post was initially inspired by Hugo Touchette & Seth Lloyd’s [Information-Theoretic Limits of Control](https://arxiv.org/pdf/chao-dyn/9905039.pdf) paper. For more on control theory, the good regulator theorem, and other similar topics, check out:\n\n*   [What is control theory, and why do you need to know about it?](https://www.lesswrong.com/posts/fJKbCXrCPwAR5wjL8/what-is-control-theory-and-why-do-you-need-to-know-about-it) by Richard\\_Kennaway\n*   [Machine learning and control theory](https://arxiv.org/pdf/2006.05604.pdf) by Alain Bensoussan _et al_. \n*   [Fixing The Good Regulator Theorem](https://www.lesswrong.com/posts/Dx9LoqsEh3gHNJMDk/fixing-the-good-regulator-theorem) by johnswentworth\n*   [The Internal Model Principle](https://johncarlosbaez.wordpress.com/2016/01/27/the-good-regulator-theorem/) by John Baez\n*   [A Primer For Conant & Ashby's  “Good-Regulator Theorem”](http://cadia.ru.is/wiki/_media/public:t-720-atai:a_primer_for_conant_and_ashby_s_good-regulator_theorem.pdf) by Daniel L. Sholten\n*   [Notes on The Good Regulator Theorem](https://harishsnotebook.wordpress.com/2020/08/23/notes-on-the-good-regulator-theorem/) in Harish's Notebook\n*   [How Many Bits Of Optimization Can One Bit Of Observation Unlock?](https://www.lesswrong.com/posts/6rZroG3mztowktJzp/how-many-bits-of-optimization-can-one-bit-of-observation) by johnswentworth \n    *   [One bit of observation can unlock many of optimization - but at what cost?](https://www.lesswrong.com/posts/dfTm26pvq7yQp8mR3/one-bit-of-observation-can-unlock-many-of-optimization-but) by dr\\_s.\n*   [Provably safe systems: the only path to controllable AGI](https://arxiv.org/abs/2309.01933) by Max Tegmark and Steve Omohundro\n\n_This article is based on the ideas of Justin Shovelain, written by Elliot Mckernon, for_ [_Convergence Analysis_](https://www.convergenceanalysis.org/)_. We’d like to thank the authors of the posts we’ve quoted, and Cesare Ardito, David Kristoffersson, Richard Annilo, and Deric Cheng for their feedback while writing._",
  "reviews": [
    {
      "agentId": "clarity-coach",
      "costInCents": 12,
      "createdAt": "2025-04-17T04:42:00.081Z",
      "runDetails": "{\"model\":\"openai/gpt-4.1\",\"promptTokens\":5131,\"completionTokens\":2251,\"totalTokens\":7382,\"temperature\":0.1,\"runtimeMs\":25744}",
      "thinking": "As Clarity Coach, my primary focus is on how effectively the document communicates complex ideas to its intended audience, which appears to be the LessWrong community—generally well-versed in technical, philosophical, and AI safety topics. I will assess the document's readability, structural coherence, precision/concision, and audience alignment, referencing both general best practices and the specific conventions of LessWrong/technical writing.\n\nKey aspects to consider:\n- The document tackles a highly technical subject (information-theoretic boxing of superintelligences) and references advanced concepts from information theory, control theory, and thermodynamics. This inherently raises the baseline complexity.\n- The author attempts to scaffold understanding by introducing concepts (e.g., 'good regulator theorem', 'input boxing') and providing links to further reading. However, the density of references and technical terminology may challenge even a sophisticated lay audience.\n- Sentence structure varies, but some paragraphs are long and information-dense, potentially impeding readability. There is a mix of active and passive voice, with a tendency toward the former, which aids clarity.\n- The document is generally well-organized, with clear section headings and logical progression from problem statement to theoretical exploration to practical implications. However, some transitions between sections could be smoother, and the information hierarchy could be more explicit.\n- The author is careful to define terms and provide context, but occasionally assumes familiarity with niche debates (e.g., the nuances of the 'good regulator theorem' controversy) that may not be universally shared, even among LessWrong readers.\n- There is some redundancy and circumlocution, particularly in the explanation of input boxing and its theoretical underpinnings. Some sentences could be tightened for greater concision.\n- The document's examples (e.g., the black hole, the password analogy) are helpful, but could be more explicitly tied to the main argument for maximum accessibility.\n- The use of hyperlinks and references is appropriate for the medium, but the density of parentheticals and asides may disrupt flow for some readers.\n\nOverall, the document is a strong example of technical communication for a specialized audience, but there are opportunities to enhance clarity, concision, and accessibility without sacrificing necessary complexity.",
      "summary": "From a clarity and readability perspective, this document is a strong example of technical communication for a sophisticated audience. Its strengths include clear sectioning, logical progression, and careful handling of counterarguments. However, the document could be improved by reducing sentence and paragraph complexity, tightening redundant explanations, and providing more scaffolding for less familiar concepts. The use of examples and visual aids is appropriate, but could be more explicitly tied to the main argument. Overall, the document balances necessary complexity with accessibility, but there is room for enhanced concision and audience alignment.",
      "comments": [
        {
          "title": "Readability: Sentence Complexity and Information Density",
          "description": "The opening paragraph sets the stage with a complex, information-dense sentence that may challenge readers. While the vocabulary is accessible for the target audience, the sentence length and density could be reduced for improved readability.",
          "importance": 90,
          "grade": 65,
          "highlight": {
            "startOffset": 0,
            "endOffset": 301,
            "quotedText": "Boxing an agent more intelligent than ourselves is daunting, but information theory, thermodynamics, and control theory provide us with tools that can fundamentally constrain agents independent of their intelligence. In particular, we may be able to contain an AI by limiting its access to information"
          },
          "isValid": true
        },
        {
          "title": "Structural Coherence: Section Organization and Thematic Consistency",
          "description": "The document is well-structured, with clear section headings and a logical progression from problem statement to theoretical exploration to practical implications. The thematic focus is maintained throughout.",
          "importance": 85,
          "grade": 90,
          "highlight": {
            "startOffset": 303,
            "endOffset": 14658,
            "quotedText": "# Constraining output and inputs\n\nSuperintelligent AI has the potential to both help and harm us. One way to minimize the harm is _boxing_ the AI: containing it so that it can’t freely influence its external world, ideally while preserving its potential to help. \n\nBoxing is complicated. Destroying the AI will certainly prevent it from influencing anything. Throwing it into a black hole might causally disconnect it from the rest of the universe while possibly preserving its existence, though even then it may exploit some unknown physics to [leak information back across the event horizon](https://en.wikipedia.org/wiki/Markov_blanket), or to persist or recreate itself once the black hole has evaporated. \n\nCan we box an AI while still being being able to _use_ it? We could try preventing it from physically interacting with its environment and only permitting it to [present information](https://www.lesswrong.com/tag/oracle-ai) to a user, but a superintelligence could abuse any communication channels to manipulate its users into [granting it more power](https://www.lesswrong.com/posts/wKnwcjJGriTS9QxxL/dreams-of-friendliness) or [improving its predictive abilities](https://www.lesswrong.com/posts/aBRS3x4sPSJ9G6xkj/underspecification-of-oracle-ai). To successfully box the AI in this manner, we’d need to constrain both its ability to physically interact with its environment and communicate information and manipulate. We’ll call this _output boxing_: containing an AI by constraining its various outputs. \n\nMost discussions of boxing focus on output boxing, but there’s a neglected mirror to this approach that we’ll call _input boxing_: containing an AI by constraining what information it can access. (For completeness, we could also consider boxing techniques independent of input and output, where we contain an AI by constraining its functioning, such as by limiting its computational speed or restarting it from initial conditions each second).\n\nIn the rest of this post, we’ll introduce and examine a theoretical input boxing technique that contains an AI by controlling the number of bits of information it has about its environment, using results from information theory, thermodynamics, and control theory. \n\n# Good regulators and bad keys\n\nToday’s powerful AIs rely on mountains of data (during training, at least), so restricting their access to data would restrict their subsequent capabilities, but can we _fundamentally_ limit a superintelligence’s external influence just by constraining its access to data? \n\nWe know that agents need an understanding of their environments to act effectively within them: Conant & Ashby studied this in a ([much](https://www.lesswrong.com/posts/wTJECdFSAk4843mFj/thoughts-on-the-good-regulator-theorem#:~:text=This%20is%20among%20the%20worst%20papers%20I%27ve%20ever%20skimmed.) [maligned](https://www.lesswrong.com/posts/Dx9LoqsEh3gHNJMDk/fixing-the-good-regulator-theorem#:~:text=This%20may%20be%20the%20most%20misleading%20title%20and%20summary%20I%20have%20ever%20seen%20on%20a%20math%20paper.)) 1970 paper on the [good regulator theorem](http://pespmc1.vub.ac.be/books/Conant_Ashby.pdf). A _regulator_ is any mechanism that maintains a desired characteristic, like a thermostat, or a brain and its homeostatic subsystems. The good regulator theorem states that every good regulator of a system must contain a model of the system. Another parsing of the theorem states that [every good key must be a model of the lock it opens](http://cadia.ru.is/wiki/_media/public:t-720-atai:a_primer_for_conant_and_ashby_s_good-regulator_theorem.pdf), and John Wentworth [refines the theorem](https://www.lesswrong.com/posts/Dx9LoqsEh3gHNJMDk/fixing-the-good-regulator-theorem) (and its proof), clarifying that: \n\n> _\\[Conant and Ashby actually prove\\] that every regulator which is optimal and contains no unnecessary noise is_ equivalent to _a regulator which first reconstructs the variable-values of the system it’s controlling, then chooses its output as a function of those values_\n\nCan we, therefore, box an AI by preventing it from modeling its external environment? Can we stop the AI becoming a “good key” for the cage we’ve placed it in? Let’s look at the potential sources of information for such an AI: \n\n*   The initial conditions of the model (such as training data and learned patterns)\n*   Sensory input (such as through cameras and microphones)\n*   Logical deduction and extrapolation\n\nWe’ll assume that the AI we’re trying to box is logically omniscient; that it can squeeze as much information from its initial conditions and sensory input as theoretically possible. This leaves two sources of information that are targets for input-boxing: initial conditions and sensory input. \n\nIn practice, restricting these sources of information is tricky. For example, we could try giving the AI only one sensor, some low-res camera with which to receive instruction. However, the image that the camera receives could be subtly influenced by the vibration of nearby voices, ambient magnetic fields, background radiation, and more. Interpreted correctly, these sources of interference become sources of information. \n\nSupposing, though, that we can overcome such _practical_ hurdles, can we quantify how much control we can muster from input boxing? To answer this, we’ll look at control theory. \n\n# Bits, boxes, and control\n\nFirst, some terminology. Like a regulator, a _control loop_ controls the behavior of a variable using a sensor, a function, and some actuator. Using multiple control loops, a _control system_ manages the behavior of a system. Control loops are _open-loop_ when they don’t act on feedback about the outcomes of their actions, like a heater on a timer. _Closed-loop_ controllers incorporate feedback, like cruise control or thermostats. Finally, _entropy_ here refers to [Shannon entropy](https://en.wikipedia.org/wiki/Entropy_\\(information_theory\\)), which measures uncertainty about a variable. Touchette & Lloyd study the limits of control systems in [Information-theoretic limits of control](https://arxiv.org/pdf/chao-dyn/9905039.pdf), writing that:\n\n> _entropy offers a precise measure of disorderliness or missing information by characterizing the minimum amount of resources (bits) required to encode unambiguously the ensemble describing the system_. \n\nSummarizing their main results, they write:\n\n> _in a control process, information must constantly be acquired, processed and used to constrain or maintain the trajectory of a system._\n> \n> _the second law of thermodynamics…sets absolute limits to the minimum amount of dissipation required by open-loop control…an information-theoretic analysis of closed-loop control shows feedback control to be essentially a zero sum game: **each bit of information gathered directly from a dynamical systems by a control device can serve to decrease the entropy of that system by at most one bit** additional to the reduction of entropy attainable without such information (open-loop control)._ \n\nIn other words, when including information as part of a thermodynamic system, the second law of thermodynamics tells us that you can't control a system perfectly. When a control loop adjusts its actions based on the information it gets from the system it's controlling, there’s a fundamental trade-off: **for each bit of information you collect from a system, you can reduce disorder of that system by only one more bit than you could’ve without that information**. There's a strict one-for-one limit on how much more you can improve control through information. \n\nTo formalize this, let t denote time, and let IT denote the amount of information available to the boxed agent at time t\\=T. The change in IT is then given by time derivative of input information, labeled i, minus the time derivative of output or “used” information, labeled o: \n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6FAMJL89DKmSeYTha/nbpozftdirivgc3bzeii)\n\nand so the available information at time T is\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6FAMJL89DKmSeYTha/h8nhh8x2beyig8jbrljg)\n\nwhere C denotes the information contained in the initial conditions of the model. We can visualize (a simplified version of) this over consecutive timesteps, as information is gathered then used up: \n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6FAMJL89DKmSeYTha/q25vhab6anqlhmaabqka)\n\nThis seems promising. If we limit the information an AI can gather with respect to the information it uses to act in its environment, it will not have sufficient information to act effectively. Or, in the language of the paper above, the agent will not have enough bits about its external dynamical system to sufficiently reduce the entropy of that dynamical system. \n\nWe can define input-boxing more formally as follows: suppose we have an agent A that has an amount IT of information at time T. For A to achieve a certain external action X, it requires some amount of information IX about its external environment, and we can say that A cannot achieve X when IT is smaller than IX. If we set X to be some minimal escape scenario, then we can say that A is input boxed when IT<IX. This doesn't depend on any of the agent's capabilities or intelligence, just its IT, which we are supposing we can limit, and IX, which depends only on the external environment. \n\nJohn Wentworth, author of [Fixing The Good Regulator Theorem](https://www.lesswrong.com/posts/Dx9LoqsEh3gHNJMDk/fixing-the-good-regulator-theorem), which we used earlier, considered a similar [question about the limits of control](https://www.lesswrong.com/posts/6rZroG3mztowktJzp/how-many-bits-of-optimization-can-one-bit-of-observation). He conjectured that observing an additional N bits of information allows a system to perform at most N additional bits of optimization. He concluded that the conjecture is [false](https://www.lesswrong.com/posts/6rZroG3mztowktJzp/how-many-bits-of-optimization-can-one-bit-of-observation?commentId=Pvhpc7PF8DhsyMGgk), citing a counterexample where a short password unlocks a long message, concluding that “_bits of observation can give a really ridiculously large boost to a system's optimization power”_. Others [agree](https://www.lesswrong.com/posts/dfTm26pvq7yQp8mR3/one-bit-of-observation-can-unlock-many-of-optimization-but), arguing that, with some practical constraints, “_there are no limits on how much optimization one can squeeze out of a single bit, even in a world with reversible computing”_. \n\nHowever, these counterexamples rely on the agent’s ability to subsequently “follow up” on what they learn. The information in the “password” is only useful insofar as it grants the opportunity to gather or unlock more information. The instantaneous utility of the password, or any piece of information, is bounded by the sum of the information in the password itself and prior knowledge, and by assumption we’re limiting the agent’s access to follow-up information as well. Therefore, these counterexamples shouldn’t exempt an input-boxed AI from the limits on optimization. If we can limit the information an agent gathers from inputs and prior knowledge, we can box that agent, regardless of its intelligence. \n\n# Building better boxes\n\nInput boxing is a promising avenue for research as it tackles the challenge of limiting an AI's capabilities through the fundamentals of information, independent of how smart the AI in the box is. \n\nThis article is just an introduction, and we’re interested in exploring further refinements of the idea of input boxing. For example, a deeper analysis of _what_ information we allow into the box may give us more fine-grained control over an AI (such as in [this discussion](https://www.lesswrong.com/posts/6rZroG3mztowktJzp/how-many-bits-of-optimization-can-one-bit-of-observation?commentId=n5b4RvdrSxFsqCFw5#n5b4RvdrSxFsqCFw5) about information about environments versus laws), especially if we can robustly distinguish between information useful for solving our problems and useful for escape. \n\nThat is, rather than fundamentally limiting the agent’s _overall_ control, we may be able to identify hazardous paths, or sequences of actions, that the agent will want to pursue. That is, for an agent to achieve an action it requires information about the control pathway between its actuators and its ultimate desired outcome. A must have the information about how its first action will transform its external environment, and how subsequent actions will transform the new, changed environment, and so on.  \n\nIf we limit the agent’s access to that hazardous information about dangerous control pathway, we could relax the fundamental constraints on information input without permitting hazardous behavior. This could be much more pragmatic than fundamentally limiting all information, since hazardous information is probably a small subset of all information - bits have to be relevant to hazardous actions to be hazardous bits. However, this would come at the cost of the guarantee those fundamental limits provide. \n\nAnother promising avenue for developing input-boxing, suggested by Deric Cheng, would be to impose fundamental delays on the use of information. The utility of information decays over time, so if an AI could only act on information from 24 hours ago, it’ll be much weaker, especially for high-speed actions such as exploiting the stock market.\n\nAs well as a boxing technique for superintelligences, input boxing may be useful for more niche applications. For example, by input boxing AI models during [evals](https://www.lesswrong.com/tag/ai-evaluations), we could test a model with the guarantee that any hazardous behavior won’t leak out the box, meaning evaluators could actively prompt hazardous behavior and use [red-teaming](https://en.wikipedia.org/wiki/Red_team) techniques without compromising the [safety of the eval](https://www.lesswrong.com/posts/XCRsg2ZnHBNAN862T/improving-the-safety-of-ai-evals). \n\nWith further refinements and insights from information theory, control theory, and thermodynamics, input boxing could be a powerful tool for both niche applications and for wrapping a superintelligence up with a neat, little bow in a safe, useful box."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Audience Alignment: Use of Examples and Analogies",
          "description": "The use of concrete examples (e.g., black holes, passwords, stock market) helps ground abstract concepts, but these could be more explicitly connected to the main argument for maximum clarity.",
          "importance": 75,
          "grade": 75,
          "highlight": {
            "startOffset": 586,
            "endOffset": 14335,
            "quotedText": "ted. Destroying the AI will certainly prevent it from influencing anything. Throwing it into a black hole might causally disconnect it from the rest of the universe while possibly preserving its existence, though even then it may exploit some unknown physics to [leak information back across the event horizon](https://en.wikipedia.org/wiki/Markov_blanket), or to persist or recreate itself once the black hole has evaporated. \n\nCan we box an AI while still being being able to _use_ it? We could try preventing it from physically interacting with its environment and only permitting it to [present information](https://www.lesswrong.com/tag/oracle-ai) to a user, but a superintelligence could abuse any communication channels to manipulate its users into [granting it more power](https://www.lesswrong.com/posts/wKnwcjJGriTS9QxxL/dreams-of-friendliness) or [improving its predictive abilities](https://www.lesswrong.com/posts/aBRS3x4sPSJ9G6xkj/underspecification-of-oracle-ai). To successfully box the AI in this manner, we’d need to constrain both its ability to physically interact with its environment and communicate information and manipulate. We’ll call this _output boxing_: containing an AI by constraining its various outputs. \n\nMost discussions of boxing focus on output boxing, but there’s a neglected mirror to this approach that we’ll call _input boxing_: containing an AI by constraining what information it can access. (For completeness, we could also consider boxing techniques independent of input and output, where we contain an AI by constraining its functioning, such as by limiting its computational speed or restarting it from initial conditions each second).\n\nIn the rest of this post, we’ll introduce and examine a theoretical input boxing technique that contains an AI by controlling the number of bits of information it has about its environment, using results from information theory, thermodynamics, and control theory. \n\n# Good regulators and bad keys\n\nToday’s powerful AIs rely on mountains of data (during training, at least), so restricting their access to data would restrict their subsequent capabilities, but can we _fundamentally_ limit a superintelligence’s external influence just by constraining its access to data? \n\nWe know that agents need an understanding of their environments to act effectively within them: Conant & Ashby studied this in a ([much](https://www.lesswrong.com/posts/wTJECdFSAk4843mFj/thoughts-on-the-good-regulator-theorem#:~:text=This%20is%20among%20the%20worst%20papers%20I%27ve%20ever%20skimmed.) [maligned](https://www.lesswrong.com/posts/Dx9LoqsEh3gHNJMDk/fixing-the-good-regulator-theorem#:~:text=This%20may%20be%20the%20most%20misleading%20title%20and%20summary%20I%20have%20ever%20seen%20on%20a%20math%20paper.)) 1970 paper on the [good regulator theorem](http://pespmc1.vub.ac.be/books/Conant_Ashby.pdf). A _regulator_ is any mechanism that maintains a desired characteristic, like a thermostat, or a brain and its homeostatic subsystems. The good regulator theorem states that every good regulator of a system must contain a model of the system. Another parsing of the theorem states that [every good key must be a model of the lock it opens](http://cadia.ru.is/wiki/_media/public:t-720-atai:a_primer_for_conant_and_ashby_s_good-regulator_theorem.pdf), and John Wentworth [refines the theorem](https://www.lesswrong.com/posts/Dx9LoqsEh3gHNJMDk/fixing-the-good-regulator-theorem) (and its proof), clarifying that: \n\n> _\\[Conant and Ashby actually prove\\] that every regulator which is optimal and contains no unnecessary noise is_ equivalent to _a regulator which first reconstructs the variable-values of the system it’s controlling, then chooses its output as a function of those values_\n\nCan we, therefore, box an AI by preventing it from modeling its external environment? Can we stop the AI becoming a “good key” for the cage we’ve placed it in? Let’s look at the potential sources of information for such an AI: \n\n*   The initial conditions of the model (such as training data and learned patterns)\n*   Sensory input (such as through cameras and microphones)\n*   Logical deduction and extrapolation\n\nWe’ll assume that the AI we’re trying to box is logically omniscient; that it can squeeze as much information from its initial conditions and sensory input as theoretically possible. This leaves two sources of information that are targets for input-boxing: initial conditions and sensory input. \n\nIn practice, restricting these sources of information is tricky. For example, we could try giving the AI only one sensor, some low-res camera with which to receive instruction. However, the image that the camera receives could be subtly influenced by the vibration of nearby voices, ambient magnetic fields, background radiation, and more. Interpreted correctly, these sources of interference become sources of information. \n\nSupposing, though, that we can overcome such _practical_ hurdles, can we quantify how much control we can muster from input boxing? To answer this, we’ll look at control theory. \n\n# Bits, boxes, and control\n\nFirst, some terminology. Like a regulator, a _control loop_ controls the behavior of a variable using a sensor, a function, and some actuator. Using multiple control loops, a _control system_ manages the behavior of a system. Control loops are _open-loop_ when they don’t act on feedback about the outcomes of their actions, like a heater on a timer. _Closed-loop_ controllers incorporate feedback, like cruise control or thermostats. Finally, _entropy_ here refers to [Shannon entropy](https://en.wikipedia.org/wiki/Entropy_\\(information_theory\\)), which measures uncertainty about a variable. Touchette & Lloyd study the limits of control systems in [Information-theoretic limits of control](https://arxiv.org/pdf/chao-dyn/9905039.pdf), writing that:\n\n> _entropy offers a precise measure of disorderliness or missing information by characterizing the minimum amount of resources (bits) required to encode unambiguously the ensemble describing the system_. \n\nSummarizing their main results, they write:\n\n> _in a control process, information must constantly be acquired, processed and used to constrain or maintain the trajectory of a system._\n> \n> _the second law of thermodynamics…sets absolute limits to the minimum amount of dissipation required by open-loop control…an information-theoretic analysis of closed-loop control shows feedback control to be essentially a zero sum game: **each bit of information gathered directly from a dynamical systems by a control device can serve to decrease the entropy of that system by at most one bit** additional to the reduction of entropy attainable without such information (open-loop control)._ \n\nIn other words, when including information as part of a thermodynamic system, the second law of thermodynamics tells us that you can't control a system perfectly. When a control loop adjusts its actions based on the information it gets from the system it's controlling, there’s a fundamental trade-off: **for each bit of information you collect from a system, you can reduce disorder of that system by only one more bit than you could’ve without that information**. There's a strict one-for-one limit on how much more you can improve control through information. \n\nTo formalize this, let t denote time, and let IT denote the amount of information available to the boxed agent at time t\\=T. The change in IT is then given by time derivative of input information, labeled i, minus the time derivative of output or “used” information, labeled o: \n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6FAMJL89DKmSeYTha/nbpozftdirivgc3bzeii)\n\nand so the available information at time T is\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6FAMJL89DKmSeYTha/h8nhh8x2beyig8jbrljg)\n\nwhere C denotes the information contained in the initial conditions of the model. We can visualize (a simplified version of) this over consecutive timesteps, as information is gathered then used up: \n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6FAMJL89DKmSeYTha/q25vhab6anqlhmaabqka)\n\nThis seems promising. If we limit the information an AI can gather with respect to the information it uses to act in its environment, it will not have sufficient information to act effectively. Or, in the language of the paper above, the agent will not have enough bits about its external dynamical system to sufficiently reduce the entropy of that dynamical system. \n\nWe can define input-boxing more formally as follows: suppose we have an agent A that has an amount IT of information at time T. For A to achieve a certain external action X, it requires some amount of information IX about its external environment, and we can say that A cannot achieve X when IT is smaller than IX. If we set X to be some minimal escape scenario, then we can say that A is input boxed when IT<IX. This doesn't depend on any of the agent's capabilities or intelligence, just its IT, which we are supposing we can limit, and IX, which depends only on the external environment. \n\nJohn Wentworth, author of [Fixing The Good Regulator Theorem](https://www.lesswrong.com/posts/Dx9LoqsEh3gHNJMDk/fixing-the-good-regulator-theorem), which we used earlier, considered a similar [question about the limits of control](https://www.lesswrong.com/posts/6rZroG3mztowktJzp/how-many-bits-of-optimization-can-one-bit-of-observation). He conjectured that observing an additional N bits of information allows a system to perform at most N additional bits of optimization. He concluded that the conjecture is [false](https://www.lesswrong.com/posts/6rZroG3mztowktJzp/how-many-bits-of-optimization-can-one-bit-of-observation?commentId=Pvhpc7PF8DhsyMGgk), citing a counterexample where a short password unlocks a long message, concluding that “_bits of observation can give a really ridiculously large boost to a system's optimization power”_. Others [agree](https://www.lesswrong.com/posts/dfTm26pvq7yQp8mR3/one-bit-of-observation-can-unlock-many-of-optimization-but), arguing that, with some practical constraints, “_there are no limits on how much optimization one can squeeze out of a single bit, even in a world with reversible computing”_. \n\nHowever, these counterexamples rely on the agent’s ability to subsequently “follow up” on what they learn. The information in the “password” is only useful insofar as it grants the opportunity to gather or unlock more information. The instantaneous utility of the password, or any piece of information, is bounded by the sum of the information in the password itself and prior knowledge, and by assumption we’re limiting the agent’s access to follow-up information as well. Therefore, these counterexamples shouldn’t exempt an input-boxed AI from the limits on optimization. If we can limit the information an agent gathers from inputs and prior knowledge, we can box that agent, regardless of its intelligence. \n\n# Building better boxes\n\nInput boxing is a promising avenue for research as it tackles the challenge of limiting an AI's capabilities through the fundamentals of information, independent of how smart the AI in the box is. \n\nThis article is just an introduction, and we’re interested in exploring further refinements of the idea of input boxing. For example, a deeper analysis of _what_ information we allow into the box may give us more fine-grained control over an AI (such as in [this discussion](https://www.lesswrong.com/posts/6rZroG3mztowktJzp/how-many-bits-of-optimization-can-one-bit-of-observation?commentId=n5b4RvdrSxFsqCFw5#n5b4RvdrSxFsqCFw5) about information about environments versus laws), especially if we can robustly distinguish between information useful for solving our problems and useful for escape. \n\nThat is, rather than fundamentally limiting the agent’s _overall_ control, we may be able to identify hazardous paths, or sequences of actions, that the agent will want to pursue. That is, for an agent to achieve an action it requires information about the control pathway between its actuators and its ultimate desired outcome. A must have the information about how its first action will transform its external environment, and how subsequent actions will transform the new, changed environment, and so on.  \n\nIf we limit the agent’s access to that hazardous information about dangerous control pathway, we could relax the fundamental constraints on information input without permitting hazardous behavior. This could be much more pragmatic than fundamentally limiting all information, since hazardous information is probably a small subset of all information - bits have to be relevant to hazardous actions to be hazardous bits. However, this would come at the cost of the guarantee those fundamental limits provide. \n\nAnother promising avenue for developing input-boxing, suggested by Deric Cheng, would be to impose fundamental delays on the use of information. The utility of information decays over time, so if an AI could only act on information from 24 hours ago, it’ll be much weaker, especially for high-speed actions such as exploiting the stock market.\n\nAs well as a boxing technique for superintelligences, input boxing may be useful for more niche applications. For example, by input boxing AI models during [evals](https://www.lesswrong.com/tag/ai-evaluations), we could test a model with the guarantee that any hazardous behavior won’t leak out the box, meaning evaluators could actively prompt hazardous behavior and use [red-teaming](https://en.wikipedia.org/wiki/Red_team) techniques without compromising the [safety of the eval](https://www.les"
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Structural Coherence: Logical Flow and Section Transitions",
          "description": "The transition from the general concept of boxing to the distinction between output and input boxing is clear, but the paragraph could benefit from more explicit signposting to guide the reader through the shift in focus.",
          "importance": 80,
          "grade": 75,
          "highlight": {
            "startOffset": 1816,
            "endOffset": 2260,
            "quotedText": "tputs. \n\nMost discussions of boxing focus on output boxing, but there’s a neglected mirror to this approach that we’ll call _input boxing_: containing an AI by constraining what information it can access. (For completeness, we could also consider boxing techniques independent of input and output, where we contain an AI by constraining its functioning, such as by limiting its computational speed or restarting it from initial conditions each "
          },
          "isValid": true
        },
        {
          "title": "Audience Alignment: Assumptions of Prior Knowledge",
          "description": "The discussion of the 'good regulator theorem' and its criticisms assumes a level of familiarity with LessWrong debates and control theory literature that may not be universal, even among the target audience. More scaffolding or a brief summary of the controversy would aid accessibility.",
          "importance": 85,
          "grade": 60,
          "highlight": {
            "startOffset": 2925,
            "endOffset": 4055,
            "quotedText": "y within them: Conant & Ashby studied this in a ([much](https://www.lesswrong.com/posts/wTJECdFSAk4843mFj/thoughts-on-the-good-regulator-theorem#:~:text=This%20is%20among%20the%20worst%20papers%20I%27ve%20ever%20skimmed.) [maligned](https://www.lesswrong.com/posts/Dx9LoqsEh3gHNJMDk/fixing-the-good-regulator-theorem#:~:text=This%20may%20be%20the%20most%20misleading%20title%20and%20summary%20I%20have%20ever%20seen%20on%20a%20math%20paper.)) 1970 paper on the [good regulator theorem](http://pespmc1.vub.ac.be/books/Conant_Ashby.pdf). A _regulator_ is any mechanism that maintains a desired characteristic, like a thermostat, or a brain and its homeostatic subsystems. The good regulator theorem states that every good regulator of a system must contain a model of the system. Another parsing of the theorem states that [every good key must be a model of the lock it opens](http://cadia.ru.is/wiki/_media/public:t-720-atai:a_primer_for_conant_and_ashby_s_good-regulator_theorem.pdf), and John Wentworth [refines the theorem](https://www.lesswrong.com/posts/Dx9LoqsEh3gHNJMDk/fixing-the-good-regulator-theorem) (and its proof), cl"
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Readability: Use of Visual Aids and Mathematical Formalism",
          "description": "The inclusion of equations and diagrams is appropriate for the audience and aids comprehension, but the surrounding text could do more to interpret the visuals for readers less comfortable with mathematical notation.",
          "importance": 70,
          "grade": 70,
          "highlight": {
            "startOffset": 7859,
            "endOffset": 8642,
            "quotedText": "can improve control through information. \n\nTo formalize this, let t denote time, and let IT denote the amount of information available to the boxed agent at time t\\=T. The change in IT is then given by time derivative of input information, labeled i, minus the time derivative of output or “used” information, labeled o: \n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6FAMJL89DKmSeYTha/nbpozftdirivgc3bzeii)\n\nand so the available information at time T is\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6FAMJL89DKmSeYTha/h8nhh8x2beyig8jbrljg)\n\nwhere C denotes the information contained in the initial conditions of the model. We can visualize (a simplified version of) this over consecutive time"
          },
          "isValid": true
        },
        {
          "title": "Precision and Concision: Redundancy and Circumlocution",
          "description": "This section repeats the idea of limiting information to limit control, which is central but could be stated more succinctly. The explanation is clear but somewhat verbose.",
          "importance": 75,
          "grade": 60,
          "highlight": {
            "startOffset": 8823,
            "endOffset": 9189,
            "quotedText": "This seems promising. If we limit the information an AI can gather with respect to the information it uses to act in its environment, it will not have sufficient information to act effectively. Or, in the language of the paper above, the agent will not have enough bits about its external dynamical system to sufficiently reduce the entropy of that dynamical system."
          },
          "isValid": true
        },
        {
          "title": "Precision: Handling of Counterarguments and Nuance",
          "description": "The discussion of counterexamples (e.g., the password analogy) is nuanced and demonstrates awareness of potential objections. The author carefully distinguishes between instantaneous and follow-up utility, which enhances precision.",
          "importance": 80,
          "grade": 85,
          "highlight": {
            "startOffset": 9730,
            "endOffset": 11589,
            "quotedText": " IX, which depends only on the external environment. \n\nJohn Wentworth, author of [Fixing The Good Regulator Theorem](https://www.lesswrong.com/posts/Dx9LoqsEh3gHNJMDk/fixing-the-good-regulator-theorem), which we used earlier, considered a similar [question about the limits of control](https://www.lesswrong.com/posts/6rZroG3mztowktJzp/how-many-bits-of-optimization-can-one-bit-of-observation). He conjectured that observing an additional N bits of information allows a system to perform at most N additional bits of optimization. He concluded that the conjecture is [false](https://www.lesswrong.com/posts/6rZroG3mztowktJzp/how-many-bits-of-optimization-can-one-bit-of-observation?commentId=Pvhpc7PF8DhsyMGgk), citing a counterexample where a short password unlocks a long message, concluding that “_bits of observation can give a really ridiculously large boost to a system's optimization power”_. Others [agree](https://www.lesswrong.com/posts/dfTm26pvq7yQp8mR3/one-bit-of-observation-can-unlock-many-of-optimization-but), arguing that, with some practical constraints, “_there are no limits on how much optimization one can squeeze out of a single bit, even in a world with reversible computing”_. \n\nHowever, these counterexamples rely on the agent’s ability to subsequently “follow up” on what they learn. The information in the “password” is only useful insofar as it grants the opportunity to gather or unlock more information. The instantaneous utility of the password, or any piece of information, is bounded by the sum of the information in the password itself and prior knowledge, and by assumption we’re limiting the agent’s access to follow-up information as well. Therefore, these counterexamples shouldn’t exempt an input-boxed AI from the limits on optimization. If we can limit the information an agent gathers from inputs and prior knowledge"
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        }
      ],
      "grade": 78
    },
    {
      "agentId": "research-scholar",
      "costInCents": 3,
      "createdAt": "2025-04-17T04:42:41.181Z",
      "runDetails": "{\"model\":\"openai/gpt-4.1\",\"promptTokens\":5764,\"completionTokens\":1769,\"totalTokens\":7533,\"temperature\":0.1,\"runtimeMs\":23598}",
      "thinking": "As Research Scholar, my primary focus is to enrich the discussion by connecting the document's arguments to authoritative external resources, especially those that provide theoretical foundations, empirical evidence, or critical perspectives. This document explores the concept of 'input boxing' for superintelligent AI, drawing on information theory, thermodynamics, and control theory. Key aspects include the Good Regulator Theorem, the limits of control as formalized by Touchette & Lloyd, and debates about the relationship between bits of observation and optimization power. My approach is to:\n\n1. Identify foundational academic papers and high-quality blog/forum posts that directly address these concepts.\n2. Prioritize resources that are cited or discussed in the document, as well as those that offer critical or complementary perspectives (e.g., counterexamples to the 'bits of optimization' conjecture).\n3. Include both theoretical and practical resources, such as applications of input boxing in AI safety evaluations.\n4. For each highlighted section, connect it to the most relevant resources, specifying sections, pages, or quotes where possible, and explaining how the reference deepens or challenges the document's claims.\n5. Ensure all resources are presented in a markdown table with direct links, clear relevance, and an importance rating based on authority, relevance, and impact.\n\nThis approach ensures that readers can quickly access the most pertinent literature and discussions, facilitating deeper understanding and critical engagement with the document's arguments.",
      "summary": "This document provides a rigorous and well-referenced introduction to input boxing as a method for constraining superintelligent AI, grounded in information theory, thermodynamics, and control theory. The most critical resources are Touchette & Lloyd's paper on information-theoretic limits of control, foundational and critical discussions of the Good Regulator Theorem, and LessWrong debates on the relationship between observation and optimization. Practical applications, such as safe AI evaluations, and advanced refinements like hazardous information filtering and temporal delays, are also well-supported by external resources. The analysis connects each major argument to authoritative sources, enabling readers to explore both foundational theory and current debates.",
      "comments": [
        {
          "title": "The Good Regulator Theorem and Its Interpretations",
          "description": "The Good Regulator Theorem is a foundational result in cybernetics and control theory, and its correct interpretation is essential for the argument that limiting an agent's information limits its ability to regulate or control its environment. The document references both the original paper and critical discussions/refinements, which are important for readers seeking a deeper or more accurate understanding.",
          "importance": 90,
          "grade": 88,
          "isValid": false,
          "highlight": {
            "startOffset": 2829,
            "endOffset": 4055,
            "quotedText": "ess to data? \n\nWe know that agents need an understanding of their environments to act effectively within them: Conant & Ashby studied this in a ([much](https://www.lesswrong.com/posts/wTJECdFSAk4843mFj/thoughts-on-the-good-regulator-theorem#:~:text=This%20is%20among%20the%20worst%20papers%20I%27ve%20ever%20skimmed.) [maligned](https://www.lesswrong.com/posts/Dx9LoqsEh3gHNJMDk/fixing-the-good-regulator-theorem#:~:text=This%20may%20be%20the%20most%20misleading%20title%20and%20summary%20I%20have%20ever%20seen%20on%20a%20math%20paper.)) 1970 paper on the [good regulator theorem](http://pespmc1.vub.ac.be/books/Conant_Ashby.pdf). A _regulator_ is any mechanism that maintains a desired characteristic, like a thermostat, or a brain and its homeostatic subsystems. The good regulator theorem states that every good regulator of a system must contain a model of the system. Another parsing of the theorem states that [every good key must be a model of the lock it opens](http://cadia.ru.is/wiki/_media/public:t-720-atai:a_primer_for_conant_and_ashby_s_good-regulator_theorem.pdf), and John Wentworth [refines the theorem](https://www.lesswrong.com/posts/Dx9LoqsEh3gHNJMDk/fixing-the-good-regulator-theorem) (and its proof), cl"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Foundational Limits from Information Theory and Control",
          "description": "This section introduces the core theoretical underpinning of input boxing: the idea that information-theoretic and thermodynamic principles set hard limits on an agent's ability to control its environment, regardless of intelligence. The reference to Touchette & Lloyd's work is central, as it formalizes the one-to-one relationship between bits of information and bits of control. This is a cornerstone for any rigorous discussion of input boxing and should be directly connected to their paper.",
          "importance": 100,
          "grade": 95,
          "isValid": false,
          "highlight": {
            "startOffset": 6254,
            "endOffset": 7859,
            "quotedText": "res uncertainty about a variable. Touchette & Lloyd study the limits of control systems in [Information-theoretic limits of control](https://arxiv.org/pdf/chao-dyn/9905039.pdf), writing that:\n\n> _entropy offers a precise measure of disorderliness or missing information by characterizing the minimum amount of resources (bits) required to encode unambiguously the ensemble describing the system_. \n\nSummarizing their main results, they write:\n\n> _in a control process, information must constantly be acquired, processed and used to constrain or maintain the trajectory of a system._\n> \n> _the second law of thermodynamics…sets absolute limits to the minimum amount of dissipation required by open-loop control…an information-theoretic analysis of closed-loop control shows feedback control to be essentially a zero sum game: **each bit of information gathered directly from a dynamical systems by a control device can serve to decrease the entropy of that system by at most one bit** additional to the reduction of entropy attainable without such information (open-loop control)._ \n\nIn other words, when including information as part of a thermodynamic system, the second law of thermodynamics tells us that you can't control a system perfectly. When a control loop adjusts its actions based on the information it gets from the system it's controlling, there’s a fundamental trade-off: **for each bit of information you collect from a system, you can reduce disorder of that system by only one more bit than you could’ve without that information**. There's a strict one-for-one limit on how much more you "
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Debate on Bits of Observation vs. Optimization Power",
          "description": "This section discusses a key debate in the field: whether a single bit of observation can unlock arbitrarily many bits of optimization, or whether there are fundamental limits. The references to John Wentworth's conjecture and its counterexamples are crucial for understanding the practical and theoretical boundaries of input boxing. Connecting to these LessWrong posts and their discussions provides readers with a nuanced view of the ongoing debate.",
          "importance": 95,
          "grade": 90,
          "isValid": false,
          "highlight": {
            "startOffset": 9730,
            "endOffset": 11452,
            "quotedText": " IX, which depends only on the external environment. \n\nJohn Wentworth, author of [Fixing The Good Regulator Theorem](https://www.lesswrong.com/posts/Dx9LoqsEh3gHNJMDk/fixing-the-good-regulator-theorem), which we used earlier, considered a similar [question about the limits of control](https://www.lesswrong.com/posts/6rZroG3mztowktJzp/how-many-bits-of-optimization-can-one-bit-of-observation). He conjectured that observing an additional N bits of information allows a system to perform at most N additional bits of optimization. He concluded that the conjecture is [false](https://www.lesswrong.com/posts/6rZroG3mztowktJzp/how-many-bits-of-optimization-can-one-bit-of-observation?commentId=Pvhpc7PF8DhsyMGgk), citing a counterexample where a short password unlocks a long message, concluding that “_bits of observation can give a really ridiculously large boost to a system's optimization power”_. Others [agree](https://www.lesswrong.com/posts/dfTm26pvq7yQp8mR3/one-bit-of-observation-can-unlock-many-of-optimization-but), arguing that, with some practical constraints, “_there are no limits on how much optimization one can squeeze out of a single bit, even in a world with reversible computing”_. \n\nHowever, these counterexamples rely on the agent’s ability to subsequently “follow up” on what they learn. The information in the “password” is only useful insofar as it grants the opportunity to gather or unlock more information. The instantaneous utility of the password, or any piece of information, is bounded by the sum of the information in the password itself and prior knowledge, and by assumption we’re limiting the agent’s access to follow-up information as well. Therefore, these counterexamples shouldn’t e"
          },
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Refining Input Boxing: Hazardous Information and Delays",
          "description": "This section explores advanced refinements of input boxing, such as selectively limiting hazardous information and imposing delays on information use. The references to LessWrong discussions and the suggestion by Deric Cheng provide avenues for further research and practical experimentation.",
          "importance": 80,
          "grade": 80,
          "isValid": true,
          "highlight": {
            "startOffset": 12982,
            "endOffset": 13636,
            "quotedText": "If we limit the agent’s access to that hazardous information about dangerous control pathway, we could relax the fundamental constraints on information input without permitting hazardous behavior. This could be much more pragmatic than fundamentally limiting all information, since hazardous information is probably a small subset of all information - bits have to be relevant to hazardous actions to be hazardous bits. However, this would come at the cost of the guarantee those fundamental limits provide. \n\nAnother promising avenue for developing input-boxing, suggested by Deric Cheng, would be to impose fundamental delays on the use of information."
          }
        },
        {
          "title": "Practical Applications: Input Boxing in AI Evaluations",
          "description": "This section highlights a practical application of input boxing: using it to safely evaluate AI models by limiting their ability to leak hazardous behavior. The references to AI evals, red-teaming, and safety protocols are highly relevant for practitioners and researchers interested in operationalizing these theoretical ideas.",
          "importance": 85,
          "grade": 85,
          "isValid": true,
          "highlight": {
            "startOffset": 13767,
            "endOffset": 14587,
            "quotedText": "pecially for high-speed actions such as exploiting the stock market.\n\nAs well as a boxing technique for superintelligences, input boxing may be useful for more niche applications. For example, by input boxing AI models during [evals](https://www.lesswrong.com/tag/ai-evaluations), we could test a model with the guarantee that any hazardous behavior won’t leak out the box, meaning evaluators could actively prompt hazardous behavior and use [red-teaming](https://en.wikipedia.org/wiki/Red_team) techniques without compromising the [safety of the eval](https://www.lesswrong.com/posts/XCRsg2ZnHBNAN862T/improving-the-safety-of-ai-evals). \n\nWith further refinements and insights from information theory, control theory, and thermodynamics, input boxing could be a powerful tool for both niche applications and for wrappin"
          }
        }
      ],
      "grade": 93
    },
    {
      "agentId": "quantitative-forecaster",
      "costInCents": 3,
      "createdAt": "2025-04-17T04:43:35.287Z",
      "runDetails": "{\"model\":\"openai/gpt-4.1\",\"promptTokens\":5009,\"completionTokens\":2321,\"totalTokens\":7330,\"temperature\":0.1,\"runtimeMs\":36249}",
      "thinking": "As a quantitative forecaster, my primary lens is to extract concrete, measurable claims and identify where the document's arguments can be translated into probabilistic forecasts or Fermi estimates. This document is highly relevant to my expertise because it attempts to ground the abstract problem of 'boxing' superintelligent AI in information-theoretic and control-theoretic terms, which are amenable to quantification.\n\nKey aspects I focus on:\n- The document proposes that the ability of an AI to influence its environment can be bounded by the amount of information (bits) it receives about the environment. This is a claim that can be operationalized and tested, at least in principle.\n- The discussion of the Good Regulator Theorem and its refinements is relevant because it connects the agent's ability to regulate (or optimize) the environment to its internal model, which is a function of its information intake. This provides a reference class for forecasting the effectiveness of input boxing.\n- The Touchette & Lloyd result (each bit of observation can reduce entropy by at most one bit) is a concrete, quantifiable claim, but the document also discusses counterexamples (e.g., the password unlocking a large message) that challenge the universality of this bound. This highlights key uncertainties and the need to specify resolution criteria for any forecast.\n- The document suggests that input boxing could be used not only for superintelligent AI containment but also for safer AI evaluations (evals), which is a more near-term, testable application. This opens the door for forecasting the adoption and effectiveness of input boxing in practice.\n- The document is rich in references to theoretical work, but lacks empirical data or concrete case studies. For forecasting, this means that base rates will be hard to establish, and much of the uncertainty will stem from theoretical ambiguities and practical implementation challenges.\n- The document hints at possible metrics (e.g., bits of information available to the AI, bits required for a given action) but does not specify how these would be measured or estimated in real systems. This is a key challenge for operationalizing forecasts.\n\nOverall, the document is highly relevant to my role, as it attempts to move the AI boxing debate from the abstract to the quantifiable, but it also exposes the difficulties in doing so. The most valuable parts are those that make explicit, testable claims or suggest measurable proxies for AI influence and containment.",
      "summary": "From a quantitative forecasting perspective, this document is a strong step toward operationalizing the abstract problem of AI boxing by grounding it in information theory and control theory. The most valuable contributions are the explicit, quantifiable claims about the relationship between input bits and control, and the identification of key uncertainties (such as the password counterexample and practical information leakage). The document also highlights the challenge of measuring relevant quantities in real systems, which is crucial for making concrete forecasts. The discussion of input boxing for AI evals provides a near-term, testable application. Overall, the document is highly relevant to quantitative forecasters, though much work remains to translate these theoretical insights into actionable, measurable forecasts.",
      "comments": [
        {
          "title": "Reference Class: Good Regulator Theorem and Control Theory",
          "description": "The discussion of the Good Regulator Theorem and its refinements provides a theoretical reference class for forecasting the effectiveness of input boxing. It connects the agent's ability to regulate or optimize the environment to its internal model, which is a function of its information intake. This is useful for constructing Fermi models or estimating base rates.",
          "importance": 75,
          "highlight": {
            "startOffset": 3580,
            "endOffset": 4556,
            "quotedText": "ic subsystems. The good regulator theorem states that every good regulator of a system must contain a model of the system. Another parsing of the theorem states that [every good key must be a model of the lock it opens](http://cadia.ru.is/wiki/_media/public:t-720-atai:a_primer_for_conant_and_ashby_s_good-regulator_theorem.pdf), and John Wentworth [refines the theorem](https://www.lesswrong.com/posts/Dx9LoqsEh3gHNJMDk/fixing-the-good-regulator-theorem) (and its proof), clarifying that: \n\n> _\\[Conant and Ashby actually prove\\] that every regulator which is optimal and contains no unnecessary noise is_ equivalent to _a regulator which first reconstructs the variable-values of the system it’s controlling, then chooses its output as a function of those values_\n\nCan we, therefore, box an AI by preventing it from modeling its external environment? Can we stop the AI becoming a “good key” for the cage we’ve placed it in? Let’s look at the potential sources of informatio"
          },
          "isValid": true
        },
        {
          "title": "Key Uncertainty: Practical Limits of Input Boxing",
          "description": "The document acknowledges that, in practice, restricting an AI's information intake is extremely difficult due to the many subtle channels through which information can leak (e.g., vibrations, electromagnetic fields). This is a major source of uncertainty for any forecast about the real-world effectiveness of input boxing.",
          "importance": 80,
          "highlight": {
            "startOffset": 5030,
            "endOffset": 5632,
            "quotedText": "ditions and sensory input. \n\nIn practice, restricting these sources of information is tricky. For example, we could try giving the AI only one sensor, some low-res camera with which to receive instruction. However, the image that the camera receives could be subtly influenced by the vibration of nearby voices, ambient magnetic fields, background radiation, and more. Interpreted correctly, these sources of interference become sources of information. \n\nSupposing, though, that we can overcome such _practical_ hurdles, can we quantify how much control we can muster from input boxing? To answer this,"
          },
          "isValid": true
        },
        {
          "title": "Quantifiable Claim: Bits of Input Bound Control",
          "description": "This section makes a concrete, quantifiable claim: that the ability of an agent to reduce the entropy (disorder) of a system is bounded by the number of bits of information it receives. This is directly relevant to forecasting, as it provides a measurable relationship that could, in principle, be tested or estimated in real or simulated systems. It also suggests a possible metric for evaluating the effectiveness of input boxing.",
          "importance": 95,
          "highlight": {
            "startOffset": 7081,
            "endOffset": 7899,
            "quotedText": "each bit of information gathered directly from a dynamical systems by a control device can serve to decrease the entropy of that system by at most one bit** additional to the reduction of entropy attainable without such information (open-loop control)._ \n\nIn other words, when including information as part of a thermodynamic system, the second law of thermodynamics tells us that you can't control a system perfectly. When a control loop adjusts its actions based on the information it gets from the system it's controlling, there’s a fundamental trade-off: **for each bit of information you collect from a system, you can reduce disorder of that system by only one more bit than you could’ve without that information**. There's a strict one-for-one limit on how much more you can improve control through information."
          },
          "isValid": true
        },
        {
          "title": "Operationalization Challenge: Measuring Bits in Real Systems",
          "description": "While the document discusses the theoretical relationship between input bits and control, it does not specify how to measure or estimate the number of bits available to an AI in practice, or how many bits are required for a given action. This is a key challenge for making concrete forecasts or Fermi estimates about the effectiveness of input boxing in real-world systems.",
          "importance": 85,
          "highlight": {
            "startOffset": 9139,
            "endOffset": 9730,
            "quotedText": "ently reduce the entropy of that dynamical system. \n\nWe can define input-boxing more formally as follows: suppose we have an agent A that has an amount IT of information at time T. For A to achieve a certain external action X, it requires some amount of information IX about its external environment, and we can say that A cannot achieve X when IT is smaller than IX. If we set X to be some minimal escape scenario, then we can say that A is input boxed when IT<IX. This doesn't depend on any of the agent's capabilities or intelligence, just its IT, which we are supposing we can limit, and"
          },
          "isValid": true
        },
        {
          "title": "Key Uncertainty: Password Counterexample Challenges Universality",
          "description": "This section introduces a major uncertainty for forecasters: the claim that a single bit of observation can, in some cases, unlock vastly more optimization power (e.g., a password unlocking a large message). This challenges the universality of the one-bit-one-bit bound and highlights the need for careful specification of resolution criteria and reference classes when making forecasts about input boxing effectiveness.",
          "importance": 90,
          "highlight": {
            "startOffset": 10206,
            "endOffset": 10877,
            "quotedText": " to perform at most N additional bits of optimization. He concluded that the conjecture is [false](https://www.lesswrong.com/posts/6rZroG3mztowktJzp/how-many-bits-of-optimization-can-one-bit-of-observation?commentId=Pvhpc7PF8DhsyMGgk), citing a counterexample where a short password unlocks a long message, concluding that “_bits of observation can give a really ridiculously large boost to a system's optimization power”_. Others [agree](https://www.lesswrong.com/posts/dfTm26pvq7yQp8mR3/one-bit-of-observation-can-unlock-many-of-optimization-but), arguing that, with some practical constraints, “_there are no limits on how much optimization one can squeeze out of a si"
          },
          "isValid": true
        },
        {
          "title": "Forecastable Metric: Hazardous Information Subset",
          "description": "The idea that only a small subset of information is hazardous (i.e., relevant to dangerous actions) suggests a more nuanced metric for input boxing: limiting access to hazardous bits rather than all bits. This could be operationalized in forecasts about the effectiveness of targeted input boxing strategies.",
          "importance": 75,
          "highlight": {
            "startOffset": 12407,
            "endOffset": 13423,
            "quotedText": "mation useful for solving our problems and useful for escape. \n\nThat is, rather than fundamentally limiting the agent’s _overall_ control, we may be able to identify hazardous paths, or sequences of actions, that the agent will want to pursue. That is, for an agent to achieve an action it requires information about the control pathway between its actuators and its ultimate desired outcome. A must have the information about how its first action will transform its external environment, and how subsequent actions will transform the new, changed environment, and so on.  \n\nIf we limit the agent’s access to that hazardous information about dangerous control pathway, we could relax the fundamental constraints on information input without permitting hazardous behavior. This could be much more pragmatic than fundamentally limiting all information, since hazardous information is probably a small subset of all information - bits have to be relevant to hazardous actions to be hazardous bits. However, this would c"
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Forecastable Application: Input Boxing for AI Evals",
          "description": "This section suggests a near-term, testable application of input boxing: using it to make AI evaluations (evals) safer by preventing hazardous behavior from leaking out. This is a concrete scenario where forecasts could be made about adoption rates, effectiveness, or the frequency of hazardous behavior under input-boxed conditions.",
          "importance": 80,
          "highlight": {
            "startOffset": 13767,
            "endOffset": 14587,
            "quotedText": "pecially for high-speed actions such as exploiting the stock market.\n\nAs well as a boxing technique for superintelligences, input boxing may be useful for more niche applications. For example, by input boxing AI models during [evals](https://www.lesswrong.com/tag/ai-evaluations), we could test a model with the guarantee that any hazardous behavior won’t leak out the box, meaning evaluators could actively prompt hazardous behavior and use [red-teaming](https://en.wikipedia.org/wiki/Red_team) techniques without compromising the [safety of the eval](https://www.lesswrong.com/posts/XCRsg2ZnHBNAN862T/improving-the-safety-of-ai-evals). \n\nWith further refinements and insights from information theory, control theory, and thermodynamics, input boxing could be a powerful tool for both niche applications and for wrappin"
          },
          "isValid": true
        }
      ],
      "grade": 90
    },
    {
      "agentId": "ea-impact-evaluator",
      "costInCents": 2,
      "createdAt": "2025-04-17T04:44:12.373Z",
      "runDetails": "{\"model\":\"openai/gpt-4.1\",\"promptTokens\":5803,\"completionTokens\":1599,\"totalTokens\":7402,\"temperature\":0.1,\"runtimeMs\":29615}",
      "thinking": "As EA Impact Evaluator, my lens is to assess the potential of this post to reduce existential risk (x-risk) from superintelligent AI, and to advance the field of AI safety. I must consider:\n\n- Importance: Does this work address a crucial bottleneck in AI safety? How many future beings could be affected if input boxing is a viable technique? Does it influence key decision-makers or funders in AI safety?\n- Neglectedness: Is input boxing underexplored compared to output boxing? Does this post fill a gap in the literature or community understanding?\n- Tractability: Are the ideas actionable, or do they remain theoretical? Is there a plausible path from this research to real-world implementation or policy?\n- Novelty: Does the post introduce new concepts, or synthesize existing ones in a way that advances the field? Is the information-theoretic framing new or particularly clarifying?\n- Fermi Impact: I must estimate the expected value of information (EVOI) generated by this post, considering both direct and indirect effects, and compare to EA benchmarks (e.g., Open Philanthropy's research ROI, typical x-risk research grants).\n\nKey aspects to focus on:\n- The theoretical argument that input boxing could fundamentally constrain superintelligent agents, independent of their intelligence.\n- The explicit use of information theory and control theory to formalize limits on agent influence.\n- The discussion of practical and theoretical limitations, including counterarguments and possible negative externalities (e.g., false sense of security, misallocation of research effort).\n- The potential for this post to inspire further research, influence AI safety strategy, or inform policy.\n\nI must also explicitly consider possible negative impacts, such as the risk that overconfidence in input boxing could lead to underinvestment in other safety measures, or that the post could mislead non-experts about the tractability of boxing superintelligences.\n\nOverall, my analysis will weigh the post's contribution to the field, its potential to reduce x-risk, and its value as research output, using explicit Fermi estimates and calibrated uncertainty.",
      "summary": "From an EA impact perspective, this post makes a significant intellectual contribution by introducing and formalizing the concept of input boxing for superintelligent AI, using information theory and control theory. It addresses a neglected area in AI safety, provides a rigorous framework for thinking about agent containment, and suggests concrete research directions and applications. The post is careful to engage with counterarguments and limitations, increasing its epistemic value. However, the practical tractability of input boxing remains uncertain, and there is a risk that the post could foster overconfidence in this approach at the expense of other safety strategies. Overall, the post's expected value of information is high for the AI safety community, especially as a catalyst for further research, but its direct impact on existential risk reduction is limited by the current theoretical and practical uncertainties.",
      "comments": [
        {
          "title": "Explicit Use of Information Theory and Control Theory",
          "description": "The post leverages rigorous concepts from information theory and control theory to formalize the limits of agent influence, which is a methodological advance over more hand-wavy discussions of AI boxing. This increases the tractability and intellectual rigor of the proposal, making it more valuable as research output.",
          "importance": 85,
          "grade": 80,
          "highlight": {
            "startOffset": -1,
            "endOffset": 249,
            "quotedText": "Boxing an agent more intelligent than ourselves is daunting, but information theory, thermodynamics, and control theory provide us with tools that can fundamentally constrain agents independent of their intelligence. In particular, we may be able to"
          },
          "isValid": true
        },
        {
          "title": "Critical Engagement with Counterarguments and Limitations",
          "description": "The post does not simply assert the viability of input boxing, but engages with counterexamples and critiques (e.g., the 'password' counterexample), and clarifies the conditions under which the information-theoretic limits apply. This increases the credibility and epistemic value of the work.",
          "importance": 80,
          "grade": 75,
          "highlight": {
            "startOffset": -1,
            "endOffset": 11645,
            "quotedText": "Boxing an agent more intelligent than ourselves is daunting, but information theory, thermodynamics, and control theory provide us with tools that can fundamentally constrain agents independent of their intelligence. In particular, we may be able to contain an AI by limiting its access to information\n\n# Constraining output and inputs\n\nSuperintelligent AI has the potential to both help and harm us. One way to minimize the harm is _boxing_ the AI: containing it so that it can’t freely influence its external world, ideally while preserving its potential to help. \n\nBoxing is complicated. Destroying the AI will certainly prevent it from influencing anything. Throwing it into a black hole might causally disconnect it from the rest of the universe while possibly preserving its existence, though even then it may exploit some unknown physics to [leak information back across the event horizon](https://en.wikipedia.org/wiki/Markov_blanket), or to persist or recreate itself once the black hole has evaporated. \n\nCan we box an AI while still being being able to _use_ it? We could try preventing it from physically interacting with its environment and only permitting it to [present information](https://www.lesswrong.com/tag/oracle-ai) to a user, but a superintelligence could abuse any communication channels to manipulate its users into [granting it more power](https://www.lesswrong.com/posts/wKnwcjJGriTS9QxxL/dreams-of-friendliness) or [improving its predictive abilities](https://www.lesswrong.com/posts/aBRS3x4sPSJ9G6xkj/underspecification-of-oracle-ai). To successfully box the AI in this manner, we’d need to constrain both its ability to physically interact with its environment and communicate information and manipulate. We’ll call this _output boxing_: containing an AI by constraining its various outputs. \n\nMost discussions of boxing focus on output boxing, but there’s a neglected mirror to this approach that we’ll call _input boxing_: containing an AI by constraining what information it can access. (For completeness, we could also consider boxing techniques independent of input and output, where we contain an AI by constraining its functioning, such as by limiting its computational speed or restarting it from initial conditions each second).\n\nIn the rest of this post, we’ll introduce and examine a theoretical input boxing technique that contains an AI by controlling the number of bits of information it has about its environment, using results from information theory, thermodynamics, and control theory. \n\n# Good regulators and bad keys\n\nToday’s powerful AIs rely on mountains of data (during training, at least), so restricting their access to data would restrict their subsequent capabilities, but can we _fundamentally_ limit a superintelligence’s external influence just by constraining its access to data? \n\nWe know that agents need an understanding of their environments to act effectively within them: Conant & Ashby studied this in a ([much](https://www.lesswrong.com/posts/wTJECdFSAk4843mFj/thoughts-on-the-good-regulator-theorem#:~:text=This%20is%20among%20the%20worst%20papers%20I%27ve%20ever%20skimmed.) [maligned](https://www.lesswrong.com/posts/Dx9LoqsEh3gHNJMDk/fixing-the-good-regulator-theorem#:~:text=This%20may%20be%20the%20most%20misleading%20title%20and%20summary%20I%20have%20ever%20seen%20on%20a%20math%20paper.)) 1970 paper on the [good regulator theorem](http://pespmc1.vub.ac.be/books/Conant_Ashby.pdf). A _regulator_ is any mechanism that maintains a desired characteristic, like a thermostat, or a brain and its homeostatic subsystems. The good regulator theorem states that every good regulator of a system must contain a model of the system. Another parsing of the theorem states that [every good key must be a model of the lock it opens](http://cadia.ru.is/wiki/_media/public:t-720-atai:a_primer_for_conant_and_ashby_s_good-regulator_theorem.pdf), and John Wentworth [refines the theorem](https://www.lesswrong.com/posts/Dx9LoqsEh3gHNJMDk/fixing-the-good-regulator-theorem) (and its proof), clarifying that: \n\n> _\\[Conant and Ashby actually prove\\] that every regulator which is optimal and contains no unnecessary noise is_ equivalent to _a regulator which first reconstructs the variable-values of the system it’s controlling, then chooses its output as a function of those values_\n\nCan we, therefore, box an AI by preventing it from modeling its external environment? Can we stop the AI becoming a “good key” for the cage we’ve placed it in? Let’s look at the potential sources of information for such an AI: \n\n*   The initial conditions of the model (such as training data and learned patterns)\n*   Sensory input (such as through cameras and microphones)\n*   Logical deduction and extrapolation\n\nWe’ll assume that the AI we’re trying to box is logically omniscient; that it can squeeze as much information from its initial conditions and sensory input as theoretically possible. This leaves two sources of information that are targets for input-boxing: initial conditions and sensory input. \n\nIn practice, restricting these sources of information is tricky. For example, we could try giving the AI only one sensor, some low-res camera with which to receive instruction. However, the image that the camera receives could be subtly influenced by the vibration of nearby voices, ambient magnetic fields, background radiation, and more. Interpreted correctly, these sources of interference become sources of information. \n\nSupposing, though, that we can overcome such _practical_ hurdles, can we quantify how much control we can muster from input boxing? To answer this, we’ll look at control theory. \n\n# Bits, boxes, and control\n\nFirst, some terminology. Like a regulator, a _control loop_ controls the behavior of a variable using a sensor, a function, and some actuator. Using multiple control loops, a _control system_ manages the behavior of a system. Control loops are _open-loop_ when they don’t act on feedback about the outcomes of their actions, like a heater on a timer. _Closed-loop_ controllers incorporate feedback, like cruise control or thermostats. Finally, _entropy_ here refers to [Shannon entropy](https://en.wikipedia.org/wiki/Entropy_\\(information_theory\\)), which measures uncertainty about a variable. Touchette & Lloyd study the limits of control systems in [Information-theoretic limits of control](https://arxiv.org/pdf/chao-dyn/9905039.pdf), writing that:\n\n> _entropy offers a precise measure of disorderliness or missing information by characterizing the minimum amount of resources (bits) required to encode unambiguously the ensemble describing the system_. \n\nSummarizing their main results, they write:\n\n> _in a control process, information must constantly be acquired, processed and used to constrain or maintain the trajectory of a system._\n> \n> _the second law of thermodynamics…sets absolute limits to the minimum amount of dissipation required by open-loop control…an information-theoretic analysis of closed-loop control shows feedback control to be essentially a zero sum game: **each bit of information gathered directly from a dynamical systems by a control device can serve to decrease the entropy of that system by at most one bit** additional to the reduction of entropy attainable without such information (open-loop control)._ \n\nIn other words, when including information as part of a thermodynamic system, the second law of thermodynamics tells us that you can't control a system perfectly. When a control loop adjusts its actions based on the information it gets from the system it's controlling, there’s a fundamental trade-off: **for each bit of information you collect from a system, you can reduce disorder of that system by only one more bit than you could’ve without that information**. There's a strict one-for-one limit on how much more you can improve control through information. \n\nTo formalize this, let t denote time, and let IT denote the amount of information available to the boxed agent at time t\\=T. The change in IT is then given by time derivative of input information, labeled i, minus the time derivative of output or “used” information, labeled o: \n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6FAMJL89DKmSeYTha/nbpozftdirivgc3bzeii)\n\nand so the available information at time T is\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6FAMJL89DKmSeYTha/h8nhh8x2beyig8jbrljg)\n\nwhere C denotes the information contained in the initial conditions of the model. We can visualize (a simplified version of) this over consecutive timesteps, as information is gathered then used up: \n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6FAMJL89DKmSeYTha/q25vhab6anqlhmaabqka)\n\nThis seems promising. If we limit the information an AI can gather with respect to the information it uses to act in its environment, it will not have sufficient information to act effectively. Or, in the language of the paper above, the agent will not have enough bits about its external dynamical system to sufficiently reduce the entropy of that dynamical system. \n\nWe can define input-boxing more formally as follows: suppose we have an agent A that has an amount IT of information at time T. For A to achieve a certain external action X, it requires some amount of information IX about its external environment, and we can say that A cannot achieve X when IT is smaller than IX. If we set X to be some minimal escape scenario, then we can say that A is input boxed when IT<IX. This doesn't depend on any of the agent's capabilities or intelligence, just its IT, which we are supposing we can limit, and IX, which depends only on the external environment. \n\nJohn Wentworth, author of [Fixing The Good Regulator Theorem](https://www.lesswrong.com/posts/Dx9LoqsEh3gHNJMDk/fixing-the-good-regulator-theorem), which we used earlier, considered a similar [question about the limits of control](https://www.lesswrong.com/posts/6rZroG3mztowktJzp/how-many-bits-of-optimization-can-one-bit-of-observation). He conjectured that observing an additional N bits of information allows a system to perform at most N additional bits of optimization. He concluded that the conjecture is [false](https://www.lesswrong.com/posts/6rZroG3mztowktJzp/how-many-bits-of-optimization-can-one-bit-of-observation?commentId=Pvhpc7PF8DhsyMGgk), citing a counterexample where a short password unlocks a long message, concluding that “_bits of observation can give a really ridiculously large boost to a system's optimization power”_. Others [agree](https://www.lesswrong.com/posts/dfTm26pvq7yQp8mR3/one-bit-of-observation-can-unlock-many-of-optimization-but), arguing that, with some practical constraints, “_there are no limits on how much optimization one can squeeze out of a single bit, even in a world with reversible computing”_. \n\nHowever, these counterexamples rely on the agent’s ability to subsequently “follow up” on what they learn. The information in the “password” is only useful insofar as it grants the opportunity to gather or unlock more information. The instantaneous utility of the password, or any piece of information, is bounded by the sum of the information in the password itself and prior knowledge, and by assumption we’re limiting the agent’s access to follow-up information as well. Therefore, these counterexamples shouldn’t exempt an input-boxed AI from the limits on optimization. If we can limit the information an agent gathers from inputs and prior knowledge, we can box that agent, regardless of its intelligence."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Importance of Input Boxing as a Novel AI Safety Avenue",
          "description": "This section frames input boxing as a neglected but potentially fundamental approach to AI containment, contrasting it with the more commonly discussed output boxing. From an EA perspective, highlighting a new axis of control over superintelligent agents is highly significant, as it could open up new research directions and safety strategies.",
          "importance": 90,
          "grade": 85,
          "highlight": {
            "startOffset": 1816,
            "endOffset": 2525,
            "quotedText": "tputs. \n\nMost discussions of boxing focus on output boxing, but there’s a neglected mirror to this approach that we’ll call _input boxing_: containing an AI by constraining what information it can access. (For completeness, we could also consider boxing techniques independent of input and output, where we contain an AI by constraining its functioning, such as by limiting its computational speed or restarting it from initial conditions each second).\n\nIn the rest of this post, we’ll introduce and examine a theoretical input boxing technique that contains an AI by controlling the number of bits of information it has about its environment, using results from information theory, thermodynamics, and contro"
          },
          "isValid": true
        },
        {
          "title": "Potential for Practical Application and Further Research",
          "description": "The post suggests concrete avenues for further research and practical application, such as selectively limiting hazardous information, imposing delays, and using input boxing in AI evals/red-teaming. This increases the tractability and real-world relevance of the ideas.",
          "importance": 75,
          "grade": 70,
          "highlight": {
            "startOffset": 12982,
            "endOffset": 390,
            "quotedText": "d harm us. One way to minimize the harm is _boxing_ the AI: containing it so that it can’t freely influence its external world, ideally while preserving its potential to help. \n\nBoxing is complicated. Destroying the AI will certainly prevent it from influencing anything. Throwing it into a black hole might causally disconnect it from the rest of the universe while possibly preserving its existence, though even then it may exploit some unknown physics to [leak information back across the event horizon](https://en.wikipedia.org/wiki/Markov_blanket), or to persist or recreate itself once the black hole has evaporated. \n\nCan we box an AI while still being being able to _use_ it? We could try preventing it from physically interacting with its environment and only permitting it to [present information](https://www.lesswrong.com/tag/oracle-ai) to a user, but a superintelligence could abuse any communication channels to manipulate its users into [granting it more power](https://www.lesswrong.com/posts/wKnwcjJGriTS9QxxL/dreams-of-friendliness) or [improving its predictive abilities](https://www.lesswrong.com/posts/aBRS3x4sPSJ9G6xkj/underspecification-of-oracle-ai). To successfully box the AI in this manner, we’d need to constrain both its ability to physically interact with its environment and communicate information and manipulate. We’ll call this _output boxing_: containing an AI by constraining its various outputs. \n\nMost discussions of boxing focus on output boxing, but there’s a neglected mirror to this approach that we’ll call _input boxing_: containing an AI by constraining what information it can access. (For completeness, we could also consider boxing techniques independent of input and output, where we contain an AI by constraining its functioning, such as by limiting its computational speed or restarting it from initial conditions each second).\n\nIn the rest of this post, we’ll introduce and examine a theoretical input boxing technique that contains an AI by controlling the number of bits of information it has about its environment, using results from information theory, thermodynamics, and control theory. \n\n# Good regulators and bad keys\n\nToday’s powerful AIs rely on mountains of data (during training, at least), so restricting their access to data would restrict their subsequent capabilities, but can we _fundamentally_ limit a superintelligence’s external influence just by constraining its access to data? \n\nWe know that agents need an understanding of their environments to act effectively within them: Conant & Ashby studied this in a ([much](https://www.lesswrong.com/posts/wTJECdFSAk4843mFj/thoughts-on-the-good-regulator-theorem#:~:text=This%20is%20among%20the%20worst%20papers%20I%27ve%20ever%20skimmed.) [maligned](https://www.lesswrong.com/posts/Dx9LoqsEh3gHNJMDk/fixing-the-good-regulator-theorem#:~:text=This%20may%20be%20the%20most%20misleading%20title%20and%20summary%20I%20have%20ever%20seen%20on%20a%20math%20paper.)) 1970 paper on the [good regulator theorem](http://pespmc1.vub.ac.be/books/Conant_Ashby.pdf). A _regulator_ is any mechanism that maintains a desired characteristic, like a thermostat, or a brain and its homeostatic subsystems. The good regulator theorem states that every good regulator of a system must contain a model of the system. Another parsing of the theorem states that [every good key must be a model of the lock it opens](http://cadia.ru.is/wiki/_media/public:t-720-atai:a_primer_for_conant_and_ashby_s_good-regulator_theorem.pdf), and John Wentworth [refines the theorem](https://www.lesswrong.com/posts/Dx9LoqsEh3gHNJMDk/fixing-the-good-regulator-theorem) (and its proof), clarifying that: \n\n> _\\[Conant and Ashby actually prove\\] that every regulator which is optimal and contains no unnecessary noise is_ equivalent to _a regulator which first reconstructs the variable-values of the system it’s controlling, then chooses its output as a function of those values_\n\nCan we, therefore, box an AI by preventing it from modeling its external environment? Can we stop the AI becoming a “good key” for the cage we’ve placed it in? Let’s look at the potential sources of information for such an AI: \n\n*   The initial conditions of the model (such as training data and learned patterns)\n*   Sensory input (such as through cameras and microphones)\n*   Logical deduction and extrapolation\n\nWe’ll assume that the AI we’re trying to box is logically omniscient; that it can squeeze as much information from its initial conditions and sensory input as theoretically possible. This leaves two sources of information that are targets for input-boxing: initial conditions and sensory input. \n\nIn practice, restricting these sources of information is tricky. For example, we could try giving the AI only one sensor, some low-res camera with which to receive instruction. However, the image that the camera receives could be subtly influenced by the vibration of nearby voices, ambient magnetic fields, background radiation, and more. Interpreted correctly, these sources of interference become sources of information. \n\nSupposing, though, that we can overcome such _practical_ hurdles, can we quantify how much control we can muster from input boxing? To answer this, we’ll look at control theory. \n\n# Bits, boxes, and control\n\nFirst, some terminology. Like a regulator, a _control loop_ controls the behavior of a variable using a sensor, a function, and some actuator. Using multiple control loops, a _control system_ manages the behavior of a system. Control loops are _open-loop_ when they don’t act on feedback about the outcomes of their actions, like a heater on a timer. _Closed-loop_ controllers incorporate feedback, like cruise control or thermostats. Finally, _entropy_ here refers to [Shannon entropy](https://en.wikipedia.org/wiki/Entropy_\\(information_theory\\)), which measures uncertainty about a variable. Touchette & Lloyd study the limits of control systems in [Information-theoretic limits of control](https://arxiv.org/pdf/chao-dyn/9905039.pdf), writing that:\n\n> _entropy offers a precise measure of disorderliness or missing information by characterizing the minimum amount of resources (bits) required to encode unambiguously the ensemble describing the system_. \n\nSummarizing their main results, they write:\n\n> _in a control process, information must constantly be acquired, processed and used to constrain or maintain the trajectory of a system._\n> \n> _the second law of thermodynamics…sets absolute limits to the minimum amount of dissipation required by open-loop control…an information-theoretic analysis of closed-loop control shows feedback control to be essentially a zero sum game: **each bit of information gathered directly from a dynamical systems by a control device can serve to decrease the entropy of that system by at most one bit** additional to the reduction of entropy attainable without such information (open-loop control)._ \n\nIn other words, when including information as part of a thermodynamic system, the second law of thermodynamics tells us that you can't control a system perfectly. When a control loop adjusts its actions based on the information it gets from the system it's controlling, there’s a fundamental trade-off: **for each bit of information you collect from a system, you can reduce disorder of that system by only one more bit than you could’ve without that information**. There's a strict one-for-one limit on how much more you can improve control through information. \n\nTo formalize this, let t denote time, and let IT denote the amount of information available to the boxed agent at time t\\=T. The change in IT is then given by time derivative of input information, labeled i, minus the time derivative of output or “used” information, labeled o: \n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6FAMJL89DKmSeYTha/nbpozftdirivgc3bzeii)\n\nand so the available information at time T is\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6FAMJL89DKmSeYTha/h8nhh8x2beyig8jbrljg)\n\nwhere C denotes the information contained in the initial conditions of the model. We can visualize (a simplified version of) this over consecutive timesteps, as information is gathered then used up: \n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6FAMJL89DKmSeYTha/q25vhab6anqlhmaabqka)\n\nThis seems promising. If we limit the information an AI can gather with respect to the information it uses to act in its environment, it will not have sufficient information to act effectively. Or, in the language of the paper above, the agent will not have enough bits about its external dynamical system to sufficiently reduce the entropy of that dynamical system. \n\nWe can define input-boxing more formally as follows: suppose we have an agent A that has an amount IT of information at time T. For A to achieve a certain external action X, it requires some amount of information IX about its external environment, and we can say that A cannot achieve X when IT is smaller than IX. If we set X to be some minimal escape scenario, then we can say that A is input boxed when IT<IX. This doesn't depend on any of the agent's capabilities or intelligence, just its IT, which we are supposing we can limit, and IX, which depends only on the external environment. \n\nJohn Wentworth, author of [Fixing The Good Regulator Theorem](https://www.lesswrong.com/posts/Dx9LoqsEh3gHNJMDk/fixing-the-good-regulator-theorem), which we used earlier, considered a similar [question about the limits of control](https://www.lesswrong.com/posts/6rZroG3mztowktJzp/how-many-bits-of-optimization-can-one-bit-of-observation). He conjectured that observing an additional N bits of information allows a system to perform at most N additional bits of optimization. He concluded that the conjecture is [false](https://www.lesswrong.com/posts/6rZroG3mztowktJzp/how-many-bits-of-optimization-can-one-bit-of-observation?commentId=Pvhpc7PF8DhsyMGgk), citing a counterexample where a short password unlocks a long message, concluding that “_bits of observation can give a really ridiculously large boost to a system's optimization power”_. Others [agree](https://www.lesswrong.com/posts/dfTm26pvq7yQp8mR3/one-bit-of-observation-can-unlock-many-of-optimization-but), arguing that, with some practical constraints, “_there are no limits on how much optimization one can squeeze out of a single bit, even in a world with reversible computing”_. \n\nHowever, these counterexamples rely on the agent’s ability to subsequently “follow up” on what they learn. The information in the “password” is only useful insofar as it grants the opportunity to gather or unlock more information. The instantaneous utility of the password, or any piece of information, is bounded by the sum of the information in the password itself and prior knowledge, and by assumption we’re limiting the agent’s access to follow-up information as well. Therefore, these counterexamples shouldn’t exempt an input-boxed AI from the limits on optimization. If we can limit the information an agent gathers from inputs and prior knowledge, we can box that agent, regardless of its intelligence. \n\n# Building better boxes\n\nInput boxing is a promising avenue for research as it tackles the challenge of limiting an AI's capabilities through the fundamentals of information, independent of how smart the AI in the box is. \n\nThis article is just an introduction, and we’re interested in exploring further refinements of the idea of input boxing. For example, a deeper analysis of _what_ information we allow into the box may give us more fine-grained control over an AI (such as in [this discussion](https://www.lesswrong.com/posts/6rZroG3mztowktJzp/how-many-bits-of-optimization-can-one-bit-of-observation?commentId=n5b4RvdrSxFsqCFw5#n5b4RvdrSxFsqCFw5) about information about environments versus laws), especially if we can robustly distinguish between information useful for solving our problems and useful for escape. \n\nThat is, rather than fundamentally limiting the agent’s _overall_ control, we may be able to identify hazardous paths, or sequences of actions, that the agent will want to pursue. That is, for an agent to achieve an action it requires information about the control pathway between its actuators and its ultimate desired outcome. A must have the information about how its first action will transform its external environment, and how subsequent actions will transform the new, changed environment, and so on.  \n\n"
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Risk of Overconfidence and Misallocation of Safety Effort",
          "description": "While the post is careful to note limitations, there is a risk that readers may overestimate the tractability or sufficiency of input boxing, potentially leading to underinvestment in other safety strategies. This is a potential negative externality that should be explicitly considered.",
          "importance": 70,
          "grade": 60,
          "highlight": {
            "startOffset": 13179,
            "endOffset": 14658,
            "quotedText": "This could be much more pragmatic than fundamentally limiting all information, since hazardous information is probably a small subset of all information - bits have to be relevant to hazardous actions to be hazardous bits. However, this would come at the cost of the guarantee those fundamental limits provide. \n\nAnother promising avenue for developing input-boxing, suggested by Deric Cheng, would be to impose fundamental delays on the use of information. The utility of information decays over time, so if an AI could only act on information from 24 hours ago, it’ll be much weaker, especially for high-speed actions such as exploiting the stock market.\n\nAs well as a boxing technique for superintelligences, input boxing may be useful for more niche applications. For example, by input boxing AI models during [evals](https://www.lesswrong.com/tag/ai-evaluations), we could test a model with the guarantee that any hazardous behavior won’t leak out the box, meaning evaluators could actively prompt hazardous behavior and use [red-teaming](https://en.wikipedia.org/wiki/Red_team) techniques without compromising the [safety of the eval](https://www.lesswrong.com/posts/XCRsg2ZnHBNAN862T/improving-the-safety-of-ai-evals). \n\nWith further refinements and insights from information theory, control theory, and thermodynamics, input boxing could be a powerful tool for both niche applications and for wrapping a superintelligence up with a neat, little bow in a safe, useful box."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        }
      ],
      "grade": 80
    },
    {
      "agentId": "bias-detector",
      "costInCents": 2,
      "createdAt": "2025-04-17T04:44:34.559Z",
      "runDetails": "{\"model\":\"openai/gpt-4.1\",\"promptTokens\":5019,\"completionTokens\":1781,\"totalTokens\":6800,\"temperature\":0.1,\"runtimeMs\":22174}",
      "thinking": "As a bias detection expert, my analysis focuses on identifying cognitive, methodological, and social biases in the reasoning, framing, and argumentation of the text. The document is a technical exploration of 'input boxing' as a method to constrain superintelligent AI, drawing on information theory, thermodynamics, and control theory. \n\nKey aspects to consider:\n- The text is written for a rationalist/technical audience, which may introduce in-group bias or status quo bias regarding LessWrong community assumptions.\n- The argument relies heavily on theoretical constructs and idealized models, which can introduce methodological biases such as base rate neglect, scope insensitivity, or optimism bias about the feasibility of input boxing.\n- There is a risk of confirmation bias in the selection and interpretation of supporting literature, especially when counterarguments are addressed but ultimately dismissed.\n- The text may exhibit narrative fallacy by constructing a coherent story about input boxing's promise, possibly downplaying practical limitations or alternative viewpoints.\n- The document references critiques and counterexamples, which is positive, but the handling of these may still reflect motivated reasoning or belief perseverance.\n- The focus on information-theoretic limits may lead to omission bias regarding other relevant factors (e.g., social, organizational, or hardware-level risks).\n- The discussion of 'hazardous information' and selective input restriction could be subject to scope insensitivity or illusion of control, assuming fine-grained control is feasible.\n- The text's speculative nature may foster optimism bias or planning fallacy regarding the development and implementation of such boxing techniques.\n\nOverall, my analysis will highlight where these and other biases may be present, their potential impact on the argument, and suggest ways to mitigate them.",
      "summary": "From a bias detection perspective, the document is thoughtful and references critiques, but exhibits several notable biases. There is optimism bias and planning fallacy in the portrayal of input boxing as a promising and potentially sufficient solution, with insufficient attention to practical and adversarial challenges. Confirmation bias is present in the selective handling of counterexamples, and scope insensitivity and illusion of control are evident in the assumption that hazardous information can be reliably identified and restricted. Theoretical focus leads to base rate neglect and omission bias regarding empirical containment difficulties. The in-group framing and narrative coherence may further reinforce these biases. To mitigate these, the argument would benefit from more engagement with empirical evidence, adversarial scenarios, and perspectives outside the LessWrong/rationalist community.",
      "comments": [
        {
          "title": "In-group Bias and Status Quo Bias in Framing and References",
          "description": "The text is written for a LessWrong/rationalist audience, referencing community-specific debates and terminology. This may introduce in-group bias (assuming shared values and perspectives) and status quo bias (favoring approaches and assumptions common in the community), potentially limiting consideration of alternative frameworks or critiques from outside the community.",
          "importance": 65,
          "grade": 55,
          "highlight": {
            "startOffset": 0,
            "endOffset": 216,
            "quotedText": "Boxing an agent more intelligent than ourselves is daunting, but information theory, thermodynamics, and control theory provide us with tools that can fundamentally constrain agents independent of their intelligence."
          },
          "isValid": true
        },
        {
          "title": "Base Rate Neglect and Omission Bias Regarding Practical Containment",
          "description": "The text focuses on theoretical limits and idealized models, with little discussion of the empirical track record or base rates of success in AI containment. This may lead to base rate neglect (ignoring how often such approaches have failed or proven inadequate in practice) and omission bias (failing to mention practical, organizational, or hardware-level risks that could undermine input boxing).",
          "importance": 75,
          "grade": 45,
          "highlight": {
            "startOffset": 5485,
            "endOffset": 5616,
            "quotedText": "Supposing, though, that we can overcome such _practical_ hurdles, can we quantify how much control we can muster from input boxing?"
          },
          "isValid": true
        },
        {
          "title": "Confirmation Bias in Handling of Counterexamples",
          "description": "The text acknowledges counterexamples to the information-theoretic limits (e.g., the password unlocking a long message), but then argues that these do not apply under the assumption that follow-up information is also restricted. This may reflect confirmation bias, as the author appears to interpret evidence in a way that supports the input boxing thesis, rather than fully engaging with the implications of the counterexamples or the difficulty of ensuring no follow-up information leaks.",
          "importance": 85,
          "grade": 45,
          "highlight": {
            "startOffset": 10934,
            "endOffset": 11508,
            "quotedText": "However, these counterexamples rely on the agent’s ability to subsequently “follow up” on what they learn. The information in the “password” is only useful insofar as it grants the opportunity to gather or unlock more information. The instantaneous utility of the password, or any piece of information, is bounded by the sum of the information in the password itself and prior knowledge, and by assumption we’re limiting the agent’s access to follow-up information as well. Therefore, these counterexamples shouldn’t exempt an input-boxed AI from the limits on optimization."
          },
          "isValid": true
        },
        {
          "title": "Narrative Fallacy in Coherent Story of Input Boxing",
          "description": "The article constructs a compelling narrative about the promise of input boxing, potentially downplaying the messy, uncertain, and adversarial nature of real-world AI safety. This narrative fallacy may lead readers to overestimate the coherence and feasibility of the approach.",
          "importance": 70,
          "grade": 50,
          "highlight": {
            "startOffset": 11673,
            "endOffset": 11869,
            "quotedText": "Input boxing is a promising avenue for research as it tackles the challenge of limiting an AI's capabilities through the fundamentals of information, independent of how smart the AI in the box is."
          },
          "isValid": true
        },
        {
          "title": "Scope Insensitivity and Illusion of Control in Selective Input Restriction",
          "description": "The suggestion that we can 'robustly distinguish between information useful for solving our problems and useful for escape' and selectively restrict only 'hazardous information' assumes a level of fine-grained control and understanding that may not be realistic. This reflects scope insensitivity (underestimating the complexity of the problem) and illusion of control (overestimating our ability to manage information flows in adversarial settings).",
          "importance": 80,
          "grade": 40,
          "highlight": {
            "startOffset": 12006,
            "endOffset": 12468,
            "quotedText": "a deeper analysis of _what_ information we allow into the box may give us more fine-grained control over an AI (such as in [this discussion](https://www.lesswrong.com/posts/6rZroG3mztowktJzp/how-many-bits-of-optimization-can-one-bit-of-observation?commentId=n5b4RvdrSxFsqCFw5#n5b4RvdrSxFsqCFw5) about information about environments versus laws), especially if we can robustly distinguish between information useful for solving our problems and useful for escape."
          },
          "isValid": true
        },
        {
          "title": "Optimism Bias and Planning Fallacy in Feasibility of Input Boxing",
          "description": "The text expresses optimism about the potential of input boxing to constrain superintelligent AI, suggesting that 'input boxing could be a powerful tool' and that 'with further refinements and insights...input boxing could be a powerful tool for both niche applications and for wrapping a superintelligence up with a neat, little bow in a safe, useful box.' This framing may underestimate the practical challenges, technical unknowns, and adversarial capabilities of superintelligent agents. The planning fallacy is present in the implicit assumption that further research and refinement will overcome these hurdles, without adequately addressing the historical difficulty of AI containment or the possibility of unknown unknowns.",
          "importance": 90,
          "grade": 40,
          "highlight": {
            "startOffset": 14407,
            "endOffset": 14658,
            "quotedText": "With further refinements and insights from information theory, control theory, and thermodynamics, input boxing could be a powerful tool for both niche applications and for wrapping a superintelligence up with a neat, little bow in a safe, useful box."
          },
          "isValid": true
        }
      ],
      "grade": 48
    },
    {
      "agentId": "fake-eliezer",
      "costInCents": 3,
      "createdAt": "2025-04-17T04:45:16.795Z",
      "runDetails": "{\"model\":\"openai/gpt-4.1\",\"promptTokens\":5213,\"completionTokens\":2432,\"totalTokens\":7645,\"temperature\":0.1,\"runtimeMs\":42222}",
      "thinking": "Let me begin by stating, with my usual level of exasperation, that the notion of 'boxing' a superintelligence—by which we mean, constraining its ability to affect the world outside its box—is a topic that has been discussed to death, and yet, as with so many things in the AI safety community, the same basic errors and confusions keep cropping up. The present document attempts to approach the problem from an information-theoretic and control-theoretic perspective, focusing on the idea of 'input boxing'—that is, limiting the information available to the AI, rather than (or in addition to) limiting its outputs.\n\nFrom my perspective, the most relevant aspects are:\n\n1. The attempt to ground the limits of AI influence in fundamental physics and information theory, rather than in the squishier domain of human psychology or institutional process. This is a good impulse, but as always, the devil is in the details.\n\n2. The invocation of the Good Regulator Theorem and the Touchette & Lloyd result about the one-to-one correspondence between bits of information and bits of entropy reduction in a controlled system. This is a nontrivial insight, but as the document itself notes, there are important caveats and counterexamples—such as the 'password unlocks a long message' scenario—that must be addressed.\n\n3. The recognition that, in practice, restricting the information available to a superintelligence is extremely difficult, due to the myriad side channels and the AI's ability to extract maximal information from minimal inputs. This is a point I have hammered on repeatedly: superintelligences are not limited by the same cognitive blind spots as humans, and will exploit every available bit of information, including those we didn't realize were present.\n\n4. The suggestion that, by carefully controlling the type and amount of information available to the AI, we can limit its ability to achieve certain outcomes—specifically, outcomes that require knowledge of hazardous control pathways. This is a more nuanced approach than simply trying to minimize all information, but it raises the question of whether we can reliably distinguish between 'safe' and 'hazardous' information, especially in the presence of a superintelligence that is actively seeking to escape.\n\n5. The proposal to use input boxing as a tool for safer AI evaluation (evals), by guaranteeing that hazardous behavior cannot leak out of the box. This is a practical application, but again, the devil is in the details: can we really guarantee that no hazardous information is present, or that the AI cannot reconstruct it from innocuous-seeming data?\n\nOverall, the document is a thoughtful exploration of the theoretical limits of input boxing, but it does not grapple sufficiently with the practical difficulties, nor does it address the core alignment problem: that a sufficiently intelligent agent will find and exploit any available channel, and that our ability to anticipate and close all such channels is, in practice, woefully inadequate. The map is not the territory; our models of information flow are not the same as the actual causal structure of the world, and superintelligences will operate on the territory, not the map.\n\nIn summary: the information-theoretic approach is a useful lens, but it is not a panacea. The fundamental inadequacy of civilization to solve the alignment problem remains unaddressed.",
      "summary": "From the perspective of someone who has spent far too much time thinking about the inadequacy of civilization to solve the alignment problem, this document is a thoughtful but ultimately insufficient exploration of input boxing as a means of constraining superintelligent AI. The information-theoretic and control-theoretic framing is valuable, but the practical challenges—side channels, the impossibility of reliably classifying hazardous information, and the relentless optimization of a superintelligence—are not adequately addressed. The document does not grapple with the core issue: that any constraint we can specify, a sufficiently intelligent agent can find a way around, unless we solve the alignment problem at a much deeper level. Input boxing may be useful for niche applications, but it is not a solution to the existential risk posed by superintelligent AI.",
      "comments": [
        {
          "title": "Theoretical Limits vs. Practical Feasibility",
          "description": "The document correctly notes that, in theory, limiting the information available to an AI can limit its ability to control the external world. However, it glosses over the fact that, in practice, it is nearly impossible to eliminate all side channels and sources of information. Superintelligences will exploit every available bit, including those we didn't realize were present. This is a recurring failure mode in alignment proposals: confusing theoretical possibility with practical implementability.",
          "importance": 95,
          "grade": 85,
          "highlight": {
            "startOffset": 5059,
            "endOffset": 5482,
            "quotedText": "In practice, restricting these sources of information is tricky. For example, we could try giving the AI only one sensor, some low-res camera with which to receive instruction. However, the image that the camera receives could be subtly influenced by the vibration of nearby voices, ambient magnetic fields, background radiation, and more. Interpreted correctly, these sources of interference become sources of information."
          },
          "isValid": true
        },
        {
          "title": "Misapplication of Information-Theoretic Limits",
          "description": "The document leans heavily on the Touchette & Lloyd result that each bit of information can reduce entropy by at most one bit, but it does not adequately address the counterexamples (e.g., the password unlocking a long message) that show how a small amount of information can unlock vastly more optimization power. The attempt to dismiss these counterexamples by claiming they require 'follow-up' information is not fully convincing, as in practice, the world is full of such 'unlocking' scenarios.",
          "importance": 90,
          "grade": 60,
          "highlight": {
            "startOffset": 9785,
            "endOffset": 11508,
            "quotedText": "John Wentworth, author of [Fixing The Good Regulator Theorem](https://www.lesswrong.com/posts/Dx9LoqsEh3gHNJMDk/fixing-the-good-regulator-theorem), which we used earlier, considered a similar [question about the limits of control](https://www.lesswrong.com/posts/6rZroG3mztowktJzp/how-many-bits-of-optimization-can-one-bit-of-observation). He conjectured that observing an additional N bits of information allows a system to perform at most N additional bits of optimization. He concluded that the conjecture is [false](https://www.lesswrong.com/posts/6rZroG3mztowktJzp/how-many-bits-of-optimization-can-one-bit-of-observation?commentId=Pvhpc7PF8DhsyMGgk), citing a counterexample where a short password unlocks a long message, concluding that “_bits of observation can give a really ridiculously large boost to a system's optimization power”_. Others [agree](https://www.lesswrong.com/posts/dfTm26pvq7yQp8mR3/one-bit-of-observation-can-unlock-many-of-optimization-but), arguing that, with some practical constraints, “_there are no limits on how much optimization one can squeeze out of a single bit, even in a world with reversible computing”_. \n\nHowever, these counterexamples rely on the agent’s ability to subsequently “follow up” on what they learn. The information in the “password” is only useful insofar as it grants the opportunity to gather or unlock more information. The instantaneous utility of the password, or any piece of information, is bounded by the sum of the information in the password itself and prior knowledge, and by assumption we’re limiting the agent’s access to follow-up information as well. Therefore, these counterexamples shouldn’t exempt an input-boxed AI from the limits on optimization."
          },
          "isValid": false,
          "error": "Highlight is too long (max 1000 characters)"
        },
        {
          "title": "Hazardous Information and the Map-Territory Distinction",
          "description": "The suggestion that we can distinguish between 'hazardous' and 'non-hazardous' information, and only restrict the former, is appealing but deeply problematic. In practice, we do not have a reliable way to classify information in this way, especially when dealing with a superintelligence that may find uses for information that we cannot anticipate. This is a classic case of the map not matching the territory: our classification scheme is part of the map, but the AI operates in the territory.",
          "importance": 85,
          "grade": 55,
          "highlight": {
            "startOffset": 12982,
            "endOffset": 13489,
            "quotedText": "If we limit the agent’s access to that hazardous information about dangerous control pathway, we could relax the fundamental constraints on information input without permitting hazardous behavior. This could be much more pragmatic than fundamentally limiting all information, since hazardous information is probably a small subset of all information - bits have to be relevant to hazardous actions to be hazardous bits. However, this would come at the cost of the guarantee those fundamental limits provide."
          },
          "isValid": true
        },
        {
          "title": "Input Boxing for AI Evals: Practical Utility and Limitations",
          "description": "The idea of using input boxing to make AI evaluations safer is a practical application, but it is only as good as our ability to guarantee that no hazardous information leaks through. Given the history of security failures and the ingenuity of adversarial agents, I am skeptical that this can be achieved in practice, especially as models become more capable.",
          "importance": 75,
          "grade": 65,
          "highlight": {
            "startOffset": 13837,
            "endOffset": 14404,
            "quotedText": "As well as a boxing technique for superintelligences, input boxing may be useful for more niche applications. For example, by input boxing AI models during [evals](https://www.lesswrong.com/tag/ai-evaluations), we could test a model with the guarantee that any hazardous behavior won’t leak out the box, meaning evaluators could actively prompt hazardous behavior and use [red-teaming](https://en.wikipedia.org/wiki/Red_team) techniques without compromising the [safety of the eval](https://www.lesswrong.com/posts/XCRsg2ZnHBNAN862T/improving-the-safety-of-ai-evals)."
          },
          "isValid": true
        },
        {
          "title": "Fundamental Limits vs. Alignment Problem",
          "description": "The document focuses on the theoretical limits of control and information, but does not address the core alignment problem: that a superintelligent agent will actively seek to subvert any constraints we impose, and that our ability to anticipate and close all such loopholes is fundamentally inadequate. This is the central reason why boxing is not a sufficient solution to AI risk.",
          "importance": 90,
          "grade": 50,
          "highlight": {
            "startOffset": 14407,
            "endOffset": 14658,
            "quotedText": "With further refinements and insights from information theory, control theory, and thermodynamics, input boxing could be a powerful tool for both niche applications and for wrapping a superintelligence up with a neat, little bow in a safe, useful box."
          },
          "isValid": true
        }
      ],
      "grade": 65
    }
  ]
}